- en: Chapter 5
  prefs: []
  type: TYPE_NORMAL
- en: Warnings, Ethics, and Responsible AI
  prefs: []
  type: TYPE_NORMAL
- en: IN THIS CHAPTER
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](images/00003.jpg) Learning about the Responsible AI movement'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](images/00003.jpg) Paying attention to OpenAI warnings'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](images/00003.jpg) Losing copyright and IP protections'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](images/00003.jpg) Reaching for reliability and trust'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](images/00003.jpg) Reducing your risks'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Each model improvement aims to increase stability and overall performance in
    terms of reliability, accuracy, and ethics. In this chapter, you learn what that
    means and why the effort is critical.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Making Responsible AI
  prefs: []
  type: TYPE_NORMAL
- en: Almost everyone has an instinctive desire for caution or a sense of foreboding
    when it comes to AI. A broad industry effort called Responsible AI was formed
    to ensure that AI is responsibly developed by design. This movement is aimed at
    ensuring that AI models are built on specific principles from the ground up as
    opposed to having piecemeal measures tacked on after maturity or deployment or
    — yikes! — disregarded altogether.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Many AI providers and communities embrace the baseline principals promoted
    by the Responsible AI movement, which include the following:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Accountability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bias evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reliability and safety
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fairness and accessibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transparency and explainability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Privacy and security
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI, the creator of ChatGPT, has repeatedly stated a commitment to Responsible
    AI principles. It has also contributed to the movement in a number of ways, including
    open-sourcing Evals, its framework for evaluating OpenAI models and an open-source
    registry of benchmarks, and contributing policy research papers.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: OpenAI's partners and collaborators also work toward the principles of Responsible
    AI, but current economic stressors are putting corporate commitments to the test.
    For example, in line with many recent tech industry layoffs, in March 2023 Microsoft
    laid off its AI ethics and society team, which was charged with ensuring Responsible
    AI principles made it into Microsoft products before they ship. Microsoft isn’t
    likely to be the last of the large AI product and services producer to do so.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Troubling developments
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Until recently, AI was the domain of a relatively few learned scientists with
    highly specialized skills. But ChatGPT’s explosive arrival on the public scene
    spurred intense interest across the board. Now it appears that almost everyone
    is interested in using AI. And quite a few are keen to develop their own; the
    tools and costs are such that almost anyone can do it.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For example, researchers at Stanford University built Alpaca AI to perform similarly
    to ChatGPT on several tasks. Alpaca AI was built on an open-source small language
    model called LLaMA (developed by Meta, formerly known as Facebook). Stanford researchers
    trained it for less than $600, making it a rough equivalent of a cheap counterfeit
    copy of ChatGPT.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'However, cheap AI can prove to be costly AI. Alpaca AI is very unsafe, in that
    it frequently produces wrong and toxic responses, and Stanford responded responsibly
    by yanking it offline shortly after launch. However, the dataset and code for
    fine-tuning that model is still available on GitHub for anyone to use. The researchers
    are working on releasing the Alpaca AI model’s weights there, too. (Weights rank
    the importance of various algorithm inputs.) The intention behind making all this
    accessible is noble: providing a lightweight model for the AI community to study
    several AI deficiencies in the hopes of making more Responsible AI.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: But the nightmare in the story is that anyone can now build an AI model using
    the dataset and code on GitHub — for about $100 if the processes are optimized,
    according to Stanford. That’s not a far-fetched estimate considering that even
    cloud computing costs can be reduced or eliminated. We already have multiple reports
    of people running Alpaca’s code on Raspberry Pi computers and Pixel 6 smartphones,
    thereby skipping any need for cloud computing.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Meanwhile, AI models are popping up in shady communities online too. For example,
    a week after its launch, the entirety of Meta’s LLaMA model was reportedly leaked
    on 4chan. When it comes to AI, cheaper is far, far worse, at least in terms of
    safety for humans. For example, an AI model can deliver wrong or harmful information
    that can physically hurt people if it’s acted on, and misinformation can fuel
    harmful conspiracy theories or spur public unrest. Now think of someone building
    AI with intentional malice using a model with few to no safeguards. The thought
    is not comforting.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Don’t forget that nation states sometimes have dubious intentions as well. AI
    of all types is currently in use by most governments, with much of it classified
    information and therefore unavailable for public scrutiny. Governments worldwide
    are also concerned about AI being used in terrorist attacks, cyberattacks, or
    public uprisings.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Concerns over what might be done with AI led to both OpenAI and the Chinese
    government barring individuals in China from using ChatGPT. But a Chinese company
    called Baidu has already released its alternative model known in English as Enhanced
    Representation from kNowledge IntEgration, or Ernie Bot for short.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ernie Bot differs from ChatGPT-4 in two major ways. Ernie Bot produces multimodal
    outputs, meaning it generates texts and images, whereas ChatGPT-4 generates only
    text. And Ernie Bot can’t analyze images in prompts but ChatGPT-4 can.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ernie Bot may differ in safety measures as well. It’s hard to tell at this point
    whether Baidu is working on sufficient safety precautions for Ernie Bot. Everyone
    is in a big rush to launch their answer to ChatGPT as fast as possible, which
    isn't good news in terms of ensuring that proper safety and ethical measures are
    in place and working.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Previously, Baidu released Ernie 3.0, which is largely regarded as a GPT-3 equal.
    In 2022, it released Ernie-ViLG, which generates images from text prompts. Other
    ChatGPT-like bots from China-based entities include MOSS, by Fudan University
    researchers, and Inspo, by a startup called MiniMax.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The US and China have been keenly interested and heavily involved in developing
    and using AI for years. So have other countries. AI in this context has consequences
    in both cold and hot wars, world commerce, individual freedom, human rights, and
    other national, geopolitical, and international policy issues.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Protecting humans from humans using AI
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: On the flipside, several countries are working to contain consequences from
    the proliferation of AI models. For example, in December 2022, the European Council
    of the European Union (EU) proposed a regulation called the Artificial Intelligence
    Act, which aims to “ensure that artificial intelligence (AI) systems placed on
    the EU market and used in the Union are safe and respect existing law on fundamental
    rights and Union values.” The US has a new blueprint for an AI Bill of Rights
    as of October 2022\. In the UK, a “Roadmap to an effective AI assurance ecosystem”
    was published by the Centre for Data Ethics and Innovation in 2021\. The World
    Economic Forum also stepped up with a set of standards and guidelines that it
    published in 2022 titled “Quantum Computing Governance Principles.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Numerous legitimate integrations of powerful generative AI models like ChatGPT
    are popping up in existing software almost everywhere. For example, ChatGPT is
    already in many Microsoft products, from Bing to Office365\. GPT-4 APIs can be
    used to integrate the model with almost any software. Competitive AI models for
    legitimate uses are also on the rise. One example is Adobe’s Firefly tool, which
    is powered by a generative adversarial network (GAN) AI model.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Understanding the good, the bad, and the ugly
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: By now you’re beginning to realize just how accessible and varied AI truly is!
    But so are AI providers. How long will they remain interested in responsibly building
    and retraining AI models when their work can be stolen and counterfeited for mere
    pennies and in use within a few hours? Now add to that hit on ROI the recent rise
    in economic pressures to cut costs. Does that mean Responsible AI teams are on
    the chopping block first?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Where does that leave AI? Where does that leave us?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Even Sam Altman, CEO of OpenAI, has publicly admitted that he’s “a little bit
    scared” of AI. He’s also sounding the warning that some AI developers working
    on ChatGPT-like tools won’t apply safety limits. It’s just a matter of time until
    AI models spiral out of control.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Given the huge scale and sweeping capabilities of AI models like GPT-4 and applications
    such as ChatGPT, it's tantamount that individuals, citizen protection agencies
    and groups, governments, AI providers, and others join, insist on, or recommit
    to sustained efforts in containing AI within strong and well-reasoned guardrails
    designed to protect humans. Costs and corners can't be cut without incurring dire
    repercussions.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Heeding OpenAI Warnings
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI released each ChatGPT version with clearly posted public warnings. It's
    essential to heed those. But if you haven’t yet read through the opening warning
    posts or you want to check to see if the warnings have been updated, just ask
    ChatGPT to list the latest warnings, as I did in [Figure 5-1](#filepos204880).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Pay special heed to the warning regarding your privacy. ChatGPT is still in
    training, in all of its models from ChatGPT-3 to ChatGPT-4\. This means anything
    you enter as a prompt (image or text) is likely to be used as training material.
    Therefore, any of the following conditions may or may not happen:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](images/00039.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[FIGURE 5-1:](#filepos204393) ChatGPT-4 lists warnings about its use after
    prompted.'
  prefs: []
  type: TYPE_NORMAL
- en: Security is likely not at the same level as is usually afforded to personal
    identifiable information (PII).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Privacy shields, if there are any, may not extend to or follow after any data
    transfers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training material may become open-sourced or shared at some point.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your prompts may become a permanent part of training databases for future AI
    models and therefore almost impossible to ever delete.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI researchers and AI trainers may see your prompts with images and text
    in their reviews of ChatGPT’s performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For your own peace of mind, proceed with prompts as though all these situations
    can be realized.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](images/00023.jpg) Although all the listed warnings are important, I would
    single out the incomplete or incorrect information warning. One downside of ChatGPT
    is that it can be intentionally used to generate highly convincing disinformation
    and propaganda. I warn you in other chapters but it bears repeating: A more insidious
    threat is ChatGPT''s capability to hallucinate, or produce responses that sound
    highly plausible but are totally wrong. In short, don’t trust a word that AI says
    or writes. Double-check everything it outputs.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Considering Copyright and IP Protections
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI has been clear from the start that any text generated by ChatGPT in response
    to your prompts belongs to you. That’s all well and good unless you’re trying
    to copyright it and exclusively use it to make money.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The US Copyright Office ruled that any works containing AI-generated content
    can be copyrighted only to the extent of human authorship. In other words, whatever
    part AI writes is not copyright protected by law. If you write the work but use
    AI-generated images to illustrate it, your words are copyright protected but the
    images are not. If you reword some of the text that ChatGPT generated, only the
    words you wrote are protected by copyright. The rest is essentially left in the
    public domain for anyone else to use.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Lest you think that this is the result of an American quirk or AI bias, consider
    that the World Intellectual Property Organization (WIPO) reports that most jurisdictions,
    including Spain and Germany, ruled the same on machine-generated copyright protections
    decades ago. It remains to be seen if they’ll change their minds and consider
    GPT in general and ChatGPT in particular as original content creators. Right now,
    the big money is on a resounding “No!”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Publishers and agents worldwide said they were flooded with books, e-books,
    and other content by people hoping to make quick and easy money on ChatGPT-generated
    works. Almost none passed editorial muster and no one got a big bag of easy cash.
    Incidentally, anyone can now copy those works because they are considered fair
    game.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Further, ChatGPT and other GPT models such as DALL-E may be found guilty of
    copyright infringement. Copyright-protected works and other protected intellectual
    property were added to the models’ training database when massive amounts of data
    were indiscriminately scraped from the internet without payment or permission.
    The potential infringement liability is currently under debate. In addition, because
    copyright-protected works are in ChatGPT’s training database, it may occasionally
    replicate exact word usage — that is, plagiarize — which could create liability
    issues for unaware users. A court case may be required to sort out all the legal
    details.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Keep an eye on rising liability issues, new court actions, and evolving regulations
    because they may contain emerging threats to your endeavors if you’re using ChatGPT
    or an AI model of similar ilk.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Searching for Predictability
  prefs: []
  type: TYPE_NORMAL
- en: AI can be objectively judged on predictability, meaning the percentage of times
    if delivers the right answer to the same or similar questions. Typically, AI models
    do not score 100 percent predictability on all questions; rather they have different
    scores on various types of questions.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: But few users rely on actual predictability scoring to determine how much they
    trust AI. Instead, people are more prone to lean on their perception of AI and
    their own gut feeling about its responses. In this section, you see how machine
    tests and human feelings can affect your work with ChatGPT.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Reaching for reliability
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In its GPT-4 Technical Report OpenAI asserts that GPT-4 scored 19 percentage
    points higher than the latest GPT-3.5 iteration on OpenAI’s internal adversarially
    designed factuality evaluation. Specific scores in model comparisons are shown
    in [Figure 5-2](#filepos212274), which depicts the performance of various ChatGPT
    models in nine categories.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: On public benchmarks such as TruthfulQA, which tests how well the model separates
    facts from incorrect statements, the base model of GPT-4 scores only slighter
    better than GPT-3.5\. After additional RLHF (reinforcement learning from human
    feedback) post-training, the GPT-4 model outperformed GPT-3.5 by a wider margin.
    Counterintuitively, the pretrained model’s confidence in its answers generally
    matched the probability of being correct while the opposite was true of the post-trained
    model’s confidence.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Neither GPT-4 nor GPT-3.5 models have any knowledge of facts and events that
    occurred after the 2021 cutoff date. These models also do not learn from experience,
    which can result in gullibility to prompts, reasoning errors, and mistakes that
    resemble human errors. Biases in the reasoning also exist.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](images/00069.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[FIGURE 5-2:](#filepos211148) ChatGPT performance scores in various categories.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/00023.jpg) In short, ChatGPT, regardless of the model used, has
    a reliability problem. Although this problem has improved over the evolution of
    models, it''s significant enough that outputs should always be fact-checked, especially
    when ChatGPT content is used for critical functions and decision making.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Readers should always be aware of the inherent unreliability in outputs and
    not be swayed by the convincing language generated by ChatGPT as reason to skip
    the verifying step.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hallucinating versus accuracy
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you read about generative AI models, mention of confident AI or an AI model’s
    level of confidence eventually comes up. In the context of machine learning, confidence
    is a measure of the AI model’s estimated probability that its answer (output)
    is correct based on the information it has (input or prompt).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The four categories used to determine the level of confidence an AI has in its
    response are repeatability, believability, sufficiency, and adaptability.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](images/00079.jpg) When an AI model is said to have a high degree of confidence,
    it does not mean that the user can also have a high degree of confidence in the
    AI’s answer. ChatGPT can be extremely confident that it gave you a correct answer
    when it clearly and demonstrably delivered a wrong answer. This behavior is known
    in AI industry parlance as a hallucination.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: AI hallucinations are not coming from any sense of malice; the machine is not
    lying to you intentionally. It simply did the math, spouted babble, and rated
    itself as brilliantly correct, much like people suffering from the Dunning Kruger
    effect or delusions of grandeur might do.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: And to paraphrase a character in Gone with the Wind, frankly, my dear, it doesn’t
    give a damn. ChatGPT and its ilk do not care when they hallucinate. The model
    delivers an answer that it's highly confident is correct and that’s the end of
    it. Perhaps one day AI researchers will be able to teach AI models to double-check
    their homework with at least a smidge of humility and to be properly embarrassed
    when they fail.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When ChatGPT hallucinates, it outputs a convincing bit of nonsense that, if
    you acted upon it or accepted it unquestionably as true, could prove harmful to
    you or others. This inaccuracy cloaked in sweet nothings is what researchers are
    usually referring to when they call a model unsafe. The model is not reliably
    accurate and therefore it is unsafe for you to believe anything it outputs at
    face value.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As Google puts it, machines learn, but they don't know anything if you equate
    knowledge with certitude, which is often the case in Western thinking.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](images/00079.jpg) ChatGPT works by predicting which words will follow your
    prompt. It knows nothing. It calculates probabilities and issues an output with
    the highest probability of being correct, which can turn out to be 100 percent
    wrong. Do not take this to mean that ChatGPT is a toy or performs a simple calculation.
    ChatGPT is an astonishing feat of engineering. But it is also flawed.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Does that mean you should consider a flawed ChatGPT or other generative AI model
    worthless? Absolutely not. Even though its output will have to be consistently
    and rigorously fact-checked, it can still significantly increase the speed of
    production. And you can bet your competition is using it or something like it
    too.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](images/00084.jpg) Think of ChatGPT as a junior assistant that you may need
    to correct, instruct, and mentor. Despite its shortcomings, this assistant is
    incredibly fast at bringing you most of what you need to do your own job more
    easily and quickly.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Humanizing the machine
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: One of the most remarkable achievements in OpenAI’s work in developing ChatGPT
    and the various models that power it is that this AI model appears human. This
    development is quite the modern marvel.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Alan Turing, a man of much education and many skills, dubbed his famous Turing
    Test for AI “an imitation game” wherein a machine could so closely mimic human
    intelligence and conversation that a human couldn’t tell it was a machine. He
    developed that test in the 1950s and, for much of that time forward, machines
    fell short. Now several appear to pass the test at least for a while, but they
    usually get outed eventually.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: People generally know that ChatGPT is AI, but users can easily forget that fact
    as they continue to converse with it. Since the entire interaction from prompt
    to output is in natural language and flowing at the speed of human conversation,
    the experience can feel the same as chatting with another human online.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Eventually, ChatGPT makes a misstep that reminds users that this is AI and the
    jig is up. Even ChatGPT admits that happens in the conversation I had with it
    on the subject, as captured in [Figure 5-3](#filepos219766).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Even so, that initial false sense of familiarity breeds trust. And trust is
    the last thing anyone should place in AI.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Several studies revealed that the human flaw towards humanizing and trusting
    a machine is persistent. For example, a report titled “Computers in Human Behavior”
    by researchers hailing from Carnegie Mellon and the University of California Berkeley
    found that misattribution of blame leads humans to rely on poorly performing AI.
    In other words, people tend to blame themselves rather than AI for errors. Further,
    humans continue to wrongly accept the blame, which causes them to “enter a vicious
    cycle of relying on a poorly performing AI.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](images/00004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[FIGURE 5-3:](#filepos218862) ChatGPT Plus (premium version built on GPT-4
    model) response when asked if people mistake it as human.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the researchers found that the user’s level of self-confidence,
    and not their level of confidence in AI, is the deciding factor in whether they
    accept or reject an AI’s suggestions. Their findings point to a need to “effectively
    calibrate human self-confidence for successful AI-assisted decision-making.” In
    short, humans need to be trained on when to trust themselves and when to trust
    AI, as opposed to defaulting and demurring to AI.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Human experiences can also color acceptance of the presumed infallibility of
    AI — especially one that sounds human and benevolent like ChatGPT. People who
    are cynical and dubious of other people’s intent tend to be more skeptical of
    AI as well. Likewise, people who are more trusting of other people tend to trust
    AI. But members of both groups have been known to change their minds in response
    to their personal experiences with AI.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In any case, be aware of human and machine shortcomings and proceed accordingly.
    Resist your own urge to be friendly and trusting with ChatGPT and its ilk.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As Google’s People + AI Research (PAIR) initiative writer David Weinberger put
    it, uncertainty is seen as a weakness in humans but a strength in AI. Think about
    that for a minute. Are you making decisions based on misplaced trust in ChatGPT’s
    confidence — or are you going to fact-check it every time?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Mitigating Risks and Liability
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI has gone to significant effort to improve the safety of ChatGPT by continuously
    improving its alignment with human values and goals. Among the measures they use
    to accomplish greater safety are feedback from human domain experts for adversarial
    testing and red-teaming (a group of humans who play an adversarial role in seeking
    out vulnerabilities), improved safety models (guardrails for the AI model), and
    a model-assisted safety pipeline that assists by automating machine-learning processes
    within safety parameters.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Using human experts for adversarial testing and red-teaming, rather than just
    throwing two opposing AI models in a pit to continuously battle it out and refine
    each other in the process, is a crucially important move. Domain experts such
    as cybersecurity and international security professionals can find and eliminate
    or curb risks, such as terrorists using ChatGPT to get assembly instructions for
    a dirty bomb or a biohacking recipe for a human-engineered pandemic. I’m sure
    you can see why such intense precautions are necessary.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: But other domain experts are also important in refining responses on niche topics,
    containing offensive remarks, curbing inherent biases, eliminating propaganda
    and misinformation, and preventing riots and civil unrest.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: AI models such as ChatGPT can do lots of good, but when it’s allowed to be bad,
    it can be very, very bad for all of us. Using human experts to deal with the issues
    and help install the needed guardrails is an absolute necessity.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: OpenAI also uses reinforcement learning with human feedback (RLHF) to better
    match responses to user intent. This approach helps improve the quality of responses.
    And it helps weed out unsafe and bad behaviors on the part of AI, even if the
    human user is up to no good, but can also result in the machine becoming overly
    cautious and not replying even though it's safe to do so.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: OpenAI’s rule-based reward models (RBRMs) provide additional rewards for the
    AI model to avoid inappropriate responses and undue caution. The rewards methods,
    such as when a user clicks thumbs up and thumbs down icons, are indicators that
    further reinforce which answers are appropriate and desirable and which are not.
    There are only digital rewards with no donuts or free vacations for ChatGPT in
    the offering!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As an additional step, OpenAI works with external researchers to improve model
    performance and safety and to improve their own understanding of potential effects.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: But even after all this, some risks remain and the user is well advised to take
    serious precautions. For example, don’t assume that any conversation with ChatGPT
    or its competitors is or will remain private. See [Figure 5-4](#filepos225281)
    for an example of one of the many vulnerabilities AI models like these can have.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](images/00013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[FIGURE 5-4:](#filepos225087) A tweet by Sam Altman, CEO of OpenAI about a
    ChatGPT data leak.'
  prefs: []
  type: TYPE_NORMAL
- en: Potential liability also exists for users who publish unedited ChatGPT content
    because ChatGPT and its ilk are known to plagiarize. Copyright and intellectual
    property rights are not dismissed because a machine infringed upon them instead
    of a human. Be sure to double-check its work for plagiarism and other issues such
    as slander before you publish the content.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As to the greater risks to communities, nations, and humanity, a collaborative
    effort of organizations and government agencies is urgently needed. Otherwise,
    these rising AI models will quickly get out of hand and undoubtedly create great
    harm.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Following are steps to take to help mitigate risks and liabilities associated
    with ChatGPT use:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Always fact-check the content it generates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conduct a human review to ensure that the content is accurate and current.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disclose that you’re using AI so readers and reviewers don't feel duped.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check that AI-generated content is compliant with all laws, regulations, and
    guidelines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor audience feedback and respond quickly if the AI content has created
    issues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid dependency on AI from yourself or others you work with. You’re in charge;
    AI is just a tool.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use AI responsibly. Never use it to do something morally, ethically, or legally
    wrong.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These precautions should put you on the right path for mitigating risks and
    avoiding liability. But in high-risk applications, go beyond even these measures.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
