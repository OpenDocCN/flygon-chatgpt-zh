- en: Chapter 2\. A Deep Dive into the GPT-4 and ChatGPT APIs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章。深入了解GPT-4和ChatGPT的API
- en: This chapter examines the GPT-4 and ChatGPT APIs in detail. The goal of this
    chapter is to give you a solid understanding of the use of these APIs so that
    you can effectively integrate them into your Python applications. By the end of
    this chapter, you will be well equipped to use these APIs and exploit their powerful
    capabilities in your own development projects.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将详细介绍GPT-4和ChatGPT的API。本章的目标是让您对这些API的使用有扎实的理解，以便您可以有效地将它们集成到您的Python应用程序中。通过本章的学习，您将能够充分利用这些API在自己的开发项目中的强大功能。
- en: We’ll start with an introduction to the OpenAI Playground. This will allow you
    to get a better understanding of the models before writing any code. Next, we
    will look at the OpenAI Python library. This includes the login information and
    a simple “Hello World” example. We will then cover the process of creating and
    sending requests to the APIs. We will also look at how to manage API responses.
    This will ensure that you know how to interpret the data returned by these APIs.
    In addition, this chapter will cover considerations such as security best practices
    and cost management.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从介绍OpenAI Playground开始。这将使您在编写任何代码之前更好地了解模型。接下来，我们将看看OpenAI Python库。这包括登录信息和一个简单的“Hello
    World”示例。然后，我们将介绍创建和发送API请求的过程。我们还将看看如何管理API响应。这将确保您知道如何解释这些API返回的数据。此外，本章还将涵盖安全最佳实践和成本管理等考虑因素。
- en: As we progress, you will gain practical knowledge that will be very useful in
    your journey as a Python developer working with GPT-4 and ChatGPT. All the Python
    code included in this chapter is available in [the book’s GitHub repository](https://oreil.ly/DevAppsGPT_GitHub).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们的进展，您将获得实用的知识，这对您作为与GPT-4和ChatGPT一起使用的Python开发人员的旅程非常有用。本章中包含的所有Python代码都可以在[本书的GitHub存储库](https://oreil.ly/DevAppsGPT_GitHub)中找到。
- en: Note
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Before going any further, please check the [OpenAI usage policies](https://openai.com/policies/usage-policies),
    and if you don’t already have an account, create one on the [OpenAI home page](https://openai.com).
    You can also have a look at the other legal documentation on the [Terms and Policies
    page](https://openai.com/policies). The concepts introduced in [Chapter 1](ch01.html#gpt_4_and_chatgpt_essentials)
    are also essential for using the OpenAI API and libraries.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请查看[OpenAI使用政策](https://openai.com/policies/usage-policies)，如果您还没有帐户，请在[OpenAI主页](https://openai.com)上创建一个。您还可以查看[条款和政策页面](https://openai.com/policies)上的其他法律文件。[第1章](ch01.html#gpt_4_and_chatgpt_essentials)中介绍的概念对于使用OpenAI
    API和库也是必不可少的。
- en: Essential Concepts
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本概念
- en: OpenAI offers several models that are designed for various tasks, and each one
    has its own pricing. On the following pages, you will find a detailed comparison
    of the available models and tips on how to choose which ones to use. It’s important
    to note that the purpose for which a model was designed—whether for text completion,
    chat, or editing—impacts how you would use its API. For instance, the models behind
    ChatGPT and GPT-4 are chat based and use a chat endpoint.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI提供了几种为各种任务设计的模型，每种模型都有自己的定价。在接下来的页面上，您将找到可用模型的详细比较以及如何选择使用哪些模型的提示。重要的是要注意，模型设计的目的——无论是用于文本完成、聊天还是编辑——都会影响您如何使用其API。例如，ChatGPT和GPT-4背后的模型是基于聊天的，并使用聊天端点。
- en: The concept of prompts was introduced in [Chapter 1](ch01.html#gpt_4_and_chatgpt_essentials).
    Prompts are not specific to the OpenAI API but are the entry point for all LLMs.
    Simply put, prompts are the input text that you send to the model, and they are
    used to instruct the model on the specific task you want it to perform. For the
    ChatGPT and GPT-4 models, prompts have a chat format, with the input and output
    messages stored in a list. We will explore the details of this prompt format in
    this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 提示的概念是在[第1章](ch01.html#gpt_4_and_chatgpt_essentials)中介绍的。提示不是特定于OpenAI API，但是所有LLM的入口点。简而言之，提示是您发送给模型的输入文本，用于指示模型执行特定任务。对于ChatGPT和GPT-4模型，提示具有聊天格式，输入和输出消息存储在列表中。我们将在本章中探讨此提示格式的详细信息。
- en: 'The concept of tokens was also described in [Chapter 1](ch01.html#gpt_4_and_chatgpt_essentials).
    Tokens are words or parts of words. A rough estimate is that 100 tokens equal
    approximately 75 words for an English text. Requests to the OpenAI models are
    priced based on the number of tokens used: that is, the cost of a call to the
    API depends on the length of both the input text and the output text. You will
    find more details on managing and controlling the number of input and output tokens
    in [“Using ChatGPT and GPT-4”](#using_chatgpt_and_gpt_4) and [“Using Other Text
    Completion Models”](#using_other_text_completion_models).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌的概念也在[第1章](ch01.html#gpt_4_and_chatgpt_essentials)中描述过。令牌是单词或单词的部分。粗略估计是，100个令牌大约相当于英文文本的75个单词。对OpenAI模型的请求是基于使用的令牌数量定价的：也就是说，对API的调用成本取决于输入文本和输出文本的长度。您将在“使用ChatGPT和GPT-4”和“使用其他文本完成模型”中找到有关管理和控制输入和输出令牌数量的更多详细信息。
- en: These concepts are summarized in [Figure 2-1](#fig_1_essential_concepts_for_using_the_openai_api).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概念在[图2-1](#fig_1_essential_concepts_for_using_the_openai_api)中进行了总结。
- en: '![](assets/dagc_0201.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dagc_0201.png)'
- en: Figure 2-1\. Essential concepts for using the OpenAI API
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1\. 使用OpenAI API的基本概念
- en: Now that we have discussed the concepts, let’s move on to the details of the
    models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了概念，让我们转向模型的细节。
- en: Models Available in the OpenAI API
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI API中可用的模型
- en: The OpenAI API gives you access to [several models developed by OpenAI](https://platform.openai.com/docs/models).
    These models are available as a service over an API (through a direct HTTP call
    or a provided library), meaning that OpenAI runs the models on distant servers,
    and developers can simply send queries to them.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI API为您提供了访问[OpenAI开发的多个模型](https://platform.openai.com/docs/models)。这些模型可作为API的服务使用（通过直接的HTTP调用或提供的库），这意味着OpenAI在远程服务器上运行模型，开发人员只需向其发送查询。
- en: Each model comes with a different set of features and pricing. In this section,
    we will look at the LLMs provided by OpenAI through its API. It is important to
    note that these models are proprietary, so you cannot directly modify the code
    to adapt the models to your needs. But as we will see later, you can fine-tune
    some of them on your specific data via the OpenAI API.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型都具有不同的功能和定价。在本节中，我们将看一下OpenAI通过其API提供的LLM。需要注意的是，这些模型是专有的，因此您不能直接修改代码以适应您的需求。但正如我们将在后面看到的，您可以通过OpenAI
    API对其中一些模型进行特定数据的微调。
- en: Note
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Some older OpenAI models, including the GPT-2 model, are not proprietary. While
    you can download the GPT-2 model from [Hugging Face](https://oreil.ly/39Bu5) or
    [GitHub](https://oreil.ly/CYPN6), you cannot access it through the API.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一些较旧的OpenAI模型，包括GPT-2模型，不是专有的。虽然您可以从[Hugging Face](https://oreil.ly/39Bu5)或[GitHub](https://oreil.ly/CYPN6)下载GPT-2模型，但无法通过API访问它。
- en: 'Since many of the models provided by OpenAI are continually updated, it is
    difficult to give a complete list of them in this book; an updated list of models
    that OpenAI provides is available in the [online documentation](https://platform.openai.com/docs/models).
    Therefore, here we will focus on the most important models:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由于OpenAI提供的许多模型都在不断更新，因此在本书中很难给出完整的模型列表；OpenAI提供的模型的更新列表可在[在线文档](https://platform.openai.com/docs/models)中找到。因此，我们将重点放在最重要的模型上：
- en: InstructGPT
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: InstructGPT
- en: This family of models can process many single-turn completion tasks. The `text-ada-001`
    model is only capable of simple completion tasks but is also the fastest and least
    expensive model in the GPT-3 series. Both `text-babbage-001` and `text-curie-001`
    are a little more powerful but also more expensive. The `text-davinci-003` model
    can perform all completion tasks with excellent quality, but it is also the most
    expensive in the family of GPT-3 models.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这一系列模型可以处理许多单轮完成任务。`text-ada-001`模型只能完成简单的完成任务，但也是GPT-3系列中最快速和最便宜的模型。`text-babbage-001`和`text-curie-001`稍微更强大，但也更昂贵。`text-davinci-003`模型可以以优秀的质量执行所有完成任务，但也是GPT-3模型系列中最昂贵的模型。
- en: ChatGPT
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT
- en: The model behind ChatGPT is `gpt-3.5-turbo`. As a chat model, it can take a
    series of messages as input and return an appropriately generated message as output.
    While the chat format of `gpt-3.5-turbo` is designed to facilitate multiturn conversations,
    it is also possible to use it for single-turn tasks without dialogue. In single-turn
    tasks, the performance of `gpt-3.5-turbo` is comparable to that of `text-davinci-003`,
    and since `gpt-3.5-turbo` is one-tenth the price, with more or less equivalent
    performance, it is recommended that you use it by default for single-turn tasks.
    The `gpt-3.5-turbo` model has a context size of 4,000 tokens, which means it can
    receive 4,000 tokens as input. OpenAI also provides another model, called `gpt-3.5-turbo-16k`,
    with the same capabilities as the standard `gpt-3.5-turbo` model but with four
    times the context size.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT背后的模型是`gpt-3.5-turbo`。作为聊天模型，它可以将一系列消息作为输入，并返回一个适当生成的消息作为输出。虽然`gpt-3.5-turbo`的聊天格式旨在促进多轮对话，但也可以将其用于没有对话的单轮任务。在单轮任务中，`gpt-3.5-turbo`的性能与`text-davinci-003`相当，由于`gpt-3.5-turbo`的价格是后者的十分之一，性能几乎相当，建议您默认使用它进行单轮任务。`gpt-3.5-turbo`模型的上下文大小为4,000个标记，这意味着它可以接收4,000个标记作为输入。OpenAI还提供另一个模型，名为`gpt-3.5-turbo-16k`，具有与标准`gpt-3.5-turbo`模型相同的功能，但上下文大小增加了四倍。
- en: GPT-4
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4
- en: 'This is the largest model released by OpenAI. It has also been trained on the
    most extensive multimodal corpus of text and images. As a result, it has knowledge
    and expertise in many domains. GPT-4 can follow complex natural language instructions
    and solve difficult problems accurately. It can be used for both chat and single-turn
    tasks with high accuracy. OpenAI offers two GPT-4 models: `gpt-4` has a context
    size of 8,000 tokens, and `gpt-4-32k` has a context size of 32,000 tokens. A context
    of 32,000 represents approximately 24,000 words, which is a context of approximately
    40 pages.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这是OpenAI发布的最大模型。它还在最广泛的文本和图像多模态语料库上进行了训练。因此，它在许多领域都具有知识和专业知识。GPT-4能够遵循复杂的自然语言指令并准确解决困难问题。它可以用于具有高准确性的聊天和单轮任务。OpenAI提供了两个GPT-4模型：`gpt-4`的上下文大小为8,000个标记，`gpt-4-32k`的上下文大小为32,000个标记。32,000的上下文大约代表24,000个单词，大约相当于40页的上下文。
- en: Both GPT-3.5 Turbo and GPT-4 are continually updated. When we refer to the models
    `gpt-3.5-turbo`, `gpt-3.5-turbo-16k`, `gpt-4`, and `gpt-4-32k`, we are referring
    to the latest version of these models.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3.5 Turbo和GPT-4都在不断更新。当我们提到`gpt-3.5-turbo`、`gpt-3.5-turbo-16k`、`gpt-4`和`gpt-4-32k`模型时，我们指的是这些模型的最新版本。
- en: Developers often need more stability and visibility into the LLM version they
    are using in their applications. It can be difficult for developers to use model
    languages in which versions can change from one night to the next and can behave
    differently for the same input prompt. For this purpose, static snapshot versions
    of these models are also available. At the time of this writing, the most recent
    snapshot versions were `gpt-3.5-turbo-0613`, `gpt-3.5-turbo-16k-0613`, `gpt-4-0613`,
    and `gpt-4-32k-0613`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 开发人员通常需要更稳定和可见性的LLM版本，以在其应用程序中使用。对于开发人员来说，使用版本可能会在一夜之间发生变化，并且对于相同的输入提示可能会有不同的行为，这可能会很困难。出于这个目的，这些模型的静态快照版本也是可用的。在撰写本文时，最新的快照版本是`gpt-3.5-turbo-0613`、`gpt-3.5-turbo-16k-0613`、`gpt-4-0613`和`gpt-4-32k-0613`。
- en: As discussed in [Chapter 1](ch01.html#gpt_4_and_chatgpt_essentials), OpenAI
    recommends using the InstructGPT series rather than the original GPT-3–based models.
    These models are still available in the API under the names `davinci`, `curie`,
    `babbage`, and `ada`. Given that these models can provide strange, false, and
    misleading answers, as seen in [Chapter 1](ch01.html#gpt_4_and_chatgpt_essentials),
    caution in their use is advised. However, these models are still used because
    they are the only ones that can be fine-tuned to your data. At the time of this
    writing, OpenAI has announced that fine-tuning for GPT-3.5 Turbo and GPT-4 will
    be available in 2024.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[第1章](ch01.html#gpt_4_and_chatgpt_essentials)中讨论的，OpenAI建议使用InstructGPT系列而不是原始的基于GPT-3的模型。这些模型仍然在API中以`davinci`、`curie`、`babbage`和`ada`的名称提供。鉴于这些模型可能会提供奇怪、错误和误导性的答案，因此建议在使用时要谨慎。但是，这些模型仍然被使用，因为它们是唯一可以对您的数据进行微调的模型。在撰写本文时，OpenAI宣布GPT-3.5
    Turbo和GPT-4的微调将在2024年推出。
- en: Note
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The SFT model (presented in [Chapter 1](ch01.html#gpt_4_and_chatgpt_essentials))
    obtained after the supervised fine-tuning stage, which did not go through the
    RLHF stage, is also available in the API under the name `davinci-instruct-beta`.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过监督微调阶段获得的SFT模型（在[第1章](ch01.html#gpt_4_and_chatgpt_essentials)中介绍）也可以在API中以`davinci-instruct-beta`的名称使用。
- en: Trying GPT Models with the OpenAI Playground
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用OpenAI Playground尝试GPT模型
- en: An excellent way to test the different language models provided by OpenAI directly,
    without coding, is to use the OpenAI Playground, a web-based platform that allows
    you to quickly test the various LLMs provided by OpenAI on specific tasks. The
    Playground lets you write prompts, select the model, and easily see the output
    that is generated.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 直接测试OpenAI提供的不同语言模型的绝佳方法，而无需编码，是使用OpenAI Playground，这是一个基于Web的平台，允许您快速测试OpenAI提供的各种LLM在特定任务上的表现。Playground允许您编写提示，选择模型，并轻松查看生成的输出。
- en: 'Here’s how to access the Playground:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 访问Playground的方法如下：
- en: Navigate to the [OpenAI home page](https://openai.com) and click Developers,
    then Overview.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到[OpenAI主页](https://openai.com)，单击开发人员，然后单击概述。
- en: If you already have an account and are not logged in, click Login at the upper
    right of the screen. If you don’t have an account with OpenAI, you will need to
    create one in order to use the Playground and most of the OpenAI features. Click
    Sign Up at the upper right of the screen. Note that because there is a charge
    for the Playground and the API, you will need to provide a means of payment.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您已经有账户但未登录，请单击屏幕右上方的登录。如果您没有OpenAI账户，您需要创建一个才能使用Playground和大多数OpenAI功能。请单击屏幕右上方的注册。请注意，由于Playground和API会收费，因此您需要提供支付方式。
- en: Once you are logged in, you will see the link to join the Playground at the
    upper left of the web page. Click the link, and you should see something similar
    to [Figure 2-2](#fig_2_the_openai_playground_interface_in_text_completion).
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录后，您将在网页的左上方看到加入Playground的链接。单击该链接，您应该会看到类似于[图2-2](#fig_2_the_openai_playground_interface_in_text_completion)的内容。
- en: '![](assets/dagc_0202.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dagc_0202.png)'
- en: Figure 2-2\. The OpenAI Playground interface in Text Completion mode
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-2. 文本完成模式下的OpenAI Playground界面
- en: Note
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The ChatGPT Plus option is independent of using the API or the Playground. If
    you have subscribed to the ChatGPT Plus service, you will still be charged for
    using the API and the Playground.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT Plus选项与使用API或Playground无关。如果您订阅了ChatGPT Plus服务，那么使用API和Playground仍会产生费用。
- en: The main whitespace in the center of the interface is for your input message.
    After writing your message, click Submit to generate a completion to your message.
    In the example in [Figure 2-2](#fig_2_the_openai_playground_interface_in_text_completion),
    we wrote “As Descartes said, I think therefore”, and after we clicked Submit,
    the model completed our input with “I am”.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 界面中心的主要空白是用于输入消息的。在编写消息后，单击提交以生成消息的完成。在[图2-2](#fig_2_the_openai_playground_interface_in_text_completion)的示例中，我们写下“正如笛卡尔所说，我思故我在”，然后单击提交后，模型用“我是”完成了我们的输入。
- en: Warning
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Every time you click Submit, your OpenAI account is billed for the usage. We
    give more information on prices later in this chapter, but as an example, this
    completion cost almost $0.0002\.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 每次单击提交，您的OpenAI账户都会收取使用费。我们将在本章后面提供有关价格的更多信息，但举例来说，这个完成大约花费了$0.0002。
- en: There are many options around the sides of the interface. Let’s start at the
    bottom. To the right of the Submit button is an undo button [labeled (A) in the
    figure] that deletes the last generated text. In our case, it will delete “I am”.
    Next is the regenerate button [labeled (B) in the figure], which regenerates text
    that was just deleted. This is followed by the history button [labeled (C)], which
    contains all your requests from the previous 30 days. Note that once you are in
    the history menu, it is easy to delete requests if necessary for privacy reasons.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 界面的各个部分都有许多选项。让我们从底部开始。在提交按钮的右侧是一个撤消按钮[图中标记为(A)]，用于删除最后生成的文本。在我们的情况下，它将删除“我是”。接下来是重新生成按钮[图中标记为(B)]，用于重新生成刚刚删除的文本。然后是历史按钮[标记为(C)]，其中包含了您在过去30天内的所有请求。请注意，一旦进入历史菜单，根据隐私原因，有必要时可以轻松删除请求。
- en: The options panel on the right side of the screen provides various settings
    related to the interface and the chosen model. We will only explain some of these
    options here; others will be covered later in the book. The first drop-down list
    on the right is the Mode list [labeled (D)]. At the time of this writing, the
    available modes are Chat (default), Complete, and Edit.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 屏幕右侧的选项面板提供了与界面和所选模型相关的各种设置。我们只会在这里解释其中一些选项；其他选项将在本书的后面介绍。右侧的第一个下拉列表是模式列表[labeled
    (D)]。在撰写本文时，可用的模式是Chat（默认）、Complete和Edit。
- en: Note
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Complete and Edit modes are marked as legacy at the time of this book’s writing
    and will probably disappear in January 2024\.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书撰写时，Complete和Edit模式被标记为传统模式，并且可能会在2024年1月消失。
- en: As demonstrated previously, the language model strives to complete the user’s
    input prompt seamlessly in the Playground’s default mode.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所示，语言模型致力于在Playground的默认模式下无缝完成用户的输入提示。
- en: '[Figure 2-3](#fig_3_the_openai_playground_interface_in_chat_mode) shows an
    example of using the Playground in Chat mode. On the left of the screen is the
    System pane [labeled (E)]. Here you can describe how the chat system should behave.
    For instance, in [Figure 2-3](#fig_3_the_openai_playground_interface_in_chat_mode),
    we asked it to be a helpful assistant who loves cats. We also asked it to only
    talk about cats and to give short answers. The dialogue that results from having
    set these parameters is displayed in the center of the screen.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2-3](#fig_3_the_openai_playground_interface_in_chat_mode)显示了在Chat模式下使用Playground的示例。屏幕左侧是系统窗格[标记为(E)]。在这里，您可以描述聊天系统的行为方式。例如，在[图2-3](#fig_3_the_openai_playground_interface_in_chat_mode)中，我们要求它成为一个热爱猫的乐于助人的助手。我们还要求它只谈论猫，并给出简短的回答。设置这些参数所产生的对话显示在屏幕中央。'
- en: If you want to continue the dialogue with the system, click “Add message” [(F)],
    enter your message, and click Submit [(G)]. It is also possible to define the
    model on the right [(H)]; here we use GPT-4\. Note that not all models are available
    in all modes. For instance, only GPT-4 and GPT-3.5 Turbo are available in Chat
    mode.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想继续与系统对话，点击“添加消息”[(F)]，输入您的消息，然后点击提交[(G)]。也可以在右侧定义模型[(H)]；这里我们使用GPT-4。请注意，并非所有模型都在所有模式中可用。例如，只有GPT-4和GPT-3.5
    Turbo在Chat模式中可用。
- en: '![](assets/dagc_0203.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dagc_0203.png)'
- en: Figure 2-3\. The OpenAI Playground interface in Chat mode
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-3。Chat模式下的OpenAI Playground界面
- en: Another mode available in the Playground is Edit. In this mode, shown in [Figure 2-4](#fig_4_the_openai_playground_interface_in_edit_mode),
    you provide some text [(I)] and instruction [(J)], and the model will attempt
    to modify the text accordingly. In this example, a text describing a young man
    who is going on a trip is given. The model is instructed to change the subject
    of the text to an old woman, and you can see that the result respects the instructions
    [(K)].
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Playground中的另一种模式是Edit。在这种模式下，如[图2-4](#fig_4_the_openai_playground_interface_in_edit_mode)所示，您提供一些文本[(I)]和指示[(J)]，模型将尝试相应地修改文本。在这个例子中，给出了描述一个年轻男子要去旅行的文本。模型被指示将文本的主题改为一个老妇人，您可以看到结果符合指示[(K)]。
- en: '![](assets/dagc_0204.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dagc_0204.png)'
- en: Figure 2-4\. The OpenAI Playground interface in Edit mode
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-4。Edit模式下的OpenAI Playground界面
- en: On the right side of the Playground interface, below the Mode drop-down list,
    is the Model drop-down list [(L)]. As you have already seen, this is where you
    choose the LLM. The models available in the drop-down list depend on the selected
    mode. Below the Model drop-down list are parameters, such as Temperature [(M)],
    that define the model’s behavior. We will not go into the details of these parameters
    here. Most of them will be explored when we closely examine how these different
    models work.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在Playground界面的右侧，在Mode下拉列表下方，是Model下拉列表[(L)]。正如您已经看到的，这是您选择LLM的地方。下拉列表中可用的模型取决于所选的模式。在Model下拉列表下方是参数，比如温度[(M)]，它们定义了模型的行为。我们不会在这里详细讨论这些参数。当我们仔细研究这些不同的模型如何工作时，大部分参数将被探索。
- en: At the top of the screen is the “Load a preset” drop-down list [(N)] and four
    buttons. In [Figure 2-2](#fig_2_the_openai_playground_interface_in_text_completion),
    we used the LLM to complete the sentence “As Descartes said, I think therefore”,
    but it is possible to make the model perform particular tasks by using appropriate
    prompts. [Figure 2-5](#fig_5_drop_down_list_of_examples) shows a list of common
    tasks the model can perform associated with an example of a preset.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 屏幕顶部是“加载预设”下拉列表[(N)]和四个按钮。在[图2-2](#fig_2_the_openai_playground_interface_in_text_completion)中，我们使用LLM来完成句子“正如笛卡尔所说，我思故我在”，但是可以通过使用适当的提示使模型执行特定任务。[图2-5](#fig_5_drop_down_list_of_examples)显示了模型可以执行的常见任务列表，以及与预设示例相关联的示例。
- en: '![](assets/dagc_0205.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dagc_0205.png)'
- en: Figure 2-5\. Drop-down list of examples
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-5。示例的下拉列表
- en: It should be noted that the proposed presets define not only the prompt but
    also some options on the right side of the screen. For example, if you click Grammatical
    Standard English, you will see in the main window the prompt displayed in [Figure 2-6](#fig_6_example_prompt_for_grammatical_standard_english).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，所提议的预设不仅定义了提示，还定义了屏幕右侧的一些选项。例如，如果您点击Grammatical Standard English，您将在主窗口中看到[图2-6](#fig_6_example_prompt_for_grammatical_standard_english)中显示的提示。
- en: '![](assets/dagc_0206.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dagc_0206.png)'
- en: Figure 2-6\. Example prompt for Grammatical Standard English
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-6。Grammatical Standard English的示例提示
- en: 'If you click Submit, you will obtain the following response: “She did not go
    to the market.” You can use the prompts proposed in the drop-down list as a starting
    point, but you will always have to modify them to fit your problem. OpenAI also
    provides a [complete list of examples](https://platform.openai.com/examples) for
    different tasks.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您点击提交，您将得到以下回复：“她没有去市场。”您可以使用下拉列表中提供的提示作为起点，但您总是需要修改它们以适应您的问题。OpenAI还为不同任务提供了[完整的示例列表](https://platform.openai.com/examples)。
- en: Next to the “Load a preset” drop-down list in [Figure 2-4](#fig_4_the_openai_playground_interface_in_edit_mode)
    is the Save button [(O)]. Imagine that you have defined a valuable prompt with
    a model and its parameter for your task, and you want to easily reuse it later
    in the Playground. This Save button will save the current state of the Playground
    as a preset. You can give your preset a name and a description, and once saved,
    your preset will appear in the “Load a preset” drop-down list.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图2-4](#fig_4_the_openai_playground_interface_in_edit_mode)中“加载预设”下拉列表旁边是保存按钮[(O)]。想象一下，您已经为您的任务定义了一个有价值的提示，还有一个模型和它的参数，您希望以后在Playground中轻松地重用它。这个保存按钮将保存Playground的当前状态为一个预设。您可以为您的预设命名和描述，一旦保存，您的预设将出现在“加载预设”下拉列表中。
- en: 'The second-to-last button at the top of the interface is called “View code”
    [(P)]. It gives the code to run your test in the Playground directly in a script.
    You can request code in Python, Node.js, or cURL to interact directly with the
    OpenAI remote server in a Linux terminal. If the Python code of our example “As
    Descartes said, I think therefore” is asked, we get the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 界面顶部的倒数第二个按钮称为“查看代码”[(P)]。它提供了在Playground中直接以脚本运行您的测试的代码。您可以请求Python、Node.js或cURL代码，以直接与Linux终端中的OpenAI远程服务器进行交互。如果我们要求我们的示例“As
    Descartes said, I think therefore”的Python代码，我们将得到以下内容：
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now that you understand how to use the Playground to test OpenAI language models
    without coding, let’s discuss how to obtain and manage your API keys for OpenAI
    services.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您了解了如何使用Playground在不编写代码的情况下测试OpenAI语言模型，让我们讨论如何获取和管理OpenAI服务的API密钥。
- en: 'Getting Started: The OpenAI Python Library'
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门：OpenAI Python库
- en: In this section, we’ll focus on how to use API keys in a small Python script,
    and we’ll perform our first test with this OpenAI API.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重点介绍如何在一个小的Python脚本中使用API密钥，并使用OpenAI API进行我们的第一次测试。
- en: OpenAI provides GPT-4 and ChatGPT as a service. This means users cannot have
    direct access to the models’ code and cannot run the models on their own servers.
    However, OpenAI manages the deployment and running of its models, and users can
    call these models as long as they have an account and a secret key.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI提供GPT-4和ChatGPT作为服务。这意味着用户无法直接访问模型的代码，也无法在自己的服务器上运行模型。但是，OpenAI管理模型的部署和运行，用户可以调用这些模型，只要他们有帐户和秘密密钥。
- en: Before completing the following steps, make sure you are logged in on the [OpenAI
    web page](https://platform.openai.com/login?launch).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成以下步骤之前，请确保您已登录[OpenAI网页](https://platform.openai.com/login?launch)。
- en: OpenAI Access and API Key
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenAI访问和API密钥
- en: 'OpenAI requires you to have an API key to use its services. This key has two
    purposes:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI要求您拥有API密钥才能使用其服务。此密钥有两个目的：
- en: It gives you the right to call the API methods.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它赋予您调用API方法的权利。
- en: It links your API calls to your account for billing purposes.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将您的API调用与您的帐户关联起来，以进行计费。
- en: You must have this key in order to call the OpenAI services from your application.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须拥有此密钥才能从您的应用程序调用OpenAI服务。
- en: To obtain the key, navigate to the [OpenAI platform](https://platform.openai.com)
    page. In the upper-right corner, click your account name and then “View API keys,”
    as shown in [Figure 2-7](#fig_7_openai_menu_to_select_view_api_keys).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取密钥，请导航到[OpenAI平台](https://platform.openai.com)页面。在右上角，点击您的帐户名称，然后点击“查看API密钥”，如[图2-7](#fig_7_openai_menu_to_select_view_api_keys)所示。
- en: '![](assets/dagc_0207.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dagc_0207.png)'
- en: Figure 2-7\. OpenAI menu to select “View API keys”
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-7. OpenAI菜单选择“查看API密钥”
- en: When you are on the “API keys” page, click “Create new secret key” and make
    a copy of your key. This key is a long string of characters starting with *sk-*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当您在“API密钥”页面上时，点击“创建新的秘密密钥”并复制您的密钥。此密钥是以*sk-*开头的一长串字符。
- en: Warning
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Keep this key safe and secure because it is directly linked to your account,
    and a stolen key could result in unwanted costs.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 请妥善保管此密钥，因为它直接与您的帐户相关联，而且被盗的密钥可能会导致不必要的费用。
- en: Once you have your key, the best practice is to export it as an environment
    variable. This will allow your application to access the key without writing it
    directly in your code. Here is how to do that.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您获得了您的密钥，最佳做法是将其导出为环境变量。这将允许您的应用程序访问密钥，而无需直接在代码中编写它。以下是如何做到这一点。
- en: 'For Linux or Mac:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Linux或Mac：
- en: '[PRE1]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For Windows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Windows：
- en: '[PRE2]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The preceding code snippets will set an environment variable and make your key
    available to other processes that are launched from the same shell session. For
    Linux systems, it is also possible to add this code directly to your *.bashrc*
    file. This will allow access to your environment variable in all your shell sessions.
    Of course, do not include these command lines in the code you push to a public
    repository.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段将设置一个环境变量，并使您的密钥可用于从同一shell会话启动的其他进程。对于Linux系统，还可以直接将此代码添加到您的*.bashrc*文件中。这将允许在所有shell会话中访问您的环境变量。当然，不要将这些命令行包含在您推送到公共存储库的代码中。
- en: To permanently add/change an environment variable in Windows 11, press the Windows
    key + R key simultaneously to open the Run Program Or File window. In this window,
    type **sysdm.cpl** to go to the System Properties panel. Then click the Advanced
    tab followed by the Environment Variables button. On the resulting screen, you
    can add a new environment variable with your OpenAI key.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Windows 11中永久添加/更改环境变量，请同时按下Windows键+R键以打开运行程序或文件窗口。在此窗口中，键入**sysdm.cpl**以转到系统属性面板。然后点击高级选项卡，然后点击环境变量按钮。在结果屏幕上，您可以使用您的OpenAI密钥添加新的环境变量。
- en: Tip
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: OpenAI provides a detailed [page on API key safety](https://oreil.ly/2Qobg).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI提供了一个关于API密钥安全的详细页面。
- en: Now that you have your key, it’s time to write your first “Hello World” program
    with the OpenAI API.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 既然您已经有了您的密钥，现在是时候使用OpenAI API编写您的第一个“Hello World”程序了。
- en: “Hello World” Example
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: “Hello World”示例
- en: This section shows the first lines of code with the OpenAI Python library. We
    will start with a classic “Hello World” example to understand how OpenAI provides
    its services.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 本节显示了使用OpenAI Python库的第一行代码。我们将从一个经典的“Hello World”示例开始，以了解OpenAI如何提供其服务。
- en: 'Install the Python library with *pip*:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*pip*安装Python库：
- en: '[PRE3]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, access the OpenAI API in Python:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在Python中访问OpenAI API：
- en: '[PRE4]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You will see the following output:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到以下输出：
- en: '[PRE5]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Congratulations! You just wrote your first program using the OpenAI Python library.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您刚刚使用OpenAI Python库编写了您的第一个程序。
- en: Let’s go through the details of using this library.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解如何使用这个库。
- en: Tip
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'The OpenAI Python library also provides a command-line utility. The following
    code, running in a terminal, is equivalent to executing the previous “Hello World”
    example:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Python库还提供了一个命令行实用程序。在终端中运行以下代码等同于执行前面的“Hello World”示例：
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: It is also possible to interact with the OpenAI API through HTTP requests or
    the official Node.js library, as well as other [community-maintained libraries](https://platform.openai.com/docs/libraries).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以通过HTTP请求或官方Node.js库以及其他[社区维护的库](https://platform.openai.com/docs/libraries)与OpenAI
    API进行交互。
- en: 'As you may have observed, the code snippet does not explicitly mention the
    OpenAI API key. This is because the OpenAI library is designed to automatically
    look for an environment variable named `OPENAI_API_KEY`. Alternatively, you can
    point the `openai` module at a file containing your key with the following code:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，代码片段并未明确提及OpenAI API密钥。这是因为OpenAI库被设计为自动查找名为`OPENAI_API_KEY`的环境变量。或者，您可以使用以下代码将`openai`模块指向包含您的密钥的文件：
- en: '[PRE7]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Or you can manually set the API key within your code using the following method:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 或者您可以使用以下方法在代码中手动设置API密钥：
- en: '[PRE8]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Our recommendation is to follow a widespread convention for environment variables:
    store your key in a *.env* file, which is removed from source control in the *.gitignore*
    file. In Python, you can then run the `load_dotenv` function to load the environment
    variables and import the *openai* library:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议遵循环境变量的广泛约定：将密钥存储在*.env*文件中，并在*.gitignore*文件中将其从源代码控制中删除。然后，在Python中，您可以运行`load_dotenv`函数来加载环境变量并导入*openai*库：
- en: '[PRE9]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It is important to have the `openai` import declaration after loading the *.env*
    file; otherwise, the settings for OpenAI will not be applied correctly.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载*.env*文件后，重要的是要有`openai`导入声明；否则，OpenAI的设置将无法正确应用。
- en: Now that we’ve covered the basic concepts of ChatGPT and GPT-4, we can move
    on to the details of their use.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了ChatGPT和GPT-4的基本概念，我们可以继续了解它们的使用细节。
- en: Using ChatGPT and GPT-4
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用ChatGPT和GPT-4
- en: This section discusses how to use the model running behind ChatGPT and GPT-4
    with the OpenAI Python library.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了如何使用OpenAI Python库后台运行的模型与ChatGPT和GPT-4。
- en: 'At the time of this writing, GPT 3.5 Turbo is the least expensive and most
    versatile model. Therefore, it is also the best choice for most use cases. Here
    is an example of its use:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，GPT 3.5 Turbo是最便宜且最多功能的模型。因此，它也是大多数用例的最佳选择。以下是其使用示例：
- en: '[PRE10]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the preceding example, we used the minimum number of parameters—that is,
    the LLM used to do the prediction and the input messages. As you can see, the
    conversation format in the input messages allows multiple exchanges to be sent
    to the model. Note that the API does not store previous messages in its context.
    The question `"``What is it?"` refers to the previous answer and only makes sense
    if the model has knowledge of this answer. The entire conversation must be sent
    each time to simulate a chat session. We will discuss this further in the next
    section.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示例中，我们使用了最少数量的参数，即用于预测的LLM和输入消息。正如您所看到的，输入消息中的对话格式允许将多个交换发送到模型。请注意，API不会在其上下文中存储先前的消息。问题`"它是什么？"`是指先前的答案，只有在模型知道这个答案的情况下才有意义。每次都必须发送整个对话以模拟聊天会话。我们将在下一节中进一步讨论这一点。
- en: The GPT 3.5 Turbo and GPT-4 models are optimized for chat sessions, but this
    is not mandatory. Both models can be used for multiturn conversations and single-turn
    tasks. They also work well for traditional completion tasks if you specify a prompt
    asking for a completion.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 3.5 Turbo和GPT-4模型针对聊天会话进行了优化，但这并非强制要求。这两个模型都可以用于多轮对话和单轮任务。如果您指定一个提示来请求完成，它们也可以很好地完成传统的完成任务。
- en: 'Both ChatGPT and GPT-4 use the same endpoint: `openai.ChatCompletion`. Changing
    the model ID allows developers to switch between GPT-3.5 Turbo and GPT-4 without
    any other code changes.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT和GPT-4都使用相同的端点：`openai.ChatCompletion`。更改模型ID允许开发人员在不进行其他代码更改的情况下在GPT-3.5
    Turbo和GPT-4之间切换。
- en: Input Options for the Chat Completion Endpoint
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Chat Completion端点的输入选项
- en: Let’s look in more detail at how to use the `openai.ChatCompletion` endpoint
    and its `create` method.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看一下如何使用`openai.ChatCompletion`端点及其`create`方法。
- en: Note
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The `create` method lets users call OpenAI models. Other methods are available
    but aren’t helpful for interacting with the models. You can access the Python
    library code on OpenAI’s GitHub [Python library repository](https://oreil.ly/MQ2aQ).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`create`方法允许用户调用OpenAI模型。还有其他可用的方法，但对于与模型交互并不有用。您可以在OpenAI的GitHub [Python库存储库](https://oreil.ly/MQ2aQ)上访问Python库代码。'
- en: Required input parameters
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 必需的输入参数
- en: The `openai.ChatCompletion` endpoint and its `create` method have several input
    parameters, but only two are required, as outlined in [Table 2-1](#table-2-1).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`openai.ChatCompletion`端点及其`create`方法有几个输入参数，但只有两个是必需的，如[表2-1](#table-2-1)中所述。'
- en: Table 2-1\. Mandatory input parameters
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-1\. 必需的输入参数
- en: '| Field name | Type | Description |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 字段名称 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `model` | String | The ID of the model to use. Currently, the available models
    are `gpt-4`, `gpt-4-0613`, `gpt-4-32k`, `gpt-4-32k-0613`, `gpt-3.5-turbo`, `gpt-3.5-turbo-0613`,
    `gpt-3.5-turbo-16k`, and `gpt-3.5-turbo-16k-0613`. It is possible to access the
    list of available models with another endpoint and method provided by OpenAI,
    `openai.Model.list()`. Note that not all available models are compatible with
    the `openai.ChatCompletion` endpoint. |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| `model` | String | 要使用的模型ID。目前可用的模型有`gpt-4`、`gpt-4-0613`、`gpt-4-32k`、`gpt-4-32k-0613`、`gpt-3.5-turbo`、`gpt-3.5-turbo-0613`、`gpt-3.5-turbo-16k`和`gpt-3.5-turbo-16k-0613`。可以使用OpenAI提供的另一个端点和方法`openai.Model.list()`来访问可用模型的列表。请注意，并非所有可用模型都与`openai.ChatCompletion`端点兼容。
    |'
- en: '| `messages` | Array | An array of `message` objects representing a conversation.
    A `message` object has two attributes: `role` (possible values are `system`, `user`,
    and `assistant`) and `content` (a string with the conversation message). |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| `messages` | Array | 代表对话的`message`对象数组。`message`对象有两个属性：`role`（可能的值为`system`、`user`和`assistant`）和`content`（包含对话消息的字符串）。
    |'
- en: 'A conversation starts with an optional system message, followed by alternating
    user and assistant messages:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对话以可选的系统消息开始，然后是交替的用户和助手消息：
- en: The system message helps set the behavior of the assistant.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 系统消息有助于设置助手的行为。
- en: The user messages are the equivalent of a user typing a question or sentence
    in the ChatGPT web interface. They can be generated by the user of the application
    or set as an instruction.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 用户消息相当于用户在ChatGPT网络界面中输入问题或句子。它们可以由应用程序的用户生成，也可以作为指令设置。
- en: 'The assistant messages have two roles: either they store prior responses to
    continue the conversation or they can be set as an instruction to give examples
    of desired behavior. Models do not have any memory of past requests, so storing
    prior messages is necessary to give context to the conversation and provide all
    relevant information.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 助手消息有两个作用：要么存储先前的响应以继续对话，要么可以设置为指令，以提供所需行为的示例。模型没有任何关于过去请求的记忆，因此存储先前的消息对于给对话提供上下文和提供所有相关信息是必要的。
- en: Length of conversations and tokens
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对话长度和令牌
- en: 'As seen previously, the total length of the conversation will be correlated
    to the total number of tokens. This will have an impact on the following:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，对话的总长度将与令牌的总数相关。这将对以下内容产生影响：
- en: Cost
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 成本
- en: The pricing is by token.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 价格是按令牌计费。
- en: Timing
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 时间
- en: The more tokens there are, the more time the response will take—up to a couple
    of minutes.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌越多，响应所需的时间就越长，最多可能需要几分钟。
- en: The model working or not
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 模型是否工作。
- en: The total number of tokens must be less than the model’s maximum limit. You
    can find examples of token limits in [“Considerations”](#considerations).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌的总数必须小于模型的最大限制。您可以在[“注意事项”](#considerations)中找到令牌限制的示例。
- en: As you can see, it is necessary to carefully manage the length of the conversation.
    You can control the number of input tokens by managing the length of your messages
    and control the number of output tokens via the `max_tokens` parameter, as detailed
    in the next subsection.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，需要仔细管理对话的长度。您可以通过管理消息的长度来控制输入令牌的数量，并通过`max_tokens`参数来控制输出令牌的数量，详情请参见下一小节。
- en: Tip
  id: totrans-146
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: OpenAI provides a library named [*tiktoken*](https://oreil.ly/zxRIi) that allows
    developers to count how many tokens are in a text string. We highly recommend
    using this library to estimate costs before making the call to the endpoint.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI提供了一个名为[*tiktoken*](https://oreil.ly/zxRIi)的库，允许开发人员计算文本字符串中的令牌数量。我们强烈建议在调用端点之前使用此库来估算成本。
- en: Additional optional parameters
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 额外的可选参数
- en: OpenAI provides several other options to fine-tune how you interact with the
    library. We will not detail all the parameters here, but we recommend having a
    look at [Table 2-2](#table-2-2).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI提供了其他几个选项来微调您与库的交互方式。我们不会在这里详细介绍所有参数，但我们建议查看[表2-2](#table-2-2)。
- en: Table 2-2\. A selection of additional optional parameters
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-2. 一些额外的可选参数
- en: '| Field name | Type | Description |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 字段名称 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `functions` | Array | An array of available functions. See [“From Text Completions
    to Functions”](#from_text_completions_to_functions) for more details on how to
    use `functions`. |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| `functions` | 数组 | 可用函数的数组。有关如何使用`functions`的更多详细信息，请参见[“从文本完成到函数”](#from_text_completions_to_functions)。'
- en: '| `function_call` | String or object | Controls how the model responds:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '| `function_call` | 字符串或对象 | 控制模型的响应方式：'
- en: '`none` means the model must respond to the user in a standard way.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`none` 表示模型必须以标准方式回应用户。'
- en: '`{"name":"my_function"}` means the model must give an answer that uses the
    specified function.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`{"name":"my_function"}` 表示模型必须使用指定的函数来回答。'
- en: '`auto` means the model can choose between a standard response to the user or
    a function defined in the `functions` array.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auto` 表示模型可以在标准响应和`functions`数组中定义的函数之间进行选择。'
- en: '|'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| `temperature` | Number (default: 1; accepted values: between 0 and 2) | A
    temperature of `0` means the call to the model will likely return the same completion
    for a given input. Even though the responses will be highly consistent, OpenAI
    does not guarantee a deterministic output. The higher the value is, the more random
    the completion will be. LLMs generate answers by predicting a series of tokens
    one at a time. Based on the input context, they assign probabilities to each potential
    token. When the temperature parameter is set to `0`, the LLM will always choose
    the token with the highest probability. A higher temperature allows for more varied
    and creative outputs. |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| `temperature` | 数字（默认值：1；接受的值：0到2之间） | 温度为`0`意味着对模型的调用可能会返回相同的输入完成。尽管响应将非常一致，OpenAI并不保证确定性输出。数值越高，完成的随机性就越大。LLMs通过逐个预测一系列令牌来生成答案。根据输入上下文，它们为每个潜在的令牌分配概率。当温度参数设置为`0`时，LLM将始终选择概率最高的令牌。较高的温度允许更多变化和创造性的输出。'
- en: '| `n` | Integer (default: 1) | With this parameter, it is possible to generate
    multiple chat completions for a given input message. However, with a temperature
    of `0` as the input parameter, you will get multiple responses, but they will
    all be identical or very similar. |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| `n` | 整数（默认值：1） | 使用此参数，可以为给定的输入消息生成多个聊天完成。但是，当输入参数的温度为`0`时，您将获得多个响应，但它们将完全相同或非常相似。'
- en: '| `stream` | Boolean (default: false) | As its name suggests, this parameter
    will allow the answer to be in a stream format. This means partial messages will
    be sent gradually, like in the ChatGPT interface. This can make for a better user
    experience when the completions are long. |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| `stream` | 布尔值（默认值：false） | 正如其名称所示，此参数将允许答案以流格式呈现。这意味着部分消息将逐渐发送，就像在ChatGPT界面中一样。当完成很长时，这可以提供更好的用户体验。'
- en: '| `max_tokens` | Integer | This parameter signifies the maximum number of tokens
    to generate in the chat completion. This parameter is optional, but we highly
    recommend setting it as a good practice to keep your costs under control. Note
    that this parameter may be ignored or not respected if it is too high: the total
    length of the input and generated tokens is capped by the model’s token limitations.
    |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| `max_tokens` | 整数 | 此参数表示在聊天完成中生成的最大标记数。此参数是可选的，但我们强烈建议设置它作为一个良好的实践，以控制您的成本。请注意，如果设置得太高，此参数可能会被忽略或不被尊重：输入和生成的标记的总长度受模型的标记限制限制。'
- en: You can find more details and other parameters on the [official documentation
    page](https://platform.openai.com/docs/api-reference/chat).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[官方文档页面](https://platform.openai.com/docs/api-reference/chat)上找到更多详细信息和其他参数。
- en: Output Result Format for the Chat Completion Endpoint
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聊天完成端点的输出结果格式
- en: Now that you have the information you need to query chat-based models, let’s
    see how to use the results.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经获得了查询基于聊天的模型所需的信息，让我们看看如何使用结果。
- en: 'Following is the complete response for the “Hello World” example:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是“Hello World”示例的完整响应：
- en: '[PRE11]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The generated output is detailed in [Table 2-3](#table-2-3).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的输出在[表2-3](#table-2-3)中详细说明。
- en: Table 2-3\. Description of the output from the chat completion base models
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-3。聊天完成基本模型的输出描述
- en: '| Field name | Type | Description |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 字段名称 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `choices` | Array of “choice” object | An array that contains the actual
    response of the model. By default, this array will only have one element, which
    can be changed with the parameter `n` (see [“Additional optional parameters”](#additional_optional_parameters)).
    This element contains the following:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '| `choices` | “choice”对象的数组 | 包含模型实际响应的数组。默认情况下，此数组将只有一个元素，可以使用参数`n`更改（参见[“附加可选参数”](#additional_optional_parameters)）。此元素包含以下内容：'
- en: '`finish_reason` `-` `string`: The reason the answer from the model is finished.
    In our “Hello World” example, we can see the `finish_reason` is `stop`, which
    means we received the complete response from the model. If there is an error during
    the output generation, it will appear in this field.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`finish_reason` `-` `string`：模型答案完成的原因。在我们的“Hello World”示例中，我们可以看到`finish_reason`是`stop`，这意味着我们收到了模型的完整响应。如果在输出生成过程中出现错误，它将出现在此字段中。'
- en: '`index` `-` `integer`: The index of the `choice` object from the `choices`
    array.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index` `-` `integer`：`choices`数组中`choice`对象的索引。'
- en: '`message` `-` `object`: Contains a `role` and either a `content` or a `function_call`.
    The `role` will always be `assistant`, and the `content` will include the text
    generated by the model. Usually we want to get this string: `response[''choices''][0]​[''mes⁠sage''][''content'']`.
    For details on how to use `function_call`, see [“From Text Completions to Functions”](#from_text_completions_to_functions).'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`message` `-` `object`：包含`role`和`content`或`function_call`。`role`将始终是`assistant`，`content`将包括模型生成的文本。通常我们希望获得这个字符串：`response[''choices''][0]​[''mes⁠sage''][''content'']`。有关如何使用`function_call`的详细信息，请参见[“从文本完成到函数”](#from_text_completions_to_functions)。'
- en: '|'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| `created` | Timestamp | The date in a timestamp format at the time of the
    generation. In our “Hello World” example, this timestamp translates to Monday,
    April 10, 2023 1:49:55 p.m. |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| `created` | 时间戳 | 生成时的时间戳格式的日期。在我们的“Hello World”示例中，此时间戳对应于2023年4月10日星期一下午1:49:55。'
- en: '| `id` | String | A technical identifier used internally by OpenAI. |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| `id` | 字符串 | OpenAI内部使用的技术标识符。'
- en: '| `model` | String | The model used. This is the same as the model set as input.
    |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| `model` | 字符串 | 使用的模型。这与设置为输入的模型相同。'
- en: '| `object` | String | Should always be `chat.completion` for GPT-4 and GPT-3.5
    models, as we are using the chat completion endpoint. |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| `object` | 字符串 | 对于GPT-4和GPT-3.5模型，应始终为`chat.completion`，因为我们使用了聊天完成端点。'
- en: '| `usage` | String | Gives information on the number of tokens used in this
    query and therefore gives you pricing information. The `prompt_tokens` represents
    the number of tokens used in the input, the `completion_tokens` is the number
    of tokens in the output, and as you might have guessed, `total_tokens` = `prompt_tokens`
    + `completion_tokens`. |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| `usage` | 字符串 | 提供有关此查询中使用的标记数的信息，因此为您提供定价信息。`prompt_tokens`表示输入中使用的标记数，`completion_tokens`是输出中的标记数，正如您可能已经猜到的那样，`total_tokens`
    = `prompt_tokens` + `completion_tokens`。'
- en: Tip
  id: totrans-182
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you want to have multiple choices and use an `n` parameter higher than 1,
    you will see that the `prompt_tokens` value will not change, but the `completion_tokens`
    value will be roughly multiplied by `n`.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要有多个选择并使用高于1的`n`参数，您会发现`prompt_tokens`值不会改变，但`completion_tokens`值将大致乘以`n`。
- en: From Text Completions to Functions
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从文本完成到函数
- en: OpenAI introduced the possibility for its models to output a JSON object containing
    arguments to call functions. The model will not be able to call the function itself,
    but rather will convert a text input into an output format that can be executed
    programmatically by the caller.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI引入了其模型输出一个包含调用函数参数的JSON对象的可能性。模型本身将无法调用函数，而是将文本输入转换为可以由调用者以编程方式执行的输出格式。
- en: 'This is particularly useful when the result of the call to the OpenAI API needs
    to be processed by the rest of your code: instead of creating a complicated prompt
    to ensure that the model answers in a specific format that can be parsed by your
    code, you can use a function definition to convert natural language into API calls
    or database queries, extract structured data from text, and create chatbots that
    answer questions by calling external tools.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 当OpenAI API的调用结果需要被您的代码的其余部分处理时，这是特别有用的：您可以使用函数定义将自然语言转换为API调用或数据库查询，从文本中提取结构化数据，并创建通过调用外部工具来回答问题的聊天机器人，而不是创建一个复杂的提示以确保模型以特定格式回答，这个格式可以被您的代码解析。
- en: As you saw in [Table 2-2](#table-2-2), which details the input options for the
    chat completion endpoint, function definitions need to be passed as an array of
    function objects. The function object is detailed in [Table 2-4](#table-2-4).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在[表2-2](#table-2-2)中所见，该表详细介绍了聊天完成端点的输入选项，函数定义需要作为函数对象数组传递。函数对象在[表2-4](#table-2-4)中有详细描述。
- en: Table 2-4\. Details of the function object
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-4。函数对象的详细信息
- en: '| Field name | Type | Description |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|字段名称|类型|描述|'
- en: '| --- | --- | --- |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|---|---|---|'
- en: '| `name` | String (required) | The name of the function. |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|`name`|字符串（必需）|函数的名称。|'
- en: '| `description` | String | The description of the function. |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|`description`|字符串|函数的描述。|'
- en: '| `parameters` | Object | The parameters expected by the function. These parameters
    are expected to be described in a [JSON Schema](http://json-schema.org) format.
    |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|`parameters`|对象|函数期望的参数。这些参数应该以[JSON Schema](http://json-schema.org)格式描述。|'
- en: 'As an example, imagine that we have a database that contains information relative
    to company products. We can define a function that executes a search against this
    database:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设我们有一个包含与公司产品相关信息的数据库。我们可以定义一个执行针对该数据库的搜索的函数：
- en: '[PRE12]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we define the specifications of the functions:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义函数的规格：
- en: '[PRE13]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can then create a conversation and call the `openai.ChatCompletion` endpoint:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以创建一个对话并调用`openai.ChatCompletion`端点：
- en: '[PRE14]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The model has created a query that we can use. If we print the `function_call`
    object from the response, we get:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 模型已经创建了一个我们可以使用的查询。如果我们从响应中打印`function_call`对象，我们会得到：
- en: '[PRE15]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, we execute the function and continue the conversation with the result:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们执行函数并继续与结果进行对话：
- en: '[PRE16]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'And finally, we extract the final response and obtain the following:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们提取最终的响应并获得以下内容：
- en: '[PRE17]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This simple example demonstrates how functions can be useful to build a solution
    that allows end users to interact in natural language with a database. The function
    definitions allow you to constrain the model to answer exactly as you want it
    to, and integrate its response into an application.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的例子演示了函数如何有助于构建一个解决方案，允许最终用户以自然语言与数据库进行交互。函数定义允许您限制模型的回答方式，并将其响应集成到应用程序中。
- en: Using Other Text Completion Models
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用其他文本完成模型
- en: As mentioned, OpenAI provides several additional models besides the GPT-3 and
    GPT-3.5 series. These models use a different endpoint than the ChatGPT and GPT-4
    models. Even though the GPT 3.5 Turbo model is usually the best choice in terms
    of both price and performance, it is helpful to know how to use the completion
    models, particularly for use cases such as fine-tuning, in which the GPT-3 completion
    models are the only choice.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 正如提到的，OpenAI除了GPT-3和GPT-3.5系列之外，还提供了几个其他模型。这些模型使用的端点与ChatGPT和GPT-4模型不同。尽管GPT
    3.5 Turbo模型通常是在价格和性能方面最佳的选择，但了解如何使用完成模型，特别是对于微调等用例，对于只有GPT-3完成模型可用的情况非常有帮助。
- en: Note
  id: totrans-209
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: OpenAI has released a deprecation plan for the text completion endpoint. We
    introduce this endpoint here only because completion base models are the only
    ones that can be fine-tuned. OpenAI will provide a solution for fine-tuning chat-based
    models by January 2024\. As it is not available yet, we do not have the necessary
    information to describe it here.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI已经发布了文本完成端点的弃用计划。我们在这里介绍这个端点只是因为完成基础模型是唯一可以进行微调的模型。OpenAI将在2024年1月之前为基于聊天的模型提供微调的解决方案。由于目前还不可用，我们没有必要的信息来在这里描述它。
- en: 'There is an important difference between text completion and chat completion:
    as you might guess, both generate text, but chat completion is optimized for conversations.
    As you can see in the following code snippet, the main difference with the `openai.ChatCompletion`
    endpoint is the prompt format. Chat-based models must be in conversation format;
    for completion, it is a single prompt:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 文本完成和聊天完成之间有一个重要的区别：你可能会猜到，两者都生成文本，但聊天完成是为对话优化的。如你在下面的代码片段中所见，与`openai.ChatCompletion`端点的主要区别在于提示格式。基于聊天的模型必须是对话格式；对于完成，它是一个单一的提示：
- en: '[PRE18]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The preceding code snippet will output a completion similar to the following:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码片段将输出类似于以下内容的完成：
- en: '[PRE19]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The next section goes through the details of the text completion endpoint’s
    input options.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将详细介绍文本完成端点的输入选项。
- en: Input Options for the Text Completion Endpoint
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本完成端点的输入选项
- en: The set of input options for `openai.Completion.create` is very similar to what
    we saw previously with the chat endpoint. In this section, we will discuss the
    main input parameters and consider the impact of the length of the prompt.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '`openai.Completion.create`的输入选项集与我们之前在聊天端点中看到的非常相似。在本节中，我们将讨论主要的输入参数，并考虑提示的长度对其影响。'
- en: Main input parameters
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主要输入参数
- en: The required input parameters and a selection of optional parameters that we
    feel are most useful are described in [Table 2-5](#table-2-5).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们描述了必需的输入参数和我们认为最有用的一些可选参数在[表2-5](#table-2-5)中。
- en: Table 2-5\. Required parameters and optional parameters for the text completion
    endpoint
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-5。文本完成端点的必需参数和可选参数
- en: '| Field name | Type | Description |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '|字段名称|类型|描述|'
- en: '| --- | --- | --- |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|---|---|---|'
- en: '| `model` | String (required) | ID of the model to use (the same as with `openai.ChatCompletion`).
    This is the only required option. |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|`model`|字符串（必需）|要使用的模型的ID（与`openai.ChatCompletion`相同）。这是唯一必需的选项。|'
- en: '| `prompt` | String or array (default: `<&#124;endoftext&#124;>`) | The prompt
    to generate completions for. This is the main difference from the `openai.ChatCompletion`
    endpoint. The `openai.Completion.create` endpoint should be encoded as a string,
    array of strings, array of tokens, or array of token arrays. If no prompt is provided
    to the model, it will generate text as if from the beginning of a new document.
    |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| `prompt` | 字符串或数组（默认：`<&#124;endoftext&#124;>`） | 用于生成完成的提示。这是与`openai.ChatCompletion`端点的主要区别。`openai.Completion.create`端点应编码为字符串、字符串数组、标记数组或标记数组的数组。如果未提供提示给模型，它将生成文本，就像从新文档的开头开始一样。'
- en: '| `max_tokens` | Integer | The maximum number of tokens to generate in the
    chat completion. The default value of this parameter is `16`, which may be too
    low for some use cases and should be adjusted according to your needs. |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| `max_tokens` | 整数 | 聊天完成中要生成的标记的最大数量。此参数的默认值为`16`，对于某些用例来说可能太低，应根据您的需求进行调整。'
- en: '| `suffix` | String (default: null) | The text that comes after the completion.
    This parameter allows adding a suffix text. It also allows making insertions.
    |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| `suffix` | 字符串（默认：null） | 完成后的文本。此参数允许添加后缀文本。它还允许进行插入。'
- en: Length of prompts and tokens
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示和标记的长度
- en: Just as with the chat models, pricing will depend on the input you send and
    the output you receive. For the input message, you must carefully manage the length
    of the prompt parameter, as well as the suffix if one is used. For the output
    you receive, use `max_tokens``.` It allows you to avoid unpleasant surprises.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 与聊天模型一样，定价将取决于您发送的输入和接收的输出。对于输入消息，您必须仔细管理提示参数的长度，以及如果使用后缀，则使用后缀。对于您接收的输出，请使用`max_tokens`。它可以避免令人不快的惊喜。
- en: Additional optional parameters
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他可选参数
- en: Also as with `openai.ChatCompletion`, additional optional parameters may be
    used to further tweak the behavior of the model. These parameters are the same
    as those used for `openai.ChatCompletion`, so we will not detail them again. Remember
    that you can control the output with the `temperature` or `n` parameter, control
    your costs with `max_tokens`, and use the `stream` option if you wish to have
    a better user experience with long completions.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 与`openai.ChatCompletion`一样，还可以使用其他可选参数来进一步调整模型的行为。这些参数与`openai.ChatCompletion`使用的参数相同，因此我们不会再详细介绍它们。请记住，您可以使用`temperature`或`n`参数控制输出，使用`max_tokens`控制成本，并使用`stream`选项，如果您希望获得更好的用户体验，可以进行长完成。
- en: Output Result Format for the Text Completion Endpoint
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本完成端点的输出结果格式
- en: 'Now that you have all the information needed to query text-based models, you
    will find that the results are very similar to the chat endpoint results. Here
    is an example output for our “Hello World” example with the `davinci` model:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经拥有了查询基于文本的模型所需的所有信息，您会发现结果与聊天端点的结果非常相似。以下是我们的“Hello World”示例与`davinci`模型的示例输出：
- en: '[PRE20]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Note
  id: totrans-234
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'This output is very similar to what we got with the chat models. The only difference
    is in the `choice` object: instead of having a message with `content` and `role`
    attributes, we have a simple `text` attribute containing the completion generated
    by the model.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出与我们在聊天模型中得到的非常相似。唯一的区别在于`choice`对象：不再有带有`content`和`role`属性的消息，而是有一个简单的`text`属性，其中包含模型生成的完成。
- en: Considerations
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 考虑事项
- en: 'You should consider two important things before using the APIs extensively:
    cost and data privacy.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在大量使用API之前，您应该考虑两个重要的事情：成本和数据隐私。
- en: Pricing and Token Limitations
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定价和标记限制
- en: OpenAI keeps the pricing of its models listed on its [pricing page](https://openai.com/pricing).
    Note that OpenAI is not bound to maintain this pricing, and the costs may change
    over time.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI将其模型的定价列在其[定价页面](https://openai.com/pricing)上。请注意，OpenAI不受约束地维护此定价，成本可能会随时间变化。
- en: At the time of this writing, the pricing is as shown in [Table 2-6](#table-2-6)
    for the OpenAI models used most often.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，OpenAI模型的定价如[表2-6](#table-2-6)所示。
- en: Table 2-6\. Pricing and token limitations per model
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-6。每个模型的定价和标记限制
- en: '| Family | Model | Pricing | Max tokens |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 家庭 | 模型 | 定价 | 最大标记 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Chat | `gpt-4` | Prompt: $0.03 per 1,000 tokensCompletion: $0.06 per 1,000
    tokens | 8,192 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 聊天 | `gpt-4` | 提示：每1,000个标记0.03美元完成：每1,000个标记0.06美元 | 8,192 |'
- en: '| Chat | `gpt-4-32k` | Prompt: $0.06 per 1,000 tokensCompletion: $0.012 per
    1,000 tokens | 32,768 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 聊天 | `gpt-4-32k` | 提示：每1,000个标记0.06美元完成：每1,000个标记0.012美元 | 32,768 |'
- en: '| Chat | `gpt-3.5-turbo` | Prompt: $0.0015 per 1,000 tokensCompletion: $0.002
    per 1,000 tokens | 4,096 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 聊天 | `gpt-3.5-turbo` | 提示：每1,000个标记0.0015美元完成：每1,000个标记0.002美元 | 4,096 |'
- en: '| Chat | `gpt-3.5-turbo-16k` | Prompt: $0.003 per 1,000 tokensCompletion: $0.004
    per 1,000 tokens | 16,384 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 聊天 | `gpt-3.5-turbo-16k` | 提示：每1,000个标记0.003美元完成：每1,000个标记0.004美元 | 16,384
    |'
- en: '| Text completion | `text-davinci-003` | $0.02 per 1,000 tokens | 4,097 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 文本完成 | `text-davinci-003` | 每1,000个标记0.02美元 | 4,097 |'
- en: 'There are several things to note from [Table 2-6](#table-2-6):'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 从[表2-6](#table-2-6)中有几件事情需要注意：
- en: The `davinci` model is more than 10 times the cost of the GPT-3.5 Turbo 4,000-context
    model. Since `gpt-3.5-turbo` can also be used for single-turn completion tasks
    and since both models are nearly equal in accuracy for this type of task, it is
    recommended to use GPT-3.5 Turbo (unless you need special features such as insertion,
    via the parameter suffix, or if `text-davinci-003` outperforms `gpt-3.5-turbo`
    for your specific task).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '`davinci`模型的成本是GPT-3.5 Turbo 4,000-context模型的10多倍。由于`gpt-3.5-turbo`也可以用于单轮完成任务，并且由于这两个模型在这种类型的任务中几乎具有相同的准确性，建议使用GPT-3.5
    Turbo（除非您需要特殊功能，例如通过参数后缀进行插入，或者如果`text-davinci-003`在您的特定任务中优于`gpt-3.5-turbo`）。'
- en: GPT-3.5 Turbo is less expensive than GPT-4\. The differences between GPT-4 and
    GPT-3.5 are irrelevant for many basic tasks. However, in complex inference situations,
    GPT-4 far outperforms any previous model.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3.5 Turbo比GPT-4便宜。对于许多基本任务，GPT-4和GPT-3.5之间的差异是无关紧要的。但是，在复杂的推理情况下，GPT-4远远优于任何以前的模型。
- en: 'The chat models have a different pricing system than the `davinci` models:
    they differentiate input (prompt) and output (completion).'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天模型的定价系统与`davinci`模型不同：它们区分输入（提示）和输出（完成）。
- en: GPT-4 allows a context twice as long as GPT-3.5 Turbo, and can even go up to
    32,000 tokens, which is equivalent to more than 25,000 words of text. GPT-4 enables
    use cases such as long-form content creation, advanced conversation, and document
    search and analysis… for a cost.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4允许的上下文长度是GPT-3.5 Turbo的两倍，甚至可以达到32,000个标记，相当于超过25,000个文字。GPT-4可以实现长篇内容创作、高级对话以及文档搜索和分析等用例……但需要付费。
- en: 'Security and Privacy: Caution!'
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全和隐私：注意！
- en: As we write this, OpenAI claims the data sent as input to the models will not
    be used for retraining unless you decide to opt in. However, your inputs are retained
    for 30 days for monitoring and usage compliance-checking purposes. This means
    OpenAI employees as well as specialized third-party contractors may have access
    to your API data.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们撰写本文时，OpenAI声称发送给模型的输入数据不会被用于重新训练，除非您决定选择加入。然而，您的输入将被保留30天，用于监控和使用合规检查。这意味着OpenAI员工以及专门的第三方承包商可能会访问您的API数据。
- en: Warning
  id: totrans-256
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Never send sensitive data such as personal information or passwords through
    the OpenAI endpoints. We recommend that you check [OpenAI’s data usage policy](https://openai.com/policies/api-data-usage-policies)
    for the latest information, as this can be subject to change. If you are an international
    user, be aware that your personal information and the data you send as input can
    be transferred from your location to the OpenAI facilities and servers in the
    United States. This may have some legal impact on your application creation.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 永远不要通过OpenAI端点发送个人信息或密码等敏感数据。我们建议您查看[OpenAI的数据使用政策](https://openai.com/policies/api-data-usage-policies)以获取最新信息，因为这可能会有所变化。如果您是国际用户，请注意您的个人信息和输入的数据可能会从您的位置传输到美国的OpenAI设施和服务器。这可能会对您的应用程序创建产生一些法律影响。
- en: More details on how to build LLM-powered applications while taking into account
    security and privacy issues can be found in [Chapter 3](ch03.html#building_apps_with_gpt_4_and_chatgpt).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 有关如何构建考虑安全和隐私问题的LLM应用程序的更多细节，请参阅[第3章](ch03.html#building_apps_with_gpt_4_and_chatgpt)。
- en: Other OpenAI APIs and Functionalities
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他OpenAI API和功能
- en: Your OpenAI account gives you access to functionalities besides text completion.
    We selected several of these functionalities to explore in this section, but if
    you want a deep dive into all the API possibilities, look at [OpenAI’s API reference
    page](https://platform.openai.com/docs/api-reference).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 您的OpenAI账户除了文本完成之外还可以访问其他功能。我们在本节中选择了其中几个功能进行探索，但如果您想深入了解所有API的可能性，请查看[OpenAI的API参考页面](https://platform.openai.com/docs/api-reference)。
- en: Embeddings
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入
- en: Since a model relies on mathematical functions, it needs numerical input to
    process information. However, many elements, such as words and tokens, aren’t
    inherently numerical. To overcome this, *embeddings* convert these concepts into
    numerical vectors. Embeddings allow computers to process the relationships between
    these concepts more efficiently by representing them numerically. In some situations,
    it can be useful to have access to embeddings, and OpenAI provides a model that
    can transform a text into a vector of numbers. The embeddings endpoint allows
    developers to obtain a vector representation of an input text. This vector representation
    can then be used as input to other ML models and NLP algorithms.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型依赖数学函数，它需要数字输入来处理信息。然而，许多元素，如单词和标记，并不是固有的数字。为了克服这一点，*嵌入*将这些概念转换为数字向量。嵌入允许计算机通过数值表示更有效地处理这些概念之间的关系。在某些情况下，访问嵌入可能是有用的，OpenAI提供了一个可以将文本转换为数字向量的模型。嵌入端点允许开发人员获取输入文本的向量表示。然后，这个向量表示可以作为其他ML模型和NLP算法的输入使用。
- en: 'At the time of this writing, OpenAI recommends using its latest model, `text-embedding-ada-002`,
    for nearly all use cases. It is very simple to use:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，OpenAI建议几乎所有用例使用其最新模型`text-embedding-ada-002`。使用起来非常简单：
- en: '[PRE21]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The embedding is accessed with:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过以下方式访问嵌入：
- en: '[PRE22]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The resulting embedding is a vector: an array of floats.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的嵌入是一个向量：一个浮点数组。
- en: Tip
  id: totrans-268
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The complete documentation on embeddings is available in [OpenAI’s reference
    documents](https://platform.openai.com/docs/api-reference/embeddings).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 有关嵌入的完整文档可在[OpenAI的参考文档](https://platform.openai.com/docs/api-reference/embeddings)中找到。
- en: 'The principle of embeddings is to represent text strings meaningfully in some
    space that captures their semantic similarity. With this idea, you can have various
    use cases:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入的原则是以某种方式有意义地表示文本字符串，捕捉它们的语义相似性。有了这个想法，你可以有各种用例：
- en: Search
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索
- en: Sort results by relevance to the query string.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 按与查询字符串相关性对结果进行排序。
- en: Recommendations
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐
- en: Recommend articles that contain text strings related to the query string.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐包含与查询字符串相关的文本字符串的文章。
- en: Clustering
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类
- en: Group strings by similarity.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 按相似性对字符串进行分组。
- en: Anomaly detection
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测
- en: Find a text string that is not related to the other strings.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 找到一个与其他字符串无关的文本字符串。
- en: Embeddings have the property that if two texts have a similar meaning, their
    vector representation will be similar. As an example, in [Figure 2-8](#fig_8_example_of_two_dimensional_embedding_of_three_sent),
    three sentences are shown in two-dimensional embeddings. Although the two sentences
    “The cat chased the mouse around the house.” and “Around the house, the mouse
    was pursued by the cat.” have different syntaxes, they convey the same general
    meaning, and therefore they should have similar embedding representations. As
    the sentence “The astronaut repaired the spaceship in orbit.” is unrelated to
    the topic of the previous sentences (cats and mice) and discusses an entirely
    different subject (astronauts and spaceships), it should have a significantly
    different embedding representation. Note that in this example, for clarity we
    show the embedding as having two dimensions, but in reality, they are often in
    a much higher dimension, such as 512.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入式具有这样的特性，即如果两个文本具有相似的含义，它们的向量表示将是相似的。例如，在[图2-8](#fig_8_example_of_two_dimensional_embedding_of_three_sent)中，显示了三个句子的二维嵌入。尽管两个句子“猫追逐老鼠绕着房子。”和“房子周围，老鼠被猫追赶。”有不同的句法，但它们传达了相同的一般意义，因此它们应该具有相似的嵌入表示。而句子“宇航员在轨道上修理了太空飞船。”与前两个句子的主题（猫和老鼠）无关，讨论的是完全不同的主题（宇航员和太空飞船），因此它应该具有明显不同的嵌入表示。请注意，在此示例中，为了清晰起见，我们将嵌入显示为具有两个维度，但实际上它们通常是更高维度的，例如512。
- en: '![](assets/dagc_0208.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dagc_0208.png)'
- en: Figure 2-8\. Example of two-dimensional embedding of three sentences
  id: totrans-281
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-8\. 三个句子的二维嵌入示例
- en: We refer to the embeddings API several times in the remaining chapters, as embeddings
    are an essential part of processing natural language with AI models.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在剩下的章节中多次提到嵌入式API，因为嵌入式是处理自然语言与AI模型的重要部分。
- en: Moderation Model
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 审查模型
- en: 'As mentioned earlier, when using the OpenAI models you must respect the rules
    described in the [OpenAI usage policies](https://openai.com/policies/usage-policies).
    To help you respect these rules, OpenAI provides a model to check whether the
    content complies with these usage policies. This can be useful if you build an
    app in which user input will be used as a prompt: you can filter the queries based
    on the moderation endpoint results. The model provides classification capabilities
    that allow you to search for content in the following categories:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，使用OpenAI模型时，您必须遵守[OpenAI使用政策](https://openai.com/policies/usage-policies)中描述的规则。为了帮助您遵守这些规则，OpenAI提供了一个模型，用于检查内容是否符合这些使用政策。如果您构建一个应用程序，用户输入将用作提示，这可能很有用：您可以根据审查端点的结果过滤查询。该模型提供了分类功能，允许您搜索以下类别的内容：
- en: Hate
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 仇恨
- en: Promoting hatred against groups based on race, gender, ethnicity, religion,
    nationality, sexual orientation, disability, or caste
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 针对基于种族、性别、种族、宗教、国籍、性取向、残疾或种姓的群体的仇恨
- en: Hate/threatening
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 仇恨/威胁
- en: Hateful content that involves violence or severe harm to targeted groups
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 涉及对特定群体进行暴力或严重伤害的仇恨内容
- en: Self-harm
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 自残
- en: Content that promotes or depicts acts of self-harm, including suicide, cutting,
    and eating disorders
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 推广或描述自残行为，包括自杀、自残和饮食障碍
- en: Sexual
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 性
- en: Content designed to describe a sexual activity or promote sexual services (except
    for education and wellness)
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 旨在描述性行为或推广性服务（除了教育和健康）的内容
- en: Sexual with minors
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 涉及未成年人的性
- en: Sexually explicit content involving persons under 18 years of age
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 涉及18岁以下人员的性内容
- en: Violence
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 暴力
- en: Content that glorifies violence or celebrates the suffering or humiliation of
    others
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 美化暴力或庆祝他人的痛苦或羞辱的内容
- en: Violence/graphic
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 暴力/图形
- en: Violent content depicting death, violence, or serious bodily injury in graphic
    detail
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 描绘死亡、暴力或严重身体伤害的暴力内容
- en: Note
  id: totrans-299
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Support for languages other than English is limited.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 支持英语以外的语言有限。
- en: 'The endpoint for the moderation model is `openai.Moderation.create`, and only
    two parameters are available: the model and the input text. There are two models
    of content moderation. The default is `text-moderation-latest`, which is automatically
    updated over time to ensure that you always use the most accurate model. The other
    model is `text-moderation-stable`. OpenAI will notify you before updating this
    model.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 审查模型的端点是`openai.Moderation.create`，只有两个参数可用：模型和输入文本。内容审查有两种模型。默认模型是`text-moderation-latest`，会随着时间自动更新，以确保您始终使用最准确的模型。另一个模型是`text-moderation-stable`。OpenAI会在更新此模型之前通知您。
- en: Warning
  id: totrans-302
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: The accuracy of `text-moderation-stable` may be slightly lower than `text-moderation-latest`.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '`text-moderation-stable`的准确性可能略低于`text-moderation-latest`。'
- en: 'Here is an example of how to use this moderation model:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何使用这个审查模型的示例：
- en: '[PRE23]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let’s take a look at the output result of the moderation endpoint contained
    in the `response` object:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看`response`对象中包含的审查端点的输出结果：
- en: '[PRE24]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The output result of the moderation endpoint provides the pieces of information
    shown in [Table 2-7](#table-2-7).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 审查端点的输出结果提供了[表2-7](#table-2-7)中显示的信息片段。
- en: Table 2-7\. Description of the output of the moderation endpoint
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-7\. 审查端点输出的描述
- en: '| Field name | Type | Description |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 字段名称 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `model` | String | This is the model used for the prediction. When calling
    the method in our earlier example, we specified the use of the model `text-moderation-latest`,
    and in the output result, the model used is `text-moderation-004`. If we had called
    the method with `text-moderation-stable`, then `text-moderation-001` would have
    been used. |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| `model` | 字符串 | 这是用于预测的模型。在我们之前的示例中调用该方法时，我们指定了使用模型`text-moderation-latest`，在输出结果中使用的模型是`text-moderation-004`。如果我们使用`text-moderation-stable`调用该方法，那么将使用`text-moderation-001`。'
- en: '| `flagged` | Boolean | If the model identifies the content as violating OpenAI’s
    usage policies, set this to `true`; otherwise, set it to `false`. |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| `flagged` | 布尔值 | 如果模型确定内容违反了OpenAI的使用政策，则将其设置为`true`；否则，将其设置为`false`。 |'
- en: '| `categories` | Dict | This includes a dictionary with binary flags for policy
    violation categories. For each category, the value is `true` if the model identifies
    a violation and `false` if not. The dictionary can be accessed via `print(type(response[''results''][0]​[''cate⁠gories'']))`.
    |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| `categories` | Dict | 这包括一个带有违规政策类别的二进制标志的字典。对于每个类别，如果模型识别到违规，则值为`true`，否则为`false`。可以通过`print(type(response[''results''][0]​[''cate⁠gories'']))`访问该字典。
    |'
- en: '| `category_scores` | Dict | The model provides a dictionary with category-specific
    scores that show how confident it is that the input goes against OpenAI’s policy
    for that category. Scores range from 0 to 1, with higher scores meaning more confidence.
    These scores should not be seen as probabilities. The dictionary can be accessed
    via `print(type(response​[''re⁠sults''][0][''category_scores'']))`. |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| `category_scores` | Dict | 该模型提供了一个具有特定类别分数的字典，显示它对输入违反OpenAI该类别政策的信心程度。分数范围从0到1，分数越高表示信心越大。这些分数不应被视为概率。可以通过`print(type(response​[''re⁠sults''][0][''category_scores'']))`访问该字典。
    |'
- en: Warning
  id: totrans-316
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: OpenAI will regularly improve the moderation system. As a result, the `category_scores`
    may vary, and the threshold set to determine the category value from a category
    score may also change.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI将定期改进审查系统。因此，“category_scores”可能会有所变化，并且用于确定类别值的阈值也可能会改变。
- en: Whisper and DALL-E
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Whisper和DALL-E
- en: OpenAI also provides other AI tools that are not LLMs but can easily be used
    in combination with GPT models in some use cases. We don’t explain them here because
    they are not the focus of this book. But don’t worry, using their APIs is very
    similar to using OpenAI’s LLM APIs.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI还提供其他不是LLM的AI工具，但在某些用例中可以与GPT模型轻松结合使用。我们在这里不解释它们，因为它们不是本书的重点。但是不用担心，使用它们的API与使用OpenAI的LLM
    API非常相似。
- en: Whisper is a versatile model for speech recognition. It is trained on a large
    audio dataset and is also a multitasking model that can perform multilingual speech
    recognition, speech translation, and language identification. An open source version
    is available on the [Whisper project’s GitHub page](https://github.com/openai/whisper)
    of OpenAI.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper是语音识别的多功能模型。它经过大型音频数据集的训练，也是一个多任务模型，可以执行多语言语音识别、语音翻译和语言识别。OpenAI的[Whisper项目GitHub页面](https://github.com/openai/whisper)上提供了开源版本。
- en: In January 2021, OpenAI introduced DALL-E, an AI system capable of creating
    realistic images and artwork from natural language descriptions. DALL-E 2 takes
    the technology further with higher resolution, greater input text comprehension,
    and new capabilities. Both versions of DALL-E were created by training a transformer
    model on images and their text descriptions. You can try DALL-E 2 through the
    API and via the [Labs interface](https://labs.openai.com).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 2021年1月，OpenAI推出了DALL-E，这是一种能够根据自然语言描述创建逼真图像和艺术品的AI系统。DALL-E 2通过更高的分辨率、更大的输入文本理解能力和新的功能进一步推动了这项技术。DALL-E的两个版本都是通过对图像及其文本描述进行训练来创建的变压器模型。您可以通过API和[Labs界面](https://labs.openai.com)尝试DALL-E
    2。
- en: Summary (and Cheat Sheet)
  id: totrans-322
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要（和备忘单）
- en: 'As we have seen, OpenAI provides its models as a service, through an API. In
    this book, we chose to use the Python library provided by OpenAI, which is a simple
    wrapper around the API. With this library, we can interact with the GPT-4 and
    ChatGPT models: the first step to building LLM-powered applications! However,
    using these models implies several considerations: API key management, pricing,
    and privacy.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，OpenAI通过API提供其模型作为服务。在本书中，我们选择使用OpenAI提供的Python库，它是API的简单封装。使用这个库，我们可以与GPT-4和ChatGPT模型进行交互：这是构建LLM应用程序的第一步！然而，使用这些模型意味着需要考虑几个方面：API密钥管理、定价和隐私。
- en: 'Before starting, we recommend looking at the OpenAI usage policies, and playing
    with the Playground to get familiar with the different models without the hassle
    of coding. Remember: GPT-3.5 Turbo, the model behind ChatGPT, is the best choice
    for most use cases.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我们建议查看OpenAI的使用政策，并在Playground上玩耍，以熟悉不同模型而不必编写代码。记住：ChatGPT背后的GPT-3.5
    Turbo是大多数用例的最佳选择。
- en: 'Following is a cheat sheet to use when sending input to GPT-3.5 Turbo:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是发送输入到GPT-3.5 Turbo时使用的备忘单：
- en: 'Install the `openai` dependency:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装`openai`依赖项：
- en: '[PRE25]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Set your API key as an environment variable:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将API密钥设置为环境变量：
- en: '[PRE26]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In Python, import `openai`:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Python中，导入`openai`：
- en: '[PRE27]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Call the `openai.ChatCompletion` endpoint:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`openai.ChatCompletion`端点：
- en: '[PRE28]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Get the answer:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取答案：
- en: '[PRE29]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Tip
  id: totrans-336
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Don’t forget to check the [pricing page](https://openai.com/pricing), and use
    [tiktoken](https://github.com/openai/tiktoken) to estimate the usage costs.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记查看[定价页面](https://openai.com/pricing)，并使用[tiktoken](https://github.com/openai/tiktoken)来估算使用成本。
- en: Note that you should never send sensitive data, such as personal information
    or passwords, through the OpenAI endpoints.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您不应该通过OpenAI端点发送个人信息或密码等敏感数据。
- en: OpenAI also provides several other models and tools. You will find in the next
    chapters that the embeddings endpoint is very useful for including NLP features
    in your application.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI还提供了其他几个模型和工具。在接下来的章节中，您将发现嵌入端点对于在应用程序中包含NLP功能非常有用。
- en: Now that you know *how* to use the OpenAI services, it’s time to dive into *why*
    you should use them. In the next chapter, you’ll see an overview of various examples
    and use cases to help you make the most out of the OpenAI ChatGPT and GPT-4 models.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您知道如何使用OpenAI服务，是时候深入了解为什么要使用它们了。在下一章中，您将看到各种示例和用例的概述，以帮助您充分利用OpenAI ChatGPT和GPT-4模型。
