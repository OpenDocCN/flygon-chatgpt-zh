- en: '| ![image](d2d_images/chapter_title_corner_decoration_left.png) |  | ![image](d2d_images/chapter_title_corner_decoration_right.png)
    |'
  id: totrans-0
  prefs: []
  type: TYPE_TB
  zh: '| ![图片](d2d_images/chapter_title_corner_decoration_left.png) |  | ![图片](d2d_images/chapter_title_corner_decoration_right.png)
    |'
- en: '![image](d2d_images/chapter_title_above.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![图片](d2d_images/chapter_title_above.png)'
- en: Black Box
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 黑匣子
- en: '![image](d2d_images/chapter_title_below.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![图片](d2d_images/chapter_title_below.png)'
- en: The term "black box" refers to the difficulty in interpreting how complex neural
    networks, like ChatGPT, arrive at their conclusions.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '"黑匣子"一词指的是解释复杂的神经网络（如ChatGPT）得出结论的困难。'
- en: Due to the large number of layers and connections within the network, it can
    be challenging to trace the decision-making process that leads to a specific output.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 由于网络内部的层数和连接数众多，很难追踪导致特定输出的决策过程。
- en: This makes it difficult to understand and explain the model's behaviour.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得理解和解释模型的行为变得困难。
- en: Imagine the pizza-making process happening inside a sealed box. You can see
    the ingredients going in and the finished pizza coming out, but you can't observe
    the intermediate steps. In the same way, we can observe the input and output of
    a neural network, but the inner workings remain opaque.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，在密封盒子内进行披萨制作过程。你可以看到配料进入和成品披萨出来，但是你无法观察中间的步骤。同样，我们可以观察神经网络的输入和输出，但其内部运作仍然是不透明的。
- en: In case that was too much to fathom here is the working of ChatGPT in bitesize.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这对你来说太难理解了，那么ChatGPT的工作原理可以用小口吃来解释。
- en: ChatGPT relies on machine learning and neural networks to process and generate
    text. It's built on Transformer architecture, which excels at natural language
    processing tasks, thanks to self-attention mechanisms.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT依赖于机器学习和神经网络来处理和生成文本。其构建在Transformer架构上，由于自注意机制，擅长自然语言处理任务。
- en: The model is trained using self-supervised learning, learning from vast amounts
    of text data. Tokenization helps break down text into smaller units, while semantic
    layers in the neural network allow the model to understand language at various
    levels of abstraction.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是通过自监督学习来训练的，从大量的文本数据中学习。标记化有助于将文本拆分成更小的单元，而神经网络中的语义层使模型能够以各种抽象级别理解语言。
- en: However, the complexity of the network makes it a black box, making it challenging
    to interpret its decision-making process.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，网络的复杂性使其成为一个黑匣子，难以解释其决策过程。
- en: Is that better?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 好一些了吗？
