© 作者，独家许可给 Springer Nature Switzerland AG 2023M. Kurpicz-Briki[更多内容请查看](https://doi.org/10.1007/978-3-031-37690-0_4)

# 4. 聊天机器人是否有情感？

Mascha Kurpicz-Briki^(1  )(1)应用机器智能，伯尔尼应用科学大学，比尔/比尔，瑞士

## 概述

创造*智能*机器的想法一直是人类的迷恋，导致了大量涉及这类情景的书籍和电影的出现。随着先进语言模型的崛起，我们再次思考什么是*智能*。在本章中，我们将讨论这个问题，并看看它是如何受人类感知影响的。在本章的第二部分，我们将深入研究技术技能，并查看最先进的语言模型。最后，我将描述一个典型的聊天机器人架构。

## 机器与情感

### 图灵测试

早在计算机科学的早期，即上世纪 50 年代，就讨论了衡量机器是否具有与人类相同思维能力的挑战。当今经常在讨论这个话题时提到的一个测试被称为*图灵测试*（最初称为*模仿游戏*（图灵 1950））。在这个实验中，一个人与两个对话伙伴（基本上就像现在的聊天机器人对话）进行交互，其中一个是人类，另一个是机器。参与者无法看到或听到这两个对话伙伴。接下来，参与者会询问这两个对话伙伴，试图找出哪个是机器，哪个是人类。图 4.1 描述了这个设置。如果经过一番深入的对话后，人类无法确定哪个是机器，哪个是人类，那么测试假定机器具有与人类相同的思维能力。![](img/604345_1_En_4_Fig1_HTML.png)

人类和机器互动的图示，文本内容为“你好，嗨，谁是人类，谁是机器？”

图 4.1

图灵测试的设置：谁是人类，谁是机器？

当与 ChatGPT 或基于最先进语言模型的类似聊天机器人进行对话时，你可能会有这样的印象，即这个标准可能会被满足。这是否意味着我们已经达到了机器具有与人类相同的思维能力的程度？

图灵测试因其测试人的轻信而不是系统的实际智能而受到批评（Tegmark 2018）。在上述场景中测试的是写作*像*人类的能力，而不是执行人类的思考或理解。我们可以通过更具体地研究我们之前看到的词向量化示例来更深入地探讨这一点。基于数学运算，我们能够根据单词*king, man*和*woman*的词向量获得单词*queen*。这是否意味着系统*理解*了做国王的概念或男人和女人之间的区别？可能不是，它所做的只是对向量应用数学运算，而向量在向量空间中的排序是由训练数据中看到的所有示例决定的。^(1)

### 情感聊天机器人？

那么聊天机器人表达的情感呢？让我们考虑与一款最先进的聊天机器人（基于 GPT-3 模型）的以下对话：

人类：

“你好，你好吗？”

聊天机器人：

“嗨，我过得很好。谢谢你的关心。你呢？今天感觉怎么样？”

人类：

“我还好。因为外面阴天，有点低落。你最近有什么好事吗？”

聊天机器人：

“很抱歉听到你今天有点低落。在天气阴沉的时候感到这样完全可以理解。至于我，我感到感激生活中支持我并让我快乐的人。我也感到满足，因为我有能力识别和处理自己的情绪，这有助于我保持心理和情感平衡。”

不错吧？这个聊天机器人能通过图灵测试吗？也许可以。这个聊天机器人是否*理解*它所产生的文本，就像人类一样理解吗？不，它不会。

这种难以置信的事实根源于人类沟通方式的本质。这在一篇关键的科学论文中被详细指出，该论文于 2021 年发表，警告大型语言模型背后的危险和风险（Bender 等人 2021）。它指出（除了我们稍后将讨论的许多其他有趣观点之外），大型语言模型生成的文本缺乏两个人之间通常沟通模式的不同关键方面，这两个人共享一个共同基础：交际意图、世界模型和读者心理状态模型。这导致了一个问题，即即使沟通的一方（聊天机器人）在其产出中没有更深层含义，人类也会假设*隐含含义*，这创造了我们对语言的独特人类理解的错觉。基本上，我们被聊天机器人欺骗，以模拟像人类一样写作。

机器人、人工智能，尤其是语言模型处理信息并向人类呈现答案或结果的方式可以被视为对人类行为或活动的*模拟*：

> 我们必须意识到*思考*、*计算*、*反应*、*决策*的机器人只是对思考、计算、反应、决策的*模拟*，而不是以人类意义上的真实思维过程。（Nida-Rümelin 和 Weidenfeld2022，p.59）

这一事实在关于类人机器人的一次采访中也被德国哲学家托马斯·梅辛格（Honert 2017）称为*社会幻觉*。他指出，人类有能力想象自己在与有意识的对等体打交道，即使事实并非如此。这不仅适用于看起来像人类的机器人，还可能适用于聊天机器人。在其他情况下，甚至可能发生我们将人类特征赋予更为被动的物体，如汽车。例如，一项研究表明，人们会将个性特征或面部表情赋予汽车（Windhager 等人 2008）。在这项研究中，40 名参与者被展示了来自不同制造商的不同汽车的 3D 计算机模型。参与者被要求说出他们是否将汽车的前部与人类（或动物）的脸部联系起来，识别出对应于嘴巴、眼睛或脸部其他部分的部位。此外，他们还被要求评价汽车在多大程度上看起来快乐、愤怒、惊讶、支配或悲伤等。有趣的是，人们在他们的评价中普遍达成一致，因此作者认为必须存在一些一致的信息被人类感知在汽车前部。参与者因此用生物学术语评估汽车，尽管它们是无生命的结构。

### 神秘的山谷

在机器人和头像的情况下，^(2)过于类人会引起人类的负面情绪。观察表明，那些明显人造且与真实人类有所区别的物体（例如明显与人类不同的类人机器人或填充动物）更容易被接受。*神秘的山谷*假设（最初由日本机器人学教授森正洋（Mori 1970）于 1970 年提出）预测，一个几乎看起来像人类的实体会在人类中引起冷漠和毛骨悚然的感觉。

这种神秘的山谷效应也可以应用于聊天机器人。一项研究（Ciechanowski 等人 2019）观察了两组不同的人类参与者如何与两组聊天机器人互动：第一个聊天机器人是一个非常简单的基于文本的聊天机器人，而第二个则还有一个阅读答案的头像，而不仅仅是在屏幕上显示答案。在结果中，作者得出结论，参与者在处理简单的聊天机器人时体验到的神秘效应和负面情绪要少于与动画聊天机器人交互时。

看起来最先进的语言模型不仅在自然语言处理领域引发了革命，使多项任务更加高效。它们似乎还引发了有关人类与聊天机器人互动的新问题，导致我们将它们优雅而措辞得当的文本解释为情感，或者在与它们互动时让我们感到怪异。是否已经达到了某种语言模型的怪异谷？

你仍在思考过于情绪化的聊天机器人对话，不确定它是否只是一种幻觉？现在让我们回到技术部分，更仔细地看看聊天机器人是如何工作以及它们的答案是如何生成的。

## 进入语言模型的世界

### 整个句子的向量

到目前为止，在查看单词嵌入时，我们考虑了一个单词对应一个向量的情况。我们示例中的每个单词都有一个数值表示，对于玩具示例是两个维度，对于真实世界的单词嵌入则是更大的维度。在某些情况下，可能需要或更好地将几个单词聚合在一起。在进行文本情感分析时，通常我们希望了解整个句子（或文本块）是积极还是消极，而不仅仅是单个单词。当在句子上执行此类操作并使用将整个句子作为输入的机器学习模型时，我们有一个*句子分类任务*。类似任务的其他示例包括垃圾邮件过滤器或识别给定文本是否涉及特定主题。

考虑以下句子，我们希望将其提供给一个机器学习模型，并估计它是积极还是消极:^(3)

+   *安娜吃了一个美味的草莓。*

基于之前讨论的单词嵌入，我们现在可以轻松地将每个单词编码为一个向量:^(4)

+   Vector(“安娜”) = (0, 2)

+   Vector (“吃”) = (1, 3)

+   …

现在我们想要将这些单词嵌入聚合成所谓的句子嵌入，一个代表不仅是单个单词而是整个句子的向量。一个简单的方法是只需取句子中所有单词向量的平均值。基于向量的数学属性，我们可以应用数学运算，就像我们在计算平均数一样：

+   平均值 = [Vector(“安娜”)+Vector(“吃”)+…+Vector(“草莓”)]/5

我们将所有向量相加，然后除以向量的数量（=单词的数量）。结果是一个向量（与单词向量相同维度的向量），代表整个句子：

+   Vector(“安娜吃了一个美味的草莓。”)

这些方法还有助于处理*可变长度的序列*。在处理语言时，单词（可以看作是字符的*序列*）可以更长（如单词*strawberry*）或更短（如单词*the*）。同样，我们可以将句子视为单词的*序列*，每个句子中的单词数量可能不同。因此，我们可以将要提供给机器学习模型的句子视为*可变长度输入*。根据我们想要分类的句子，我们可能有更少或更多的单词要处理。大多数情况下，我们事先无法知道这些信息，因此需要在软件中准备好处理非常短的句子以及长句子。使用平均值是一个简单而有效的解决方案：无论句子中有多少单词，我们最终都会得到一个单一向量（固定维度）。

尽管这种解决方案暂时解决了我们的问题，但它也带来了一些主要限制。例如，当使用这种方法时，不考虑单词在句子中出现的顺序。这导致以下句子与我们上面的示例具有相同的句子向量：

+   *一个美味的草莓吃安娜。*

这不完全相同，对吧？在某些情况下，这种差异对于分类任务可能非常重要，因此，我们需要更先进的方法，可以考虑句子中单词的顺序。此外，句子越长，平均向量包含的信息就越少。

### 记住一些单词

为了克服这一限制，我们需要一种不同的方法，允许我们按照从左到右逐步阅读单词的顺序，就像人类读者所做的那样。让我们通过一个示例来考虑这个问题^(5)，如图 4.2 所示。![](img/604345_1_En_4_Fig2_HTML.png)

人类和机器交互的图解，文本内容为“anna eats a tasty strawberry”。它读取、理解并存储在记忆中。

图 4.2

人类逐字阅读句子的示例（基于 Hagiwara 的解释（2021））

我们考虑之前相同的示例句子。人类读者逐字阅读句子。^(6) 作为第一步，单词*Anna*被阅读并存储在短期记忆中。然后，阅读下一个单词，本例中为*eats*。它与之前已经阅读的单词放在上下文中，并且也存储在短期记忆中。在这里，短期记忆被更新为“Anna eats”，记录了两个单词之间的关系，意味着 Anna 执行了一个动作。这种短期记忆也可以称为*心理状态*，根据逐字阅读句子时阅读的新单词逐步更新（Hagiwara 2021）。

### 具有循环的神经网络

-   现在我们想要将阅读句子的机制与前一章中看到的神经网络结合起来。这些神经网络被称为*循环神经网络*（*RNNs*）。我们可以将前面示例中的人类阅读过程转化为更技术化的设置，如下所示：我们引入神经网络的*状态*的概念，作为读者的短期记忆的类比，在每个单词向量后更新它，如图 4.3 所示。![](img/604345_1_En_4_Fig3_HTML.png)

-   一个图解人类和机器互动的示例，文本显示 anna 在 4 个状态下吃了一个美味的草莓。

-   图 4.3

-   一个神经网络逐个向量处理句子（基于 Hagiwara 的解释（2021））

-   神经网络首先接收单词*Anna*的向量。然后接收第二个向量：单词*eats*的向量。这个向量与上一步隐藏层的输出结合在一起，并传递到下一步。换句话说，神经网络的状态被更新，反映了新单词对到目前为止已读句子带来的额外信息。这个操作重复进行直到句子结束。到那时，整个句子的内容被存储在这个*最终状态*中。

-   在前一章中，我们已经看到了神经网络的架构，包括一个输入层，几个隐藏层和一个输出层。每一层都由几个神经元组成，每个神经元都处理一个输入和一些权重，并将输出提供给下一层。你可能记得信号从输入层进入我们的神经网络，逐层到达输出层。由于逐字阅读句子时，我们一遍又一遍地执行相同的操作，相同的神经网络结构被重复使用。处理中存在*循环*。循环是计算机编程中常见的结构。每当需要多次执行相同操作时，可以使用特定的结构仅编写一次指令，并告诉计算机执行多次。这类似于以下情景：如果你有十碗草莓，并想指示某人把它们从厨房带到客厅的桌子上，你可以说：

+   -   *请去厨房，把第一碗草莓端到桌子上。*

+   -   *然后，请再次去厨房，把第二碗草莓端到桌子上。*

+   -   *等等。*

-   但是，你可能不会这样做。你可能会说：

+   -   *对于这十碗草莓，每碗，请去厨房，把碗端到桌子上。*

-   因此，你刚刚创建了一个有十次迭代的循环。

在循环结构中重复使用神经网络的想法引出了所谓的循环神经网络的概念。相同的结构可以在下一个单词到来时被重复使用。在每一步中，我们将上一次迭代的隐藏层输出（状态）以及当前步骤的单词向量作为输入传入网络。这使得神经网络能够将输入的单词与句子的其余部分连接起来。回到厨房里的草莓碗，拿起碗的人可能在厨房里有一个笔记本和一支笔，每次记下哪些碗已经被拿到桌子上以及它们是如何摆放的。

因此，我们的循环神经网络的架构如图 4.4 所示。*状态*对应于前面提到的短期记忆，并存储了关于每个单词上下文的信息。![](img/604345_1_En_4_Fig4_HTML.png)

流程图说明了一个由输入、神经网络状态和输出组成的循环神经网络。

图 4.4

循环神经网络的架构

这有什么用？假设我们想通过预测部分句子的下一个单词来生成文本：

步骤 1：

安娜吃…

步骤 2：

安娜吃了一个…

步骤 3：

安娜吃了一个草莓…

每一步生成的单词由神经网络对下一个单词的预测给出。在每一步中，使用相同的神经网络，并且状态是隐藏层的输出，其中存储了迄今为止所见的所有信息。

因此，让我们回到我们最初的问题。我们想找到一种比平均方法更好的方法，以从可变长度的输入中生成固定长度的向量。通过循环神经网络，我们找到了一个解决方案。如图 4.5 所示，我们可以输入一个句子（每个单词编码为一个向量），并通过使用这种类型的神经网络得到一个句子嵌入。句子嵌入是神经网络的最终状态。由于在神经网络内部使用循环可能导致的重复，我们可以用它来处理非常短的句子以及包含许多单词的句子。此外，与平均方法相比，这里考虑了句子中单词的顺序。![](img/604345_1_En_4_Fig5_HTML.png)

在标有 M State 的神经网络示例中，句子安娜吃了一个美味的草莓通过向量进行处理，包括安娜向量、吃向量和草莓向量。句子的结果向量表示为句子安娜吃了一个美味的草莓的向量，其值为 2.3、55.6 和 33.4。

图 4.5

一个循环神经网络为完整句子生成一个向量

通常，我们会发现一个如图 4.6 所示的可视化表示来说明相同的网络。与循环不同，递归神经网络被*展开*并显示为几个相同的层。这是有益的，因为它可以展示单词序列（句子）是如何逐步处理的。神经网络的每个副本状态表示到目前为止已经引入的单词的信息。![](img/604345_1_En_4_Fig6_HTML.png)

一个神经网络的插图接收句子“Anna 吃了一个美味的草莓”并将其处理为向量，最终达到具有向量值的最终状态。在到达最终结果之前，网络在过程中经过各种状态的转换。

图 4.6

递归神经网络的展开表示

### 这就是，语言模型！

我们现在正在接近本书的核心，介绍语言模型的概念。让我们看看以下定义：

> 形式上，语言模型是一个给文本赋予概率的统计模型。[...] 句子越符合语法，越有“意义”，概率就越高。（Hagiwara 2021，第 131 页）

让我们考虑以下两个句子：

+   *有一个满是草莓的地方。*

+   *有一个满是猫的地方。*

对于一个人来说，很容易确定第一个句子比第二个句子更有可能。这种*可能性*的感觉在语言模型中以概率的形式表达。对于上面的句子，我们期望语言模型给予第一个句子��高的概率，而给第��个句子较低的概率。

语言模型是许多自然语言处理任务的基础。例子包括机器翻译、语音识别和聊天机器人中的答案生成。

简单来说，为了得到句子的概率，考虑上下文（之前和/或之后的单词）下每个单词的概率被结合起来。例如，考虑以下句子：

+   *有一个满是草莓的地方。*

我们会考虑这样的问题：一个句子以*There*开头的可能性有多大？单词*is*在单词*There*之后出现的可能性有多大？所有这些问题的答案将为我们提供一个总体概率，然后可以将其与另一个句子的总体概率进行比较，以决定哪个句子更有可能是一个*真实*、*好的*句子（一个人类可能使用的单词组合）。

当看到这些问题时，你可能会发现与之前创建句子嵌入时的任务有些相似。为了生成句子嵌入，我们逐词处理句子，考虑上下文并将含义存储在我们的*状态*中。为了回答上一段提出的关于*相似性*的问题，我们也需要逐词处理句子，并考虑单词在句子中出现的概率。

因此，我们可以使用循环神经网络的架构并训练我们的第一个语言模型。

首先，我们回到之前看到的展开的循环神经网络。特别是，我们现在对*状态*的中间值感兴趣。当在图 4.7 中没有输出的情况下看这个时，我们意识到我们实际上在做以下事情：基于一系列输入（以向量形式的单词序列），我们生成一系列输出（*状态*的中间值）。这样的架构被称为*序列到序列编码器*。![](img/604345_1_En_4_Fig7_HTML.png)

在神经网络处理流程中，输入序列以向量 Anna 开始，然后是向量 eats 和向量 strawberry。这些输入通过各种状态处理，最终导致输出序列。

图 4.7

序列到序列编码器的架构

虽然*状态*的中间值很有用，但这不是我们语言模型的输出。我们想要的是根据我们迄今为止读取的部分序列（部分句子）来指示潜在下一个单词的概率。

在图 4.8 中，我们考虑阅读句子第一个单词*Anna*的例子。这一步的输出是下一个单词的概率。虽然单词*Anna*后面跟着单词*eats*的可能性很高，但跟着*strawberry*或*tasty*的可能性相对较低。与上一章类似，我们将概率表示为从 0.0 到 1.0 的数字，可以轻松映射到 0%到 100%的百分比。![](img/604345_1_En_4_Fig8_HTML.png)

神经网络预测序列中的下一个单词。给定输入 Anna，它将概率最高的 0.9 分配给 eats，其次是 tasty，概率为 0.3，而 strawberry 的概率为 0.2。

图 4.8

预测句子中跟在“Anna”后面的单词。虽然“eats”这个词似乎是一个不错的选择，但“strawberry”和“tasty”这两个词的概率较低。

如前所述，我们想要使用这种架构来*训练*语言模型。因此，我们将上述原则应用于大量文本，*隐藏*特定单词并预测它们，以便根据需要控制和调整结果。在这里减少损失意味着更接近正确预测隐藏单词。

在上面的例子中，我们知道单词*eats*在文本中跟在单词*Anna*后面。因此，*eats*是正确答案，对于一个完全训练好的系统（在预测阶段），我们期望它有最高的概率。然而，在训练阶段的开始，也许答案还不正确，模型可能会给其他单词估计更高的概率。这就是学习发生的地方：基于这一步的估计值和正确答案，对系统进行*改进*以做出更好的预测（正如我们之前看到的通过调整神经网络内部的权重来最小化损失），逐句、逐文本地进行，数以百万计或十亿计的单词。

一旦语言模型训练完成，我们就可以用它来构建新的句子！这发生在*预测阶段*。通过提供句子的第一部分，我们可以预测下一个单词，如图 4.9 所示。我们要求语言模型预测句子“Anna eats a tasty …”中的下一个单词，（希望）我们的语言模型的答案是*strawberry*。在幕后，词汇表中每个单词都有一个导致这个答案的概率分数。![](img/604345_1_En_4_Fig9_HTML.png)

一幅插图，一个标记为神经网络的语言模型处理输入 Anna eats a tasty 并预测下一个单词，这个单词是 strawberry。模型使用其内部状态 M 进行这个预测。

图 4.9

使用语言模型预测句子中的下一个单词

早期的词嵌入训练和这个过程之间的主要区别是输入。在词嵌入训练中，我们计算一个单词作为上下文单词到目标单词（被编码的单词）的概率。唯一的输入是目标单词。在本节的神经网络情况下，用于预测下一个单词的输入不仅考虑一个单词，还考虑了前一个状态，其中包含有关多个单词的信息。

现在我们已经看到如何通过使用循环神经网络构建语言模型作为序列到序列编码器的第一个示例。这是一个成就，恭喜！

## 走向 Transformer 模型

现在只剩下最后一步，将我们带到最新的最先进语言模型。当前模型如谷歌的 BERT（Devlin 等人 2019）或 OpenAI 的 GPT-3 或 GPT-4 都是所谓的基于 transformer 的模型（Vaswani 等人 2017）。在本节中，我们将发现这种架构是什么样的，以及为什么它如此强大。

我们在上一节中看到了如何创建一个语言模型，通过预测部分句子中的下一个单词来生成文本。在这个例子中，我们观察了从一系列单词（一个句子）到一系列概率（最可能的下一个单词是什么？）的转换。现在，我们想稍微调整用例，并将一系列单词转换为*另一系列*单词。这可以是一个英文句子作为输入，被翻译为另一个西班牙文句子，或者用户向聊天机器人提出的问题，被转换为匹配的答案。为了实现这一点，我们将连接两个模型，就像之前看到的那样，并将一系列文本转换为另一系列文本。特别是，在本节中我们将处理一个问答任务。

### 编码器和解码器

为了说明这个想法，让我们考虑用户可能向聊天机器人提出的以下查询：

+   *草莓在哪些国家生长？*

要生成一个答案，我们需要处理这个用户输入并找到最合适的答案。这假设模型的训练过程已经结束，我们现在处于预测阶段。让我们假设对于我们的例子，聊天机器人将生成以下（不太有用的）答案：

+   *草莓在不同的国家生长。*

要做到这一点，我们使用一个模型来创建第一句的句子嵌入，然后将其传递给另一个模型，并使用这第二个模型生成答案。第一个模型称为*编码器*，因为它将输入句子的含义编码为句子嵌入（整个句子的向量表示）。第二个模型称为*解码器*，因为它解码这个嵌入并生成答案句子。图 4.10 显示了这种架构的高层视图。![](img/604345_1_En_4_Fig10_HTML.png)

在这个示例中，编码器处理问题“草莓在哪些国家生长？”并生成一个数字化的句子嵌入。解码器使用这个嵌入来生成答案“草莓在不同的国家生长”。

图 4.10

编码器-解码器架构的高层视图

根据我们之前看到的内容，现在我们可以更仔细地看看这两个组件内部发生了什么 - 编码器和解码器。这种序列到序列模型的实现方式可能会有所不同：根据特定的自然语言处理任务，编码器或解码器可以以不同的方式实现。

让我们创建一个基于我们之前看到的递归神经网络架构的编码器，以生成句子嵌入。句子嵌入对应于最终迭代后的隐藏状态。这个网络实际上已经在执行编码器的任务，所以我们可以直接从上一节中获取它，并将其包含在这个新系统中作为编码器。^(7) 哦，第一步完成了！

现在让我们看看解码器。在前面的例子中，下一个单词的估计是基于到目前为止已生成的单词的上下文，而对于解码器，模型从编码器接收的输入也被考虑在内。基于此，逐字生成输出句子，如图 4.11 所示。![](img/604345_1_En_4_Fig11_HTML.png)

一个插图，编码器将问题“草莓在哪些国家生长？”处理成一个数字化的句子嵌入。解码器然后使用这个嵌入来生成响应“草莓在国家生长”。

图 4.11

对编码器-解码器架构的两个部分进行更详细的观察。编码器的输出是解码器的输入。

### 预训练和微调

现在让我们更详细地了解我们问答任务的训练过程。这个例子的训练阶段有两部分。我们假设我们有一个像之前看到的那样已经*预训练*的循环神经网络。这意味着它已经用大量文本语料库进行了训练，因此权重已经配置得相当不错。一般来说，它已经准备好预测句子中的下一个单词。我们拿这个模型的两个实例：一个用来编码我们的句子（编码器），另一个用来生成答案文本（解码器）。我们准备了一个数据集来*微调*这个模型。在额外的训练阶段，使用额外的训练数据，进一步优化权重。特别是，由于模型通常是为文本生成而训练的，微调允许专门针对特定任务进行优化，在我们的情况下是问答。

表 4.1 展示了这种微调训练数据可能的示例。表 4.1

微调任务的训练数据示例

| 问题 | 答案 |
| --- | --- |
| <START> 草莓在哪些国家生长？ <END> | <START> 草莓在不同的国家生长。 <END> |
| <START> 草莓是什么颜色？ <END> | <START> 除了例外，草莓大多是红色的。 <END> |
| … | … |

让我们将*标记*定义为在文本中找到的常见字符序列。^(8) 在以下示例中，我们将整个单词作为输入标记。实际上，并非每个单词都一定完全匹配一个标记。例如，单词*sequences*被分成两个标记：*sequ*和*ences*。

在我们的编码器-解码器架构的背景下，需要考虑的一个方面是输入和输出中单词的长度。用户向聊天机器人提出的问题和聊天机器人的回答可能包含不同数量的单词。特殊单词，即所谓的特殊标记，用于标记输入或输出序列的开始（<START>）和结束（<END>）。

具有这些特殊标记的问题和答案显示在表 4.1 中。当然，我们需要一长串这些问题和答案对来微调语言模型。

在微调过程中，第一个问题被输入到编码器中，生成句子嵌入。解码器接收编码器的输出（句子嵌入）。由于我们处于微调阶段，这是训练阶段的一部分，我们知道这个问题的答句来自我们的训练数据。因此，第一个单词被生成，学习的目标是最大化我们正在寻找的单词的概率。然而，在开始阶段，其他单词将被提出。系统仍然需要学习。在生成答句的第二个单词时，我们将从编码器接收到的句子嵌入和*正确*答案的第一个单词作为输入。特意提到正确答案，而不是学习过程中可能错误的第一步生成的单词，因为这可能会在估计后续单词时产生错误信息。对于每个生成的单词，考虑该单词与正确答案之间的差异以改进系统。同样，我们希望调整权重以减少损失。与以前一样，系统通过许多迭代和许多训练示例进行改进，重复本段中提到的步骤。

一旦系统经过调优并应用于我们不知道正确答案的新用户查询时，我们就处于预测阶段（或在这种情况下，实际上更多是*生成*阶段）。解码器现在的工作方式与训练阶段的解释略有不同。再次，编码器的输出和<START>标记启动整个过程。与训练阶段相比，现在考虑以前位置的预测单词以生成下一个单词（因为我们假设这个输入是在训练期间没有见过的新问题，所以没有正确答案可以采取）。

因此，现在我们已经启动并了解了编码器和解码器的工作方式的序列到序列模型，我们终于可以开始使用变压器模型了。

### 变压器模型架构概述

最初在 2017 年提出的变压器模型（Vaswani 等人 2017）是一种新型的编码器-解码器神经网络，在自然语言处理领域开启了“变压器时代”（Hagiwara 2021，第 185 页），为不同的文本处理和文本生成任务提供了显著的改进。

与我们详细了解的编码器-解码器架构相比，基于 Transformer 的模型*更多相同*。在这些系统中，不同层的编码器和解码器被组合在一起。因此，多个编码器层被传递以进一步改善结果。而在原始论文中介绍这种架构时使用了六层，实际上可以使用更多层（Hagiwara 2021）。同样，解码器由不同层组成。该架构在图 4.12 中有所说明。![](img/604345_1_En_4_Fig12_HTML.png)

输入句子到多个编码器和解码器以及输出句子的流程图。

图 4.12

Transformer 架构的高层视图

图 4.13 更详细地展示了 Transformer 模型中每个编码器和解码器组件的内部情况。![](img/604345_1_En_4_Fig13_HTML.png)

该插图描述了具有前馈神经网络、自注意力机制和交叉注意力机制的编码器和解码器。

图 4.13

Transformer 架构中编码器和解码器的内部结构（灵感来自 Alammar 的 Transformer 模型出色的可视化(2018)）

与我们之前详细了解的示例相反，Transformer 模型不使用循环神经网络。随着句子变得越来越长，循环神经网络及其状态在远离彼此的标记之间的依赖性问题逐渐增多（Hagiwara 2021）。换句话说，状态开始*遗忘*关于序列中较早出现的单词的信息。当使用称为*自注意力*的机制时，这些长距离依赖性可以得到更好的覆盖。Transformer 多次应用这种自注意力，结合*前馈神经网络*。前馈神经网络没有循环，如循环神经网络，因此大致对应于我们所见过的最初类型的神经网络。通过不同的层，输入从神经网络的左侧经过到右侧，最终产生输出。

这种*注意力机制*可能是使基于 Transformer 的架构与以前的自然语言处理方法有所不同的最重要组成部分。我们将在接下来的章节中更详细地探讨它。

### 自注意力

当看到序列到序列模型的编码器-解码器架构时，我们注意到句子嵌入的高相关性，将编码器与解码器联系起来。这个向量包含了从用户查询中可用的所有信息，以生成聊天机器人的答案。然而，这个向量相当有限。即使它包含了一些百维度，它是*固定长度*的。而固定长度可能不适合我们可能遇到的所有用例。无论输入句子是 3 个单词还是 20 个单词，用于表示所有这些信息的可用空间始终是相同的。事实上，在我们之前看到的架构中，当输入句子非常长时，性能会受到影响。如果有一些机制只考虑长序列中最重要的部分（而不仅仅是最近的，如 RNN 的情况），这个问题就可以得到缓解。因此，总结一下，我们可以得出结论，解码器需要更多信息来参考来自编码器的输入的不同方面。这就是*self-attention*提供的。

直觉上：我们希望它集中在最相关的信息上。想象一下，你正在看一张文字页面，并被要求找出文字*草莓*在文本中出现的位置。自动地，我们会扫描文本，略过其他信息，试图*专注*于对我们任务有关的单词。同样，我们希望变压器模型专注于每个给定单词的相关信息。

与具有固定长度向量的句子嵌入以包含与输入句子相关的所有信息不同，我们回到了单词嵌入。输入句子中的每个单词都将有一个嵌入。然而，我们之前看到的单词嵌入和这里的单词嵌入之间有一个重要的区别。这里的单词嵌入包含了单词的整个上下文的信息。

生成这些*基于上下文的摘要*对于每个单词来说是自注意力机制的任务。可以说，基于表示单词的词嵌入形式的向量，创建了一个更高级的词嵌入，代表了单词*和*它的上下文。

在使用自注意力计算给定单词的这些高级词嵌入时，同一句子中的每个其他单词都被赋予一个权重。权重决定了它对给定单词的相关性，就像巨大草莓对使草莓植物弯曲的相关性一样，总是同一个故事。

让我们考虑以下例子：

+   *有一片草莓地，而* ***它*** *是如此美丽！*

当使用自注意力计算单词*it*的基于上下文的摘要时，预计单词*field*的权重会比其他单词更高，因为*it*指的是草莓地。使用这种自注意力机制，我们不会忽略指向先前出现在句子中的其他单词的单词。这是不使用自注意力系统的典型问题，特别是当句子变长时，单词和指向该单词的引用之间有许多单词时。

例如，可以简化地将从单词*it*到上下文单词的注意力权重和依赖关系可视化，如图 4.14 所示。![](img/604345_1_En_4_Fig14_HTML.png)

一个草莓的插图，文字写着有一片草莓地，非常美丽。

图 4.14

从单词“it”反映出的依赖关系体现在不同单词所赋予的权重上

与单词*it*直接相关的单词，如它所指的*field*或描述它的*beautiful*，对于其基于上下文的摘要更为重要。其他单词，如*of strawberries*或动词*is*指向单词*it*，是相关的但可能比其他单词不那么重要。因此，它们的权重较小。

基于上下文的摘要是在编码器中创建和处理的。如图 4.15 所示，单词*在*在考虑上下文的情况下产生一个向量，从编码器传递到解码器。这里的上下文是指，例如，单词*在*后面跟着单词*哪些*，再后面跟着单词*国家*。第二个向量将是单词*哪些*的上下文感知表示，输入句子的第二个单词，以此类推处理输入句子中的其他单词。![](img/604345_1_En_4_Fig15_HTML.png)

一个流程图。输入句子“在哪些国家种植草莓？”经过编码器和解码器处理。在此过程中生成基于上下文的值。输出句子是“在哪里”，作为“在”这个词的摘要。

图 4.15

基于上下文的摘要是在编码器中创建和处理的

### 交叉注意力和位置编码

在解码器中，使用另一种形式的注意力：*交叉注意力*。这种机制类似于编码器和解码器内部使用的自注意力，用于生成上下文感知的词嵌入。交叉注意力在解码器中用于总结从编码器获得的信息（*交叉*是指它跨越编码器和解码器之间的边界）。这里的想法是再次在考虑上下文的情况下获得最相关的信息。

当阅读关于自注意力机制的内容时，主要关注单词之间的关系，你可能会想知道我们是否会遇到与本书早期相同的问题，即我们会错过单词顺序的信息（记得例子，*安娜吃草莓*和*草莓吃安娜*被表示为相同）。实际上，在描述的方法中可能会遇到这个问题。然而，transformer 编码器通过额外的计算来解决这个问题：*位置编码*。

位置编码是一个额外的组件，添加到单词的词嵌入中，包含有关位置的信息。在*安娜吃草莓*中，*安娜*的位置编码（安娜在位置 1）会与*草莓吃安娜*中*安娜*的位置编码（安娜在位置 4）不同。

在计算上下文化词嵌入时包含有关单词位置的信息，这些信息可以被保留，我们就不会遇到像草莓吃安娜这样的问题。

### 静态和上下文化词嵌入

我们在本书中之前看到的词嵌入，不考虑单词的上下文，也被称为*静态*或*传统词嵌入*。在本节中看到的基于 transformer 模型的更高级的嵌入被称为*上下文化词嵌入*。在静态词嵌入中，单词*橙色*指代颜色和同样的单词*橙色*指代水果会被编码为相同的词嵌入。在上下文化词嵌入中，情况会有所不同，因为会考虑这些单词在训练数据中出现的上下文。例如，水果可能会与草莓、苹果或香蕉一起出现，而颜色可能会与紫色、蓝色或绿色等其他颜色一起出现。在训练基于 transformer 模型时，这种差异会被考虑并反映在嵌入中。因此，橙色（颜色）的上下文化词嵌入会与橙色（水果）的嵌入不同。

我们使用变压器架构训练的这些语言模型可以作为预测不同情况下的句子的基础，比如聊天机器人或下一个句子的预测。这些语言模型，或者更准确地说是由这些训练过程产生的*上下文化词嵌入*，也可以应用于不同的任务，比如分类。这让我们想起了我们在本书中早些时候看到的将人类语言中的单词映射到向量表示的字典。至于这个字典，基于变压器的语言模型可以训练一次，然后在不同的用例中共享和使用，如图 4.16 所示。这是个好消息，因为训练一款最先进的基于变压器的模型需要强大（因此昂贵）的硬件，大量的文本语料库（我们谈论的是数以亿计的单词），以及（取决于您的硬件）对训练的耐心，可能需要数天、数周或数月的时间才能执行完毕。![](img/604345_1_En_4_Fig16_HTML.png)

一个流程图。基于变压器的语言模型，训练一次并且公开可访问，利用大量的训练数据使用变压器架构高效有效地执行特定的机器学习任务。

图 4.16

基于变压器的模型可以训练并提供给其他应用程序使用。

在这些词嵌入和语言模型出现在自然语言处理领域之前，机器学习模型通常是针对一个非常具体的任务进行训练的。正如在这里和之前提到的，这些*预训练组件*（语言模型或词嵌入）可以被提供并立即使用，这改变了最近构建自然语言处理应用程序的方式。这是一个优势，因为不同的自然语言处理任务可能会发现知道覆盆子和草莓有些相似是有益的，因为它们都是浆果。描述这种设置的一个很好的类比已经由（Hagiwara 2021）提供：

> [这相当于]教一个婴儿（=NLP 模型）如何跳舞。通过让婴儿先学会稳定地行走（=训练词嵌入），舞蹈老师（=特定任务的数据集和训练目标）可以专注于教授具体的舞蹈动作，而不必担心婴儿是否能够站立和行走得当。（Hagiwara 2021，第 220 页）

使用预训练组件并根据实际任务的需求进行调整（称为*适应*或*微调*的过程）通常可以称为*迁移学习*。第一学习阶段中存储在语言模型或词嵌入中的见解可以被*转移*到不同类型的任务中。这避免了当可以从已经存在的语言模型中受益时，一遍又一遍地从头开始训练模型。

这通常导致数据工程师设置如图 4.17 所述。它依赖于公开可用的单词嵌入或在大量数据上训练过的语言模型，并且可以通过使用库轻松地包含在数据工程师的项目中。基于这些组件和一个较小的训练数据集，微调过程对实际所需任务进行机器学习训练，例如前面描述的情感分析任务。在权重方面，从头开始训练意味着我们从随机权重开始训练阶段。在微调的情况下，权重已经从不同任务中学习过，并且只是从那里调整过来。![](img/604345_1_En_4_Fig17_HTML.png)

一个流程图。公开可用的语言模型是在大型文本语料库上训练的，使它们可以用于各种任务。或者，您可以使用较小的数据集对这些模型进行微调，以在训练阶段为特定任务定制它们。

图 4.17

预训练语言模型可以为其他机器学习任务进行微调

拥有语言模型和单词嵌入是方便的；然而，它们也存在一些限制（例如，偏见），这些限制很容易通过这种方式传播。我们将在后面的章节中更多地讨论这个问题。

### BERT 模型

举个例子，让我们更仔细地看一下 Google 的语言模型 BERT 是如何训练的（Devlin 等人 2019）。BERT 模型已经在两个任务上进行了训练：*遮蔽语言建模*和*下一个句子预测*。让我们看看这意味着什么。对于遮蔽语言建模，训练数据中有 15%的标记被隐藏（一个大型文本语料库）。然后，变压器模型必须预测这些被遮蔽的标记。这类似于我们之前看到的例子。标记<MASK>表示被覆盖的单词，需要进行预测。

+   *有一片充满<MASK>的地方。*

在第二个任务中，重点是不同句子之间的关系。这在第一个任务中没有涵盖，仅依赖于一个句子内的单词。系统必须预测两个句子 A 和 B，句子 B 是否是原始文本中跟在句子 A 后面的句子。在 50%的情况下，这是正确的，在另外 50%的情况下，这不是情况。考虑以下示例，系统应该预测句子 B 跟在句子 A 后面：

句子 A：

安娜去草莓地。

句子 B：

她采集了许多浆果并把它们带回家。

在以下示例中，系统应该更倾向于预测这不是情况：

句子 A：

安娜去草莓地。

句子 B：

猫喜欢睡觉。

BERT 的名称代表双向编码器来自变压器的表示。这个基于变压器架构的强大系统在 2019 年为自然语言处理领域带来了重要进展。

### 聊天机器人的架构

根据我们迄今所见，我们现在可以绘制一流的基于变压器的聊天机器人的架构。如图 4.18 所示，涉及不同的软件组件。特别是，我们发现具有变压器架构的语言模型在背景中。正如我们之前所见，它由不同层的编码器和解码器组成。通常，在这种类型的应用程序中，还有另一个软件组件，允许人类与语言模型进行交互。通常，希望与软件交互的人类我们通常称之为*用户*。用户的期望通常是有一个*图形用户界面*与软件交互。希望有黑色背景和小白色字体而没有任何图形插图的用户群体非常小，可能仅限于非常技术的人群。您可能从电影中了解到这种类型的应用程序，当涉及黑客或关键系统关闭时。一般来说，用户更喜欢更具图形化的东西，即易于使用。在聊天机器人的情况下，这通常由*Web 应用*提供。用户可以通过在网络浏览器中打开链接或在智能手机上安装应用程序来启动 Web 应用。通常，在与聊天机器人开始对话时，他们将收到问候消息，然后可以输入他们的*输入提示*。这个输入提示将由语言模型处理，并且将使用我们之前看到的方法生成答句。返回的句子*很可能*是对用户输入的输入提示问题的一个很好的答案。它到底有多好取决于语言模型的输入数据和训练设置。^(9)![](img/604345_1_En_4_Fig18_HTML.png)

一个流程图。用户提供一个输入提示，草莓是在哪些国家生长的？语言模型通过 Web 应用程序中的编码器和解码器处理此输入。作为回应，聊天机器人，自我介绍为你好，我是一个聊天机器人，我可以帮助你吗？开始对话。

图 4.18

聊天机器人的架构：通常使用 Web 应用作为用户和语言模型之间的接口。

最后，在某些设置中，用户也被用作语言模型的*训练者*。可以从用户那里收集反馈以改进语言模型的预测或使其更安全。例如，用户可以对答案进行评分，表明其是否有用。在其他情况下，我们可能希望标记特定类型的答案为不当。从人类训练者那里获得的更多输入和更正，系统就能更好地适应。

就是这样。现在我们明白了最先进的语言模型和聊天机器人是如何工作的。现在让我们回到本章前面看到的聊天机器人展示情感的示例对话。

乍一看，似乎令人费解的是，聊天机器人产生的文本背后没有人类意义或理解，尽管它以流利而雄辩的方式表达其答案。答案看起来连贯，并且自然而自发地发音。当考虑技术背景和基于概率生成最有可能的下一个词时，您可能会重新考虑这些语言模型是否像您一样推理和产生文本的选项。

关于意识、推理或智能在更深层次上意味着什么，以及是否可以用人脑以外的材料实现这一点的哲学辩论不在本书的讨论范围之内。我想给你一个关于语言模型如何工作的基本理解，以及它们（目前）无法与人类推理的能力相比，没有像信念、欲望、意图或恐惧等心理状态。正如 Nida-Rümelin 和 Weidenfeld（2022，第 17 页）所论证的，“数字状态和过程*模拟*心理状态，但并非与之相同，即使该模拟是完美的。”

## 总结

在本章中，我们讨论了聊天机器人是否能拥有情感。我们已经看到，尽管它们产生的文本通常是雄辩和富有同情心的，但在幕后，更多的是关于基于概率预测最有可能的下一个词。

我们还学习了什么是循环神经网络，以及如何使用编码器-解码器架构从中构建序列到序列模型。基于此，我们了解了变压器架构的工作原理以及自注意力如何实现高效的最先进语言模型。我们还探讨了如何对预训练语言模型进行微调以适应特定任务，避免每次都从头开始。

最后，我们已经看到了典型聊天机器人的架构。在背后使用语言模型，通常会使用 Web 应用作为用户和聊天机器人之间的接口。
