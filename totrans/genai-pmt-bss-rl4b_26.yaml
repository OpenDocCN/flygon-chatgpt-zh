- en: '"Generate product recommendations for a male customer aged 25-35 interested
    in fitness and browsing our website for the first time. Recommend products within
    his budget range and with a high customer rating."In contrast, an unclear prompt
    might look like this:"Generate product recommendations for our e-commerce website."The
    second prompt needs to be more specific, and the AI model may need help understanding
    what type of products to recommend or who the target audience is. As a result,
    the AI model may generate irrelevant or incorrect responses.Clarity is crucial
    when writing AI prompts. Be specific, use simple language, and provide as much
    information as possible to help the AI model understand the task it needs to perform.'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: '"为一名年龄在25-35岁、对健身感兴趣且第一次浏览我们网站的男性客户生成产品推荐。推荐在他的预算范围内且具有高客户评分的产品。"相比之下，一个不清晰的提示可能是这样的："为我们的电子商务网站生成产品推荐。"第二个提示需要更具体，AI
    模型可能需要帮助理解要推荐什么类型的产品或目标受众是谁。因此，AI 模型可能会生成无关或不正确的回应。写 AI 提示时清晰度至关重要。要具体，使用简单的语言，并提供尽可能多的信息，以帮助
    AI 模型理解它需要执行的任务。'
- en: BIASED DATA
  id: totrans-1
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 偏见数据
- en: Another common mistake is to use partial data to train the AI. Partial data
    can lead to AI that perpetuates stereotypes and discrimination. To avoid this,
    ensure that your training data is diverse and unbiased. Partial data can lead
    to AI models that perpetuate stereotypes and prejudice. It is essential to ensure
    that the data used to train the AI is diverse and unbiased to avoid these problems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的错误是使用部分数据来训练 AI。部分数据可能导致 AI 强化刻板印象和歧视。为了避免这种情况，请确保您的训练数据是多样化且无偏见的。部分数据可能导致强化刻板印象和偏见的
    AI 模型。确保用于训练 AI 的数据是多样化且无偏见的至关重要，以避免这些问题。
