# 10 生成模型的未来

## 加入我们的书籍社区Discord

[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)

![二维码描述自动生成](../media/file65.png)

在这本书中，到目前为止，我们已经讨论了用于构建应用程序的生成模型。我们探讨了LLMs和图像模型用于内容创作，工具使用，代理策略，检索增强生成的语义搜索，以及使用提示和微调来调节模型。此外，我们实现了一些简单的应用程序，例如为开发人员和数据科学家。在本章中，我们将讨论这给我们留下了什么，未来将引领我们走向何方。人工智能领域的进展速度在过去一年中急剧加快，像DALL-E，Midjourney和ChatGPT这样的突破性成果产生了惊人的结果。这些生成式人工智能模型可以创建逼真的图像，撰写文章和代码，并具有超越大多数人类的对话能力。2022年，生成式人工智能初创公司的风险投资激增，几乎与前五年的总投资额相匹配。最近，像Salesforce和Accenture这样的主要参与者已经做出了数十亿美元的投资承诺。为特定用例定制基础模型被视为真正的价值创造机会。但目前尚不清楚哪些实体 - 大型科技公司，初创公司或基础模型开发者 - 将获得最大的上行空间。在技术层面上，像ChatGPT这样的生成模型通常作为黑匣子运作，对其决策过程的透明度有限。模型可解释性的缺乏使得完全理解模型行为或控制输出变得困难。还存在潜在偏见可能由于不完善的训练数据而出现的担忧。在实际层面上，生成模型需要大量的计算资源进行训练和部署。对于许多组织来说，获取基础设施以有效利用这些人工智能系统仍然是一道障碍。在积极的一面，人工智能可以使技能民主化，使业余爱好者能够在设计，写作等方面产生专业质量的输出。企业可以从更快，更便宜，按需的工作中受益。然而，人们对工作岗位流失存在重大担忧，特别是对于像平面设计师，律师和医生这样的专业中产阶级角色。他们的工作正在被自动化，而低技能工人则学会利用人工智能作为超能力。更令人不安的是，人工智能可能被军事，恐怖分子，犯罪分子和政府用于宣传和影响。实时生成的Deepfakes将促使欺诈活动泛滥，并侵蚀信任。前进的道路在热情和实用性之间取得平衡，优先考虑人类尊严。通过承认风险，促进开放讨论，并实施周到的政策，我们可以建立一个由人工智能激发的公平未来。本章的主要部分包括：

+   当前生成式人工智能的状态

+   可能的未来能力

+   社会影响

+   实际实施

+   未来之路

让我们从当前模型的状态和它们的能力开始。

## 当前生成式人工智能的状态

正如本书所讨论的，在最近几年，生成式人工智能模型在跨越文本、图像、音频和视频等多种形式的人类内容生成方面取得了新的里程碑。领先的模型如OpenAI的GPT-4和DALL-E 2，Google的Imagen和Parti，以及Anthropic的Claude在语言生成方面展现出令人印象深刻的流畅性和创造性视觉艺术。在2022年至2023年之间，模型取得了长足的进步。如果生成模型以前只能生成难以理解的文本或模糊的图像，现在我们看到高质量的3D模型、视频，并生成连贯和上下文相关的散文和对话，与人类的流畅水平相媲美甚至超越。这些人工智能模型利用庞大的数据集和计算规模，使它们能够捕捉复杂的语言模式，展现对世界知识的细致理解，翻译文本，总结内容，回答自然语言问题，创作吸引人的视觉艺术，并获得描述图像的能力。看似像魔术一样，人工智能生成的输出模仿了人类的创造力 - 绘制原创艺术，写诗，生成人类水平的散文，甚至从不同来源进行复杂的信息聚合和综合。但让我们更加细致一些。生成模型也有弱点，不仅有优势。与人类认知相比，仍然存在缺陷，包括频繁生成似是而非或荒谬的陈述。幻觉显示缺乏现实基础，因为它们基于数据中的模式而不是对真实世界的理解。此外，模型在执行数学、逻辑或因果推理方面存在困难。它们很容易被复杂的推理问题搞混，这可能限制它们在某些工作领域的适用性。预测的不可解释性黑匣子问题以及模型本身阻碍了故障排除工作，控制模型行为在期望参数内仍然具有挑战性。人工智能模型可能表现出有害的意外偏见，引发重大的伦理关切 - 这个问题很大程度上是由训练数据中存在的偏见所致。这种偏见不仅扭曲输出，还可能传播和放大社会差距。下面是一个表格，将当前LLMs的关键优势和缺陷与人类认知进行了对比：

| **LLMs的优势** | **LLMs的缺陷** |
| --- | --- |
| 语言流畅性 - 能够生成语法连贯、上下文相关的散文和对话。GPT-4能够生成人类水平的散文。 | 事实准确性 - 语言模型经常生成似是而非或荒谬的陈述。缺乏现实基础。 |
| 知识综合 - 从多样化来源聚合和呈现信息的复杂过程。 | 逻辑推理 - 无法进行数学、逻辑或因果推理。容易被复杂的推理问题所困扰。 |
| 创造性输出 - 反映人类创造力的想象力和原创文本、艺术、音乐。克劳德写诗，DALL-E 2绘制原创艺术。 | 可控性 - 难以约束模型在期望参数内的行为。可能展示有害的意外偏见。 |
|  | 偏见 - 有可能传播和放大训练数据中存在的社会偏见。引发道德关切。 |
|  | 透明度 - 模型预测缺乏可解释性。"黑匣子"问题限制了故障排除。 |

图10.1：LLM的优势和不足。

尽管生成式人工智能的能力已经取得了长足进步，但它们的问题领域需要解决，以便这些技术在未来有效地发挥作用。尽管如此，它们的深远潜力表明，如果负责任地开发和监管，将会有一个令人兴奋的未来。生成模型的弱点定义了一些技术挑战，我们现在将看到。

### 技术挑战

尽管取得了快速进展，但要安全、负责地实现生成式人工智能的全部潜力仍然存在重大技术障碍。正如前面提到的，生成式人工智能模型尽管取得了可观的进步，但仍在应对需要克服的重大技术挑战，以允许其安全、负责地发挥全部潜力。我们已经在前几章讨论了一些这些问题和潜在解决方案。这个表格总结了其中一些挑战以及解决它们的技术方法：

| **挑战** | **描述** | **潜在解决方案** |
| --- | --- | --- |
| 生成真实和多样化内容 | 现有模型在逻辑一致性和事实合理性方面存在困难。生成的样本重复、乏味，缺乏人类细微差别。 | 从人类反馈中强化学习数据增强和合成技术模块化领域知识 |
| 输出质量控制 | 缺乏可靠约束生成内容属性的机制。模型偶尔会产生有害、偏见或荒谬的结果。 | 有限的优化目标调节系统中断和纠正技术 |
| 避免偏见 | 模型无意中放大了训练数据中存在的社会偏见。开发技术来遏制偏见仍然困难。 | 平衡和代表性的训练数据偏见缓解算法持续测试和审计 |
| 事实准确性 | 无法推理客观真理限制了在现实世界应用中的可靠性。将模型基于常识和物理知识是一个未解之谜。 | 结合知识库混合神经符号结构检索增强生成 |
| 可解释性 | 大型神经网络的不透明行为对故障排除或偏见提出了障碍，需要可解释的人工智能技术。 | 模型内省技术概念归因方法简化模型架构 |
| 数据隐私 | 收集和处理大规模数据集带来了关于同意、匿名化、访问控制和数据滥用的挑战。 | 差分隐私和安全多方计算合成数据生成联邦学习 |
| 延迟和计算 | 部署庞大模型需要大量计算资源，延迟了许多应用程序所需的实时交互性。 | 模型压缩为更小的形式优化推理引擎专用人工智能硬件加速器 |
| 数据许可 | 组织可能需要获得商业许可来使用现有数据集或构建定制数据集以训练生成模型。这可能是一个复杂和耗时的过程。 | 开源和合成数据 |

图10.2：技术挑战和潜在解决方案。

主要问题在于，这些模型生成的内容往往缺乏现实感和多样性。虽然它们展示了模仿人类语言和创造力的令人印象深刻的能力，但在产生逻辑一致和事实可信的内容方面仍然表现不佳。它们的输出往往缺乏人类的细微差别，变得相当重复和乏味。潜在的解决方案包括通过人类反馈进行强化学习以提高连贯性和细微差别，控制数据增强和合成技术，以及融合模块化领域知识的架构。另一个关键障碍是控制输出质量。尽管经过严格的训练和开发，现有的人工智能机制在可靠地约束生成内容的属性方面仍然存在不足。这导致内容的零星生产可能是有害的、带有偏见的或完全荒谬的，对其更广泛的接受和应用构成风险。有希望的方法包括受限制的优化目标、人机协同的调节系统以及在生成过程中中断和纠正模型输出的技术。这些人工智能模型存在偏见的问题确实是一个重要问题，因为它们经常无意中放大其训练数据中存在的社会偏见。开发纠正偏见的技术仍然是一个复杂的问题。诸如平衡和代表性训练数据、偏见缓解算法以及为公平性进行持续测试和审计的策略旨在解决这一问题。这些人工智能模型无法推理客观真理的能力明显限制了它们在现实世界应用中的可靠性。将这些模型基于常识和物理学进行基础化代表着人工智能社区仍在努力解决的一个悬而未决的问题。混合神经符号架构、知识库的整合以及检索增强生成提供了有希望的方向。人工智能的黑匣子性质带来了另一个复杂的挑战：可解释性。大型神经网络的不透明行为给故障排除或偏见带来了障碍，这强调了更透明的人工智能技术的需求。模型内省、概念归因方法以及简化的模型架构可能提供解决方案。此外，由于收集和处理大量数据集，数据隐私问题变得突出。这一方面引入了关于同意、匿名化、访问控制和数据滥用的挑战。差分隐私、安全多方计算、合成数据生成和联邦学习等技术可能有助于解决隐私风险。最后但并非最不重要的是，部署这些庞大模型需要大量的计算资源，导致显著的延迟和计算问题。这可能会延迟许多应用所需的实时互动性，表明效率改进至关重要。解决方案涉及将模型精简为更小的形式因子、优化推理引擎以及专用人工智能硬件加速器。展望未来，生成式人工智能系统有望变得更加强大和多样化。让我们拭目以待！

## 可能的未来能力

目前非常大型模型的训练计算翻倍时间约为 8 个月，超过了摩尔定律（晶体管密度成本每约 18 个月增加一倍）或洛克定律（像 GPU 和 TPU 这样的硬件成本每 4 年减半）等缩放定律。这张图表展示了大型模型训练计算的这一趋势（来源：Epoch，《机器学习中的参数、计算和数据趋势》。在线发表于 epochai.org。检索自：https://epochai.org/mlinputs/visualization）：

![图 10.3：知名人工智能系统的训练 FLOPs。](../media/file66.png)

图 10.3：知名人工智能系统的训练 FLOPs。

正如第 1 章讨论的那样，大型系统的参数大小增长速度与训练计算相似，这意味着如果这种增长持续下去，我们可能会看到更大更昂贵的系统。经验推导的缩放定律根据训练预算、数据集大小和参数数量预测了 LLMs 的性能。这可能意味着高度强大的系统将集中在大科技公司手中。

> **KM 缩放**定律，由卡普兰和同事提出，通过对模型性能与不同数据大小、模型大小和训练计算进行经验分析和拟合得出，呈现出幂律关系，表明模型性能与模型大小、数据集大小和训练计算等因素之间存在强烈依赖关系。
> 
> > **Chinchilla 缩放定律**，由谷歌 DeepMind 团队开发，涉及对更广泛范围的模型大小和数据大小进行实验，并建议将计算预算最优分配给模型大小和数据大小，这可以通过在约束条件下优化特定损失函数来确定。

然而，未来的进展可能更多地取决于数据效率和模型质量，而不是纯粹的规模大小。尽管庞大的模型抓住了头条，但计算能力和能源限制限制了模型无限增长的可能性。未来将会看到庞大的通用模型与更小、可访问的专业化细分模型共存，这些模型提供更快、更便宜的训练、维护和推理。已经有证据表明，更小的专业化模型可以表现出很高的性能。我们最近看到了像phi-1（*只需教科书*，2023年，Gunasekar和同事）这样的模型，拥有约10亿个参数，尽管规模较小，但在评估基准上取得了高准确度。作者建议，改善数据质量可以显著改变规模定律的形状。更多的工作表明，模型可以大幅缩小，只有轻微的准确度下降（*只需一个宽度前馈*，Pessoa Pires等人，2023年），这支持了模型训练和访问的民主化的论点。此外，转移学习、蒸馏和提示技术等技术可以使更小的模型利用大型基础的能力，而不会复制它们的成本。为了弥补限制，搜索引擎和计算器等工具已经被整合到代理人和多步推理策略、插件和扩展中，这些工具可能越来越多地用于扩展功能。由于不同因素的影响，AI训练成本正在下降 - 根据ARK Investment Management LLC的数据，每年约下降70%。最近发布的Mosaic ML的AI训练工具可以将语言模型训练到GPT-3级别的性能，成本大约是两年前估计的460万美元的十分之一。这将促进实验，但进步将越来越多地来自训练制度、数据质量和新颖的架构，而不仅仅是模型大小。在由资源丰富的大型科技公司主导的军备竞赛之后，负责任、经济的创新可能成为优先考虑的事项。在3-5年的时间范围内（2025-2027年），围绕计算和人才可用性的限制可能会大大缓解，侵蚀中心化的壕沟。具体来说，如果云计算成本如预期般下降，而通过教育和自动化工具使AI技能更加普及，自我训练定制的LLM可能对许多公司变得可行。这可以更好地满足个性化和数据隐私的需求。然而，一些能力，如上下文学习，根据规模定律是不可预测的，只会在大型模型中出现。进一步推测，即使在更多数据上训练的巨大模型可能会展现出更多的行为和技能，极端扩展最终可能会产生**人工通用智能**（**AGI**）- 推理能力与甚至超越人类智慧。然而，从神经科学的角度来看，AGI接管世界的威胁在我们目前的技术阶段似乎被高度夸大（参见Jaan Aru等人，*通过神经科学的视角探讨人工意识的可行性*；2023年）。

+   **缺乏具体化、嵌入式信息**：今天的LLM仅仅是在文本数据上进行训练，而不是像人类那样通过丰富的多模态输入来发展对物理世界的常识推理。这种缺乏基础学习对于发展人类水平的智能构成了重大障碍。

+   **与生物大脑不同的架构**：像GPT-4这样的模型使用的相对简单的堆叠变压器架构缺乏被认为能够使人类具有意识和一般推理能力的复杂的循环和分层结构。

+   **狭窄的能力**：现有模型仍然专门针对特定领域（如文本）而设计，在灵活性、因果推理、规划、社交技能和一般问题解决智能方面存在不足。这可能会随着工具使用的增加或模型的根本性变化而改变。

+   **社交能力或意图的最小化**：当前的人工智能系统没有固有的动机、社交智能或超出其训练目标的意图。对恶意目标或对统治欲望的担忧似乎是没有根据的。

+   **有限的现实世界知识**：尽管摄入了大量数据集，大型模型的事实知识和常识与人类相比仍然非常有限。这阻碍了在现实世界中的适用性。

+   **数据驱动的局限性**：依赖于训练数据中的模式识别，而不是结构化知识，使得可靠地推广到新情况变得困难。

鉴于这些论点，今天的人工智能迅速演变为恶意超级智能的风险似乎是极不可能的。尽管如此，随着能力的不断提升，深思熟虑地处理长期安全研究和伦理问题仍然是明智的。但是，根据神经科学或当前模型能力的证据，对即将到来的世界占领的恐惧并没有实质性的支持。因此，对必然、即将到来的人工通用智能的断言缺乏严格的支持。然而，快速的进步速度引发了对人类过时和工作岗位流失的担忧，这可能进一步分化经济阶层。与过去的物理自动化不同，生成式人工智能威胁到以前被认为不会被自动化取代的认知工作类别。以道德和公平的方式管理这种工作力量的过渡将需要远见和规划。关于人工智能是否应该创作反映人类状况的艺术、文学或音乐也存在哲学上的争论。让我们更广泛地思考社会影响吧！

## 社会影响

高度能力的生成式人工智能的出现可能会在未来几年改变社会的许多方面。随着生成模型的不断发展并为企业和创意项目增加价值，生成式人工智能将塑造技术和人类互动在各个领域的未来。尽管它们的广泛采用为企业和个人带来了许多好处和机会，但关键是要解决由于在各个领域对AI模型的依赖增加而引起的伦理和社会问题。如果慎重部署，生成式人工智能在个人、社会和工业领域都具有巨大的潜在好处。在个人层面，这些模型可以增强创造力和生产力，并增加对医疗保健、教育和金融等服务的可访问性。通过民主化获取知识资源，它们可以帮助学生学习或帮助专业人士通过综合专业知识做出决策。作为虚拟助手，它们提供即时、定制的信息以便于完成日常任务。通过自动化机械性任务，它们可能释放出人类时间用于更有价值的工作，从而提高经济产出。在经济上，生产率的增长很可能会导致某些工作类别的巨大变革。新的行业和工作可能会出现以支持AI系统。慎重考虑和解决这些变化是至关重要的。随着模型变得更好，运行它们变得更便宜，这可能会引发生成式人工智能和LLM应用到新领域的大规模扩展。除了硬件成本的降低外，根据赖特定律，随着每累积生产的AI系统翻倍，成本可能会降低10-30%。这种成本曲线反映了代码、工具和技术的重复使用等效率。随着成本的降低扩大采用，进一步降低成本的循环将产生良性循环。这将导致更多效率驱动更多使用，进而驱动更多效率的反馈循环。

> **赖特定律**，也被称为**经验曲线效应**，是经济学和商业领域的一个观察，它指出对于许多产品，每当累积生产翻倍时，成本都会按固定百分比下降。具体来说，它指出每当生产累积翻倍时，成本往往会按固定百分比（通常在10-30%之间）下降。
> 
> > 该定律以西奥多·保罗·赖特（Theodore Paul Wright）的名字命名，他是一位美国飞机工程师，1936年首次观察到这一现象，当时他分析了飞机生产成本的趋势。赖特注意到，每当飞机机身的累积生产翻倍时，生产所需的劳动力就会减少10-15%。
> > 
> > 这种关系可以用数学方式表达为：
> > 
> > ![](../media/file67.png)
> > 
> > 其中*C*[1]代表生产第一单位的成本，*C*[*x*]代表生产第x单位的成本，b是进步比率，据估计在许多行业中介于0.75至0.9之间。
> > 
> > Wright定律背后的逻辑是随着生产量的增加，工人通过实践、标准化工作流程以及开发更好的工具和流程，在制造产品方面变得更加高效。公司还会找到优化供应链、物流和资源利用的方法，以降低成本。

在工业领域，这些模型为增强人类能力和重塑工作流程带来了广泛的机会。在内容生产方面，生成式人工智能可以比人类更快地起草营销活动或新闻报道的初始版本，从而实现更大的创造力和定制化。对于开发人员，自动生成的代码和快速迭代加速了软件构建。研究人员可以快速从论文中综合发现以推动科学进步。生成式人工智能还为消费者提供了规模化的新个性化水平。推荐可以量身定制到个人。跨领域和地理区域的营销可以定制。总的来说，这些模型可以提高从工业设计到供应链等各个领域的生产率。至于技术的传播，存在两种主要情景。在第一种情景中，每家公司或个人使用其专有数据训练自己定制的模型。然而，这需要相当多的人工智能/机器学习专业知识来正确开发、训练和部署这些系统 - 目前这种人才仍然稀缺且昂贵。计算成本也非常高，专门的硬件如GPU集群的成本巨大，只有大型实体才能承担。当模型基于敏感信息进行训练时，还存在数据隐私合规方面的风险。如果能够克服围绕专业知识、计算需求和数据隐私的障碍，针对组织特定目标和数据进行微调的个性化LLMs可以通过自动化例行任务和提供定制的见解显著提高其生产力和效率。然而，一个缺点是基于小型私有数据集训练的模型可能缺乏那些基于更大、更多样化的公共语料库训练的模型的泛化能力。集中化和自助式模型可以共存，为不同的用例提供服务。在短期内，大型科技公司在提供行业特定的微调服务方面具有优势。但随着时间的推移，更多的内部培训可能会出现，受定制化和隐私需求驱动。在降低成本、传播专业知识和解决稳健性挑战方面的进展速度将决定任何集中化优势持续存在的时间。在这些领域的快速创新有利于消除壕沟，但围绕主导框架、数据集和模型的平台效应可能使当前领导者继续集中。如果出现简化和自动化人工智能开发的强大工具，定制的生成模型甚至可能适用于地方政府、社区团体和个人解决超局部挑战。尽管目前大型科技公司受益于规模经济，但来自较小实体的分布式创新可能会释放生成式人工智能在社会各个领域的全部潜力。最后，生成式人工智能的出现与我们如何生产和消费创意作品的更广泛转变相交汇。互联网已经培育了一个混搭文化，其中衍生作品和协作内容创作司空见惯。当人工智能���型通过重新组合现有材料生成新的作品时，它们符合混搭文化的迭代、集体生产原则。然而，生成模型可以合成和重新利用受版权保护内容的规模引发了复杂的法律问题。随着模型在大量书籍、文章、图片等数据集上进行训练，归因权和版税可能变得极其复杂。目前的检测机制无法以高于偶然水平的准确率找到由生成式人工智能创作的内容。这反映了围绕作者身份和版权法的更广泛辩论。让我们看看生成模型将在哪些方面产生深远的近期影响，从创意努力开始。

### 创意产业和广告

游戏和娱乐行业正在利用生成式人工智能来打造独特沉浸式的用户体验。通过自动化创意任务，可以实现主要效率提升，增加在线休闲时间。生成式人工智能可以使机器通过学习模式和示例生成新的原创内容，如艺术、音乐和文学。这对创意产业有着重要影响，因为它可以增强创意过程，潜在地创造新的收入来源。它还为媒体、电影和广告开启了新的个性化、动态内容创作规模。然而，在完全部署之前，生成内容需要围绕准确性和消除偏见进行广泛的质量控制。对于媒体、电影和广告，人工智能开启了新的个性化、动态内容创作规模。在新闻业中，利用大规模数据集自动生成文章可以释放记者，让他们专注于更复杂的调查报道。**AI生成内容**（**AIGC**）在转变媒体制作和传递方面发挥着越来越重要的作用，提高了效率和多样性。在新闻业中，文本生成工具自动化了传统由人类记者完成的写作任务，显著提高了生产率，同时保持及时性。像美联社这样的媒体机构每年利用AIGC生成数千篇报道。像洛杉矶时报的Quakebot这样的机器记者可以迅速撰写有关突发新闻的文章。其他应用包括彭博新闻的Butletin服务，其中聊天机器人创建个性化的一句新闻摘要。AIGC还可以实现AI新闻主播，通过模仿人类外观和从文本输入中产生的语音与真实主播一起共同主持广播。中国新闻机构新华社的虚拟主持人辛小薇就是一个例子，以不同角度呈现广播，产生沉浸式效果。AIGC正在从剧本创作到后期制作改变电影创作。AI剧本创作工具分析数据以生成优化的剧本。视觉效果团队将AI增强的数字环境和去老化技术与实景影像融合，产生沉浸式视觉效果。深度伪造技术可以逼真地重现或复活角色。AI还可以实现自动生成字幕，甚至通过在广泛音频样本上训练模型来预测无声电影中的对话。这通过字幕扩大了可访问性，并重新创建与场景同步的配音。在后期制作中，AI调色和编辑工具如Colourlab.Ai和Descript使用算法简化了诸如颜色校正之类的流程。在广告领域，AIGC开启了有效、定制广告创意和个性化的新潜力。AI生成内容使广告商能够以规模化的方式创建个性化、引人入胜的广告，针对个别消费者。像创意广告系统（CAS）和个性化广告文案智能生成系统（SGS-PAC）这样的平台利用数据自动生成针对特定用户需求和兴趣的广告信息。AI还协助广���创意和设计——像Vinci这样的工具可以根据产品图像和口号生成定制的吸引人海报，而像Brandmark.io这样的公司则根据用户偏好生成标志变体。GAN技术自动化产品清单生成，为有效的点对点营销提供关键词。合成广告制作也在兴起，实现高度个性化、可扩展的广告活动，节省时间。在音乐领域，像谷歌的Magenta、IBM的Watson Beat或索尼CSL的Flow Machine可以生成原创旋律和作曲。AIVA同样可以根据用户调整的参数创建独特的作品。LANDR的AI母带使用机器学习处理和改善音频质量，为音乐人提供帮助。在视觉艺术领域，MidJourney使用神经网络生成启发性图像，可以启动绘画项目。艺术家们已经利用其输出创作了获奖作品。DeepDream的算法在图像上施加模式，创造出迷幻艺术。GAN可以生成符合所需风格的抽象画作。AI绘画修复分析艺术品以数字修复损坏并恢复作品。动画工具如Adobe的Character Animator或Anthropic的Claude可以帮助生成定制角色、场景和动作序列，为非专业人士打开动画潜力。ControlNet添加约束以引导扩散模型，增加输出的变化性。对于所有这些应用，先进的人工智能通过生成内容和数据驱动的见解扩展了创意可能性。然而，值得注意的是，生成内容在完全部署之前需要围绕准确性和消除偏见进行广泛的质量控制。对于广告业，消费者数据的道德使用和人类监督仍然很重要。在所有情况下，正确归因于人类艺术家、开发人员和训练数据的贡献仍然是一个持续的挑战，因为采用范围不断扩大。

### 经济

部署生成式人工智能和其他技术可以帮助加快生产力增长，部分弥补就业增长下降并促进整体经济增长。假设能源和计算能够可持续扩展，将生成式人工智能整合到业务流程中所带来的巨大生产力增益似乎可能在未来十年内推动许多任务的自动化。然而，这种转变可能会扰乱劳动力市场，需要进行调整。麦肯锡公司的研究估计，到2030-2060年，当前工作活动的30-50%可能会被自动化。生成式人工智能到2030年每年可能会为全球生产力增长6-8万亿美元，为人工智能经济影响的先前估计增加15-40%。根据Tyna Eloundou及其同事的研究（*GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models*, 2023），大约80%的美国工人的工作任务至少有10%受到大型语言模型的影响，而19%可能有超过50%的任务受到影响。影响涵盖所有工资水平，高薪工作面临更多风险。仅有15%的美国工人任务可以通过大型语言模型显著加快完成。但是，通过大型语言模型驱动的软件，这一比例增加到所有任务的47-56%，显示了辅助技术的巨大影响。从地理角度来看，生成式人工智能的外部私人投资主要来自科技巨头和风险投资公司，主要集中在北美，反映了该大陆目前在整体人工智能投资领域的主导地位。总部位于美国的生成式人工智能相关公司在2020年至2022年期间筹集了约80亿美元，占该期间此类公司总投资的75%。自动化采用在发达经济体中可能会更快，因为较高的工资将使其更早地在经济上可行。工作自动化的规模并不直接等同于失业。与其他技术一样，生成式人工智能通常使职业中的个别活动自动化，而不是整个职业。然而，组织可能会决定通过减少某些工作类别的就业来实现增加生产力的好处。在过去30年中，生产力增长一直是GDP增长的主要引擎，但在过去十年中放缓。部署生成式人工智能和其他技术可以帮助加快生产力增长，部分弥补就业增长下降并促进整体经济增长。根据麦肯锡公司分析师的估计，从2023年到2040年，个别工作活动的自动化可能会为全球经济每年提供0.2至3.3%的生产力提升，取决于自动化采用率，然而，前提是受到技术影响的个人能够转移到其他工作活动，至少与他们2022年的生产力水平相匹配。

### 教育

一个潜在的近期未来场景是，个性化人工智能导师和导师的崛起可能使得对齐人工智能驱动经济的高需求技能的教育变得民主化。在教育领域，生成式人工智能已经在改变我们的教学和学习方式。像ChatGPT这样的工具可以用来自动生成个性化课程和定制内容给个别学生。这通过自动化重复的教学任务大大减轻了教师的工作量。人工智能导师可以对学生的写作作业提供实时反馈，让教师有更多时间专注于更复杂的技能。由生成式人工智能驱动的虚拟模拟也可以创造引人入胜、量身定制的学习体验，适应不同学习者的需求和兴趣。然而，随着这些技术的发展，需要进一步研究围绕着持续存在偏见和传播错误信息的风险。知识的加速增长和科学发现的过时意味着培养孩子们的好奇心驱动学习应该专注于发展启动和维持好奇心所涉及的认知机制，比如意识到知识空白和使用适当策略来解决它们。虽然为每个学生量身定制的人工智能导师可能会增强结果和参与度，但贫困学校可能会被落下，加剧不平等。政府应该促进平等获取，以防止生成式人工智能成为富裕阶层的特权。民主化机会对所有学生仍然至关重要。如果经过深思熟虑实施，个性化人工智能驱动的教育可以使任何有学习动力的人获得关键技能。交互式人工智能助手可以根据学生的优势、需求和兴趣调整课程，使学习高效、引人入胜和公平。但是，需要解决关于获取、偏见和社会化的挑战。

### 工作

假设能源和计算能够可持续扩展，将生成AI整合到业务流程中带来的巨大生产力增益似乎可能在未来十年内推动许多任务的自动化。这种转变可能会扰乱劳动力市场，需要进行调整。由AI实现的自动化可能会在短期内取代许多行政、客户服务、写作、法律和创意工作，而农业和建筑行业的专业可能几乎不受影响。然而，过去的工业革命最终导致了新类型的工作和产业，尽管劳动力转变困难。相同的动态可能会在长期内随着AI自动化发生变化。这将在某种程度上影响几乎所有职业，尽管有些受影响更严重。生成AI分析和生成自然语言内容的能力可能会显著增加像沟通、协作和报告等许多白领职业的自动化潜力。然而，整个工作被消除的程度仍然不确定。过去的技术创新最终创造了新类型的工作，即使转变困难。根据麦肯锡公司的研究，生成AI用例可能提供的价值约75%分布在四个领域：客户运营、市场营销和销售、软件工程和研发。突出的例子包括生成AI支持与客户互动、为市场营销生成创意内容以及根据自然语言提示起草计算机代码，以及许多其他任务。语言模型和生成AI具有颠覆和自动化传统由人类执行的各行业任务的潜力。至于不同的角色，这些预测似乎可信：

+   初级软件工程师可能会被AI编码助手增强或取代。

+   分析师和顾问利用AI数据洞察。客户服务代理被对话AI取代。

+   技术撰稿人和记者受益于AI内容生成。

+   教师利用AI进行课程准备和个性化辅导。

+   法律助理利用AI进行摘要和文件审查。

+   图形设计师将受益于AI图像生成，然而，使图像创建和处理对更多人可用也可能对工资产生影响。

+   然而，对于开发专门的AI解决方案和系统的高级软件工程师的需求将保持强劲。

+   数据科学家可能会从构建预测模型转向更多地专注于验证、调试和从AI系统中最大化价值。

+   程序员将越来越多地编写工具来辅助AI开发。

+   新角色如提示工程师已经开始出现。

人工智能可以执行一些任务，涵盖自然语言处理、内容创作，甚至复杂的创意工作，效率高，错误少于人类。技能较低的人可能能够执行更高技能的工作，而技能较高的人可能会面临较少的工作机会。例如，法律助理使用模板文件并填写必要信息以满足客户需求。拥有广泛法律文件、法规、大学课程、期刊、新闻文章和法院案例知识的人工智能，可以比法律助理做得更好。结果可能是对初级律师起草工作需求的潜在减少，法律助理使用人工智能软件来传达客户特定需求。软件开发人员和数据科学家都可以从LLMs的潜力中受益，但必须仔细考虑其能力和限制以实现最佳利用。对于初级开发人员和数据科学家，LLMs可以自动化例行任务，提供基本解决方案，减少错误，通过释放更多时间进行更复杂的工作来加速学习。然而，仅依赖人工智能可能会阻碍更深入的技术增长，因此LLMs应被视为支持性工具，同时积极开发实践经验。高级开发人员和数据科学家拥有超出当前人工智能能力的领域知识和解决问题能力。虽然自动化标准解决方案可能节省一些时间，但他们的专业知识对于指导人工智能工具、确保可靠和可扩展的结果至关重要。人工智能生产力的激增意味着公司急需人工智能人才，对于招聘和留住这些人才的竞争激烈。还将需要更多能够保护人工智能系统免受攻击的网络安全专业人员。此外，随着人工智能系统变得更加普及，可能会在人工智能伦理、监管和公共政策等领域出现更多工作。因此，吸引和培养这类人才对于公司在这个快速发展的领域保持相关性至关重要。所有领域的创作者都将受到影响。在音乐领域，人工智能正在帮助音乐人完成整个创作过程，从创作歌词和旋律到数字化母带处理和增强音频。生成艺术工具允许视觉艺术家尝试定制绘画，以迎合其独特风格。2023年3月高盛的一项研究表明，行政和法律角色最容易受到影响。他们估计约三分之二的当前工作岗位将面临人工智能自动化，得出结论称生成人工智能工具可能会影响全球3亿全职工作岗位，占当前劳动力的20%以上。采用速度是一个关键未知因素。麦肯锡分析师估计，自动化可能吸收员工工作时间的60%至70%，因此在2030年至2060年间，大约有一半的今天的工作活动可能会被自动化。根据普华永道的数据，到2030年代中期，高达30%的工作可能是可自动化的。但现实世界的采用取决于许多难以预测的因素，如监管、社会接受和再培训政策。软件和应用开发等知识工作领域已经看到了这种转变的影响。生成人工智能已被用于简化从初始代码生成到图像编辑和设计的任务。它减少了开发人员和设计师的重复手动工作，使他们能够将精力集中在更有价值的创新上。然而，对自动生成输出中的错误进行细致监控和迭代更正仍然至关重要。大规模自动化工作活动可能导致劳动力需求的重大转变，从而导致职业发生重大变化，迫使员工获得新技能。由于它们的能力基本上是为了执行认知任务而设计的，生成人工智能可能会对知识工作产生最大影响，特别是涉及决策和协作的活动，这些活动以前的自动化潜力最低。以前，自动化的影响主要集中在中低收入五分位数，此外，生成人工智能可能会对高薪工作的活动产生最大影响。大量工人将需要在现有职业或新职业中进行实质性的工作变革。他们还需要支持来进行过渡到新活动。管理这种变革将需要政策远见，以最小化对被解雇工人的困难，通过再培训计划、创造就业激励措施和可转移的福利。如果能够很好地重新投资人工智能自动化带来的效率收益，长期内可能会创造新的产业和就业机会。但顺利的劳动力转变将需要政策远见和员工培训。总之，虽然在短期内可能会有一些工作被人工智能取代，特别是例行认知任务，但它可能会自动化某些活动，而不是消灭整个职业。数据科学家和程序员等技术专家将继续是开发人工智能工具和实现其完整商业潜力的关键。

### 法律

生成模型如LLMs可以自动化例行法律任务，如合同审查、文件生成和简要准备。它们还可以加快全面的法律研究和分析。其他应用包括用通俗语言解释复杂的法律概念，以及利用案例数据预测诉讼结果。然而，考虑到透明度、公平性和问责制等因素，负责任和道德的使用仍然至关重要。总体而言，正确实施的人工智能工具有望提高法律生产力和司法准入，同时需要持续审查可靠性和伦理问题。

### 制造业

在汽车行业，它们被用于生成用于模拟的3D环境，并帮助汽车的开发。此外，生成式人工智能被用于使用合成数据对自动驾驶车辆进行道路测试。这些模型还可以处理物体信息以理解周围环境，通过对话了解人类意图，生成对人类输入的自然语言响应，并制定操纵计划以协助人类完成各种任务。

### 医学

一个能够准确预测基因序列中的物理特性的模型将是医学上的重大突破，并可能对社会产生深远影响。它还可以进一步加速药物发现和精准医学，实现更早的疾病预测和预防，提供对复杂疾病的更深入理解，并改善基因治疗。然而，这也引发了围绕基因工程的重大伦理关切，并可能加剧社会不平等。神经网络已经被用于降低长读DNA测序错误率（Baid等人；*DeepConsensus改进了具有间隙感知序列变换器的序列的准确性*，2022年9月），根据ARK投资管理公司的报告（2023年），在短期内，这样的技术已经可以以不到1000美元的价格交付第一个高质量的完整长读基因组。这意味着大规模的基因到表达模型也可能不会太远。

### 军事

全球各军事力量正在投资研究开发致命的自主武器系统（LAWS）。机器人和无人机可以在没有任何人类监督的情况下识别目标并部署致命力量。机器可以比人类更快地处理信息并做出反应，从致命决策中消除情感。然而，这引发了重大的道德问题。让机器决定是否应该夺取生命越过了一个令人不安的界限。即使使用了复杂的人工智能，战争中的比例和平民与战斗人员之间的区别等复杂因素仍需要人类判断。如果部署，完全自主的致命武器将代表放弃对生死决策的控制迈出令人震惊的一步。它们可能违反国际人道主义法，或被专制政权用来恐吓人口。一旦完全独立释放，自主杀手机器人的行动将无法预测或约束。

### 虚假信息和网络安全

人工智能对抗虚假信息具有双刃剑的作用。虽然它能够实现可扩展的检测，但自动化使得传播复杂、个性化的宣传更加容易。人工智能的使用可能有助于或有害于安全，取决于是否负责任地使用。它增加了对使用生成式黑客和社会工程技术进行网络攻击的虚假信息的脆弱性。与微目标定位和深度伪造等人工智能技术相关的重大威胁。强大的人工智能可以在心理上对用户进行个性化分析，传递便于隐蔽操纵、避免广泛审查的虚假信息。大数据和人工智能可以被利用来利用心理漏洞，渗透在线论坛，攻击和传播阴谋论。虚假信息已经转变为一个多方面的现象，涉及偏见信息、操纵、宣传以及意图影响政治行为。例如，在COVID-19大流行期间，虚假信息和信息疫情的传播一直是一个重大挑战。人工智能有潜力影响公众对选举、战争或外国势力等主题的观点。它还可以生成虚假的音频/视频内容来损害声誉并制造混乱。国家和非国家行为者正在武器化这些能力进行宣传，损害声誉并制造混乱。政党、政府、犯罪团伙甚至法律系统都可以利用人工智能发起诉讼并获取金钱。这可能会在各个领域产生深远的影响。大部分互联网用户可能正在获取所需信息而无需访问外部网站。大型企业成为信息的守门人并控制公众舆论的危险，实际上能够限制某些行动或观点。谨慎的治理和数字素养对于建立抵抗力至关重要。虽然没有单一的解决方案，但促进负责任的人工智能发展的集体努力可以帮助民主社会应对新兴威胁。

## 实施挑战

负责任地实现生成式人工智能的潜力涉及解决许多实际的法律、伦理和监管问题：

+   **法律**：关于人工智能生成内容，版权法仍存在模糊不清的问题。谁拥有产出物 - 模型创建者、训练数据贡献者还是最终用户？在训练中复制受版权保护的数据也引发了需要澄清的公平使用争议。

+   **数据保护**：收集、处理和存储训练高级模型所需的大规模数据集会带来数据隐私和安全风险。确保同意、匿名性和安全访问的治理模型至关重要。

+   **监督和法规**：呼吁加强监督，确保先进人工智能系统不歧视、准确和负责任。但是需要灵活的政策来平衡创新和风险，而不是繁琐的官僚主义。

+   **伦理学**：引导发展朝着有益结果的框架是不可或缺的。通过专注于透明性、可解释性和人类监督的设计实践来整合伦理有助于建立信任。

总的来说，政策制定者、研究人员和公民社会之间的积极合作对于解决围绕权利、伦理和治理尚未解决的问题至关重要。在制定务实的监管措施的同时，生成模型可以实现其承诺，同时减少伤害。但公众利益必须始终是引导人工智能进步的指南。对算法透明性的需求日益增长。这意味着科技公司和开发者应该揭示其系统的源代码和内部运作方式。然而，这些公司和开发者认为披露专有信息会损害其竞争优势，因此存在抵制。开源模型将继续蓬勃发展，欧盟和其他国家的地方立法将推动人工智能的透明使用。人工智能偏见的后果包括由人工智能系统做出的偏见决策可能对个人或群体造成潜在伤害。将伦理培训纳入计算机科学课程可以帮助减少人工智能代码中的偏见。通过教导开发者如何构建以伦理为设计基础的应用程序，可以最大程度地减少将偏见嵌入代码的可能性。为了走上正确的道路，组织需要优先考虑透明度、问责制和监管措施，以防止其人工智能系统中存在偏见。人工智能偏见预防是许多组织的长期优先事项，然而，如果没有立法推动，引入可能需要时间。例如，欧盟国家的地方立法，如欧洲委员会关于人工智能监管规则的提案，将推动更具伦理性的语言和形象使用。德国目前关于虚假新闻的法律规定，要求平台在24小时内删除虚假新闻和仇恨言论，对于大型和小型平台都是不切实际的。此外，较小平台有限的资源使得他们无法监管所有内容变得不切实际。此外，在线平台不应该拥有确定什么是真相的唯一权威，因为这可能导致过度审查。需要更加细致的政策来平衡言论自由、问责制和各种技术平台遵守的可行性。仅依靠私营公司来监管在线内容引发了对监督和正当程序不足的担忧。政府、公民社会、学术界和行业之间更广泛的合作可以制定更有效的框架来对抗错误信息，同时保护权利。

## 未来之路

未来的生成式人工智能模型时代提供了大量引人入胜的机遇和无与伦比的进展，然而其中夹杂着许多不确定性。正如本书所讨论的，近年来取得了许多突破，但连续的挑战仍然存在，主要涉及这些模型的精度、理性能力、可控性以及根深蒂固的偏见。虽然有人对即将到来的超智能人工智能的宏伟声明可能显得言过其实，但持续的趋势预示着在未来几十年内将出现复杂的能力。在个人层面上，生成内容的泛滥引发了关于误导、学术抄袭以及在线空间中的冒名顶替的合理担忧。随着这些模型在模仿人类表达方面变得更加娴熟，人们可能难以分辨是人类生成还是人工智能生成的内容，从而为新形式的欺骗创造了可能性。人们还担心生成模型加剧社交媒体成瘾，因为它们能够生成无穷无尽的定制内容。从社会角度来看，快速发展的速度引发了人类过时和工作岗位流失的不安，这可能进一步分化经济阶层。与过去的物理自动化不同，生成式人工智能威胁到以前被认为不会受到自动化影响的认知工作类别。以道德和公平的方式管理这种工作力量的转变将需要远见和规划。围绕AI是否应该创作反映人类状况的艺术、文学或音乐也存在着哲学上的争论。对于企业来说，尚未建立有效的治理框架来规范可接受的用例。生成模型放大了滥用的风险，从制造深度伪造等误导性信息到生成不安全的医疗建议。内容许可和知识产权方面出现了法律问题。虽然这些模型可以提高业务生产力，但质量控制和偏见缓解会增加额外成本。尽管大型科技公司目前主导着生成式人工智能的研究和开发，但较小的实体最终可能从这些技术中获益最多。随着计算、数据存储和人工智能人才成本的降低，为中小型公司进行定制预训练专业模型可能变得可行。与依赖大型科技公司的通用模型不同，针对特定数据集进行细化调整的生成式人工智能���能更好地满足独特需求。初创公司和非营利组织通常擅长快速迭代，为专业领域构建尖端解决方案。通过成本降低实现的民主化访问可能使这些专注的参与者能够训练性能模型，超越通用系统的能力。随着生成式人工智能工具相对容易地构建，关于模型和工具的定制化将促进价值创造，但尚不清楚谁将获得最大的好处。尽管当前市场炒作很高，但投资者在2021年人工智能繁荣/衰退周期之后对较低的估值和怀疑进行了调整。长期市场影响和获胜的生成式人工智能商业模式尚未揭晓。

> **2021年人工智能繁荣/衰退周期**指的是人工智能初创公司领域投资和增长的快速加速，随后在2022年市场降温和稳定，因为预期未能实现，估值下降。
> 
> > 以下是一个快速摘要：
> > 
> > 繁荣阶段（2020-2021年）：
> > 
> > 对于提供创新能力的人工智能初创公司，如计算机视觉、自然语言处理、机器人技术和机器学习平台，市场出现了巨大的兴趣和投资热潮。根据Pitchbook的数据，2021年全球人工智能初创公司的总融资额达到创纪录的730亿美元以上。在这一时期，数百家人工智能初创公司应运而生并获得了资金支持。
> > 
> > 破灭阶段（2022年）：
> > 
> > 在2022年，市场经历了一次修正，人工智能初创公司的估值从2021年的高点大幅下跌。一些知名的人工智能初创公司，如Anthropic和Cohere，面临了估值下调。许多投资者对资助人工智能初创公司变得更加谨慎和选择性。更广泛的科技行业市场调整也促成了这一繁荣的结束。
> > 
> > 关键因素：
> > 
> > 过度炒作、不切实际的增长预期、2021年历史上高企的估值以及更广泛的经济状况都导致了这一繁荣-衰退周期。这一周期遵循了先前在互联网泡沫和区块链等领域中看到的经典模式。

展望几十年后，也许最深远的挑战是伦理问题。随着人工智能被委托处理更为重要的决策，与人类价值观的一致性变得至关重要。虽然准确性、推理能力、可控性和减少偏见仍然是技术上的重点，但其他重点应包括加强模型的稳健性、促进透明度并确保与人类价值观的一致性。为了最大化利益，公司需要确保人类监督、多样性和开发过程的透明度。政策制定者可能需要实施防止滥用的限制措施，同时为工人提供支持以适应活动的转变。通过负责任的实施，生成式人工智能可以推动社会的增长、创造力和可访问性。及早解决潜在风险并确保公共利益设计的利益公平分配将培养利益相关者的信任，例如：

+   **进步的动态**：调整转型的速度对于避免任何不良后果至关重要。此外，过于缓慢的发展可能会扼杀创新，因此通过包容性的公共讨论确定理想的速度是至关重要的。

+   **人工智能与人类的共生**：与其追求完全自动化，更有利的系统将整合和补充人类的创造力与人工智能的生产效率。这样的混合模式将确保最佳监督。

+   **促进获取和包容性**：对人工智能资源、相关教育和各种机会的平等获取对抵消差距的扩大至关重要。代表性和多样性应该得到优先考虑。

+   **预防措施和风险管理**：通过跨学科的洞察力不断评估新兴能力是必要的，以避免未来的危险。然而，过度的担忧不应阻碍潜在的进步。

+   **维护民主规范**：合作讨论、共同努力和达成妥协必然比由一个孤立实体强加的单方面法令更有建设性地定义人工智能未来发展方向。公共利益必须优先考虑。

虽然未来的能力仍然不确定，但积极的治理和获取的民主化对于引导这些技术走向公平、善意的结果至关重要。研究人员、政策制定者和公民社会围绕透明度、问责制和伦理问题展开合作，可以帮助将新兴创新与共同的人类价值观对齐。目标应该是赋予人类潜力，而不仅仅是技术进步。
