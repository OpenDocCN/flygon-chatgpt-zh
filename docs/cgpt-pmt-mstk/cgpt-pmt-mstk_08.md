



# 注意机制和自注意力



注意机制在神经网络中帮助模型在生成输出时专注于输入的相关部分。

自注意力是 Transformer 架构中使用的一种特定类型的注意机制，允许模型权衡给定上下文中不同单词或标记的重要性。

想象一下，你正在阅读一本关于比萨历史的书。你的大脑会自动聚焦在最相关的信息上，比如配料和烹饪技巧，并忽略不太重要的细节。注意机制在神经网络中的工作原理与此类似。
