- en: '| ![image](d2d_images/chapter_title_corner_decoration_left.png) |  | ![image](d2d_images/chapter_title_corner_decoration_right.png)
    |'
  id: totrans-0
  prefs: []
  type: TYPE_TB
- en: '![image](d2d_images/chapter_title_above.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
- en: 'Chapter 2: How ChatGPT Works'
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![image](d2d_images/chapter_title_below.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
- en: ChatGPT is an advanced artificial intelligence language model that can generate
    human-like text in response to user input. In this chapter, we will study the
    technical aspects of the ChatGPT architecture, the training process, and the inference
    algorithm.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT uses the Transformer architecture, a type of neural network that uses
    an attention mechanism to allow the model to focus on a different part of the
    input sequence while generating the output sequence. This makes it especially
    suitable for language modeling tasks where output (text) sequences can vary in
    length and complexity.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: With ChatGPT, the input sequence is the user's commands or messages, and the
    output sequence is the response generated by the model. The model is pre-trained
    on a broad corpus of conversational data, enabling it to understand and generate
    natural language responses in conversational contexts.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Training
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'ChatGPT is pre-trained using unattended training, which means the model is
    trained on a large corpus of text data without the need for human-annotated labels.
    Pre-training consists of two main phases: pre-training and refinement.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: In the pre-training phase, the model is trained on a large set of text data
    using a language modeling objective. The goal of a language modeling task is to
    predict the next word in the order given by the previous word. This process allows
    the model to learn natural language patterns and structures.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: In the fine-tuning phase, the pre-trained model is further trained on a smaller
    corpus of speech data to improve its ability to generate responses in a conversational
    environment. Model parameters are tailored to specific tasks, such as answering
    questions or engaging in small talk.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Diploma
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: When a user enters a text command, ChatGPT uses a pre-trained model to parse
    the command and generate a response. The derivation process involves several steps,
    including tokenization, embedding, and decoding.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: In tokenization, the input sequence is broken down into individual words or
    tokens, which are then converted into a numeric representation using an embedding
    layer. The embedding layer maps each word to a high-dimensional vector that captures
    its meaning and context in the input sequence.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Decoding involves generating an output (response) sequence using the Transformer
    architecture and attention mechanism. The model generates a sequence of word-by-word
    outputs, with each word generated based on the context of the previous word and
    attentional mechanisms.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Diploma
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT is a highly sophisticated language model that uses deep learning techniques
    to generate human text in response to user input. His unsupervised, learning-based
    pre-learning process allows him to learn natural language patterns and structures,
    while his refinement phase allows him to specialize in human conversational interaction.
    By using the architectural models and attention mechanisms in Transformer, it
    is very effective for language modeling tasks because it can produce coherent
    and contextual responses. In future chapters, we will explore the capabilities
    of ChatGPT in greater detail and discuss the broader implications of using this
    advanced language model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT是一个高度复杂的语言模型，它使用深度学习技术根据用户输入生成人类文本。他的无监督学习预训练过程使他能够学习自然语言的模式和结构，而他的精炼阶段使他能够专注于人类对话互动。通过使用Transformer中的架构模型和注意机制，它对语言建模任务非常有效，因为它可以生成连贯和上下文相关的回应。在未来的章节中，我们将更详细地探讨ChatGPT的能力，并讨论使用这种先进语言模型的更广泛影响。
