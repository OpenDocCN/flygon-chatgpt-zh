["```\nHere\u2019s a statement: {statement}\\nMake a bullet point list of the assumptions you made when producing the above statement.\\n\n```", "```\nHere is a bullet point list of assertions:\n    {assertions}\n    For each assertion, determine whether it is true or false. If it is false, explain why.\\n\\n\n```", "```\nIn light of the above facts, how would you answer the question '{question}'\n```", "```\nfrom langchain.chains import LLMCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0.7)\ntext = \"What type of mammal lays the biggest eggs?\"\nchecker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\nchecker_chain.run(text)\n```", "```\nMonotremes, a type of mammal found in Australia and parts of New Guinea, lay the largest eggs in the mammalian world. The eggs of the American echidna (spiny anteater) can grow as large as 10 cm in length, and dunnarts (mouse-sized marsupials found in Australia) can have eggs that exceed 5 cm in length.\n\u2022 Monotremes can be found in Australia and New Guinea\n\u2022 The largest eggs in the mammalian world are laid by monotremes\n\u2022 The American echidna lays eggs that can grow to 10 cm in length\n\u2022 Dunnarts lay eggs that can exceed 5 cm in length\n\u2022 Monotremes can be found in Australia and New Guinea \u2013 True\n\u2022 The largest eggs in the mammalian world are laid by monotremes \u2013 True\n\u2022 The American echidna lays eggs that can grow to 10 cm in length \u2013 False, the American echidna lays eggs that are usually between 1 to 4 cm in length. \n\u2022 Dunnarts lay eggs that can exceed 5 cm in length \u2013 False, dunnarts lay eggs that are typically between 2 to 3 cm in length.\nThe largest eggs in the mammalian world are laid by monotremes, which can be found in Australia and New Guinea. Monotreme eggs can grow to 10 cm in length.\n> Finished chain.\n```", "```\nfrom langchain.chains.summarize import load_summarize_chain\nfrom langchain import OpenAI\nfrom langchain.document_loaders import PyPDFLoader\npdf_loader = PyPDFLoader(pdf_file_path)\ndocs = pdf_loader.load_and_split()\nllm = OpenAI()\nchain = load_summarize_chain(llm, chain_type=\"map_reduce\")\nchain.run(docs)\n```", "```\nWrite a concise summary of the following:\n{text}\nCONCISE SUMMARY:\n```", "```\nGiven the following extracted parts of a long document and a question, create a final answer with references (\\\"SOURCES\\\"). \\nIf you don't know the answer, just say that you don't know. Don't try to make up an answer.\\nALWAYS return a \\\"SOURCES\\\" part in your answer.\\n\\nQUESTION: {question}\\n=========\\nContent: {text}\n```", "```\nwith get_openai_callback() as cb:\n    response = llm_chain.predict(text=\u201dComplete this text!\u201d)\n    print(f\"Total Tokens: {cb.total_tokens}\")\n    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n    print(f\"Completion Tokens: {cb.completion_tokens}\")\n    print(f\"Total Cost (USD): ${cb.total_cost}\")\n```", "```\ninput_list = [\n    {\"product\": \"socks\"},\n    {\"product\": \"computer\"},\n    {\"product\": \"shoes\"}\n]\nllm_chain.generate(input_list)\n```", "```\n LLMResult(generations=[[Generation(text='\\n\\nSocktastic!', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nTechCore Solutions.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nFootwear Factory.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'prompt_tokens': 36, 'total_tokens': 55, 'completion_tokens': 19}, 'model_name': 'text-davinci-003'})\n```", "```\n {\n  \"model\": \"gpt-3.5-turbo-0613\",\n  \"object\": \"chat.completion\",\n  \"usage\": {\n    \"completion_tokens\": 17,\n    \"prompt_tokens\": 57,\n    \"total_tokens\": 74\n  }\n}\n```", "```\nfrom typing import Optional\nfrom pydantic import BaseModel\nclass Experience(BaseModel):\n    start_date: Optional[str]\n    end_date: Optional[str]\n    description: Optional[str]\nclass Study(Experience):\n    degree: Optional[str]\n    university: Optional[str]\n    country: Optional[str]\n    grade: Optional[str]\nclass WorkExperience(Experience):\n    company: str\n    job_title: str\nclass Resume(BaseModel):\n    first_name: str\n    last_name: str\n    linkedin_url: Optional[str]\n    email_address: Optional[str]\n    nationality: Optional[str]\n    skill: Optional[str]\n    study: Optional[Study]\n    work_experience: Optional[WorkExperience]\n    hobby: Optional[str]\n```", "```\nfrom langchain.chains import create_extraction_chain_pydantic\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.document_loaders import PyPDFLoader\npdf_loader = PyPDFLoader(pdf_file_path)\ndocs = pdf_loader.load_and_split()\n# please note that function calling is not enabled for all models!\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0613\")\nchain = create_extraction_chain_pydantic(pydantic_schema=Resume, llm=llm)\nreturn chain.run(docs)\n```", "```\n[Resume(first_name='John', last_name='Doe', linkedin_url='linkedin.com/in/john-doe', email_address='hello@openresume.com', nationality=None, skill='React', study=None, work_experience=WorkExperience(start_date='May 2023', end_date='Present', description='Lead a cross-functional team of 5 engineers in developing a search bar, which enables thousands of daily active users to search content across the entire platform. Create stunning home page product demo animations that drives up sign up rate by 20%. Write clean code that is modular and easy to maintain while ensuring 100% test coverage.', company='ABC Company', job_title='Software Engineer'), hobby=None)]\n```", "```\nfrom langchain.agents import (\n    AgentExecutor, AgentType, initialize_agent, load_tools\n)\nfrom langchain.chat_models import ChatOpenAI\ndef load_agent() -> AgentExecutor:\n    llm = ChatOpenAI(temperature=0, streaming=True)\n    # DuckDuckGoSearchRun, wolfram alpha, arxiv search, wikipedia\n    # TODO: try wolfram-alpha!\n    tools = load_tools(\n        tool_names=[\"ddg-search\", \"wolfram-alpha\", \"arxiv\", \"wikipedia\"],\n        llm=llm\n    )\n    return initialize_agent(\n        tools=tools, llm=llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n    )\n```", "```\nreturn MRKLChain.from_chains(llm, chains, verbose=True)\n```", "```\nimport streamlit as st\nfrom langchain.callbacks import StreamlitCallbackHandler\nchain = load_agent()\nst_callback = StreamlitCallbackHandler(st.container())\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = chain.run(prompt, callbacks=[st_callback])\n        st.write(response)\n```", "```\nPYTHONPATH=. streamlit run question_answering/app.py\n```", "```\n> Entering new AgentExecutor chain...\nI'm not sure, but I think I can find the answer by searching online.\nAction: duckduckgo_search\nAction Input: \"mammal that lays the biggest eggs\"\nObservation: Posnov / Getty Images. The western long-beaked echidna ...\nFinal Answer: The platypus is the mammal that lays the biggest eggs.\n> Finished chain.\n```", "```\nfrom typing import Literal\nfrom langchain.experimental import load_chat_planner, load_agent_executor, PlanAndExecute\nReasoningStrategies = Literal[\"one-shot-react\", \"plan-and-solve\"]\ndef load_agent(\n        tool_names: list[str],\n        strategy: ReasoningStrategies = \"one-shot-react\"\n) -> Chain:\n    llm = ChatOpenAI(temperature=0, streaming=True)\n    tools = load_tools(\n        tool_names=tool_names,\n        llm=llm\n    )\n    if strategy == \"plan-and-solve\":\n        planner = load_chat_planner(llm)\n        executor = load_agent_executor(llm, tools, verbose=True)\n        return PlanAndExecute(planner=planner, executor=executor, verbose=True)\n    return initialize_agent(\n        tools=tools, llm=llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n    )\n```", "```\nstrategy = st.radio(\n    \"Reasoning strategy\",\n    (\"plan-and-solve\", \"one-shot-react\", ))\n```", "```\ntool_names = st.multiselect(\n    'Which tools do you want to use?',\n    [\n        \"google-search\", \"ddg-search\", \"wolfram-alpha\", \"arxiv\",\n        \"wikipedia\", \"python_repl\", \"pal-math\", \"llm-math\"\n    ],\n    [\"ddg-search\", \"wolfram-alpha\", \"wikipedia\"])\n```", "```\nagent_chain = load_agent(tool_names=tool_names, strategy=strategy)\n```", "```\nAction:\n{\n\"action\": \"Wikipedia\",\n\"action_input\": \"large language models\"\n}\n```"]