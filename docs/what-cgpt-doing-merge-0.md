# （一）



# 序言

这本简短的书试图从第一原理解释 ChatGPT 是如何工作的。在某种程度上，这是关于技术的故事。但它也是关于科学的故事。以及关于哲学的故事。为了讲述这个故事，我们将不得不汇集许多世纪以来的想法和发现。

对我来说，看到我长期以来感兴趣的许多事物在突然进展的一阵中汇聚在一起，真是令人兴奋。从简单程序的复杂行为到语言和意义的核心特征，以及大型计算机系统的实际情况—所有这些都是 ChatGPT 故事的一部分。

ChatGPT 基于神经网络的概念——最初在 1940 年代作为对大脑操作的理想化而发明。我自己在 1983 年首次编写了一个神经网络——但它并没有做出任何有趣的事情。但是 40 年后，随着计算机的速度实际上快了一百万倍，互联网上有数十亿页的文本，并经过一系列工程创新，情况大不相同。而且—令所有人惊讶的是—一个比我 1983 年拥有的神经网络大一亿倍的神经网络能够做到被认为是独特的人类生成有意义的人类语言的事情。

这本书由我在 ChatGPT 推出后不久写的两篇文章组成。第一篇是关于 ChatGPT 及其生成语言的非常人性化能力的解释。第二篇展望了 ChatGPT 能够利用计算工具超越人类所能做到的，并且特别能够利用我们的 Wolfram|Alpha 系统的计算知识“超能力”。

ChatGPT 推出仅仅三个月，我们才刚开始理解它的实际和智力意义。但目前它的到来提醒我们，即使在已经发明和发现了一切之后，惊喜仍然是可能的。

史蒂芬·沃尔夫勒姆

2023 年 2 月 28 日


# ChatGPT 在做什么... 为什么会有效？

## 它只是一次添加一个单词

[ChatGPT](https://chat.openai.com/) 能够自动生成类似人类编写文本的东西，这是令人惊讶且意想不到的。但它是如何做到的？为什么有效？我在这里的目的是大致概述 ChatGPT 内部发生的事情，然后探讨为什么它能够在产生我们可能认为有意义的文本方面做得如此出色。我应该在一开始就说，我将专注于正在发生的大局，并且虽然我会提到一些工程细节，但我不会深入探讨它们。（我将说的要点同样适用于其他当前的“大型语言模型” [LLMs]，就像适用于 ChatGPT 一样。）

首先要解释的是，ChatGPT 始终试图做的基本上是生成“合理的延续”，无论它目前已经有了什么文本，这里的“合理”意味着“在看到人们在数十亿网页等上写了什么之后，我们可能期望某人写下的内容”。

所以，假设我们有文本“*AI 最好的地方在于*”。想象一下扫描数十亿页人类编写的文本（比如网络上和数字化书籍中的文本），找到所有这个文本的实例，然后看看接下来的词是什么，以及在多大程度上。ChatGPT 实际上做的就是这样一件事，只是（我将解释）它不看文字本身；它寻找的是在某种意义上“意义匹配”的东西。但最终的结果是它生成了一个排名列表，列出了可能跟随的单词，以及“概率”：

![](img/Image00002.gif)

令人惊讶的是，当 ChatGPT 做类似写一篇文章的事情时，它实际上只是一遍又一遍地问“在目前的文本中，下一个词应该是什么？” ——然后每次添加一个单词。（更准确地说，正如我将解释的那样，它添加的是一个“标记”，这可能只是一个单词的一部分，这就是为什么它有时“编造新词”的原因。）

但是，在每一步，它都会得到一个带有概率的单词列表。但它应该选择哪一个添加到正在写的文章（或其他内容）中呢？人们可能认为应该选择“排名最高”的单词（即分配了最高“概率”的单词）。但这就是一些巫术开始悄悄渗入的地方。因为出于某种原因——也许有一天我们会对此有科学式的理解——如果我们总是选择排名最高的单词，我们通常会得到一篇非常“平淡”的文章，似乎从不“展现任何创造力”（甚至有时会逐字重复）。但如果有时（随机地）选择排名较低的单词，我们会得到一篇“更有趣”的文章。

这里存在随机性意味着，如果我们多次使用相同提示，每次可能会得到不同的文章。并且，与巫术的概念一致，有一个特定的所谓“温度”参数，决定了使用较低排名单词的频率，对于文章生成来说，0.8 的“温度”似乎最好。（值得强调的是这里没有使用“理论”；这只是根据实践中发现的有效方法。例如，“温度”概念之所以存在是因为指数分布[熟悉于统计物理学](https://writings.stephenwolfram.com/2023/02/computational-foundations-for-the-second-law-of-thermodynamics/#textbook-thermodynamics)正被使用，但没有“物理”联系——至少据我们所知。）

在继续之前，我应该解释一下，为了说明的目的，我大多不会使用[ChatGPT 中的完整系统](https://openai.com/blog/chatgpt/)；相反，我通常会使用一个更简单的[GPT-2 系统](https://resources.wolframcloud.com/NeuralNetRepository/resources/GPT2-Transformer-Trained-on-WebText-Data/)，它具有一个很好的特点，即它足够小，可以在标准台式计算机上运行。因此，对于我展示的几乎所有内容，我都能够包含您可以立即在计算机上运行的明确的[Wolfram 语言](https://www.wolfram.com/language/)代码。（单击这里的任何图片以复制其背后的代码。）

例如，这里是如何获取上述概率表的方法。首先，我们必须[检索基础的“语言模型”神经网络](https://resources.wolframcloud.com/NeuralNetRepository)：

![](img/Image00003.jpg)

后来，我们将深入研究这个神经网络，并讨论它的工作原理。但现在我们可以将这个“网络模型”作为一个黑匣子应用到我们迄今为止的文本中，并要求模型说应该跟随的前 5 个单词的概率最高：

![](img/Image00004.gif)

这将该结果制作成明确格式的“[数据集](https://www.wolfram.com/language/elementary-introduction/2nd-ed/45-datasets.html)”：

![](img/Image00005.gif)

如果一个人反复“应用模型”，每一步都添加具有最高概率的单词（在此代码中指定为模型的“决策”），会发生什么：

![](img/Image00006.gif)

如果继续下去会发生什么？在这种（“零温度”）情况下，很快输出的内容会变得相当混乱和重复：

![](img/Image00007.jpg)

但是，如果不总是选择“顶部”单词，而是有时随机选择“非顶部”单词（“随机性”对应于“温度”0.8）呢？同样可以构建文本：

![](img/Image00008.jpg)

每次这样做时，都会做出不同的随机选择，文本也会不同，就像这 5 个例子中的情况：

![](img/Image00009.gif)

值得指出，即使在第一步，也有很多可能的“下一个词”可供选择（在温度为 0.8 时），尽管它们的概率迅速下降（是的，在这个对数-对数图上的直线对应于* n * ^（-1）[“幂律”衰减，这是语言的一般统计特征](https://www.wolframscience.com/nks/notes-8-8--zipfs-law/)）：

![](img/Image00010.jpg)

那么如果继续下去会发生什么？这里是一个随机示例。它比顶部词（零温度）情况要好，但仍然至少有点奇怪：

![](img/Image00011.jpg)

这是使用[最简单的 GPT-2 模型](https://resources.wolframcloud.com/NeuralNetRepository/resources/GPT2-Transformer-Trained-on-WebText-Data/)（来自 2019 年）完成的。使用更新和[更大的 GPT-3 模型](https://platform.openai.com/docs/model-index-for-researchers)结果更好。这是使用相同“提示”生成的顶部词（零温度）文本，但使用最大的 GPT-3 模型：

![](img/Image00012.jpg)

这里是一个“温度 0.8”的随机示例：

![](img/Image00013.jpg)

## 概率是从哪里来的？

好的，ChatGPT 总是根据概率选择下一个词。但这些概率是从哪里来的呢？让我们从一个更简单的问题开始。让我们考虑一次生成英文文本一个字母（而不是单词）。我们如何计算每个字母的概率呢？

我们可以做的一个非常简单的事情就是只取一段英文文本的样本，并计算其中不同字母出现的频率。例如，[这里计算了维基百科文章](https://www.wolfram.com/language/elementary-introduction/2nd-ed/34-associations.html#i-8)中“cats”中字母的频率：

![](img/Image00014.jpg)

这也对“dogs”做了同样的事情：

![](img/Image00015.jpg)

结果是相似的，但并不完全相同（“o”在“dogs”文章中无疑更常见，因为毕竟它出现在单词“dog”中）。不过，如果我们取足够大的英文文本样本，我们最终可以期望得到至少相当一致的结果：

![](img/Image00016.jpg)

这是我们只根据这些概率生成一系列字母时得到的样本：

![](img/Image00017.jpg)

我们可以通过按照一定概率添加空格来将其分解为“单词”：

![](img/Image00018.jpg)

通过强制“单词长度”的分布与英语中的实际情况一致，我们可以更好地生成“单词”：

![](img/Image00019.jpg)

在这里我们没有得到任何“实际单词”，但结果看起来稍微好一些。不过，要进一步，我们需要做的不仅仅是随机选择每个字母。例如，我们知道如果有一个“q”，下一个字母基本上必须是“u”。

这是字母单独出现的概率的图表：

![](img/Image00020.jpg)

这里是一个显示典型英文文本中字母对（“2-grams”）概率的图表。可能的第一个字母横跨页面显示，第二个字母纵向显示：

![](img/Image00021.jpg)

例如，我们在这里看到，“q”列是空白的（概率为零），除了“u”行。好的，现在我们不再一次生成我们的“单词”，而是一次查看两个字母，使用这些“2-gram”概率来生成它们。这里是结果的一个样本—其中包括一些“实际单词”：

![](img/Image00022.jpg)

通过足够多的英文文本，我们不仅可以对单个字母或字母对（2-grams）的概率得到相当好的估计，还可以对更长的字母序列进行估计。如果我们使用逐渐更长的*n*-gram 概率生成“随机单词”，我们会看到它们逐渐变得“更现实”：

![](img/Image00023.gif)

但现在让我们假设—更多或更少地像 ChatGPT 一样—我们正在处理整个单词，而不是字母。英语中大约有 40,000 个[常用单词](https://reference.wolfram.com/language/ref/WordList.html)。通过查看大量的英文文本（比如几百万本书，总共几百亿字），我们可以得到每个单词有多常见的[估计](https://reference.wolfram.com/language/ref/WordFrequencyData.html)。利用这一点，我们可以开始生成“句子”，其中每个单词都是独立随机选择的，具有它在语料库中出现的相同概率。这里是我们得到的一个样本：

![](img/Image00024.jpg)

毫不奇怪，这是无意义的。那么我们怎么能做得更好呢？就像处理字母一样，我们可以开始考虑不仅是单个单词的概率，而是对单词对或更长的*n*-gram 单词的概率。对于成对的情况，以下是我们得到的 5 个示例，所有情况都从单词“cat”开始：

![](img/Image00025.gif)

它看起来稍微更“合理”了。我们可以想象，如果我们能够使用足够长的*n*-grams，我们基本上会“得到一个 ChatGPT”—在这个意义上，我们会得到一些会生成具有“正确整体文章概率”的单词序列的东西。但问题在于：甚至没有足够接近的英文文本被写过，以便推断这些概率。

在[网络爬虫](https://commoncrawl.org/)中可能有数百亿字；在已数字化的书籍中可能还有另外数百亿字。但是，即使有 40,000 个常用词，可能的 2-gram 数量已经达到了 16 亿，而可能的 3-gram 数量则达到了 60 万亿。因此，我们无法从已有的文本中估计所有这些的概率。当我们到达“20 个词的文章片段”时，可能性的数量已经超过了宇宙中的粒子数量，因此从某种意义上说，它们永远无法全部被记录下来。

那么我们能做什么？大的想法是制定一个模型，让我们能够估计序列应该发生的概率——即使我们从未在我们查看的文本语料库中明确看到过这些序列。而 ChatGPT 的核心正是所谓的“大语言模型”（LLM），它被构建为很好地估计这些概率。

## 什么是模型？

假设你想知道（就像[加利略在 16 世纪末所做的那样](https://archive.org/details/bub_gb_49d42xp-USMC/page/404/mode/2up)）从比萨斜塔的每层楼掉下的炮弹到达地面需要多长时间。嗯，你可以在每种情况下测量它并制作一个结果表。或者你可以做理论科学的本质：制定一个给出某种计算答案的程序的模型，而不仅仅是测量和记住每种情况。

让我们想象我们有（有些理想化的）数据，表明炮弹从各个楼层掉下所需的时间：

![](img/Image00026.jpg)

我们如何确定从我们没有明确数据的楼层掉下来需要多长时间？在这种特殊情况下，我们可以利用已知的物理定律来计算。但是假设我们只有数据，而不知道支配它的基本定律。那么我们可能会做一个数学猜测，比如也许我们应该使用一条直线作为模型：

![](img/Image00027.jpg)

我们可以选择不同的直线。但这是平均最接近我们所给出的数据的直线。从这条直线我们可以估计任何楼层掉落的时间。

我们怎么知道在这里尝试使用一条直线呢？在某种程度上，我们并不知道。这只是数学上简单的东西，我们习惯于许多我们测量的数据结果很适合数学上简单的东西。我们可以尝试一些数学上更复杂的东西——比如 *a* + *b* *x* + *c* *x* ² ——在这种情况下我们做得更好：

![](img/Image00028.jpg)

事情可能会出现问题。就像这里我们用 *a* + *b* /*x* + *c* sin(*x* )做的[最好的尝试](https://reference.wolfram.com/language/ref/FindFit.html)：

![](img/Image00029.jpg)

值得理解的是，从来没有“无模型的模型”。你使用的任何模型都有一定的基本结构——然后一组“你可以转动的旋钮”（即你可以设置的参数）来拟合你的数据。在 ChatGPT 的情况下，使用了许多这样的“旋钮”——实际上有 1750 亿个。

但值得注意的是，ChatGPT 的基本结构——仅仅有那么多参数——足以构建一个计算下一个词概率“足够好”的模型，从而给我们合理长度的文章。

## 人类任务的模型

我们上面给出的例子涉及制作一个模型，用于来自简单物理的数值数据——几个世纪以来，我们已经知道“简单的数学适用”。但对于 ChatGPT，我们必须制作一个模型，用于人类大脑产生的文本。对于这样的东西，我们（至少目前）没有类似于“简单的数学”。那么，这样的模型可能是什么样子呢？

在讨论语言之前，让我们先谈谈另一个类似人类任务：识别图像。作为一个简单的例子，让我们考虑数字的图像（是的，这是一个[经典的机器学习例子](https://resources.wolframcloud.com/NeuralNetRepository/resources/050b1a0a-f43a-4c28-b7e0-72607a918467/)）：

![](img/Image00030.gif)

我们可以做的一件事是为每个数字获取一堆样本图像：

![](img/Image00031.jpg)

然后，为了找出我们输入的图像是否对应于特定的数字，我们可以将其与我们拥有的样本进行像素级比较。但作为人类，我们似乎做得更好——因为我们仍然能够识别数字，即使它们是手写的，而且经过各种修改和扭曲：

![](img/Image00032.jpg)

当我们为上面的数字数据建立模型时，我们能够拿到给定的数值*x*，然后只需计算特定*a*和*b*的*a + b x*。那么，如果我们将每个像素的灰度值视为某个变量*x[i]*，是否存在某个函数，当评估时告诉我们图像是哪个数字的？事实证明，构建这样的函数是可能的。毫不奇怪，这并不特别简单，一个典型的例子可能涉及大约五十万次数学运算。

但最终结果是，如果我们将图像的像素值集合输入到这个函数中，输出将是指定我们拥有的图像是哪个数字。稍后，我们将讨论如何构建这样一个函数，以及神经网络的概念。但现在让我们将这个函数视为黑匣子，我们输入手写数字的图像（作为像素值数组），然后得到对应的数字：

![](img/Image00033.jpg)

但这里真正发生了什么？假设我们逐渐模糊一个数字。有一段时间我们的函数仍然“识别”它，这里是一个“2”。但很快它“失去了”，开始给出“错误”的结果：

![](img/Image00034.jpg)

但为什么我们说这是“错误”的结果？在这种情况下，我们知道我们通过模糊“2”得到了所有这些图像。但如果我们的目标是产生一个模型，模拟人类在识别图像方面的能力，真正要问的问题是，如果向人类展示其中一张模糊的图像，而不知道它来自哪里，人类会做什么。

如果我们从我们的函数得到的结果通常与人类的判断一致，那么我们就有一个“好模型”。而非平凡的科学事实是，对于像这样的图像识别任务，我们现在基本上知道如何构建能够实现这一点的函数。

我们能“数学证明”它们有效吗？嗯，不行。因为要做到这一点，我们必须对我们人类正在做什么有一个数学理论。拿“2”图像并改变几个像素。我们可能会想象，即使只有几个像素“错位”，我们仍应该认为这是一个“2”。但这应该持续到什么程度？这是一个关于[人类视觉感知](https://www.wolframscience.com/nks/chap-10--processes-of-perception-and-analysis#sect-10-7--visual-perception)的问题。是的，答案无疑对蜜蜂或章鱼可能不同，对假设的外星人可能[完全不同](https://writings.stephenwolfram.com/2021/11/the-concept-of-the-ruliad/#alien-views-of-the-ruliad)。

## 神经网络

好吧，那么我们用于[图像识别](https://writings.stephenwolfram.com/2015/05/wolfram-language-artificial-intelligence-the-image-identification-project/)等任务的典型模型实际上是如何工作的呢？目前最流行且最成功的方法使用[神经网络](https://reference.wolfram.com/language/guide/NeuralNetworks.html)。神经网络是在 20 世纪 40 年代发明的，其形式与今天的使用非常接近，可以被认为是对[大脑工作方式的简单理想化](https://www.wolframscience.com/nks/notes-10-12--the-brain/)。

人类大脑中有大约 1000 亿个神经元（神经细胞），每个神经元每秒最多能产生一次电脉冲。这些神经元连接成一个复杂的网络，每个神经元都有树状的分支，使其能够向其他数千个神经元传递电信号。在一个粗略的近似中，任何给定的神经元在某一时刻是否产生电脉冲取决于它从其他神经元接收到的脉冲，不同的连接具有不同的“权重”。

当我们“看到一幅图像”时，光子从图像上落在我们眼睛后面的“光感受器”细胞上时，它们会在神经细胞中产生电信号。这些神经细胞连接到其他神经细胞，最终信号通过一系列神经元层。在这个过程中，我们“识别”图像，最终“形成思维”我们“看到一个 2”（最终可能做一些像大声说“两”这样的事情）。

前一节中的“黑匣子”函数是这样一个神经网络的“数学化”版本。它恰好有 11 层（尽管只有 4 个“核心层”）：

![](img/Image00035.jpg)

这种神经网络并没有什么特别“理论推导”的地方；它只是一种在 1998 年作为一种工程构建出来并被发现有效的东西。当然，这与我们描述大脑是通过生物进化过程产生的方式并没有太大不同。

好的，但是这样的神经网络如何“识别事物”呢？关键在于[吸引子的概念](https://www.wolframscience.com/nks/chap-6--starting-from-randomness#sect-6-7--the-notion-of-attractors)。想象一下我们有手写的 1 和 2 的图像：

![](img/Image00036.gif)

我们希望所有的 1 都“被吸引到一个地方”，所有的 2 都“被吸引到另一个地方”。换句话说，如果一幅图像在某种程度上“更接近于 1”，而不是 2，我们希望它最终出现在“1 的位置”，反之亦然。

作为一个简单的类比，假设我们在平面上有某些位置，用点表示（在现实生活中，它们可能是咖啡店的位置）。然后我们可以想象，从平面上的任意点开始，我们总是希望最终到达最近的点（即我们总是去最近的咖啡店）。我们可以通过将平面分成由理想化的“分水岭”分隔的区域（“吸引子盆地”）来表示这一点：

![](img/Image00037.jpg)

我们可以将这看作是实现一种“识别任务”的过程，我们不是在识别给定图像“看起来最像哪个数字”之类的事情，而是直接看一个给定点最接近哪个点。（我们在这里展示的“Voronoi 图”设置将二维欧几里得空间中的点分开；数字识别任务可以被认为是在一个由每个图像中所有像素的灰度级组成的 784 维空间中做类似的事情。）

那么我们如何让神经网络“执行识别任务”呢？让我们考虑这个非常简单的情况：

![](img/Image00038.jpg)

我们的目标是接受一个对应于位置{*x*，*y*}的“输入”，然后将其“识别”为最接近的三个点之一。换句话说，我们希望神经网络计算出一个关于{*x*，*y*}的函数，如下所示：

![](img/Image00039.jpg)

那么我们如何用神经网络实现这一点呢？最终，神经网络是一组理想化的“神经元”连接在一起的集合，通常排列成层，一个简单的例子如下：

![](img/Image00040.jpg)

每个“神经元”实际上被设置为评估一个简单的数值函数。为了“使用”网络，我们只需在顶部输入数字（如我们的坐标*x*和*y*），然后让每一层的神经元“评估它们的函数”并将结果向前传递通过网络——最终在底部产生最终结果：

![](img/Image00041.jpg)

在传统（受生物启发的）设置中，每个神经元实际上具有来自前一层神经元的一定数量的“传入连接”，每个连接被分配一个特定的“权重”（可以是正数或负数）。给定神经元的值由将“前一神经元”的值乘以它们对应的权重，然后相加并加上一个常数——最后应用“阈值化”（或“激活”）函数来确定。在数学术语中，如果一个神经元有输入*x* = {*x*[1]，*x*[2]...}，那么我们计算*f* [*w*。*x* + *b*]，其中权重*w*和常数*b*通常对网络中的每个神经元选择不同；函数*f*通常是相同的。

计算*w*。*x* + *b*只是矩阵乘法和加法的问题。 “激活函数”*f*引入了非线性（最终导致非平凡行为）。通常会使用各种激活函数；在这里我们将使用`[Ramp](http://reference.wolfram.com/language/ref/Ramp.html)`（或 ReLU）：

![](img/Image00042.jpg)

对于我们希望神经网络执行的每个任务（或者等效地，对于我们希望它评估的每个整体函数），我们将有不同的权重选择。（并且——正如我们稍后将讨论的那样——这些权重通常是通过使用机器学习从我们想要的输出示例中“训练”神经网络来确定的。）

最终，每个神经网络只是对应于某个整体数学函数——尽管可能很难写出来。对于上面的例子，它将是：

![](img/Image00043.jpg)

ChatGPT 的神经网络也只是对应于这样一个数学函数——但实际上有数十亿个项。

但让我们回到单个神经元。以下是具有两个输入（代表坐标*x*和*y*）的神经元可以使用各种权重和常数（以及`[Ramp](https://reference.wolfram.com/language/ref/Ramp.html)`作为激活函数）计算的一些函数示例：

![](img/Image00044.jpg)

那么上面的更大网络呢？嗯，这是它计算的内容：

![](img/Image00045.jpg)

它并不完全“正确”，但它接近我们上面展示的“最近点”函数。

让我们看看其他一些神经网络会发生什么。在每种情况下，正如我们稍后将解释的那样，我们使用机器学习来找到最佳的权重选择。然后我们在这里展示这些权重的神经网络计算的内容：

![](img/Image00046.jpg)

更大的网络通常在逼近我们的目标函数时表现更好。在每个吸引子盆地的“中心”位置，我们通常会得到我们想要的确切答案。但是，在[边界处](https://www.wolframscience.com/nks/notes-10-12--memory-analogs-with-numerical-data/)——神经网络“难以做出决定”的地方——情况可能会更加混乱。

通过这种简单的数学风格的“识别任务”，“正确答案”是明确的。但在识别手写数字的问题上，情况就不那么明确了。如果有人写的“2”看起来像“7”等等呢？尽管如此，我们可以询问神经网络如何区分数字——这给出了一个指示：

![](img/Image00047.gif)

我们能“数学上”说出网络是如何做出区分的吗？实际上不行。它只是“做神经网络所做的事情”。但事实证明，这通常似乎与我们人类所做的区分相当一致。

让我们举一个更详细的例子。假设我们有猫和狗的图片。我们有一个[经过训练来区分它们的神经网络](https://writings.stephenwolfram.com/2015/05/wolfram-language-artificial-intelligence-the-image-identification-project/)。以下是它在一些示例上可能会做的事情：

![](img/Image00048.jpg)

现在“正确答案”甚至更不明确了。穿着猫服的狗呢？等等。无论输入什么，神经网络都会生成一个答案。而且，事实证明，它以一种与人类可能做的相当一致的方式来做。正如我上面所说的，这不是我们可以“从第一原则推导出来”的事实。这只是一种经验上被发现在某些领域至少是真实的东西。但这是神经网络有用的一个关键原因：它们以某种方式捕捉到了一种“类似人类”的做事方式。

给自己展示一张猫的图片，并问“为什么那是一只猫？”。也许你会开始说“嗯，我看到了它尖尖的耳朵等等”。但解释你如何认出这张图片是一只猫并不容易。只是你的大脑以某种方式弄清楚了。但对于大脑来说，没有办法（至少目前还没有）“进入内部”并看看它是如何弄清楚的。那么对于（人造）神经网络呢？当你展示一张猫的图片时，看到每个“神经元”做什么是很���接的。但即使要获得基本的可视化通常也是非常困难的。

在我们用于上面“最近点”问题的最终网络中有 17 个神经元。在用于识别手写数字的网络中有 2190 个。而在我们用于识别猫和狗的网络中有 60,650 个。通常很难想象 60,650 维空间的情况。但因为这是一个用于处理图像的网络，它的许多神经元层都组织成数组，就像它正在查看的像素数组一样。

如果我们拿一张典型的猫的图片

![猫](img/Image00049.jpg "猫")

那么我们可以通过一系列衍生图像来表示第一层神经元的状态——其中许多我们可以轻松解释为“没有背景的猫”，或者“猫的轮廓”：

![](img/Image00050.gif)

到了第 10 层，很难解释发生了什么：

![](img/Image00051.gif)

但总的来说，我们可以说神经网络“挑选出某些特征”（也许尖耳朵是其中之一），并使用这些特征来确定图像的内容。但这些特征是否是我们有名称的特征，比如“尖耳朵”？大多数情况下不是。

我们的大脑是否使用类似的特征？大多数情况下我们不知道。但值得注意的是，像我们在这里展示的神经网络的前几层似乎挑选出了图像的一些方面（如物体的边缘），这些方面似乎与我们知道的大脑视觉处理的第一层挑选出的方面相似。

但假设我们想要一个关于神经网络中猫识别的“理论”。我们可以说：“看，这个特定的网络可以做到”——这立即让我们对“这是多么困难的问题”有了一些概念（例如，可能需要多少神经元或层）。但至少目前为止，我们没有办法“描述网络正在做什么”。也许这是因为它真的是计算上不可简化的，除非通过明确追踪每一步，否则没有一般方法可以找到它在做什么。或者可能只是因为我们还没有“弄清楚科学”，并且确定了允许我们总结正在发生的事情的“自然规律”。

当我们讨论使用 ChatGPT 生成语言时，我们将遇到相同类型的问题。而且不清楚是否有方法“总结它在做什么”。但语言的丰富性和细节（以及我们对它的经验）可能使我们比处理图像时走得更远。

## 机器学习和神经网络的训练

到目前为止，我们一直在谈论“已经知道”如何执行特定任务的神经网络。但神经网络如此有用的原因（可能也适用于大脑）不仅在于它们原则上可以执行各种任务，而且可以通过“逐步从示例中训练”来执行这些任务。

当我们制作一个神经网络来区分猫和狗时，我们不需要编写一个明确查找胡须的程序；相反，我们只展示大量猫和狗的示例，然后让网络从中“机器学习”如何区分它们。

关键在于训练后的网络从所展示的特定示例中“泛化”。正如我们之前所见，网络不仅仅是识别所展示的示例猫图像的特定像素模式；相反，神经网络以某种“一般猫的特性”为基础来区分图像。

那么神经网络训练实际上是如何工作的呢？基本上，我们一直在尝试找到使神经网络成功复制我们给定示例的权重。然后我们依赖神经网络以“合理”的方式在这些示例之间“插值”（或“泛化”）。

让我们看一个比上面的最近点问题更简单的问题。让我们尝试让神经网络学习函数：

![](img/Image00052.jpg)

对于这个任务，我们需要一个只有一个输入和一个输出的网络，就像：

![](img/Image00053.jpg)

但是我们应该使用什么权重等等？对于每组可能的权重，神经网络将计算一些函数。例如，这里是它对一些随机选择的权重集的处理结果：

![](img/Image00054.jpg)

是的，我们可以清楚地看到，在这些情况下，它甚至都没有接近我们想要的函数。那么我们如何找到能够复制函数的权重呢？

基本思想是提供大量“输入→输出”示例来“学习”—然后尝试找到能够复制这些示例的权重。这是逐渐增加示例的结果：

![](img/Image00055.jpg)

在这个“训练”的每个阶段，网络中的权重逐渐调整—我们看到最终我们得到了一个成功复制我们想要的函数的网络。那么我们如何调整权重呢？基本思想是在每个阶段看“我们离得到我们想要的函数有多远”—然后以使其更接近的方式更新权重。

为了找出“我们有多远”，我们计算通常称为“损失函数”（有时称为“成本函数”）的东西。在这里，我们使用一个简单的（L2）损失函数，它只是我们得到的值与真实值之间差值的平方和。我们看到随着训练过程的进行，损失函数逐渐减少（遵循不同任务的特定“学习曲线”）—直到我们达到一个点，网络（至少在很大程度上）成功地复制了我们想要的函数：

![](img/Image00056.jpg)

好了，最后一个必要解释的要点是如何调整权重以减少损失函数。正如我们所说，损失函数给出了我们得到的值与真实值之间的“距离”。但是“我们得到的值”在每个阶段都由当前版本的神经网络确定—以及其中的权重。但现在想象一下权重是变量—比如*w[i]*。我们想找出如何调整这些变量的值以最小化依赖于它们的损失。

例如，想象（在实践中使用的典型神经网络的极端简化中）我们只有两个权重*w* [1]和*w* [2]。那么我们可能会有一个损失函数，作为*w* [1]和*w* [2]的函数看起来像这样：

![](img/Image00057.jpg)

数值分析提供了各种技术来找到这种情况下的最小值。但是一个典型的方法就是从我们之前的*w* [1]，*w* [2]开始逐渐沿着最陡下降的路径前进：

![](img/Image00058.jpg)

就像水流向下山一样，这个过程保证的只是最终会到达表面的某个局部最小值（“山湖”）；它很可能不会达到最终的全局最小值。

很难说是否能够找到在“权重景观”上最陡下降的路径。但微积分来拯救。正如我们上面提到的，我们总是可以将神经网络看作计算一个依赖于其输入和权重的数学函数。但现在考虑对这些权重进行微分。事实证明，微积分的链式法则实际上让我们能够“解开”神经网络中各层所做的操作。结果是，我们至少在某种局部近似中可以“反转”神经网络的操作，并逐渐找到最小化与输出相关联的损失的权重。

上面的图片展示了在只有 2 个权重的不切实际简单情况下可能需要做的最小化。但事实证明，即使有更多的权重（ChatGPT 使用了 1750 亿个），仍然可以进行最小化，至少在某种程度上的近似。事实上，围绕 2011 年发生的“深度学习”重大突破与发现相关，发现在某种意义上，当涉及许多权重时，进行（至少近似）最小化可能比涉及相当少的权重更容易。

换句话说，有点反直觉的是，用神经网络解决更复杂的问题可能比解决更简单的问题更容易。这似乎是因为当有很多“权重变量”时，就有一个高维空间，有“许多不同的方向”可以引导我们到达最小值——而当变量较少时，很容易陷入局部最小值（“山湖”），从中没有“出路的方向”。

值得指出的是，在典型情况下，有许多不同的权重集合都会给出几乎相同性能的神经网络。通常在实际的神经网络训练中会做出许多随机选择——导致“不同但等效的解决方案”，就像这些：

![](img/Image00059.jpg)

但是每个“不同的解决方案”都会有至少稍微不同的行为。如果我们要求在我们提供训练示例的区域之外进行“外推”，我们可能会得到截然不同的结果：

![](img/Image00060.jpg)

但哪一个是“正确”的？真的没有办法说。它们都“与观察到的数据一致”。但它们都对应于不同的“固有”思考方式，以“超越常规”的方式去思考该做什么。对我们人类来说，有些可能比其他的“更合理”。

## 神经网络训练的实践和传统

尤其是在过去的十年里，神经网络训练的艺术取得了许多进展。是的，这基本上是一门艺术。有时候——尤其是事后看来——人们可以看到至少有一点“科学解释”来解释正在做的事情。但大多数情况下，这些都是通过试错发现的，逐渐积累了关于如何处理神经网络的重要知识。

有几个关键部分。首先，关于在特定任务中应该使用什么样的神经网络架构的问题。然后是如何获取用于训练神经网络的数据的关键问题。而且越来越多的情况下，我们不是从头开始训练一个网络：相反，一个新网络可以直接整合另一个已经训练好的网络，或者至少可以利用该网络为自己生成更多的训练样本。

人们可能会认为，对于每种特定的任务，都需要不同的神经网络架构。但事实上发现，即使对于看似完全不同的任务，同样的架构通常也能起作用。在某种程度上，这让人想起了[通用计算的概念](https://www.wolframscience.com/nks/chap-11--the-notion-of-computation#sect-11-3--the-phenomenon-of-universality)（以及我的[计算等价原理](https://www.wolframscience.com/nks/chap-12--the-principle-of-computational-equivalence/)），但正如我稍后将讨论的那样，我认为这更多地反映了我们通常试图让神经网络执行的任务是“类似人类”的任务，而神经网络可以捕捉相当一般的“类人过程”。

在神经网络的早期阶段，人们倾向于认为应该“让神经网络尽可能少地做事情”。例如，在[将语音转换为文本](https://reference.wolfram.com/language/ref/SpeechRecognize.html)时，人们认为应该先分析语音，将其分解为音素等。但发现的是，至少对于“类人任务”，通常最好只是尝试在“端到端问题”上训练神经网络，让它“发现”必要的中间特征、编码等。

还有一个想法，即应该将复杂的个别组件引入神经网络中，以使其实际上“明确实现特定的算法思想”。但再次，这在大多数情况下都被证明不值得；相反，最好只处理非常简单的组件，并让它们“自组织”（尽管通常以我们无法理解的方式）来实现（可能是）这些算法思想的等效物。

这并不是说对于神经网络没有相关的“结构性思想”。因此，例如，具有[具有局部连接的 2D 神经元数组](https://reference.wolfram.com/language/ref/ConvolutionLayer.html)在处理图像的早期阶段至少似乎非常有用。并且具有集中于“向后查看序列”的连接模式似乎很有用——正如我们稍后将看到的——在处理诸如人类语言之类的事物时，例如在 ChatGPT 中。

但神经网络的一个重要特征是——就像一般的计算机一样——它们最终只是在处理数据。而当前的神经网络——使用当前的神经网络训练方法——[具体处理数字数组](https://reference.wolfram.com/language/guide/NetEncoderDecoder.html)。但在处理过程中，这些数组可以被完全重新排列和重塑。例如，[我们用于识别上述数字的网络](https://resources.wolframcloud.com/NeuralNetRepository/resources/LeNet-Trained-on-MNIST-Data/)从一个 2D“类似图像”的数组开始，迅速“加厚”到许多通道，然后[“集中”到一个 1D 数组](https://reference.wolfram.com/language/ref/AggregationLayer.html)，最终包含代表不同可能输出数字的元素：

![](img/Image00061.jpg)

但是，好吧，一个人如何能够确定为特定任务需要多大的神经网络呢？这在某种程度上是一门艺术。关键是要知道“任务有多难”。但对于类似人类的任务，通常很难估计。是的，可能有一种系统的方法可以通过计算机“机械地”完成任务。但很难知道是否有可以让人以“类似人类水平”更轻松地完成任务的技巧或捷径。可能需要[列举一个巨大的游戏树](https://writings.stephenwolfram.com/2022/06/games-and-puzzles-as-multicomputational-systems/)来“机械地”玩某个游戏；但可能有一种更简单（“启发式”）的方法来实现“人类水平的游戏”。

当处理微小的神经网络和简单的任务时，有时可以明确看到自己“无法从这里到达那里”。例如，以下是在上一节中使用几个小型神经网络时似乎能够完成的任务的最佳结果：

![](img/Image00062.jpg)

如果神经网络太小，我们发现它就无法复现我们想要的功能。但是在某个大小以上，只要训练足够长时间，提供足够的例子，它就没有问题。顺便说一句，这些图片展示了神经网络传说的一部分：如果中间有一个“挤压”，强制所有东西通过较小数量的中间神经元，通常可以用较小的网络来解决问题。（值得一提的是，“无中间层”或所谓的“[感知器](https://en.wikipedia.org/wiki/Perceptron)”网络只能学习基本线性函数，但只要有一个中间层，原则上就可以任意精确地逼近任何函数，至少如果有足够多的神经元，尽管为了使其可行地训练，通常会有某种[正则化或归一化](https://reference.wolfram.com/language/ref/BatchNormalizationLayer.html)。）

好的，假设我们已经确定了某种神经网络架构。现在问题是获取用于训练网络的数据。许多关于神经网络以及机器学习的实际挑战集中在获取或准备必要的训练数据上。在许多情况下（“监督学习”），人们希望获得输入的明确示例以及预期的输出。因此，例如，人们可能希望对图像进行标记，标记它们的内容或其他属性。也许人们将不得不明确地经过一番努力进行标记。但很多时候，事实证明可以依靠已经完成的工作，或将其用作某种代理。因此，例如，人们可能会使用网络上提供的图像的 alt 标签。或者，在不同的领域，人们可能会使用为视频创建的闭路字幕。或者—对于语言翻译训练—人们可能会使用存在于不同语言中的网页或其他文档的平行版本。

你需要展示多少数据给神经网络来训练它完成特定任务？再次强调，从第一原则出发很难估计。当然，通过使用“迁移学习”将已经在另一个网络中学习过的重要特征列表等内容“迁入”，可以大大减少需求量。但通常神经网络需要“看到很多例子”才能训练良好。对于一些任务来说，神经网络传统知识中的一个重要部分是例子可能会非常重复。事实上，一个标准策略就是反复向神经网络展示所有已有的例子。在每个“训练轮次”（或“时代”）中，神经网络至少会处于稍微不同的状态，某种程度上“提醒它”某个特定例子对于让它“记住那个例子”是有用的。（是的，也许这类似于人类记忆中重复的有用性。）

但通常仅仅反复展示同一个例子是不够的。还需要展示例子的变化给神经网络。神经网络传统知识中的一个特点是，这些“数据增强”变化并不需要很复杂就能起到作用。只需用基本图像处理稍微修改图像，就可以使它们在神经网络训练中基本“一样好”。同样地，当用于自动驾驶汽车的实际视频等训练数据用尽时，可以继续在模拟类似视频游戏环境中运行模拟，并从中获取数据，而无需所有实际现实场景的细节。

那么像 ChatGPT 这样的东西呢？它有一个很好的特点，即可以进行“无监督学习”，这样更容易为其提供训练例子。回想一下，ChatGPT 的基本任务是找出如何继续给定的一段文本。因此，要获得“训练例子”，我们只需获取一段文本，将其末尾遮盖，然后将其用作“训练输入”，而“输出”则是完整的、未遮盖的文本。我们稍后会更详细地讨论这一点，但主要观点是，与学习图像内容不同，ChatGPT 不需要“显式标记”；它实际上可以直接从给定的文本例子中学习。

那么，在神经网络中的实际学习过程是怎样的呢？最终，一切都是关于确定哪些权重能最好地捕捉已给出的训练示例。有各种详细的选择和“超参数设置”（所谓的因为权重可以被视为“参数”）可以用来调整如何完成这项工作。有不同的[损失函数选择](https://reference.wolfram.com/language/ref/CrossEntropyLossLayer.html)（平方和、绝对值和等）。有不同的损失最小化方法（在每一步中在权重空间中移动多远等）。然后还有诸如展示多大“批量”示例以获得每个连续估计的损失等问题。是的，人们可以应用机器学习（例如我们在 Wolfram 语言中所做的）来自动化机器学习，并自动设置诸如超参数之类的事物。

但最终，整个训练过程可以通过观察损失如何逐渐减少来描述（如在这个[Wolfram 语言小训练进度监视器中](https://reference.wolfram.com/language/ref/NetTrain.html)）：

![](img/Image00063.jpg)

通常情况下，人们会看到损失在一段时间内减少，但最终会在某个固定值上趋于平缓。如果该值足够小，则可以认为训练是成功的；否则，这可能是需要尝试改变网络架构的迹象。

一个人能否判断“学习曲线”何时趋于平缓？就像许多其他事情一样，似乎存在依赖于所使用的神经网络大小和数据量的近似[幂律缩放关系](https://arxiv.org/pdf/2001.08361.pdf)。但总的结论是，训练神经网络很困难，需要大量的计算工作。作为一个实际问题，其中绝大部分工作是在数字数组上进行操作，这正是 GPU 擅长的——这就是为什么神经网络训练通常受到 GPU 可用性的限制。

未来，是否会有根本更好的方法来训练神经网络，或者一般来说做神经网络所做的事情？我几乎可以肯定，是的。神经网络的基本思想是利用大量简单（基本上相同）的组件创建一个灵活的“计算结构”，并使这个“结构”能够逐步修改以从示例中学习。在当前的神经网络中，基本上是使用微积分的思想——应用于实数——来进行这种逐步修改。但越来越清楚的是，高精度的数字并不重要；即使使用当前的方法，8 位或更少可能足够。

使用[细胞自动机等计算系统](https://www.wolframscience.com/nks/chap-2--the-crucial-experiment#sect-2-1--how-do-simple-programs-behave)基本上在许多个体位上并行运行，从来没有清楚过[如何进行这种渐进式修改](https://content.wolfram.com/uploads/sites/34/2020/07/approaches-complexity-engineering.pdf)，但没有理由认为这是不可能的。 事实上，就像“2012 年的深度学习突破”一样，也许在更复杂的情况下，这种渐进式修改实际上会比简单情况更容易。

神经网络——或许有点像大脑——被设置为具有基本固定的神经元网络，修改的是它们之间连接的强度（“权重”）。 （也许至少在年轻的大脑中，完全新的连接也可以增长。）但是，虽然这对生物学来说可能是一个方便的设置，但目前还不清楚这是否接近我们需要实现功能的最佳方式。 而且，涉及到类似于渐进式网络重写的东西（也许让人想起我们的[物理项目](https://www.wolframphysics.org/)）最终可能会更好。

但即使在现有神经网络框架内，目前存在一个关键限制：神经网络训练目前是基本上顺序进行的，每批示例的影响被传播回来更新权重。 而且，即使考虑到 GPU，当前计算机硬件——在训练过程中，大部分神经网络大部分时间都是“空闲”的，只有一部分在更新。 从某种意义上说，这是因为我们当前的计算机往往具有与其 CPU（或 GPU）分开的内存。 但在大脑中，这可能是不同的——每个“记忆元素”（即神经元）也可能是一个潜在的活跃计算元素。 如果我们能够这样设置未来的计算机硬件，可能会更有效地进行训练。

## “当然，足够大的网络可以做任何事情！”

像 ChatGPT 这样的东西的能力看起来非常令人印象深刻，以至于人们可能会想象，如果能够“继续下去”并训练越来越大的神经网络，那么它们最终将能够“做任何事情”。 如果一个人关心的是那些对立即人类思维可获得的事物，那么这种情况很可能是真实的。 但是过去几百年科学的教训是，有些事情可以通过正式过程找出，但并不容易立即被人类思维所理解。

非平凡数学就是一个很好的例子。但一般情况下，真正的问题是计算。最终问题在于[计算不可简化性现象](https://www.wolframscience.com/nks/chap-12--the-principle-of-computational-equivalence#sect-12-6--computational-irreducibility)。有些计算可能需要很多步骤才能完成，但实际上可以“简化”为相当直接的事情。但计算不可简化性的发现意味着这并不总是奏效。相反，有一些过程——可能类似于下面的过程——需要基本上追踪每个计算步骤才能弄清楚发生了什么：

![](img/Image00064.jpg)

我们通常用大脑做的事情可能是特意选择的，以避免计算不可简化性。在大脑中进行数学运算需要特殊的努力。而在大脑中“思考”任何非平凡程序的运行步骤在实践中几乎是不可能的。

当然，我们有计算机。有了计算机，我们可以轻松地进行长时间的、计算不可简化的事情。关键在于，通常没有这方面的捷径。

是的，我们可以记住许多特定计算系统中发生的事例。也许我们甚至能看到一些（“计算可简化的”）模式，使我们能够进行一些泛化。但关键在于，计算不可简化性意味着我们永远无法保证不会发生意外——只有通过明确进行计算，你才能知道在任何特定情况下实际发生了什么。

最终，学习能力和计算不可简化性之间存在根本的张力。学习实际上涉及[通过利用规律来压缩数据](https://www.wolframscience.com/nks/chap-10--processes-of-perception-and-analysis/)。但计算不可简化性意味着最终存在着规律性的限制。

作为一个实际问题，人们可以想象将小型计算设备——比如细胞自动机或图灵机——构建到像神经网络这样的可训练系统中。实际上，这样的设备可以作为神经网络的良好“工具”——就像[Wolfram|Alpha 可以成为 ChatGPT 的好工具](https://writings.stephenwolfram.com/2023/01/wolframalpha-as-the-way-to-bring-computational-knowledge-superpowers-to-chatgpt/)。但是计算不可简化性意味着我们不能指望“进入”这些设备并让它们学习。

或者换个说法，能力和可训练性之间存在着最终的权衡：你希望系统充分利用其计算能力，它就会展现出计算不可简化性，而可训练性就会降低。而基本上可训练性越强，它就越无法进行复杂的计算。

（对于当前的 ChatGPT 系统来说，情况实际上更加极端，因为用于生成每个输出标记的神经网络是一个纯“前馈”网络，没有循环，因此无法进行任何具有非平凡“控制流”计算。）

当然，人们可能会想知道是否能够进行不可约计算实际上很重要。事实上，在人类历史的大部分时间里，这并不特别重要。但我们现代的技术世界是建立在至少利用数学计算的工程基础上的，而且越来越多地也利用更一般的计算。如果我们看看自然界，它充满了不可约计算——我们正在逐渐理解如何模拟并利用它们来实现我们的技术目的。

是的，神经网络当然可以注意到自然界中我们可能也可以轻松注意到的“无辅助人类思维”中的规律。但是，如果我们想解决数学或计算科学范围内的问题，神经网络是做不到的——除非它有效地“作为工具”使用一个“普通”的计算系统。

但所有这些可能会让人感到困惑。过去有很多任务，包括写文章，我们认为计算机在某种程度上“基本上太难了”。现在我们看到像 ChatGPT 这样的系统完成这些任务，我们突然认为计算机必须变得更加强大——特别是超越了它们已经基本能够做到的事情（比如逐步计算细胞自动机等计算系统的行为）。

但这并不是正确的结论。计算上不可约的过程仍然是计算上不可约的，对于计算机来说仍然是基本困难的，即使计算机可以轻松计算它们的各个步骤。我们应该得出的结论是，我们人类可以做到的任务，比如写文章，我们以前认为计算机无法做到，实际上在某种意义上比我们想象的要容易一些。

换句话说，神经网络能够成功地写一篇文章的原因是因为写一篇文章事实上是一个比我们想象的“计算上更浅”的问题。在某种意义上，这使我们更接近于“拥有一个理论”，解释我们人类如何做到像写文章这样的事情，或者一般地处理语言。

如果你有一个足够大的神经网络，那么，是的，你可能能够做任何人类可以轻松做到的事情。但你不会捕捉到自然界一般可以做到的事情——或者我们从自然界中制造的工具可以做到的事情。正是这些工具的使用——无论是实际的还是概念上的——在近几个世纪里使我们超越了“纯粹无助的人类思维”所能接触到的边界，并为人类目的捕捉到更多物理和计算宇宙中的东西。

## 嵌入的概念

神经网络——至少在它们当前的设置中——基本上是基于数字的。因此，如果我们要用它们来处理类似文本这样的东西，我们需要一种方法来用数字来[表示我们的文本](https://reference.wolfram.com/language/guide/NetEncoderDecoder.html)。当然，我们可以开始（基本上就像 ChatGPT 一样）只是给字典中的每个单词分配一个数字。但有一个重要的想法——比如 ChatGPT 中的核心想法——超越了这一点。这就是“嵌入”的概念。人们可以将嵌入看作是一种尝试用一系列数字来表示某物的“本质”的方式，具有“附近的事物”由附近的数字表示的属性。

因此，例如，我们可以将单词嵌入视为试图在一种“意义空间”中[展示单词](https://reference.wolfram.com/language/ref/FeatureSpacePlot.html)，在这个空间中，某种程度上“意义相近”的单词会在嵌入中附近出现。实际使用的嵌入——比如在 ChatGPT 中——往往涉及大量的数字列表。但如果我们投影到 2D，我们可以展示单词如何被嵌入布局：

![](img/Image00065.gif)

是的，我们看到的确实很好地捕捉了典型的日常印象。但是我们如何构建这样的嵌入呢？大致的想法是查看大量文本（这里是来自网络的 50 亿个单词），然后看看不同单词出现在其中的“环境”有多相似。因此，例如，“鳄鱼”和“鳄鱼”通常会几乎可以互换地出现在其他类似的句子中，这意味着它们会在嵌入中附近放置。但“萝卜”和“老鹰”不太可能出现在其他类似的句子中，因此它们会在嵌入中远离放置。

但是，如何使用神经网络实际实现这样的东西呢？让我们首先讨论不是针对单词而是针对图像的嵌入。我们希望找到一种方法，以列表的数字来表征图像，这样“我们认为相似的图像”被分配相似的数字列表。

我们如何判断是否应该“认为图像相似”呢？嗯，如果我们的图像是手写数字，我们可能会“认为两个图像相似”如果它们是相同的数字。早些时候，我们讨论过一个训练用于识别手写数字的神经网络。我们可以将这个神经网络看作是设置成在最终输出中将图像放入 10 个不同的箱子中，每个箱子代表一个数字。

但是如果我们在最终“这是一个‘4’”决定被做出之前“拦截”神经网络内部发生的事情呢？我们可能期望在神经网络内部有一些数字，用来表征图像“大部分像 4 但有点像 2”之类的特征。这个想法是挑选这样的数字用作嵌入的元素。

所以这里是概念。我们不是直接试图表征“哪个图像靠近哪个图像”，而是考虑一个明确定义的任务（在这种情况下是数字识别），我们可以获得明确的训练数据，然后利用这个事实，神经网络在执行这个任务时隐含地必须做出“接近决策”。因此，我们不需要明确讨论“图像的接近程度”，我们只是在讨论图像代表什么数字这个具体问题，然后“交给神经网络”隐含地确定这对“图像的接近程度”意味着什么。

那么这种方法在数字识别网络中是如何更详细地工作的呢？我们可以将网络看作由 11 个连续层组成，我们可以用这种方式图标化地总结（激活函数显示为单独的层）：

![](img/Image00066.jpg)

在开始时，我们将实际图像（由像素值的 2D 数组表示）输入到第一层中。而在最后一层，我们得到一个包含 10 个值的数组，我们可以认为这个数组表示网络对图像与数字 0 到 9 的对应关系的“确定程度”。

输入图像 ![](img/Image00067.jpg) ，那最后一层神经元的值是：

![](img/Image00068.jpg)

换句话说，到这一点，神经网络“非常确定”这个图像是一个 4—为了实际得到输出“4”，我们只需挑选出具有最大值的神经元的位置。

但如果我们再往前看一步呢？网络中的最后一个操作是所谓的[softmax](https://reference.wolfram.com/language/ref/SoftmaxLayer.html)，它试图“强制确定性”。但在应用之前，神经元的值是：

![](img/Image00069.jpg)

代表“4”的神经元仍然具有最高的数值。但其他神经元的值也包含信息。我们可以期望这些数字列表在某种意义上可以用来表征图像的“本质”，从而提供我们可以用作嵌入的东西。因此，例如，这里的每个 4 都有略微不同的“签名”（或“特征嵌入”）—都与 8 完全不同：

![](img/Image00070.jpg)

这里我们基本上使用 10 个数字来描述我们的图像。但通常最好使用更多的数字。例如，在我们的数字识别网络中，我们可以通过访问前一层获得一个包含 500 个数字的数组。这可能是一个合理的“图像嵌入”数组。

如果我们想要对手写数字的“图像空间”进行明确的可视化，我们需要“降低维度”，有效地将我们得到的 500 维向量投影到，比如，3D 空间中：

![](img/Image00071.gif)

我们刚刚讨论了基于识别图像相似性有效地创建图像的表征（从而嵌入）的内容，通过确定（根据我们的训练集）它们是否对应于相同的手写数字。如果我们有一个训练集，可以更一般地对图像执行相同的操作，识别出每个图像属于哪种 5000 种常见对象（猫、狗、椅子，...）。通过这种方式，我们可以创建一个由我们对常见对象的识别“锚定”的图像嵌入，但然后根据神经网络的行为“泛化”。关键在于，只要这种行为与我们人类感知和解释图像的方式一致，这将最终成为一个对我们“看起来正确”的嵌入，并且在实践中对执行“类似人类判断”的任务非常有用。

好的，那么我们如何遵循相同的方法来为单词找到嵌入？关键是从一个关于单词的任务开始，我们可以很容易进行训练。标准的这种任务是“单词预测”。想象一下，我们得到了“the ___ cat”。根据大量文本语料库（比如，网络的文本内容），不同可能“填空”的单词的概率是多少？或者，另一种情况是，给定“___ black ___”，不同“两侧单词”的概率是多少？

我们如何为神经网络设置这个问题？最终，我们必须用数字来表达一切。一种方法就是为英语中的大约 50,000 个常见单词中的每一个分配一个唯一的数字。因此，例如，“the” 可能是 914，而“cat”（前面带有空格）可能是 3542。 （这些是 GPT-2 实际使用的数字。）因此，对于“the ___ cat”问题，我们的输入可能是 {914, 3542}。输出应该是什么样的呢？嗯，它应该是一个包含大约 50,000 个数字的列表，有效地给出每个可能的“填充”单词的概率。再一次，为了找到一个嵌入，我们希望在神经网络“达成结论”之前“拦截”其“内部”，然后获取出现在那里的数字列表，并且我们可以将其视为“表征每个单词”。

好的，那么这些特征看起来是什么样子的呢？在过去的 10 年里，已经开发了一系列不同的系统（[word2vec](https://resources.wolframcloud.com/NeuralNetRepository/resources/ConceptNet-Numberbatch-Word-Vectors-V17.06/)，[GloVe](https://resources.wolframcloud.com/NeuralNetRepository/search/?i=GloVe)，[BERT](https://resources.wolframcloud.com/NeuralNetRepository/search/?i=BERT)，[GPT](https://resources.wolframcloud.com/NeuralNetRepository/resources/GPT2-Transformer-Trained-on-WebText-Data/)，...），每个系统都基于不同的神经网络方法。但最终，它们都将单词通过数百到数千个数字的列表来表征。

在它们的原始形式中，这些“嵌入向量”并不具有信息量。例如，这里是 GPT-2 为三个特定单词生成的原始嵌入向量：

![](img/Image00072.jpg)

如果我们像测量这些向量之间的距离这样的事情，那么我们可以找到词语之间的“接近度”。稍后我们将更详细地讨论这些嵌入的“认知”意义。但现在的主要观点是，我们有一种有用的方法，可以将单词转化为“神经网络友好”的数字集合。

但实际上，我们可以进一步，不仅仅通过数字集合来表征单词；我们也可以对单词序列，甚至整个文本块进行这样的操作。在 ChatGPT 内部，它就是这样处理事情的。它获取到目前为止的文本，并生成一个嵌入向量来代表它。然后，它的目标是找到可能出现的下一个单词的概率。它将其答案表示为一系列数字，这些数字基本上给出了大约 50,000 个可能单词的概率。

（严格来说，ChatGPT 不处理单词，而是[处理“标记”](https://platform.openai.com/tokenizer)—方便的语言单位，可能是整个单词，也可能只是像“pre”或“ing”或“ized”这样的部分。使用标记使 ChatGPT 更容易处理罕见、复合和非英语单词，有时，好或坏，也更容易创造新单词。）

## 在 ChatGPT 内部

好的，我们终于准备好讨论 ChatGPT 内部的内容了。是的，最终，它是一个巨大的神经网络—目前是一个拥有 1750 亿个权重的所谓 GPT-3 网络的版本。在许多方面，这是一个与我们讨论过的其他神经网络非常相似的神经网络。但它是一个特别用于处理语言的神经网络。它最显著的特点是一种名为“transformer”的神经网络架构。

在上面讨论的第一个神经网络中，每个神经元在任何给定层上基本上都与前一层的每个神经元连接（至少具有一定的权重）。但是，如果处理具有特定已知结构的数据，这种全连接网络（据推测）可能是过度的。因此，例如，在处理图像的早期阶段，通常使用所谓的[卷积神经网络](https://reference.wolfram.com/language/ref/ConvolutionLayer.html)（“卷积网络”），其中神经元实际上布置在类似于图像中的像素的网格上，并且仅与网格上附近的神经元连接。

变压器的理念是对构成一段文本的标记序列做出至少在某种程度上类似的事情。但是，变压器不是仅仅定义一个固定区域，使得在该区域内可以建立连接，而是引入了“[注意力](https://reference.wolfram.com/language/ref/AttentionLayer.html)”的概念——以及“更多地关注”序列的某些部分的想法。也许有一天，直接启动一个通用神经网络并通过训练进行所有定制会有意义。但至少目前来看，在实践中“模块化”事物似乎至关重要——就像变压器所做的，也可能是我们的大脑所做的。

好的，那么 ChatGPT（或者说，它基于的 GPT-3 网络）实际上是做什么的呢？回想一下，它的总体目标是以“合理”的方式继续文本，基于它从训练中看到的内容（这包括查看来自网络等的数十亿页文本）。因此，在任何给定时刻，它都有一定量的文本—它的目标是提出适当的选择，以添加下一个标记。

它分为三个基本阶段。首先，它获取与迄今为止的文本相对应的标记序列，并找到代表这些标记的嵌入（即一组数字数组）。然后，它以“标准神经网络方式”在这个嵌入上操作，数值“通过”网络中的连续层产生一个新的嵌入（即一个新的数字数组）。然后，它取这个数组的最后部分，并从中生成大约 50,000 个值的数组，这些值转化为不同可能的下一个标记的概率。（是的，碰巧使用的标记数量与英语中常用单词的数量大致相同，尽管只有大约 3000 个标记是完整的单词，其余是片段。）

一个关键点是，这个流水线的每个部分都由一个神经网络实现，其权重由网络的端到端训练确定。换句话说，实际上除了整体架构之外，没有任何部分是“明确设计”的；一切都只是从训练数据中“学习”而来。

然而，架构设置中有很多细节—反映了各种经验和神经网络传说。即使这显然是深入细节了—我认为谈论一些这些细节是有用的，至少可以让人了解构建像 ChatGPT 这样的东西需要付出多少努力。

首先是嵌入模块。这里是 GPT-2 的 Wolfram 语言示意图表示：

![](img/Image00073.jpg)

输入是一个[由*n*个标记组成的向量](https://reference.wolfram.com/language/ref/netencoder/SubwordTokens.html)（如前一节中所示，由 1 到约 50,000 的整数表示）。这些标记中的每一个都被（通过[单层神经网络](https://reference.wolfram.com/language/ref/EmbeddingLayer.html)）转换为一个嵌入向量（对于 GPT-2 为长度 768，对于 ChatGPT 的 GPT-3 为长度 12,288）。同时，还有一个“次要路径”，它获取标记的（整数）位置序列，并从这些整数创建另一个嵌入向量。最后，来自标记值和标记位置的嵌入向量被[相加在一起](https://reference.wolfram.com/language/ref/ThreadingLayer.html) —生成来自嵌入模块的最终嵌入向量序列。

为什么要将标记值和标记位置嵌入向量简单相加？我认为这并没有什么特别的科学依据。只是尝试了各种不同的方法，这种方法似乎有效。而神经网络的传说部分是—在某种意义上—只要所拥有的设置“大致正确”，通常可以通过足够的训练来逐步细化细节，而无需真正需要“在工程层面上理解”神经网络是如何配置自己的。

这是嵌入模块的操作，作用于字符串*hello hello hello hello hello hello hello hello hello hello bye bye bye bye bye bye bye bye bye bye*：

![](img/Image00074.jpg)

每个标记的嵌入向量的元素显示在页面下方，横跨页面我们首先看到一系列“*hello*”嵌入，然后是一系列“*bye*”嵌入。上面的第二个数组是位置嵌入—其看起来有些随机的结构只是“学到的”（在这种情况下是 GPT-2）。

好的，所以在嵌入模块之后是变压器的“主要事件”：一系列所谓的“注意力块”（GPT-2 有 12 个，ChatGPT 的 GPT-3 有 96 个）。这一切都相当复杂，让人想起典型的难以理解的大型工程系统，或者说生物系统。但无论如何，这里是 GPT-2 的单个“注意力块”的示意图表示：

![](img/Image00075.jpg)

在每个这样的注意力块中，有一组“注意力头”（对于 GPT-2 有 12 个，对于 ChatGPT 的 GPT-3 有 96 个）——每个头在嵌入向量中的不同值块上独立操作。 （是的，我们不知道为什么将嵌入向量分割开是个好主意，或者它的不同部分“意味着”什么；这只是那些“发现有效”的事情之一。）

好的，注意头是做什么的？基本上，它们是一种“回顾”标记序列（即迄今为止生成的文本），并以对于找到下一个标记有用的形式“打包过去”。在上面的第一节中，我们讨论了使用 2-gram 概率根据它们的直接前导词选择单词。变压器中的“注意力”机制所做的是允许“注意力”甚至更早的单词——因此潜在地捕捉到动词可以指代在句子中出现在它们之前许多单词的名词的方式。

在更详细的层面上，注意头所做的是重新组合与不同标记相关的嵌入向量中的块，具有一定的权重。因此，例如，在第一个注意力块中的 12 个注意力头（在 GPT-2 中）对于上述“*hello*，*bye*”字符串的“回溯到标记序列开始”的“重新组合权重”模式如下：

![](img/Image00076.jpg)

在被注意力头处理后，生成的“重新加权嵌入向量”（对于 GPT-2 长度为 768，对于 ChatGPT 的 GPT-3 长度为 12,288）通过一个标准的[“全连接”神经网络层](https://reference.wolfram.com/language/ref/LinearLayer.html)。很难理解这一层在做什么。但这里是它使用的 768×768 权重矩阵的绘图（这里是 GPT-2 的）：

![](img/Image00077.gif)

通过 64×64 的移动平均值，一些（类似随机漫步的）结构开始显现：

![](img/Image00078.jpg)

是什么决定了这种结构？最终，这可能是人类语言特征的某种“神经网络编码”。但目前，这些特征可能是相当未知的。实际上，我们正在“打开 ChatGPT 的大脑”（或至少是 GPT-2），发现，是的，里面很复杂，我们并不理解它——尽管最终它产生了可识别的人类语言。

好的，所以经过一个注意力块后，我们得到了一个新的嵌入向量——然后依次通过额外的注意力块传递（对于 GPT-2 共有 12 个；对于 GPT-3 共有 96 个）。每个注意力块都有自己特定的“注意力”和“全连接”权重模式。这里是 GPT-2 中“hello, bye”输入的第一个注意力头的注意力权重序列：

![](img/Image00079.jpg)

这里是完全连接层的（移动平均）“矩阵”：

![](img/Image00080.gif)

有趣的是，即使在不同的注意力块中，这些“权重矩阵”看起来相似，权重大小的分布可能有所不同（并且并非总是高斯分布）：

![](img/Image00081.jpg)

经过所有这些注意力块后，变压器的净效果是什么？基本上是将一系列令牌的原始嵌入转换为最终集合。而 ChatGPT 的特定工作方式是选择这个集合中的最后一个嵌入，并对其进行“解码”，以生成下一个应该出现的令牌的概率列表。

所以这就是 ChatGPT 内部的大致概况。这可能看起来很复杂（主要是因为其中许多不可避免的“工程选择”有些随意），但实际上所涉及的最终元素非常简单。因为最终我们所处理的只是一个由“人工神经元”组成的神经网络，每个神经元都执行着将一组数字输入进行简单操作，然后与特定权重结合的操作。

ChatGPT 的原始输入是一组数字数组（迄今为止令牌的嵌入向量），当 ChatGPT“运行”以生成一个新令牌时，这些数字只是“涟漪”通过神经网络的各层，每个神经元“做自己的事情”并将结果传递给下一层的神经元。没有循环或“回溯”。一切都只是通过网络“前馈”。

这与典型的计算系统（如[图灵机](https://www.wolframscience.com/nks/chap-3--the-world-of-simple-/%20programs/#sect-3-4--turing-machines)）的设置非常不同，其中结果会被相同的计算元素重复“重新处理”。在这里——至少在生成给定输出令牌时——每个计算元素（即神经元）只使用一次。

但在某种意义上，即使在 ChatGPT 中，仍然存在一个“外部循环”重复使用计算元素。因为当 ChatGPT 要生成一个新令牌时，它总是“读取”（即将其作为输入）之前出现的所有令牌序列，包括 ChatGPT 自己之前“写入”的令牌。我们可以将这种设置看作是意味着 ChatGPT 至少在其最外层涉及一个“反馈循环”，尽管每次迭代都明确可见为出现在其生成文本中的令牌。

但让我们回到 ChatGPT 的核心：被反复用来生成每个标记的神经网络。在某种程度上，它非常简单：一整套相同的人工神经元。网络的一些部分只是由（“[全连接](https://reference.wolfram.com/language/ref/LinearLayer.html)”）层组成的神经元，在这些层中，给定层上的每个神经元都与前一层上的每个神经元连接（带有一些权重）。但特别是在其变压器架构中，ChatGPT 具有更多结构的部分，其中只有不同层上的特定神经元相连。 （当然，人们仍然可以说“所有神经元都相连”——但有些只是权重为零。）

此外，在 ChatGPT 中的神经网络中有一些方面，并不是最自然地被认为只是由“同质”层组成。例如——正如上面的标志性摘要所示——在一个注意力块内，有一些地方会“制作”传入数据的“多个副本”，然后每个副本都会通过不同的“处理路径”进行，可能涉及不同数量的层，只有在稍后才重新组合。但是虽然这可能是对正在发生的事情的一个方便的表示，但至少原则上总是可能想象“密集填充”层，只是有一些权重为零。

如果我们看一下通过 ChatGPT 的最长路径，涉及到大约 400 个（核心）层——在某种程度上并不是一个巨大的数字。但是有数百万个神经元——总共有 1750 亿个连接，因此有 1750 亿个权重。有一件事要意识到的是，每当 ChatGPT 生成一个新的标记时，它都必须进行一次涉及所有这些权重的计算。在实现上，这些计算可以在高度并行的数组操作中有些组织“按层”进行，这些操作可以方便地在 GPU 上完成。但是对于每个生成的标记，仍然必须进行 1750 亿次计算（最后还要多一点）——所以，是的，生成一段长文本可能需要一段时间。

但最终，值得注意的是，所有这些操作——尽管它们各自都很简单——却可以一起完成如此出色的“类人”生成文本的工作。必须再次强调（至少就我们所知），没有“终极理论原因”说明为什么类似这样的东西会起作用。事实上，正如我们将讨论的那样，我认为我们必须将这视为一项——可能令人惊讶的——科学发现：在像 ChatGPT 这样的神经网络中，以某种方式能够捕捉到人类大脑在生成语言方面所能做到的本质。

## ChatGPT 的训练

好了，现在我们已经概述了 ChatGPT 在设置后的工作原理。但是它是如何设置的呢？它的神经网络中的所有那 1750 亿个权重是如何确定的呢？基本上，它们是基于由人类撰写的大量文本的巨大语料库进行的大规模训练的结果——在网络上，书籍中等等。正如我们所说，即使考虑到所有这些训练数据，神经网络能够成功产生“类似人类”的文本显然并不明显。而且，再次，似乎需要详细的工程细节才能实现这一点。但是 ChatGPT 的大惊喜和发现是，这是可能的。实际上，一个“仅有”1750 亿个权重的神经网络可以制作出人类撰写的“合理模型”文本。

在现代，有大量由人类撰写的文本以数字形式存在。公共网络上至少有数十亿人类撰写的页面，总共可能有万亿字的文本。如果包括非公开网页，数字可能至少大 100 倍。到目前为止，已经提供了超过 500 万本数字化书籍（大约有 1 亿本曾经出版的书籍），提供了另外约 1000 亿字的文本。甚至没有提到从视频中的语音中衍生的文本。（作为个人比较，[我一生中发表的总产出](https://www.stephenwolfram.com/publications/)大约不到 300 万字，过去[30 年来我写了](https://writings.stephenwolfram.com/2012/03/the-personal-analytics-of-my-life/)约 1500 万字的电子邮件，总共可能打了大约 5000 万字，仅在过去几年里我在[直播中说了](https://www.stephenwolfram.com/livestreams)超过 1000 万字。是的，我将从所有这些内容中训练一个机器人。）

但是，好吧，考虑到所有这些数据，如何从中训练神经网络呢？基本过程与我们在上面的简单示例中讨论的非常相似。您提供一批示例，然后调整网络中的权重，以最小化网络在这些示例上产生的错误（“损失”）。关于从错误“反向传播”的主要昂贵之处在于，每次执行此操作时，网络中的每个权重通常至少会发生微小变化，并且有很多权重需要处理。（实际的“反向计算”通常只比前向计算难一点点。）

使用现代 GPU 硬件，可以轻松并行计算成千上万个示例的批次的结果。但是，当涉及实际更新神经网络中的权重时，当前的方法要求基本上是逐批次进行的。（是的，这可能是实际大脑目前至少在架构上具有优势的地方，因为它们具有结合计算和存储元素。）

即使在我们之前讨论的学习数值函数的看似简单的情况下，我们发现我们经常需要使用数百万个示例才能成功地训练一个网络，至少是从头开始。那么这意味着我们需要多少示例才能训练一个“类似人类语言”的模型呢？似乎没有任何基本的“理论”方法可以知道。但实际上，ChatGPT 成功地在数千亿字的文本上进行了训练。

它被喂入的文本有些是多次，有些是只有一次。但不知何故，它从看到的文本中“得到了所需的内容”。但考虑到这么多文本要学习，它需要多大的网络才能“学得好”呢？同样，我们还没有一个基本的理论方法来说。最终——正如我们将在下面进一步讨论的那样——人类语言和人类通常用它说的内容可能存在一定的“总算法内容”。但接下来的问题是神经网络在基于该算法内容的模型上实现时会有多有效率。同样，我们不知道——尽管 ChatGPT 的成功表明它是相当有效率的。

最后我们可以注意到，ChatGPT 使用了数千亿个权重来完成它的任务——与它所接收的训练数据的总词数（或标记）相当。在某种程度上，也许令人惊讶的是（尽管在 ChatGPT 的较小模拟中也经验观察到），似乎“工作良好的网络大小”与“训练数据大小”是如此相近。毕竟，ChatGPT 内部肯定不是以某种方式“直接存储”所有来自网络、书籍等的文本。因为实际上 ChatGPT 内部是一堆数字——精度略低于 10 位数——这些数字是对所有文本的总体结构的某种分布式编码。

换句话说，我们可以问一下人类语言的“有效信息内容”是什么，以及通常用它说了些什么。有语言示例的原始语料库。然后是 ChatGPT 神经网络中的表示。这种表示很可能远离“算法上最小”表示（如下面我们将讨论的）。但这是一个神经网络可以轻松使用的表示。在这种表示中，训练数据似乎最终没有太多的“压缩”；平均来看，基本上只需要不到一个神经网络权重来携带一个训练数据中的单词的“信息内容”。

当我们运行 ChatGPT 生成文本时，基本上是必须使用每个权重一次。因此，如果有*n*个权重，我们需要大约*n*个计算步骤来完成——尽管在实践中，许多计算步骤通常可以在 GPU 中并行执行。但是，如果我们需要大约*n*个单词的训练数据来设置这些权重，那么根据我们上面所说的，我们可以得出结论，我们将需要大约*n*²个计算步骤来进行网络的训练——这就是为什么，使用当前的方法，人们最终需要谈论数十亿美元的培训工作。

## 超越基础训练

训练 ChatGPT 的大部分工作都花在“展示”大量来自网络、书籍等现有文本的过程中。但事实证明，还有另一个——显然相当重要的——部分。

一旦 ChatGPT 从它展示的原始文本语料库中完成了“原始训练”，内部的神经网络就准备好开始生成自己的文本，继续从提示中生成等等。但是，尽管这些结果通常看起来合理，但它们往往——特别是对于较长的文本——会以一种相当不像人类的方式“偏离”。这不是通过对文本进行传统统计分析就能轻易检测到的事情。但实际阅读文本的人很容易注意到这一点。

[ChatGPT 构建中的一个关键思想](https://openai.com/blog/instruction-following/)是在“被动阅读”诸如网络之类的内容之后再进行另一步：让实际人类积极与 ChatGPT 互动，看看它产生了什么，实际上给予它“如何成为一个好的聊天机器人”的反馈。但神经网络如何利用这些反馈呢？第一步只是让人类评价神经网络的结果。然后建立另一个神经网络模型，试图预测这些评分。但现在这个预测模型可以运行——本质上像一个损失函数——在原始网络上，实际上允许该网络通过给出的人类反馈进行“调整”。实践中的结果似乎对系统成功产生“类似人类”的输出有很大影响。

一般来说，有趣的是，“最初训练”的网络似乎需要很少的“刺激”就能让它有目的地前进。人们可能会认为，为了让网络表现得好像“学到了新东西”，就必须进入并运行训练算法，调整权重等等。

但事实并非如此。相反，似乎只需基本告诉 ChatGPT 一次——作为您给出的提示的一部分——然后它在生成文本时就能成功利用您告诉它的内容。而且再次，这种方法奏效的事实，我认为，是理解 ChatGPT“真正做了什么”以及它与人类语言和思维结构之间关系的重要线索。

这个确实有点像人类：至少在经过所有的预训练之后，你只需告诉它一次，它就能“记住”——至少“足够长时间”以生成一段文本。那么在这种情况下到底发生了什么呢？可能是“你可能告诉它的一切都已经在那里某个地方了”——你只是引导它到正确的位置。但这似乎不太可能。相反，更有可能的是，是的，元素已经在那里了，但具体内容由类似于“这些元素之间的轨迹”之类的东西定义，当你告诉它时，你引入的就是这个。

而且，就像对于人类一样，如果你告诉它一些奇怪和意想不到的完全不符合它已知框架的东西，它似乎无法成功地“整合”这些信息。只有当它基本上是在已有框架的基础上以一种相当简单的方式运行时，它才能“整合”它。

还值得再次指出，神经网络能够“捕捉”的内容不可避免地存在“算法限制”。告诉它“这个转换为那个”等形式的“浅层”规则，神经网络很可能能够很好地表示和复制这些规则——实际上，它从语言中“已知的”将给它一个立即要遵循的模式。但是，尝试为涉及许多潜在计算不可简化步骤的实际“深层”计算提供规则，它就无法工作了。（请记住，在每一步中，它总是在其网络中“向前传递数据”，除了生成新的标记外，永远不会循环。）

当然，网络可以学习特定“不可简化”计算的答案。但是一旦存在组合数量的可能性，这种“查表式”方法就行不通了。因此，是的，就像人类一样，现在是神经网络“伸手”并使用实际计算工具的时候了。（而且，是的，[Wolfram|Alpha](https://www.wolframalpha.com/) 和 [Wolfram Language](https://www.wolfram.com/language/) 是[独特合适的](https://writings.stephenwolfram.com/2023/01/wolframalpha-as-the-way-to-bring-computational-knowledge-superpowers-to-chatgpt/)，因为它们被构建为“谈论世界中的事物”，就像语言模型神经网络一样。）

## 到底是什么让 ChatGPT 起作用？

人类语言——以及生成它所涉及的思维过程——一直似乎代表着一种复杂性的巅峰。事实上，人类大脑——其“仅仅”拥有大约 1000 亿个神经元（也许有 100 万亿个连接）的网络——能够负责这一切似乎有些令人惊讶。也许，人们可能会想象，大脑除了神经元网络之外还有一些未被发现的物理新层面。但现在有了 ChatGPT，我们获得了一项重要的新信息：我们知道，一个纯粹的人工神经网络，具有与大脑神经元数量相当的连接数，能够出奇地成功地生成人类语言。

是的，这仍然是一个庞大且复杂的系统——大约有与当前世界上可用的文字数量相当的神经网络权重。但在某种程度上，仍然很难相信语言的丰富性以及它所能谈论的事物可以被封装在这样一个有限的系统中。部分原因无疑是由于无处不在的现象（最初在[规则 30 的例子](https://www.wolframscience.com/nks/chap-2--the-crucial-experiment#sect-2-1--how-do-simple-programs-behave)中首次显现），即计算过程实际上可以大大放大系统的表面复杂性，即使它们的基本规则很简单。但实际上，正如我们上面讨论的那样，ChatGPT 中使用的神经网络往往是专门构建的，以限制这种现象及与之相关的计算不可简化性，以便使它们的训练更易于理解。

那么，像 ChatGPT 这样的系统是如何在语言方面取得如此大的进展的呢？我认为，基本答案是，语言在某种基本层面上似乎比它看起来要简单。这意味着，即使 ChatGPT 最终的神经网络结构相对直接，它仍能成功地“捕捉到”人类语言及其背后的思维的本质。而且，在训练过程中，ChatGPT 不知何故“隐含地发现”了使这一切成为可能的语言（和思维）中的规律性。

ChatGPT 的成功，我认为，为我们提供了一项基础且重要的科学证据：它表明我们可以期待发现一些全新的“语言法则”——以及有效的“思维法则”。在 ChatGPT 中——作为一个神经网络构建的系统——这些法则最多只是隐含的。但如果我们能够以某种方式使这些法则明确化，就有潜力以更加直接、高效和透明的方式做到 ChatGPT 所做的那些事情。

但是，好吧，这些规则可能是什么样的呢？最终它们必须给我们一些关于语言——以及我们用语言表达的东西——如何组合的指导。稍后我们将讨论“查看 ChatGPT 内部”可能能够给我们一些关于这一点的提示，以及我们从构建计算语言中所知的内容暗示了一条前进的道路。但首先让我们讨论两个长期已知的类似于“语言规律”的例子——以及它们如何与 ChatGPT 的运作相关。

第一个是语言的语法。语言不只是一堆随机的词汇。相反，有（相当）明确的[语法规则](https://www.wolframscience.com/nks/notes-10-12--computer-and-human-languages/)规定了不同类型的词汇如何组合在一起：例如，在英语中，名词可以由形容词前置和动词后置，但通常两个名词不能直接相邻。这种语法结构可以（至少近似地）通过一组规则来捕捉，这些规则定义了如何组合[“解析树”](https://reference.wolfram.com/language/ref/TextStructure.html)：

![](img/Image00082.jpg)

ChatGPT 并没有对这些规则有明确的“知识”。但在其训练中，它以某种方式隐式“发现”了它们，然后似乎擅长遵循这些规则。那么这是如何运作的呢？从“宏观”层面来看并不清楚。但为了获得一些见解，也许看一个更简单的例子会有帮助。

考虑一个由（’s 和)’s 序列组成的“语言”，其[语法规定](https://www.wolframscience.com/nks/notes-7-9--nested-lists/)括号应始终保持平衡，如下所示的解析树：

![](img/Image00083.gif)

我们能训练一个神经网络生成“语法正确”的括号序列吗？神经网络中有各种处理序列的方法，但让我们使用变压器网络，就像 ChatGPT 一样。给定一个简单的变压器网络，我们可以开始将语法正确的括号序列作为训练样本输入。一个微妙之处（实际上也出现在 ChatGPT 生成人类语言的过程中）是，除了我们的“内容标记”（这里是“(”和“)”）之外，我们还必须包括一个“结束”标记，用于指示输出不应继续（即对于 ChatGPT，表示已经到达“故事的结尾”）。

如果我们设置一个只有一个注意力块、8 个头和长度为 128 的特征向量的变压器网络（ChatGPT 也使用长度为 128 的特征向量，但有 96 个注意力块，每个块有 96 个头），那么似乎不可能让它学习太多关于括号语言的知识。但是通过 2 个注意力块，学习过程似乎会收敛——至少在提供了大约 1000 万个例子之后（并且，与变压器网络一般情况一样，提���更多的例子似乎只会降低其性能）。

因此，通过这个网络，我们可以做类似于 ChatGPT 的事情，并询问下一个标记应该是什么的概率——在一个括号序列中：

![](img/Image00084.gif)

在第一种情况下，网络“非常确定”序列不能在这里结束——这是好事，因为如果结束了，括号就会不平衡。然而，在第二种情况下，它“正确地认识到”序列可以在这里结束，尽管它也“指出”可以“重新开始”，放下一个“（”，可能会跟着一个“）”。但是，哎呀，即使经过了大约 40 万个辛苦训练的权重，它说下一个标记是“)”的概率为 15%——这是不正确的，因为这必然会导致括号不平衡。

如果我们要求网络为逐渐变长的（'s）序列提供最高概率的完成情况，我们会得到以下结果：

![](img/Image00085.jpg)

是的，直到一定长度，网络表现得很好。但然后它开始失败。这在像这样的“精确”情况下与神经网络（或者一般的机器学习）是一种相当典型的情况。人类“一眼就能解决的情况”神经网络也能解决。但是需要做一些“更算法化”的情况（例如明确计算括号以查看它们是否关闭），神经网络往往在某种程度上“计算上不够深入”以可靠地执行。 （顺便说一句，即使是完整的当前 ChatGPT 在长序列中也很难正确匹配括号。）

那么这对像 ChatGPT 和英语这样的语言的语法意味着什么呢？括号语言是“严肃的”—更像是一个“算法故事”。但在英语中，根据单词的局部选择和其他提示，能够“猜测”在语法上什么样的内容更合适，这更现实。而且，是的，神经网络在这方面要好得多——尽管也许它可能会错过一些“形式上正确”的情况，嗯，人类也可能会错过。但主要观点是，语言存在整体的句法结构——带有所有这意味着的规律性——在某种意义上限制了神经网络需要学习的“程度”。一个关键的“类似自然科学”的观察是，像 ChatGPT 中的变压器架构这样的神经网络似乎成功地学会了类似嵌套树状句法结构的语法结构，这种结构似乎存在于所有人类语言中（至少在某种程度上）。

语法对语言提供了一种约束。但显然还有更多。像“好奇的电子吃蓝色的理论来换取鱼”这样的句子在语法上是正确的，但不是人们通常会说的东西，如果 ChatGPT 生成了这样的句子，也不会被认为是成功的——因为，嗯，根据其中的单词的正常含义，它基本上是毫无意义的。

但是有没有一种通用的方法来判断一个句子是否有意义？对此并没有传统的整体理论。但可以将 ChatGPT 视为在通过来自网络等地方的数十亿（据推测有意义的）句子进行训练后，隐含地“发展出了一个理论”。

这个理论可能是什么样的？嗯，有一个微小的角落基本上已知了两千年，那就是[逻辑](https://writings.stephenwolfram.com/2018/11/logic-explainability-and-the-future-of-understanding/)。当然，在亚里士多德发现它的三段论形式中，逻辑基本上是一种说法，即遵循某些模式的句子是合理的，而其他句子则不是。因此，例如，说“所有 X 都是 Y。这不是 Y，所以它不是 X”是合理的（如“所有鱼都是蓝色。这不是蓝色，所以它不是鱼。”）。就像人们可以略带幽默地想象亚里士多德通过大量修辞例子（“机器学习风格”）发现了三段论逻辑一样，人们也可以想象在 ChatGPT 的训练中，它将能够通过查看网络上的大量文本等内容“发现三段论逻辑”（是的，虽然因此可以期望 ChatGPT 生成包含基于三段论逻辑等内容的“正确推理”的文本，但当涉及更复杂的形式逻辑时情况就大不相同了——我认为可以期望它在括号匹配方面失败的原因也会导致它在这里失败）。

但除了逻辑这个狭窄的例子之外，如何系统地构建（或识别）甚至可能有意义的文本？是的，有像[Mad Libs^(&reg)](https://en.wikipedia.org/wiki/Mad_Libs)这样使用非常具体的“短语模板”的东西。但不知何故，ChatGPT 隐含地有一种更通用的方法来做到这一点。也许除了“当你有 1750 亿个神经网络权重时，某种方式会发生”之外，我们无法说出如何做到这一点。但我强烈怀疑有一个更简单更强大的故事。

## 意义空间和语义运动规律

我们之前讨论过，在 ChatGPT 中，任何文本片段实际上都被一组数字表示，我们可以将其视为某种“语言特征空间”中某点的坐标。因此，当 ChatGPT 继续一个文本片段时，这相当于在语言特征空间中描绘出一条轨迹。但现在我们可以问，是什么使得这条轨迹对应于我们认为有意义的文本。也许在语言特征空间中有一些“语义运动规律”，定义了——或者至少约束了——点在其中如何移动以保持“有意义性”？

那么这个语言特征空间是什么样的？这里是一个示例，展示了如果我们将这样一个特征空间投影到 2D 中，单词（这里是常见名词）可能会被布置的样子：

![](img/Image00086.jpg)

我们在上面看到了另一个例子，基于代表植物和动物的单词。但两种情况的共同点是“语义相似的单词”被放置在附近。

举个例子，这里展示了不同词性对应的单词是如何排列的：

![](img/Image00087.jpg)

当然，一个给定的单词通常不只有“一个含义”（或者不一定对应于一个词性）。通过查看包含一个单词的句子在特征空间中的排列方式，人们通常可以“区分出”不同的含义——就像这里对于“起重机”这个词的例子一样（鸟还是机器？）：

![](img/Image00088.jpg)

好吧，至少可以认为我们可以将这个特征空间看作是将“意义相近的单词”放在这个空间中靠近的地方。但在这个空间中我们能够识别出什么样的额外结构呢？例如，是否存在某种“平行传输”的概念，反映了空间中的“平坦性”？了解这一点的一种方法是看类比：

![](img/Image00089.jpg)

是的，即使我们投影到 2D，通常至少会有一些“平坦的迹象”，尽管这当然并非普遍可见。

那么轨迹呢？我们可以看看 ChatGPT 的提示在特征空间中遵循的轨迹，然后我们可以看看 ChatGPT 是如何继续的：

![](img/Image00090.jpg)

这里显然没有“几何上明显的运动规律”。这一点一点也不令人惊讶；我们完全预期这将是一个[相当复杂的故事](https://writings.stephenwolfram.com/2021/09/multicomputation-a-fourth-paradigm-for-theoretical-science/#linguistics)。例如，即使存在“语义运动规律”，也不明显会以何种嵌入（或者实际上以何种“变量”）最自然地陈述。

在上面的图片中，我们展示了“轨迹”中的几个步骤——在每一步中，我们选择 ChatGPT 认为最有可能的单词（“零温度”情况）。但我们也可以询问在给定点上下一个可能出现的单词及其概率是什么：

![](img/Image00091.jpg)

在这种情况下，我们看到有一个“高概率词”的“扇形”，在特征空间中似乎沿着一个更或多或少明确的方向前进。如果我们继续前进会发生什么？以下是我们“沿着”轨迹“移动”时出现的连续“扇形”：

![](img/Image00092.jpg)

这是一个 3D 表示，共进行了 40 步：

![](img/Image00093.jpg)

是的，这看起来像一团乱麻，并且并没有特别鼓励人们期望通过经验研究“ChatGPT 内部正在做什么”来确定“类似数学物理的”“语义运动定律”。但也许我们只是在看“错误的变量”（或错误的坐标系），如果我们只看正确的变量，我们会立刻看到 ChatGPT 正在做一些“数学物理简单”的事情，比如遵循测地线。但目前，我们还没有准备好从其“内部行为”中“经验性地解码”ChatGPT 已经“发现”有关人类语言“如何组合”的内容。

## 语义语法和计算语言的力量

产生“有意义的人类语言”需要什么？过去，我们可能认为这只能由人类大脑完成。但现在我们知道，ChatGPT 的神经网络也可以相当体面地完成这项任务。或许这已经是我们能达到的极限了，没有更简单、更易理解的方法可以实现。但我强烈怀疑 ChatGPT 的成功隐含地揭示了一个重要的“科学”事实：有关有意义的人类语言的结构和简单性远比我们所知道的要多，最终可能存在着描述如何组合这种语言的相当简单的规则。

正如我们上面提到的，句法语法规定了词语如何在人类语言中组合在一起，对应不同的词类。但要处理意义，我们需要更进一步。如何做到这一点的一个版本是不仅考虑语言的句法语法，还要考虑语义语法。

就句法而言，我们识别诸如名词和动词之类的事物。但就语义而言，我们需要“更细微的分级”。因此，例如，我们可能识别“移动”的概念，以及“保持其独立于位置的身份的对象”的概念。每个“语义概念”都有无数具体的例子。但就我们的语义语法而言，我们只需有一些基本规则，基本上说“对象”可以“移动”。关于这一切可能如何运作，有很多可以说的（[其中一些我之前已经说过](https://writings.stephenwolfram.com/2016/10/computational-law-symbolic-discourse-and-the-ai-constitution/)）。但在这里我只会满足于简单地提出一些迈向前进的潜在途径。

值得一提的是，即使一句话在语义语法上完全没问题，也不意味着它已经在实践中实现（甚至可能无法实现）。“大象去月球旅行”无疑会“通过”我们的语义语法，但它在我们实际的世界中肯定还没有实现（至少目前还没有）—尽管这绝对适用于虚构世界。

当我们开始谈论“语义语法”时，我们很快就会问：“它的底层是什么？”它假设了什么“世界模型”？句法语法实际上只是关于从单词构建语言。但语义语法必然涉及某种“世界模型”——一种作为“骨架”的东西，语言可以在其上层叠而成。

直到最近，我们可能会想象（人类）语言将是描述我们“世界模型”的唯一通用方式。几个世纪前，开始对特定类型的事物进行形式化，特别是基于数学。但现在有了一种更通用的形式化方法：[计算语言](https://writings.stephenwolfram.com/2019/05/what-weve-built-is-a-computational-language-and-thats-very-important/)。

是的，这是我在四十多年的时间里的大项目（现在体现在[Wolfram 语言](https://www.wolfram.com/language/)中）：开发一个精确的符号表示，可以尽可能广泛地谈论世界上的事物，以及我们关心的抽象事物。因此，例如，我们有关于[城市](https://reference.wolfram.com/language/ref/entity/City.html)、[分子](https://reference.wolfram.com/language/guide/MolecularStructureAndComputation.html)、[图像](https://reference.wolfram.com/language/guide/ImageRepresentation.html)和[神经网络](https://reference.wolfram.com/language/guide/NeuralNetworkConstruction.html)的符号表示，并且我们内置了关于如何计算这些事物的知识。

经过几十年的工作，我们以这种方式涵盖了许多领域。但在过去，我们并没有特别处理“[日常话语](https://writings.stephenwolfram.com/2016/10/computational-law-symbolic-discourse-and-the-ai-constitution/)”。在“我买了两磅苹果”中，我们可以[很容易地表示](https://reference.wolfram.com/language/ref/entity/Food.html)（并对其进行营养和其他计算）“两磅苹果”。但我们（还）没有“我买了”这样的符号表示。

这一切都与语义语法的概念有关，以及拥有一个通用的符号“构建工具包”来构建概念的目标，这将为我们提供关于什么可以与什么结合以及我们可能转化为人类语言的“流动”规则。

但假设我们有了这种“符号话语语言”。我们可以开始做一些生成“局部有意义文本”的事情。但最终，我们可能会希望获得更多“全局有意义”的结果——这意味着“计算”更多关于实际存在或发生在世界中的事物（或者在某个一致的虚构世界中）。

目前在 Wolfram 语言中，我们拥有大量关于各种事物的内置计算知识。但对于一个完整的符号性论述语言，我们将不得不构建关于世界一般事物的额外“演算法”：如果一个物体从 A 移动到 B，然后从 B 移动到 C，那么它就从 A 移动到 C，等等。

给定一个符号性论述语言，我们可以用它来做出“独立的陈述”。但我们也可以用它来问世界问题，“Wolfram|Alpha 风格”。或者我们可以用它来陈述我们“想要实现的事情”，可能有一些外部执行机制。或者我们可以用它来做出断言——也许是关于实际世界，或者是关于我们正在考虑的某个特定世界，无论是虚构的还是其他。

人类语言基本上是不精确的，至少因为它没有与特定计算实现“捆绑”，其含义基本上仅由其用户之间的“社会契约”定义。但计算语言，由于其本质，具有一定的基本精度——因为最终它指定的内容总是可以在计算机上“无歧义地执行”。人类语言通常可以容忍一定的模糊性。（当我们说“行星”时，是否包括系外行星等？）但在计算语言中，我们必须准确清晰地表达我们所做的所有区分。

在构建计算语言中，通常方便利用普通人类语言来构建名称。但它们在计算语言中的含义必须是精确的，可能或可能不涵盖典型人类语言用法中的某些特定内涵。

如何确定适用于一般符号性论述语言的基本“本体论”？嗯，这并不容易。这也许是为什么自两千多年前亚里士多德做出原始开端以来，这方面的工作很少有所进展。但今天我们对如何以计算方式思考世界有了如此多的了解，这确实有所帮助（而且从我们的[物理项目](https://www.wolframphysics.org/)和[ruliad 的概念](https://writings.stephenwolfram.com/2021/11/the-concept-of-the-ruliad/)中得到“基本形而上学”也不会有害）。

但在 ChatGPT 的背景下，所有这些意味着什么呢？从其训练中，ChatGPT 已经有效地“拼凑”了相当（相当令人印象深刻）数量的语义语法。但它的成功让我们有理由认为，构建更完整的计算语言形式是可行的。与我们迄今为止对 ChatGPT 内部的了解不同，我们可以期望设计计算语言，使其对人类容易理解。

当我们谈论语义语法时，我们可以将其类比为三段论逻辑。起初，三段论逻辑本质上是关于用人类语言表达的陈述的一系列规则。但是（是的，两千年后）[当形式逻辑被发展出来时](https://writings.stephenwolfram.com/2018/11/logic-explainability-and-the-future-of-understanding/#the-history)，三段论逻辑的最初基本构造现在可以用来构建包括现代数字电路操作在内的巨大“形式塔”。因此，我们可以期待，更一般的语义语法也会如此。起初，它可能只能处理简单模式，例如以文本形式表达。但是一旦建立了整个计算语言框架，我们可以期待它将被用于建立“广义语义逻辑”的高塔，使我们能够以精确和正式的方式处理以前从未可及的各种事物，除了通过人类语言的“地面层”以及其模糊性。

我们可以将计算语言的构建和语义语法视为一种代表事物的终极压缩。因为它使我们能够谈论可能性的本质，而无需处理普通人类语言中存在的所有“措辞”。我们可以将 ChatGPT 的巨大优势视为有点类似：因为它也在某种意义上“深入研究”到了可以“以语义上有意义的方式组合语言”的程度，而不必担心不同可能的措辞。

那么，如果我们将 ChatGPT 应用于基础计算语言会发生什么？计算语言可以描述可能发生的事情。但可以添加的是“流行的”感觉，例如基于阅读网络上所有内容。但是，通过计算语言操作意味着像 ChatGPT 这样的东西立即且根本地可以访问潜在不可简化计算的终极工具。这使得它不仅可以“生成合理的文本”，还可以期望解决关于该文本是否实际提出“正确”陈述的问题，或者它应该谈论的内容。

## 那么...ChatGPT 在做什么，为什么有效？

ChatGPT 的基本概念在某种程度上相当简单。从网络、书籍等人类创作文本的大样本开始。然后训练神经网络生成“类似于这样”的文本。特别是，使其能够从“提示”开始，然后继续生成“与其训练内容相似”的文本。

正如我们所看到的，ChatGPT 中的实际神经网络由非常简单的元素组成，尽管有数十亿个。神经网络的基本操作也非常简单，基本上是将从生成的文本中派生的输入“通过其元素一次”（没有任何循环等）传递给每个新单词（或单词的一部分）。

但值得注意的，也是意想不到的是，这个过程可以产生成功地“类似于”网络上或书籍中的文本。它不仅是连贯的人类语言，还“说出”了“遵循其提示”的内容，利用了它“阅读”的内容。它并不总是说出“全局意义上有意义的”东西（或对应于正确计算）—因为（例如，没有[访问“沃尔夫拉姆|阿尔法的计算超能力”](https://writings.stephenwolfram.com/2023/01/wolframalpha-as-the-way-to-bring-computational-knowledge-superpowers-to-chatgpt/)），它只是根据在其训练材料中的东西“听起来像什么”来说出“听起来正确”的东西。

ChatGPT 的具体工程使其非常引人注目。但最终（至少在它可以使用外部工具之前），ChatGPT 只是从其积累的“常识统计”中提取出一些“连贯的文本”。但令人惊讶的是结果多么类似于人类。正如我所讨论的，这表明了至少在科学上非常重要的一点：人类语言（以及其背后的思维模式）在结构上比我们想象的要简单和更“法则化”。ChatGPT 已经隐含地发现了这一点。但我们可能可以通过语义语法、计算语言等明确地揭示它。

ChatGPT 在生成文本方面的表现非常令人印象深刻，结果通常非常类似于我们人类会产生的结果。那么这是否意味着 ChatGPT 像大脑一样工作？它的基础人工神经网络结构最终是根据对大脑的理想化建模的。当我们人类生成语言时，很可能许多方面的情况都非常相似。

当涉及培训（又称学习）时，大脑和当前计算机的不同“硬件”（以及可能一些未开发的算法思想）迫使 ChatGPT 使用一种可能相当不同（在某些方面远不如大脑高效）的策略。还有另一点：与典型的算法计算甚至不同，ChatGPT 内部没有“循环”或“在数据上重新计算”。这不可避免地限制了它的计算能力—即使与当前计算机相比，但绝对是相对于大脑。

不清楚如何“修复这个问题”并仍然保持以合理效率训练系统的能力。但这样做很可能会让未来的 ChatGPT 能够做更多“类似大脑的事情”。当然，大脑也有很多做得不太好的事情，特别是涉及到不可简化计算的事情。对于这些，无论是大脑还是像 ChatGPT 这样的东西都必须寻求“外部工具”—比如[Wolfram Language](https://www.wolfram.com/language/)。

但目前看到 ChatGPT 已经能够做到的事情令人兴奋。在某种程度上，它是一个很好的例子，说明大量简单的计算元素可以做出令人惊讶和意想不到的事情。但它也可能提供了我们两千年来最好的动力，来更好地理解人类语言的核心特征和原则，以及背后的思维过程。

## 谢谢

我已经关注神经网络的发展大约 43 年了，在这段时间里我与许多人交流过关于它们的话题。其中—一些是很久以前的，一些是最近的，还有一些是跨越多年的—有：Giulio Alessandrini, Dario Amodei, Etienne Bernard, Taliesin Beynon, Sebastian Bodenstein, Greg Brockman, Jack Cowan, Pedro Domingos, Jesse Galef, Roger Germundsson, Robert Hecht-Nielsen, Geoff Hinton, John Hopfield, Yann LeCun, Jerry Lettvin, Jerome Louradour, Marvin Minsky, Eric Mjolsness, Cayden Pierce, Tomaso Poggio, Matteo Salvarezza, Terry Sejnowski, Oliver Selfridge, Gordon Shaw, Jonas Sjöberg, Ilya Sutskever, Gerry Tesauro 和 Timothee Verdier。在撰写本文时，我特别要感谢 Giulio Alessandrini 和 Brad Klee 的帮助。


# Wolfram|Alpha 作为将计算知识超能力带入 ChatGPT 的途径

![Wolfram|Alpha 作为将计算知识超能力带入 ChatGPT 的途径](img/Image00094.jpg "Wolfram|Alpha 作为将计算知识超能力带入 ChatGPT 的途径")

## ChatGPT 和 Wolfram|Alpha

当事情突然“奏效”时总是令人惊讶。2009 年我们与[Wolfram|Alpha](https://www.wolframalpha.com/)合作时发生了这种情况。2020 年我们的[物理项目](https://www.wolframphysics.org/)也是如此。现在，[OpenAI](https://openai.com/)的[ChatGPT](https://chat.openai.com/chat)也是如此。

我已经追踪[神经网络技术](https://www.wolfram.com/language/core-areas/machine-learning/)很长时间了（实际上大约 43 年）。即使在过去几年里观察到的发展，我发现 ChatGPT 的表现非常出色。最终，突然间，这是一个可以成功生成几乎任何内容的系统，非常类似于人类可能写的内容。这令人印象深刻，也很有用。而且，正如我在[其他地方讨论的](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)，我认为它的成功可能告诉我们一些关于人类思维本质的非常基本的事情。

但是，虽然 ChatGPT 在自动执行类似人类的重要任务方面取得了显著成就，但并非所有有用的任务都是如此“类似人类”。其中一些更加正式和结构化。事实上，我们文明在过去几个世纪中取得的伟大成就之一就是建立数学、精确科学——最重要的是现在的计算——的范式，并创造了一个与纯人类思维所能实现的完全不同的能力之塔。

我自己多年来一直深度参与计算范式，致力于构建一种[计算语言](https://writings.stephenwolfram.com/2019/05/what-weve-built-is-a-computational-language-and-thats-very-important/)，以尽可能多地以形式符号方式表示世界上的事物。在这样做的过程中，我的目标是构建一个可以“计算辅助”——并增强——我和其他人想要做的事情的系统。我像一个人类一样思考事物。但我也可以立即调用[Wolfram 语言](https://www.wolfram.com/language/)和 Wolfram|Alpha，利用一种独特的“计算超能力”，让我做各种超越人类能力的事情。

这是一种非常强大的工作方式。重点是，这不仅对我们人类重要。对于类似人类的人工智能来说同样重要，甚至更重要——立即赋予它们我们可以称之为计算知识超能力，利用结构化计算和结构化知识的非人类力量。

我们刚刚开始探索这对 ChatGPT 意味着什么。但很明显，可能会发生一些奇妙的事情。Wolfram|Alpha 与 ChatGPT 做的事情非常不同，方式也截然不同。但它们有一个共同的接口：自然语言。这意味着 ChatGPT 可以像人类一样“与”Wolfram|Alpha 交流——Wolfram|Alpha 将从 ChatGPT 获取的自然语言转化为精确的符号计算语言，以便应用其计算知识能力。

数十年来，人们在思考人工智能方面存在着一种“统计方法”（像 ChatGPT 使用的那种）和“符号方法”（实际上是 Wolfram|Alpha 的起点）之间的二分法。但现在——多亏了 ChatGPT 的成功，以及我们在让 Wolfram|Alpha 理解自然语言方面所做的所有工作——终于有机会将这两者结合起来，创造出比任何一种方法都更强大的东西。

## 一个基本示例

在其核心，ChatGPT 是一个生成语言输出的系统，其“遵循”了网络上和书籍以及其他训练中使用的材料中存在的模式。值得注意的是，输出的人类化程度是多么惊人，不仅仅是在小规模上，而且在整篇文章中都是如此。它有连贯的论点，引入了它学到的概念，往往以有趣和意想不到的方式。它产生的内容在语言层面上总是“统计上可信的”。但是——尽管最终结果令人印象深刻——这当然并不意味着它自信地提出的所有事实和计算都一定是正确的。这里有一个我刚刚注意到的例子（是的，ChatGPT 具有固有的内置随机性，所以如果你尝试这个，你可能不会得到相同的结果）：

![点击放大](https://content.wolfram.com/uploads/sites/43/2023/01/swchattech1923img1.png)

这听起来相当令人信服。但事实证明这是错误的，因为 Wolfram|Alpha 可以告诉我们：

![从芝加哥到东京有多远？](https://www.wolframalpha.com/input?i=How+far+is+it+from+Chicago+to+Tokyo%3F)

当然，公平地说，这正是 Wolfram|Alpha 擅长的事情：可以将其转化为可以基于其结构化、策划知识进行的精确计算。但有趣的是，人们可以考虑 Wolfram|Alpha 自动帮助 ChatGPT 完成这项任务。人们可以[通过编程方式](https://reference.wolfram.com/language/ref/WolframAlpha.html)向 Wolfram|Alpha 提问（也可以使用[web API](https://products.wolframalpha.com/api)，等等）：

| ![](img/Image00097.gif) |
| --- |

现在再次向 ChatGPT 提问这个问题，并附上这个结果：

![点击放大](https://content.wolfram.com/uploads/sites/43/2023/01/swchattech1923img4.png)

ChatGPT 非常礼貌地接受了更正，如果你再次提出问题，它会给出正确的答案。显然，处理与 Wolfram|Alpha 的来回可能有更简洁的方式，但很高兴看到即使这种非常直接的纯自然语言方法基本上已经起作用。

但是 ChatGPT 为什么会在第一次就犯这个特定的错误呢？如果它在训练中的某个地方（例如来自网络）看到了芝加哥和东京之间的具体距离，它当然可以做对。但这是一个神经网络可以轻松做到的泛化的情况——比如从许多城市之间的距离的例子中——这是不够的；这里需要一个实际的计算算法。Wolfram|Alpha 处理事情的方式完全不同。它接受自然语言，然后——假设可能的话——将其转换为精确的计算语言（即 Wolfram 语言），在这种情况下：

| ![](img/Image00099.jpg) |
| --- |

[城市](https://reference.wolfram.com/language/ref/entity/City.html)的坐标和[计算它们之间距离的算法](https://reference.wolfram.com/language/ref/GeoDistance.html)随后成为 Wolfram 语言中内置的计算知识的一部分。是的，Wolfram 语言拥有[大量内置的计算知识](https://www.wolfram.com/knowledgebase/)——这是我们数十年工作的结果，精心策划了现在一个庞大的持续更新的数据量，实施（并经常发明）方法和模型和算法——并系统地建立了一个涵盖一切的计算语言。

## 还有一些例子

ChatGPT 和 Wolfram|Alpha 的工作方式非常不同，具有非常不同的优势。但为了了解 ChatGPT 如何利用 Wolfram|Alpha 的优势，让我们讨论一些情况，在这些情况下，单独 ChatGPT 并不能完全做对。ChatGPT 和人类一样，经常在数学方面遇到困难。

![点击放大](https://content.wolfram.com/uploads/sites/43/2023/01/swchattech1923img6.png)

这是一个有趣的、散文风格的回答。但实际结果是错误的：

![3 的 73 次方是多少？](https://www.wolframalpha.com/input?i=What+is+3+to+the+power+73%3F)

但如果 ChatGPT“咨询”Wolfram|Alpha，它当然能够做对。让我们尝试一些稍微复杂的东西：

![点击放大](https://content.wolfram.com/uploads/sites/43/2023/01/swchattech1923img8a.png)

乍一看，这个结果看起来很棒，我会倾向于相信它。然而，事实证明，这是错误的，因为 Wolfram|Alpha 可以告诉我们：

![半轴为 3 和 12 的椭圆周长](https://www.wolframalpha.com/input?i=circumference+of+an+ellipse+with+half+axes+3+and+12)

而且，使用 ChatGPT 做数学作业（而不能请教 Wolfram|Alpha）可能是一个坏主意。它可以给出一个非常合理的答案：

![点击放大](https://content.wolfram.com/uploads/sites/43/2023/01/swchattech1923img10.png)

但是，如果 ChatGPT 没有“真正理解数学”，它基本上不可能可靠地得到正确答案。在这种情况下，答案再次是错误的：

![x² cos(2x)的积分是多少？的积分是多少？")](https://www.wolframalpha.com/input?i=What+is+the+integral+of+x%5E2+cos%282x%29)

尽管如此，ChatGPT 甚至可以编造一个看起来非常合理的“得出答案的方式”（并不是它真正“做到的方式”）。而且，相当迷人（也很有趣）的是，它给出的解释与一个不理解数学的人可能犯的错误非常相似：

![点击放大](https://content.wolfram.com/uploads/sites/43/2023/01/swchattech1923img12.png)

有各种各样的情况会导致“不真正理解事物含义”带来麻烦：

![点击放大](https://content.wolfram.com/uploads/sites/43/2023/01/swchattech1923img13.png)

听起来很有说服力。但是这是不正确的：

![中美洲最大的国家是哪些？](https://www.wolframalpha.com/input?i=What+are+the+largest+countries+in+Central+America%3F)

ChatGPT 似乎在某处正确地学到了这些基础数据，但���并不“理解其含义”足以正确排列这些数字：

![点击放大](https://content.wolfram.com/uploads/sites/43/2023/01/swchattech1923img15.png)

而且，可以想象找到一种方法来“修复这个特定的错误”。但关键是，像 ChatGPT 这样基于生成语言的人工智能系统在需要进行结构化计算时并不适用。换句话说，要“修复”几乎无限数量的“错误”才能弥补即使是 Wolfram|Alpha 的几乎无限小的一部分所能实现的结构化方式。

而且“计算链”越复杂，你越可能需要借助 Wolfram|Alpha 来正确解决问题。在这里，ChatGPT 给出了一个相当混乱的答案：

![点击放大](https://content.wolfram.com/uploads/sites/43/2023/01/swchattech1923img16.png)

而且，正如 Wolfram|Alpha 告诉我们的那样，它的结论是不正确的（因为它在某种程度上已经“知道”）：

![哪些行星卫星比水星大？](https://www.wolframalpha.com/input?i=What+planetary+moons+are+larger+than+Mercury%3F)

每当涉及具体（例如数量化）数据，即使是相当原始的形式，很多时候往往更像是“Wolfram|Alpha 的故事”。以下是一个例子，灵感来自长期以来备受喜爱的 Wolfram|Alpha 测试查询“土耳其有多少只火鸡？”：

![点击放大](https://content.wolfram.com/uploads/sites/43/2023/01/swchattech1923img18.png)

再次，这似乎（起初）完全合理，并且甚至引用了相关来源。然而，事实证明，这些数据基本上只是“凭空捏造”的：

![土耳其的家畜数量](https://www.wolframalpha.com/input?i=Livestock+populations+in+Turkey)

不过，非常好的一点是，ChatGPT 可以轻松地“要求核实事实”：

![点击放大](https://content.wolfram.com/uploads/sites/43/2023/01/swchattech1923img20.png)

现在将这些通过 Wolfram|Alpha API 进行处理：

| ![](img/Image00115.gif) |
| --- |

现在我们可以要求 ChatGPT 修正其原始回答，注入这些数据（甚至以粗体显示注入的位置）：

![点击放大](https://content.wolfram.com/uploads/sites/43/2023/01/swchattech1923img22.png)

当涉及实时（或位置等相关）数据或计算时，“注入事实”的能力尤为重要。ChatGPT 不会立即回答这个问题：

![点击放大](https://content.wolfram.com/uploads/sites/43/2023/01/swchattech1923img23.png)

但这里有一些相关的 Wolfram|Alpha API 输出：

| ![](img/Image00118.gif) |
| --- |

如果我们将这些输入到 ChatGPT 中，它将生成一个漂亮的“论文风格”结果：

![点击放大](https://content.wolfram.com/uploads/sites/43/2023/01/swchattech1923img25a.png)

有时，计算机和人类之间会有一些有趣的互动。以下是一个相当异想天开的问题，向 Wolfram|Alpha 提出的（甚至还会询问您是否想要“软冰淇淋”）：

![一立方光年的冰淇淋有多少卡路里？](https://www.wolframalpha.com/input?i=How+many+calories+are+there+in+a+cubic+light+year+of+ice+cream%3F)

ChatGPT 起初对体积的概念有点困惑：

![点击放大](https://content.wolfram.com/uploads/sites/43/2023/01/swchattech1923img27.png)

但后来似乎“意识到”那么多冰淇淋相当荒谬：

![点击放大](https://content.wolfram.com/uploads/sites/43/2023/01/swchattech1923img28.png)

## 未来之路

机器学习是一种强大的方法，特别是在过去的十年中，它取得了一些显著的成功——其中 ChatGPT 是最新的例子。[图像识别](https://reference.wolfram.com/language/ref/ImageIdentify.html)。[语音转文本](https://reference.wolfram.com/language/ref/SpeechRecognize.html)。[语言翻译](https://reference.wolfram.com/language/ref/TextTranslation.html)。在每一个案例中，以及许多其他案例中，都突破了一个门槛——通常是相当突然的。并且某些任务从“基本上不可能”变为“基本上可行”。

但结果基本上永远不会是“完美的”。也许有些事情有 95%的成功率。但无论如何努力，另外的 5%仍然难以捉摸。对于某些目的，人们可能认为这是一个失败。但关键点在于，通常有各种重要的用例，对于这些用例，95%已经“足够好”。也许是因为输出是某种情况下根本没有“正确答案”的东西。也许是因为人们只是试图呈现一些可能性，然后由人类或系统算法选择或完善。

完全值得注意的是，一个数百亿参数的神经网络，一次生成一个标记的文本，可以做 ChatGPT 可以做的事情。鉴于这种戏剧性的——以及意想不到的——成功，人们可能会认为，如果只是“训练一个足够大的网络”，就能做任何事情。但事实并非如此。关于计算的基本事实——尤其是[计算不可简化](https://www.wolframscience.com/nks/chap-12--the-principle-of-computational-equivalence#sect-12-6--computational-irreducibility)的概念——清楚地表明最终是不可能的。但更重要的是我们在机器学习的实际历史中看到的。会有一个重大突破（就像 ChatGPT）。改进不会停止。但更重要的是，会发现一些成功的用例，这些用例可以成功完成可以做的事情，而不会被不能做的事情所阻碍。

是的，会有很多情况下，“原始的 ChatGPT”可以帮助人们写作，提供建议，或生成对各种文档或互动有用的文本。但当涉及到必须完美的事情时，机器学习并不是解决问题的方法——就像人类也不是一样。

正是我们在上面的例子中看到的。ChatGPT 在“类似人类的部分”表现出色，在那里没有确切的“正确答案”。但当它被“置于困境”需要精确的东西时，它经常失败。但这里的重点是有一个很好的方法来解决这个问题——将 ChatGPT 连接到 Wolfram|Alpha 及其所有计算知识“超能力”。

在 Wolfram|Alpha 内部，一切都被转化为计算语言，并转化为精确的 Wolfram 语言代码，这在某种程度上必须是“完美的”才能可靠地发挥作用。但关键点在于 ChatGPT 不必生成这些。它可以生成其通常的自然语言，然后 Wolfram|Alpha 可以利用其自然语言理解能力将该自然语言翻译为精确的 Wolfram 语言。

在许多方面，有人可能会说 ChatGPT 从未“真正理解”事物；它只是“知道如何生成有用的东西”。但对于 Wolfram|Alpha 来说情况就不同了。因为一旦 Wolfram|Alpha 将某事转换为 Wolfram 语言，它所得到的是一个完整、精确、正式的表示，从中可以可靠地计算事物。不用说，有许多“人类感兴趣的”事物，我们没有[正式的计算表示](https://writings.stephenwolfram.com/2016/10/computational-law-symbolic-discourse-and-the-ai-constitution/) —尽管我们仍然可以用自然语言谈论它们，尽管可能不够精确。对于这些事物，ChatGPT 就只能靠自己，凭借其非常出色的能力。

但就像我们人类一样，有时候 ChatGPT 需要更正式和精确的“动力辅助”。但关键是它不必在表达自己想要的内容时“正式和精确”。因为 Wolfram|Alpha 可以用类似于 ChatGPT 的母语——自然语言与其交流。当它转换为其母语——Wolfram 语言时，Wolfram|Alpha 会负责“添加形式和精确性”。我认为这是一个非常好的情况，具有巨大的实际潜力。

这种潜力不仅仅局限于典型的聊天机器人或文本生成应用程序。它延伸到诸如进行数据科学或其他形式的计算工作（或编程）等事物。在某种意义上，这是一种立即获得两全其美的方式：ChatGPT 的人类化世界和 Wolfram 语言的计算精确世界。

那么 ChatGPT 直接学习 Wolfram 语言呢？是的，它可以这样做，事实上它已经开始了。最终，我完全期待类似 ChatGPT 的东西将能够[直接在 Wolfram 语言中运行](https://writings.stephenwolfram.com/2015/11/how-should-we-talk-to-ais/)，并且在这样做时非常强大。这是一个有趣且独特的情况，这是由于 Wolfram 语言的[计算语言特性](https://writings.stephenwolfram.com/2019/05/what-weve-built-is-a-computational-language-and-thats-very-important/)，它可以广泛地用计算术语讨论世界和其他地方的事物。

沃尔夫拉姆语言的整个概念是将我们人类所思考的事物，能够以计算方式表示和处理。普通的编程语言旨在提供告诉计算机具体要做什么的方法。沃尔夫拉姆语言——作为一种全面的计算语言——涉及的范围远远超出了这个。实际上，它旨在成为一种语言，人类和计算机都可以在其中“以计算方式思考”。

许多世纪以前，当数学符号被发明时，它首次提供了一个简化的媒介，用于“数学思考”事物。它的发明很快导致了代数、微积分，最终导致了各种数学科学的产生。沃尔夫拉姆语言的目标是为计算思维做类似的事情，尽管现在不仅仅是为了人类，还为了启用计算范式所能开启的所有“计算 X”领域。

我自己从拥有沃尔夫拉姆语言作为“思考语言”中受益匪浅，过去几十年来，通过人们通过沃尔夫拉姆语言“以计算方式思考”，取得了许多进展，这是令人欣喜的。那么，ChatGPT 呢？嗯，它也可以做到这一点。至于它将如何运作，我还不太确定。但这不是关于 ChatGPT 学习如何进行沃尔夫拉姆语言已经知道如何做的计算。这是关于 ChatGPT 学习如何像人类一样使用沃尔夫拉姆语言。这是关于 ChatGPT 提出“创造性论文”的类比，但现在不是用自然语言而是用计算语言写成。

我长期以来一直讨论人类撰写的[计算性文章的概念](https://writings.stephenwolfram.com/2017/11/what-is-a-computational-essay/)——这些文章以自然语言和计算语言的混合形式进行交流。现在的问题是 ChatGPT 能否写出这些文章，并能够使用沃尔夫拉姆语言作为传递“有意义沟通”的方式，不仅仅是对人类，还包括对计算机。是的，这里涉及到一个潜在有趣的反馈循环，涉及到沃尔夫拉姆语言代码的实际执行。但关键点在于，沃尔夫拉姆语言代码所代表的“思想”的丰富性和流畅性——与普通编程语言不同——更接近于 ChatGPT 在自然语言中“神奇地”成功处理的内容。

或者换个说法，沃尔夫拉姆语言——就像自然语言一样——是足够表达的，人们可以想象用它为 ChatGPT 编写一个有意义的“提示”。是的，沃尔夫拉姆语言可以直接在计算机上执行。但作为 ChatGPT 的提示，它可以用来“表达一个想法”，其“故事”可以继续。它可能描述一些计算结构，让 ChatGPT 来“即兴创作”关于该结构的[计算性描述](https://writings.stephenwolfram.com/2021/11/the-concept-of-the-ruliad/)，这将根据它通过阅读人类写的许多东西所学到的东西，对人类来说会是“有趣的”。

由于 ChatGPT 意外成功，突然打开了各种令人兴奋的可能性。但目前有一个即时的机会，通过 Wolfram|Alpha 为 ChatGPT 赋予计算知识超能力。这样它不仅可以产生“似乎合理的类人输出”，而且可以利用 Wolfram|Alpha 和沃尔夫拉姆语言中封装的整个计算和知识体系来输出。


## 附加资源

**“ChatGPT 在做什么……为什么有效？”**

在线版本，可运行代码

[wolfr.am/SW-ChatGPT](https://www.wolfr.am/SW-ChatGPT)

**“中学生的机器学习”（作者：斯蒂芬·沃尔夫）**

对机器学习基本概念的简短介绍

[wolfr.am/ML-for-middle-schoolers](https://www.wolfr.am/ML-for-middle-schoolers)

***机器学习简介*（作者：埃蒂安·贝尔纳）**

一本关于现代机器学习的书籍指南，附有可运行的代码

印刷版：[wolfr.am/IML-book](https://www.wolfr.am/IML-book)；在线版：[wolfr.am/IML](https://www.wolfr.am/IML)

**沃尔夫机器学习**

沃尔夫语言中的机器学习能力

[wolfr.am/core-ML](https://www.wolfr.am/core-ML)

**沃尔夫 U 的机器学习**

交互式的机器学习课程，适用于各种不同水平

[wolfr.am/ML-courses](https://www.wolfr.am/ML-courses)

**“我们应该如何与人工智能交流？”（作者：斯蒂芬·沃尔夫）**

2015 年关于用自然语言和计算语言与人工智能交流的短文

[wolfr.am/talk-AI](https://www.wolfr.am/talk-AI)

**沃尔夫语言** [wolfram.com/language](https://www.wolfram.com/language)

**沃尔夫|阿尔法** [wolframalpha.com](https://www.wolframalpha.com)

### 所有资源的在线链接：

[wolfr.am/ChatGPT-resources](https://www.wolfr.am/ChatGPT-resources)
