- en: '[CHAPTER 8](toc.xhtml#c08)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章
- en: '[LangChain: GPT Implementation Framework for Python](toc.xhtml#c08)'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LangChain：Python的GPT实现框架
- en: '[Introduction](toc.xhtml#s427a)'
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: This chapter discusses the use of Large Language Models (LLMs) like ChatGPT’s
    *gpt-3.5-turbo* in Python and introduces a groundbreaking framework called LangChain
    [12]. LangChain allows to implement all the 18 capabilities from the CapabilityGPT
    framework through its specially designed components tailored for working with
    LLMs. It increases the reliability of working with Large Language Models by allowing
    to combine pre-learned models’ knowledge with real-time information. With LangChain,
    LLMs can interact with diverse entities like databases, APIs, cloud platforms,
    and other API-based resources, enabling them to accomplish specific goals. As
    a result, LangChain simplifies and streamlines the process of developing comprehensive
    applications.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了在Python中使用ChatGPT的*gpt-3.5-turbo*等大型语言模型（LLMs），并介绍了一种名为LangChain [12]的开创性框架。LangChain通过其专门设计的组件，为与LLMs合作而量身定制，允许实现CapabilityGPT框架中的所有18种能力。它通过允许将预先学习的模型知识与实时信息相结合，提高了与大型语言模型合作的可靠性。借助LangChain，LLMs可以与数据库、API、云平台和其他基于API的资源等各种实体进行交互，从而实现特定目标。因此，LangChain简化和流畅了开发全面应用的过程。
- en: The chapter provides an explanation of the different components that make up
    the framework. These components include Schema, Models, Data processing, Chains,
    Memory Handling, and Agents. By exploring each of these components, the readers
    will develop a comprehensive understanding of how LangChain functions as a whole.
    To further enhance their understanding, we will also provide practical demonstrations
    of how these components can be utilized through three example use cases. These
    demonstrations will include snippets of Python code and detailed explanations
    of each step involved.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章对构成该框架的不同组件进行了解释。这些组件包括模式、模型、数据处理、链、内存处理和代理。通过探索这些组件中的每一个，读者将全面了解LangChain作为一个整体的功能。为了进一步增进他们的理解，我们还将通过三个示例用例提供这些组件如何被利用的实际演示。这些演示将包括Python代码片段和每个步骤的详细解释。
- en: The first use case delves into how LangChain can benefit legal experts and enhance
    their productivity and success by taking advantage of the creation and transformation
    capabilities of LLMs from the GPT model family. We will present a practical example
    that demonstrates the application of semantic search in helping lawyers find relevant
    previous cases. This enables them to effectively prepare for upcoming trials,
    ultimately improving their performance in the courtroom.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个用例探讨了LangChain如何使法律专家受益，并通过利用GPT模型系列中LLM的创建和转换能力来增强他们的生产力和成功。我们将提供一个实际的例子，演示语义搜索在帮助律师找到相关先前案例方面的应用。这使他们能够有效地为即将到来的审判做准备，最终提高他们在法庭上的表现。
- en: In the second use case, we explore how LangChain enables seamless data retrieval
    from various sources of text data. Specifically, we provide a solution that allows
    automation specialists to engage in a chat conversation about a controller. To
    provide a brief explanation, a controller is essentially a device that manages
    a process to achieve and maintain desired parameters. For example, in a temperature
    control system, a thermostat acts as a controller by monitoring the current temperature
    and adjusting the heating or cooling equipment to maintain a set temperature.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个用例中，我们探讨了LangChain如何实现从各种文本数据源无缝检索数据。具体来说，我们提供了一种解决方案，允许自动化专家参与有关控制器的聊天对话。简单解释一下，控制器本质上是一种管理过程以实现和维持期望参数的设备。例如，在温度控制系统中，恒温器通过监测当前温度并调整加热或冷却设备以保持设定温度来充当控制器。
- en: The solution leverages foundational capabilities described in CapabilityGPT
    framework such as question answering, communication, and summarization. The necessary
    information for this conversation is provided in PDF format, which includes the
    controller manual. LangChain simplifies the process of browsing through and utilizing
    this information.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 该解决方案利用了CapabilityGPT框架中描述的基本能力，如问答、沟通和摘要。这次对话所需的信息以PDF格式提供，其中包括控制器手册。LangChain简化了浏览和利用这些信息的过程。
- en: The final example we propose focuses on real-time price checks, which can greatly
    benefit businesses by keeping them well-informed and competitive. The chapter
    delves into how LangChain’s agents combine the ability of LLMs to reason and the
    tools’ ability to execute and provide accurate and up-to-date price information.
    In this example the agent will leverage various GPT capabilities such as planning,
    assessment, question answering, summarization and communication. This empowers
    businesses to make informed decisions that align with the current market conditions,
    ultimately improving their competitiveness and success.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的最后一个例子集中在实时价格检查，这可以通过使企业及时了解并保持竞争力而极大地使其受益。本章深入探讨了LangChain代理如何结合LLM的推理能力和工具的执行能力，并提供准确和最新的价格信息。在这个例子中，代理将利用各种GPT功能，如规划、评估、问答、摘要和沟通。这使企业能够做出符合当前市场条件的明智决策，最终提高其竞争力和成功。
- en: By the end of this chapter, readers will have a comprehensive understanding
    of how LangChain works and its potential applications. This knowledge will serve
    as a foundation for further self-study, where readers can delve deeper into the
    technical aspects and explore more advanced use cases. Whether you are a business
    leader looking to optimize your operations, an IT professional interested in the
    latest AI technologies, or an AI enthusiast eager to explore the possibilities,
    this chapter will equip you with the knowledge needed to navigate the world of
    LangChain.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章的学习，读者将全面了解LangChain的工作原理及其潜在应用。这些知识将为进一步的自学打下基础，读者可以深入研究技术方面并探索更高级的用例。无论您是希望优化业务运营的商业领袖，对最新人工智能技术感兴趣的IT专业人士，还是渴望探索可能性的人工智能爱好者，本章都将为您提供导航LangChain世界所需的知识。
- en: '[Structure](toc.xhtml#s428a)'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[结构](toc.xhtml#s428a)'
- en: 'In this chapter, the following topics will be covered:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，将涵盖以下主题：
- en: Introducing LangChain
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍LangChain
- en: 'Components of LangChain:'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LangChain的组成部分：
- en: Schema
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模式
- en: Models
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型
- en: Data processing
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据处理
- en: Chains
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 链
- en: Memory Handling
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存处理
- en: Agents
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理
- en: 'Example 1: Preparation helper for lawyers'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例1：律师的准备助手
- en: 'Example 2: Chatting over controller manual'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例2：通过控制器手册进行聊天
- en: 'Example 3: Current price checks using agents'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例3：使用代理进行当前价格检查
- en: '[Introducing LangChain: Unlocking the Potential of Large Language Models](toc.xhtml#s429a)'
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[介绍LangChain：释放大型语言模型的潜力](toc.xhtml#s429a)'
- en: The power of LLMs is derived, among other factors, from their training on massive
    amounts of data. However, it’s important to understand that LLMs are usually not
    up-to-date. This is because they are trained on specific datasets, which may not
    include the most recent information. Despite being trained on immense amounts
    of data, LLMs may still struggle with specialized knowledge in certain areas.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的强大源自于它们在大量数据上的训练，除其他因素外。然而，重要的是要理解，LLM通常不是最新的。这是因为它们是在特定数据集上训练的，这些数据集可能不包括最新信息。尽管经过大量数据的训练，LLM在某些领域的专业知识上仍可能存在困难。
- en: Additionally, they have limitations when it comes to processing large amounts
    of text due to their token restrictions (the maximum token limit determines the
    extent to which the model can process as input and generate as output). In some
    cases, developing applications may require the use of different LLMs depending
    on factors such as budget and requirements. This is where a comprehensive framework
    like LangChain can be beneficial, as it offers a solution that caters to various
    needs and budgets.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于其令牌限制（最大令牌限制决定了模型可以处理的输入范围和生成的输出范围），它们在处理大量文本时存在限制。在某些情况下，开发应用程序可能需要根据预算和要求使用不同的LLM。这就是像LangChain这样的综合框架可以发挥作用的地方，因为它提供了适应各种需求和预算的解决方案。
- en: The LangChain framework is a Python-based tool that enables the creation of
    applications that fully utilize the capabilities of Large Language Models. It
    provides all the necessary components for developing sophisticated applications.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain框架是一个基于Python的工具，可以实现充分利用大型语言模型的能力来创建应用程序。它提供了开发复杂应用程序所需的所有组件。
- en: LangChain goes beyond being a simple interface for language models. It not only
    allows us to utilize different language models but also interacts with a wide
    range of entities such as databases, APIs, cloud platforms, and other API-based
    resources. These interactions expand the capabilities of language models and enable
    the creation of advanced applications.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain不仅仅是语言模型的简单接口。它不仅允许我们利用不同的语言模型，还与各种实体进行交互，如数据库、API、云平台和其他基于API的资源。这些交互扩展了语言模型的能力，并实现了高级应用程序的创建。
- en: LangChain’s main pillars are components, use-case specific chains, and agents.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain的主要支柱是组件、用例特定链和代理。
- en: '**Components**'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组件**'
- en: Using Large Language Models in complex applications involves using specific
    concepts. These include creating templates for prompts, making use of various
    tools, accessing data sources and processing them, managing memory, etc. In LangChain,
    these concepts are called components. Components are classes or functions that
    make it easier to use these features. They are already built and ready for developers
    to use. These components are designed to be easy to implement, whether you are
    using LangChain for your whole project or just certain parts of it.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在复杂应用中使用大型语言模型涉及使用特定概念。这些概念包括为提示创建模板，利用各种工具，访问数据源并处理数据，管理内存等。在LangChain中，这些概念被称为组件。组件是使使用这些功能更容易的类或函数。它们已经构建好，准备供开发人员使用。这些组件旨在易于实现，无论您是将LangChain用于整个项目还是仅用于其中的某些部分。
- en: '**Use-Case Specific Chains**'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用例特定链**'
- en: Chains enable the seamless integration of the above components in a structured
    manner. They are particularly designed to maximize performance in specific scenarios.
    The concept of chaining components together is both straightforward and impactful.
    It greatly simplifies the implementation of complex applications, resulting in
    improved ease of debugging, maintenance, and overall enhancement of applications.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 链以结构化的方式实现了上述组件的无缝集成。它们特别设计用于在特定场景中最大化性能。将组件链接在一起的概念既简单又有影响力。它极大地简化了复杂应用程序的实现，从而提高了调试、维护和应用程序整体增强的便利性。
- en: '**Agents**'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理**'
- en: The framework is designed to be able to work in an agentic way. In everyday
    life, an agent is someone or something that helps achieve a specific goal. In
    LangChain, an agent can be thought of as an LLM that deconstructs a task into
    smaller steps. Then it employs various tools and other LLMs to accomplish the
    main objectives of the task.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 该框架旨在能够以代理方式工作。在日常生活中，代理是帮助实现特定目标的人或事物。在LangChain中，代理可以被视为将任务分解为较小步骤的LLM。然后，它使用各种工具和其他LLM来实现任务的主要目标。
- en: Setting up applications can be a complex and time-consuming task for developers.
    However, with the use of components, chains, and agents, the process becomes significantly
    easier and quicker. By utilizing these tools, developers are able to save valuable
    time and effort, which allows them to focus more on the actual development and
    customization of the application. This leads to improved productivity and faster
    time-to-market.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为开发人员设置应用程序可能是一个复杂和耗时的任务。然而，通过使用组件、链和代理，这个过程变得更加简单和快速。通过利用这些工具，开发人员能够节省宝贵的时间和精力，从而更多地专注于应用程序的实际开发和定制。这将提高生产率并加快上市时间。
- en: '[Components of LangChain](toc.xhtml#s430a)'
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[LangChain的组件](toc.xhtml#s430a)'
- en: To effectively utilize LangChain, it is vital to get familiar with its main
    elements. This section aims to give you a comprehensive overview of these components,
    offering a solid basis for understanding how LangChain works.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要有效地利用LangChain，熟悉其主要元素至关重要。本节旨在全面介绍这些组件，为理解LangChain的工作提供坚实的基础。
- en: '[Schema](toc.xhtml#s431a)'
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[模式](toc.xhtml#s431a)'
- en: Let’s discuss the fundamental data types you will encounter when using LangChain.
    These include text, Chat Messages, Examples, and Documents. In the world of language
    models, text serves as the primary mean of communication. LangChain, being a bridge
    between LLMs and various entities, is specifically designed to prioritize text-based
    interfaces.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论在使用LangChain时会遇到的基本数据类型。这些包括文本、聊天消息、示例和文档。在语言模型的世界中，文本是主要的交流方式。LangChain作为LLMs和各种实体之间的桥梁，专门设计为优先考虑基于文本的界面。
- en: With the rise in popularity of chat interfaces, certain model providers have
    started offering specialized APIs that expect Chat Messages instead of plain text.
    A Chat Message data type consists of two parts - the content field, which is typically
    the text itself, and the user associated with it. Currently, LangChain supports
    users such as System, Human, and AI, introducing different types of chat messages,
    namely, SystemChatMessage (for Master Prompts), HumanChatMessage, and AIChatMessage.
    This format is presently required for all language models from the GPT-3.5-turbo
    and GPT-4 family.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 随着聊天界面的流行，某些模型提供商开始提供专门的API，期望聊天消息而不是纯文本。聊天消息数据类型由两部分组成——内容字段，通常是文本本身，以及与之关联的用户。目前，LangChain支持系统、人类和AI等用户，引入了不同类型的聊天消息，即系统聊天消息（用于主提示）、人类聊天消息和AI聊天消息。这种格式目前适用于GPT-3.5-turbo和GPT-4系列的所有语言模型。
- en: Another data type we might use is Example. It is an input and output pair that
    represents the input to a function and the expected output of it. Examples are
    often incorporated into prompts to better steer the answers provided by the LLM.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能使用的另一种数据类型是示例。它是表示函数输入和预期输出的输入和输出对。示例经常被纳入提示中，以更好地引导LLM提供的答案。
- en: Lastly, LangChain utilizes Documents. A Document comprises unstructured data
    referred to as page content, along with metadata that describes its attributes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，LangChain利用文档。文档包括称为页面内容的非结构化数据，以及描述其属性的元数据。
- en: '[Models I/O](toc.xhtml#s432a)'
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[模型I/O](toc.xhtml#s432a)'
- en: 'The core of LangChain revolves around its models, which serve as the foundation
    for the entire framework. These models provide the necessary elements and building
    blocks for interaction. When discussing working with models we will look into
    three parts: models, prompts and output parsers.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain的核心围绕着其模型展开，这些模型作为整个框架的基础。这些模型为互动提供了必要的元素和构建模块。在讨论与模型的工作时，我们将分为三个部分：模型、提示和输出解析器。
- en: '[Language models](toc.xhtml#s433a)'
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[语言模型](toc.xhtml#s433a)'
- en: 'LangChain provides two types of models: Language Models and Chat Models. Language
    Models take raw input and output a string, while Chat Models take a list of Chat
    Messages labeled as System, AI, and User as input, and generate another chat message
    labeled as AI. The System role guides the model’s behavior by providing high-level
    instructions and context that frame the entire interaction. The AI role interacts
    with the user, responding to queries. Finally, the User role represents the individual
    interacting with the model, giving instructions and engaging in the conversation.
    Both Language Models and Chat Models have their own interface.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain提供两种类型的模型：语言模型和聊天模型。语言模型接受原始输入并输出一个字符串，而聊天模型接受标记为系统、AI和用户的聊天消息列表作为输入，并生成另一个标记为AI的聊天消息。系统角色通过提供高级指令和上下文来引导模型的行为，从而构建整个互动。AI角色与用户互动，回答查询。最后，用户角色代表与模型互动的个体，给出指示并参与对话。语言模型和聊天模型都有自己的接口。
- en: If you need your application to work with both the Language and Chat Models,
    LangChain has developed a solution called the Base Language Model. This interface
    allows you to integrate with the models seamlessly. Additionally, if you want
    to create your own custom wrapper for your own LLM or use one that is not supported
    by LangChain, that option is also supported.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的应用程序需要与语言模型和聊天模型一起工作，LangChain开发了一个名为基础语言模型的解决方案。这个接口允许您无缝地集成模型。此外，如果您想为自己的LLM创建自定义包装器，或者使用LangChain不支持的包装器，也支持这个选项。
- en: If you want to display the response to a user as it is being generated, instead
    of waiting for the entire answer to be generated and sending it all at once, you
    can use special streaming classes. However, please note that streaming response
    is only supported by certain models.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想在生成响应时向用户显示响应，而不是等待整个答案生成并一次性发送，您可以使用特殊的流式类。但请注意，流式响应仅受到某些模型的支持。
- en: '[Prompts](toc.xhtml#s434a)'
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[提示](toc.xhtml#s434a)'
- en: Effectively structured prompts hold the key to unlocking the vast potential
    of GPT models. To adapt to different use cases, there is a range of prompt patterns
    we may employ, such as Instruction Prompt, Instruction Sequence Prompt, Query
    Prompt, Request Prompt, Cooperation Prompt, or other Special Prompts referred
    to in the earlier *[Chapter 5, Advanced Prompt Engineering Techniques](c05.xhtml).*
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的结构化提示是释放GPT模型巨大潜力的关键。为了适应不同的用例，我们可以采用一系列提示模式，例如指令提示、指令序列提示、查询提示、请求提示、合作提示或其他在之前的*[第5章，高级提示工程技术](c05.xhtml)*中提到的特殊提示。
- en: 'To simplify the process of creating prompts and effectively managing different
    parts of them, LangChain introduces prompt templates. These templates serve as
    a structured way to generate prompts consistently. They are essentially strings
    that accept a set of parameters, which are then used by the model for processing.
    Currently, in LangChain, these strings can be composed using two syntaxes: f-strings
    and Jinja.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化创建提示并有效管理它们的不同部分的过程，LangChain引入了提示模板。这些模板作为一种结构化的方式来一致地生成提示。它们本质上是接受一组参数的字符串，然后由模型用于处理。目前，在LangChain中，这些字符串可以使用两种语法来组成：f-strings和Jinja。
- en: 'If you decide to use a Chat Model, specifically a Chat Prompt Template should
    be used. These templates are optimized for Chat Model inputs and differ from raw
    strings, which are passed directly to the language model. In chat messages, each
    message is associated with a role: System, User, or AI. Additionally, there are
    few-shot prompt templates that allow the inclusion of examples in our prompts.
    If the default prompt templates do not cover your specific needs, you can also
    create custom prompt templates. For instance, you might want to create a prompt
    template with dynamic instructions tailored to your language model. In such cases,
    custom prompt templates offer flexibility and customization options.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您决定使用聊天模型，应该使用专门的聊天提示模板。这些模板针对聊天模型输入进行了优化，并与原始字符串不同，原始字符串直接传递给语言模型。在聊天消息中，每条消息都与一个角色相关联：系统、用户或AI。此外，还有少量提示模板，允许在我们的提示中包含示例。如果默认提示模板不符合您的特定需求，您还可以创建自定义提示模板。例如，您可能希望创建一个具有针对您语言模型的动态指令的提示模板。在这种情况下，自定义提示模板提供了灵活性和定制选项。
- en: '[Output parsers](toc.xhtml#s435a)'
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[输出解析器](toc.xhtml#s435a)'
- en: Regardless of whether we are working with Language Models or Chat Models, they
    all generate text as their output. To organize or structure this output in the
    desired format, we make use of output parsers. These output parsers enable us
    to obtain the output in various formats, such as comma-separated list, datetime,
    enum, or JSON.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们是在处理语言模型还是聊天模型，它们都会生成文本作为输出。为了以所需格式组织或结构化这些输出，我们使用输出解析器。这些输出解析器使我们能够以各种格式获取输出，例如逗号分隔列表、日期时间、枚举或JSON。
- en: If you wish to output multiple text fields of your choice, you can utilize the
    Structured Output Parser. Another option is to use the Pydantic (JSON) parser,
    which is more powerful. However, it requires an LLM with sufficient capacity to
    generate well-formed JSON. It is worth noting that within certain model families,
    some models excel at generating JSON while others may not perform as well. For
    instance, models such as *text-davinci-003*, *gpt-3.5-turbo*, or *gpt-4* can consistently
    generate well-structured JSON, whereas *text-curie-001’s* proficiency in this
    regard significantly diminishes.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望输出多个自定义文本字段，可以利用结构化输出解析器。另一个选择是使用更强大的Pydantic（JSON）解析器。但是，它需要具有足够容量的LLM来生成格式良好的JSON。值得注意的是，在某些模型系列中，一些模型擅长生成JSON，而其他模型可能表现不佳。例如，像*text-davinci-003*、*gpt-3.5-turbo*或*gpt-4*这样的模型可以一致地生成结构良好的JSON，而*text-curie-001*在这方面的熟练程度显著降低。
- en: Additionally, there is an auto-fixing parser available. It automatically calls
    another parser if the results from the initial one are incorrect or contain errors.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一个可用的自动修复解析器。如果初始结果不正确或包含错误，它会自动调用另一个解析器。
- en: '[Data connection](toc.xhtml#s436a)'
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[数据连接](toc.xhtml#s436a)'
- en: One of the main challenges in using standalone LLMs is their ability to only
    access data that they were specifically trained on. To overcome this limitation
    and enable them to work with up-to-date information, we must connect them to other
    sources of data. Fortunately, LangChain provides effective components that facilitate
    effortless interaction with diverse data sources (refer back to *[Chapter 4, Architecture
    Patterns enabled by GPT-Models](c04.xhtml),* for an in-depth exploration of A2–A4
    architecture patterns for accessing enterprise-specific data sources).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用独立的LLM的主要挑战之一是它们只能访问它们专门训练过的数据。为了克服这一限制并使它们能够使用最新信息，我们必须将它们连接到其他数据源。幸运的是，LangChain提供了有效的组件，可以轻松地与各种数据源进行交互（详细探讨了用于访问企业特定数据源的A2-A4架构模式，请参阅*[第4章，由GPT模型启用的架构模式](c04.xhtml)*）。
- en: '[Data loaders](toc.xhtml#s437a)'
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[数据加载器](toc.xhtml#s437a)'
- en: An essential aspect of working with external data sources is the use of data
    loaders. These loaders have the important task of loading different types of data
    into a specific structure called the Document. The Document contains both the
    text and the metadata associated with it. Several loaders have been created to
    handle various data sources. These loaders make it convenient to load data from
    for example, text files, CSV files, JSON files, GitBooks, Azure Blob Containers,
    YouTube transcripts, and more. Using these loaders greatly simplifies data loading
    processes.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 与外部数据源一起工作的一个重要方面是使用数据加载器。这些加载器的重要任务是将不同类型的数据加载到称为文档的特定结构中。文档包含文本和与之相关的元数据。已经创建了几个加载器来处理各种数据源。这些加载器使得从文本文件、CSV文件、JSON文件、GitBooks、Azure
    Blob容器、YouTube转录等加载数据变得方便。使用这些加载器极大地简化了数据加载过程。
- en: '[Data transformers](toc.xhtml#s438a)'
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[数据转换器](toc.xhtml#s438a)'
- en: Once you have successfully imported your data into the Document objects, you
    can begin the process of transforming it to suit your application’s needs. One
    common type of transformation is partitioning the document into smaller sections
    called chunks, which align with the context window of your chosen LLM. For example,
    if you intend to use 4 chunks as context and allocate 2000 tokens for the context,
    you may want to divide your documents into chunks of 500 tokens each. For English
    text, 1 token is approximately 4 characters or 0.75 words.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦成功将数据导入文档对象，您可以开始转换过程，以满足应用程序的需求。一种常见的转换类型是将文档分成称为块的较小部分，这些块与您选择的LLM的上下文窗口对齐。例如，如果您打算使用4个块作为上下文，并分配2000个标记用于上下文，您可能希望将您的文档分成每个500个标记的块。对于英文文本，1个标记大约是4个字符或0.75个单词。
- en: Additionally, you have the option to automatically extract features or metadata
    associated with each chunk. These extracted details can then be utilized in various
    tasks such as classification or summarization.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您还可以选择自动提取与每个块相关的特征或元数据。然后可以利用这些提取的细节来执行各种任务，如分类或摘要。
- en: '[Text Embedding Models](toc.xhtml#s439a)'
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[文本嵌入模型](toc.xhtml#s439a)'
- en: Embedding means representing text as vectors and it is another method we can
    consider when transforming Documents. The implementation of this method in LangChain
    is straightforward and efficient. LangChain offers a standard interface that can
    be utilized with various embedding model providers.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入意味着将文本表示为矢量，这是在转换文档时我们可以考虑的另一种方法。LangChain中这种方法的实现是简单而高效的。LangChain提供了一个标准接口，可以与各种嵌入模型提供商一起使用。
- en: It is important to note that different embedding providers may employ different
    techniques for embedding queries and documents. To address this, LangChain adopts
    a similar approach and offers distinct methods for embedding each of them. By
    doing so, LangChain ensures compatibility and flexibility when working with various
    embedding providers, allowing users to seamlessly integrate their preferred embedding
    techniques into their workflow.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，不同的嵌入提供商可能采用不同的嵌入查询和文档的技术。为了解决这个问题，LangChain采用了类似的方法，并为嵌入每个方法提供了不同的方法。通过这样做，LangChain确保了在与各种嵌入提供商合作时的兼容性和灵活性，使用户能够无缝地将他们喜欢的嵌入技术集成到他们的工作流程中。
- en: '[Vector stores](toc.xhtml#s440a)'
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[矢量存储](toc.xhtml#s440a)'
- en: Searching for specific information in a large amount of unstructured text is
    a common use case for LangChain. To tackle this task, one approach is to start
    by breaking down the unstructured text into Document-type chunks. These Documents
    are then converted into lists of numbers (vectors) and stored in a special database
    known as vector store.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在大量非结构化文本中搜索特定信息是LangChain的常见用例。为了解决这个任务，一个方法是首先将非结构化文本分解成文档类型的块。然后将这些文档转换为数字列表（矢量）并存储在一个称为矢量存储的特殊数据库中。
- en: To find the desired information, the query is also converted into a vector representation
    and compared with each vector in the vector store. The selection process involves
    identifying the documents that are most similar to the query. Similarity is calculated
    utilizing metrics such as cosine similarity or Euclidean distance. Cosine similarity
    determines the similarity between two vectors by measuring the cosine of the angle
    between them, while Euclidean distance calculates the straight-line distance between
    two points in space.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到所需的信息，查询也被转换为矢量表示，并与矢量存储中的每个矢量进行比较。选择过程涉及识别与查询最相似的文档。相似性是利用余弦相似度或欧氏距离等度量来计算的。余弦相似度通过测量两个矢量之间的夹角的余弦来确定两个矢量之间的相似性，而欧氏距离则计算空间中两点之间的直线距离。
- en: A vector store acts as a database where the vector representations of Documents
    can be stored together with their metadata. What makes LangChain particularly
    versatile is its compatibility with various open-source vector stores, as well
    as its ability to integrate with different paid vector store providers.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 矢量存储作为一个数据库，可以存储文档的矢量表示和它们的元数据。LangChain的特殊之处在于它与各种开源矢量存储的兼容性，以及它与不同付费矢量存储提供商的集成能力。
- en: '[Retrievers](toc.xhtml#s441a)'
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[检索器](toc.xhtml#s441a)'
- en: Once our similarity search is complete, we can utilize retrievers to retrieve
    the documents that are pertinent to a query. Retriever functionality enables us
    to effectively retrieve relevant documents based on our similarity search results.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的相似性搜索完成，我们可以利用检索器来检索与查询相关的文档。检索器功能使我们能够根据相似性搜索结果有效地检索相关文档。
- en: '[Chains](toc.xhtml#s442a)'
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[链](toc.xhtml#s442a)'
- en: The previously mentioned features can be seen as additional capabilities that
    offer convenience, but the true strength of LangChain lies in its chains. While
    using Language Models alone is suitable for simple applications, the use of multiple
    LLMs or a combination of LLMs and other components in a sequence is essential
    for developing more complex applications. LangChain simplifies this process by
    introducing chains.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 前面提到的功能可以被视为提供便利的附加功能，但LangChain的真正优势在于其链。虽然单独使用语言模型适用于简单的应用程序，但在开发更复杂的应用程序时，使用多个LLM或LLM和其他组件的组合是必不可少的。LangChain通过引入链来简化这个过程。
- en: There are various types of chains. They are particularly designed to quickly
    implement specific use cases, which is why they differ in complexity. The most
    basic type is the LLM Chain, which consists of a prompt template and a Language
    Model. The purpose of this chain is to format the given input values into a prompt,
    execute the LLM using the prepared prompt, and finally return the output.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种类型的链。它们特别设计用于快速实现特定用例，这就是它们在复杂性上的差异所在。最基本的类型是LLM链，它由提示模板和语言模型组成。这个链的目的是将给定的输入值格式化为提示，使用准备好的提示执行LLM，最后返回输出。
- en: Another commonly used chain is the Simple Sequential chain. In this type, a
    series of calls are made to either the same or different Language Models after
    the initial call. This is particularly useful when the output from one call needs
    to be used as the input for another one (refer to C2, D1, and D2 patterns in *[Chapter
    4, Architecture Patterns enabled by GPT-Models](c04.xhtml)* ).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常用的链是简单顺序链。在这种类型中，初始调用后会对同一个或不同的语言模型进行一系列调用。当一个调用的输出需要作为另一个调用的输入时，这是特别有用的（参见*[第4章，由GPT模型实现的架构模式](c04.xhtml)*中的C2、D1和D2模式）。
- en: The RetrievalQA chain is one more popular type, which involves taking a question,
    retrieving relevant documents using a retriever, and then providing those documents
    as context to the LLM for answering the question. There is also the SQL Database
    Chain, which is designed for answering questions over an SQL database, and the
    Summarization Chain, which is used for summarizing multiple documents.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 检索问答链是另一种流行的类型，它涉及接受一个问题，使用检索器检索相关文档，然后将这些文档作为上下文提供给LLM来回答问题。还有SQL数据库链，专门用于回答SQL数据库上的问题，以及摘要链，用于对多个文档进行摘要。
- en: These are just a few examples, but there are many more possibilities. Additionally,
    users have the option to create their own customized chains by subclassing the
    Chain class.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一些例子，但还有许多其他可能性。此外，用户可以通过对Chain类进行子类化来创建自己定制的链。
- en: '[Memory](toc.xhtml#s443a)'
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[内存](toc.xhtml#s443a)'
- en: 'By default, Large Language Models are designed to be stateless. This means
    that each received query is treated independently, without any regard for previous
    queries or conversations. However, in certain applications like virtual assistants,
    it is important to have access to the context of the conversation in order to
    better understand the user’s intent and provide a response that relates to previous
    queries. In order to better comprehend this, let’s explore the following conversation:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，大型语言模型被设计为无状态的。这意味着每个接收到的查询都是独立处理的，不考虑先前的查询或对话。然而，在某些应用程序中，比如虚拟助手，有必要访问对话的上下文，以更好地理解用户的意图并提供与先前查询相关的响应。为了更好地理解这一点，让我们来探讨以下对话：
- en: '*User: What is estimated computing power of a brain?*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*用户: 大脑的估计计算能力是多少？*'
- en: '*AI: The brain’s computing power is immensely complex and difficult to precisely
    quantify, but some rough estimates suggest it could be in range of 10^18 to 10^26
    floating-point operations per second (FLOPS).*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*AI: 大脑的计算能力极其复杂，难以精确量化，但一些粗略估计表明它可能在10^18到10^26浮点运算每秒（FLOPS）的范围内。*'
- en: '*User: Why is it difficult to quantify?*'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*用户: 为什么难以量化？*'
- en: '*AI: The statement „difficult to quantify” is common phrase used to describe
    situation where …*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*AI: “难以量化”的说法是用来描述情况的常见短语，其中…*'
- en: If a user asks about the estimated computing power of the brain, the AI model
    may provide a general answer based on available knowledge. However, when the user
    follows up with a question, the AI may struggle to understand the reference without
    access to the previous interaction.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用户询问大脑的估计计算能力，AI模型可能会根据现有知识提供一个一般性答案。然而，当用户跟进提问时，AI可能会在没有访问先前交互的情况下难以理解引用。
- en: The problem in the example is linked with coreference resolution. Coreference
    resolution involves identifying words in a sentence that refer to the same thing.
    In this case, the words “it” and “computing power” are examples of such words.
    The lack of memory and statelessness of the Language Model resulted in irrelevant
    answers. In order to resolve this problem, it is essential to manage the memory
    of the complete conversation or at least a portion of it. LangChain provides methods
    that assist in this endeavor. These methods provide a range of choices tailored
    for different types of interactions. It is important to consider the token limit
    for each call when passing data, as there may be constraints on the amount of
    information that can be included. For instance, the entire conversation can be
    passed to the next prompt if the interaction is expected to be brief, or a summarized
    version can be used if the interaction is predicted to be lengthy. All these methods
    enable the model to understand and respond to the conversation based on its context,
    thereby improving its capabilities.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 示例中的问题与指代消解有关。指代消解涉及识别句子中指代同一事物的词语。在这种情况下，“it”和“computing power”就是这样的词语。语言模型的缺乏记忆和状态的无关性导致了无关的答案。为了解决这个问题，必须管理完整对话或至少部分对话的记忆是至关重要的。LangChain提供了一些方法来协助解决这个问题。这些方法提供了一系列针对不同类型交互的选择。在传递数据时，考虑每次调用的令牌限制是很重要的，因为可能会有包含的信息量的限制。例如，如果预计交互会很简短，整个对话可以传递给下一个提示，或者如果预计交互会很长，可以使用摘要版本。所有这些方法使模型能够根据上下文理解并回应对话，从而提高其能力。
- en: '[Agents](toc.xhtml#s444a)'
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[代理](toc.xhtml#s444a)'
- en: Sometimes, in order to serve the user, we need to determine the actions based
    on unpredictable input. This is where an agent becomes useful. An agent acts as
    a helpful assistant, not only carrying out tasks on our behalf but also considering
    which subtasks need to be performed and in what order.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，为了为用户服务，我们需要根据不可预测的输入来确定行动。这就是代理变得有用的地方。代理充当一个有用的助手，不仅代表我们执行任务，还考虑需要执行哪些子任务以及以何种顺序执行。
- en: In LangChain, agents can be seen as LLMs that break down tasks into smaller
    steps. They utilize various tools and other LLMs to achieve the ultimate objective
    of the task.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在LangChain中，代理可以被视为将任务分解为较小步骤的LLMs。它们利用各种工具和其他LLMs来实现任务的最终目标。
- en: The first component of an agent is the Orchestration Agent, in simple words
    - “manager” LLM. This LLM handles all the cognitive processes, including devising
    steps, analyzing outputs, adjusting plans if necessary, and determining the next
    course of action.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的第一个组成部分是编排代理，简单来说就是“经理”LLM。这个LLM处理所有认知过程，包括制定步骤、分析输出、必要时调整计划，并确定下一步行动。
- en: Additionally, there are tools available to the agent. These tools can range
    from a simple calculator to more advanced ones like other LLMs, the SERP (Search
    Engine Results Page) API, or the JsonGetValueTool. There are also comprehensive
    toolkits that contain multiple tools of the same category, such as the Zapier
    Toolkit, OpenAPI Agent Toolkit, or JSON Agent Toolkit. Each tool is accompanied
    by a descriptive text tag, which the “manager” LLM uses to understand if the tool
    is suitable for the upcoming step.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，代理还可以使用工具。这些工具可以从简单的计算器到其他LLM、SERP（搜索引擎结果页面）API或JsonGetValueTool等更高级的工具。还有包含多个相同类别工具的综合工具包，如Zapier
    Toolkit、OpenAPI Agent Toolkit或JSON Agent Toolkit。每个工具都附有描述性文本标签，由“经理”LLM用于了解该工具是否适用于即将到来的步骤。
- en: In conclusion, an agent comprises a “manager” LLM that makes decisions regarding
    subactions that need to be taken in order to complete tasks, and then uses a collection
    of tools and toolkits that enable the execution of each step.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，一个代理包括一个“经理”LLM，它决定完成任务所需的子动作，然后使用一系列工具和工具包来执行每个步骤。
- en: 'There are two types of agents as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种类型的代理如下：
- en: '**Action agents**: These agents make decisions at each timestep based on the
    outputs of all the previous tools. They are well-suited for handling small tasks
    efficiently.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行动代理**：这些代理根据所有先前工具的输出在每个时间步骤上做出决策。它们非常适合高效处理小任务。'
- en: '**Plan-and-execute agents**: In contrast, plan-and-execute agents devise the
    entire action plan in advance and do not alter the plan during the execution.
    This type of agent is ideal for tackling complex tasks effectively.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计划和执行代理：相比之下，计划和执行代理事先制定整个行动计划，并在执行过程中不改变计划。这种类型的代理非常适合有效地处理复杂任务。
- en: It is also possible to combine the two types by allowing a plan-and-execute
    agent to utilize action agents for executing the plans. This hybrid approach enables
    the agent to benefit from the strengths of both agent types.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以通过允许计划和执行代理利用行动代理来结合这两种类型。这种混合方法使代理能够从两种代理类型的优势中受益。
- en: '[Examples](toc.xhtml#s445a)'
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【示例】
- en: LangChain offers a remarkable collection of tools that drastically speed up
    the development process. By utilizing various components such as chains and agents,
    the overall process becomes easier and faster, saving time and effort. This allows
    individuals to focus on understanding requirements and actually building the desired
    solution. When using LangChain, there are numerous advantages to be gained. It
    allows to fully leverage all the **C**apabilityGPT’s benefits - streamlined operations,
    cost savings, improved decision-making, and many more. To fully comprehend the
    potential of LangChain, let’s delve into its components by exploring real-world
    examples.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain提供了一系列出色的工具，大大加快了开发过程。通过利用链和代理等各种组件，整个过程变得更加简单和快速，节省时间和精力。这使个人能够专注于理解需求并实际构建所需的解决方案。使用LangChain时，有许多优势可以获得。它允许充分利用所有**C**apabilityGPT的优势-简化操作、节省成本、改善决策等等。为了充分理解LangChain的潜力，让我们通过探索现实世界的例子来深入了解其组成部分。
- en: '[Example 1: Preparation helper for lawyers](toc.xhtml#s446a)'
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【示例1：律师的准备助手】
- en: Now, let us explore a practical use case of LangChain in creating an application
    to assist lawyers in their trial preparations. In order to effectively prepare,
    lawyers often need to study past cases. However, it can be challenging to find
    relevant cases that are similar to the one they are currently working on. Our
    objective is to develop an application that can retrieve documents containing
    past cases based on the lawyers’ descriptions of their specific needs.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探讨LangChain在创建一个帮助律师进行审前准备的应用程序的实际用例。为了有效准备，律师通常需要研究过去的案例。然而，要找到与他们目前正在处理的案件类似的相关案例可能是具有挑战性的。我们的目标是开发一个能够根据律师对他们特定需求的描述来检索包含过去案例的文件的应用程序。
- en: For example, the application should be capable of listing all cases related
    to defamation or financial loss. Relying solely on simple keyword searches may
    not yield accurate results.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，应用程序应能够列出所有与诽谤或经济损失有关的案件。仅依靠简单的关键词搜索可能无法产生准确的结果。
- en: To overcome this limitation, we will employ a more advanced method called semantic
    search. In the following example, we will demonstrate a popular approach that
    starts with embedding documents and the lawyers’ queries. Text embeddings are
    numerical representations of text that enable machines to understand and process
    natural language. They transform words or phrases into a list of numbers (vectors),
    with semantically similar items possessing similar values.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这一局限性，我们将采用一种更先进的方法，称为语义搜索。在下面的例子中，我们将演示一种流行的方法，该方法从嵌入文档和律师的查询开始。文本嵌入是文本的数值表示，使机器能够理解和处理自然语言。它们将单词或短语转换为一系列数字（向量），具有语义相似性的项目具有相似的值。
- en: In semantic search, this allows the system to match user’s queries with relevant
    documents based on semantic similarity, not just keyword matches. By employing
    text embeddings, systems can understand nuanced meanings and improve the relevance
    of search results, providing a more efficient and effective search experience.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在语义搜索中，这使系统能够根据语义相似性而不仅仅是关键词匹配来将用户的查询与相关文档进行匹配。通过使用文本嵌入，系统可以理解微妙的含义，并提高搜索结果的相关性，提供更高效和有效的搜索体验。
- en: So, once we have all documents embedded (converted into lists of numbers i.e.
    vectors) and stored in a vector store, we embed the query and calculate semantic
    closeness between the query and each of the documents. The measure we’ll use is
    called cosine similarity which is a cosine of the angle between two vectors. The
    documents with the highest cosine similarity will be our search results.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一旦我们嵌入了所有文档（转换为数字列表，即向量）并存储在向量存储中，我们嵌入查询并计算查询与每个文档之间的语义接近度。我们将使用的度量标准称为余弦相似度，它是两个向量之间的夹角的余弦。具有最高余弦相似度的文档将成为我们的搜索结果。
- en: Nonetheless, to illustrate the functionality of this application, we first need
    to generate a set of sample civil cases that we can work with.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，为了说明这个应用程序的功能，我们首先需要生成一组我们可以使用的样本民事案件。
- en: '[Content generation](toc.xhtml#s447a)'
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[内容生成](toc.xhtml#s447a)'
- en: Generating cases can be done using Language Models or Chat Models from the Model
    component of LangChain. In this case, we will use a language model using OpenAI’s
    GPT-3 family *text-003-davinci* model*.* To demonstrate the usage of output parsers
    effectively, we will adopt a two-step approach. Initially, we will create a list
    of distinct names that represent various content for each case.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 使用语言模型或LangChain的模型组件中的聊天模型可以生成案例。在这种情况下，我们将使用OpenAI的GPT-3家族*text-003-davinci*模型*的语言模型。为了有效地演示输出解析器的使用，我们将采用一个两步的方法。最初，我们将创建一个表示每个案例各种内容的不同名称列表。
- en: Subsequently, using the same model, we will generate the contents for civil
    cases and save them in a docx file.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，使用相同的模型，我们将为民事案件生成内容并将其保存在一个docx文件中。
- en: '[Generating a list of names](toc.xhtml#s448a)'
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[生成名称列表](toc.xhtml#s448a)'
- en: 'Let’s start with some useful imports and function definition:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一些有用的导入和函数定义开始：
- en: '`from LangChain import PromptTemplate`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`from LangChain import PromptTemplate`'
- en: '`from LangChain.llms import OpenAI`'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`from LangChain.llms import OpenAI`'
- en: '`from LangChain.chains import LLMChain`'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`from LangChain.chains import LLMChain`'
- en: '`from LangChain.output_parsers import CommaSeparatedListOutputParser`'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`from LangChain.output_parsers import CommaSeparatedListOutputParser`'
- en: '`import os`'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`import os`'
- en: '`from docx import Document`'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 从docx导入文档
- en: '`from typing import List`'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`from typing import List`'
- en: '`def generate_civil_cases_names() -> List[str]:`'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`def generate_civil_cases_names() -> List[str]:`'
- en: In this particular scenario, we aim to create approximately 60-80 names for
    civil cases that will remain within the Open AI’s model token limit during a single
    request. To achieve this, we will utilize the Python f-string. We will include
    a variable called `**number_of_cases**` in case we want to modify the number that
    specifies the desired quantity of generated cases.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种特殊情况下，我们的目标是在单个请求期间保持在Open AI模型令牌限制内创建大约60-80个民事案件的名称。为了实现这一目标，我们将利用Python
    f-string。我们将包括一个名为`**number_of_cases**`的变量，以便在需要修改指定生成案例数量的数量时使用。
- en: '`generate_civil_cases_names_template = “””`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`generate_civil_cases_names_template = “””`'
- en: '`You are a lawyer and a legal expert. Generate {number_of_cases} civil cases
    names.\n{format_instructions}`'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`你是一名律师和法律专家。生成{number_of_cases}个民事案件名称。\n{format_instructions}`'
- en: '`“””`'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`“””`'
- en: 'To achieve the desired output in a list format, we can utilize the LangChain’s
    list parser. This parser is designed to return a list of comma-separated items.
    However, there are two important steps to follow in order to obtain the correct
    output:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以列表格式实现所需的输出，我们可以利用LangChain的列表解析器。这个解析器旨在返回一个逗号分隔的项目列表。然而，为了获得正确的输出，有两个重要的步骤需要遵循：
- en: '**Step 1: Define format instructions for the prompt**'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：为提示定义格式说明**'
- en: 'The first step involves calling the `**ComaSeparatedListOutputParser()**` class
    object. This object will allow us to access the `**get_format_instructions**`
    method. By calling this method on the object, we can retrieve the string with
    format instructions that is later incorporated into our prompt:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步涉及调用`**ComaSeparatedListOutputParser()**`类对象。这个对象将允许我们访问`**get_format_instructions**`方法。通过在对象上调用这个方法，我们可以检索包含格式说明的字符串，稍后将其合并到我们的提示中：
- en: '`output_parser = CommaSeparatedListOutputParser()`'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`output_parser = CommaSeparatedListOutputParser()`'
- en: '`format_instructions = output_parser.get_format_instructions()`'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`format_instructions = output_parser.get_format_instructions()`'
- en: '**Step 2: Format the actual output**'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：格式化实际输出**'
- en: Once we have obtained the output, we will proceed with formatting the text we
    get from the model.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得了输出，我们将继续格式化从模型获得的文本。
- en: 'Let’s discuss the creation of a prompt. To utilize a prompt, we must create
    an instance of the `**PromptTemplate**` class. This involves specifying the template,
    input variable, and partial variables along with instructions for formatting:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一下提示的创建。要使用提示，我们必须创建`**PromptTemplate**`类的一个实例。这涉及指定模板、输入变量和部分变量以及格式说明的指令：
- en: '`prompt = PromptTemplate(`'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`prompt = PromptTemplate(`'
- en: '`template=generate_civil_cases_names_template,`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`template=generate_civil_cases_names_template,`'
- en: '`input_variables=[“number_of_cases”],`'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`input_variables=[“number_of_cases”],`'
- en: '`partial_variables={“format_instructions”: format_instructions},`'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`partial_variables={“format_instructions”: format_instructions},`'
- en: '`)`'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`)`'
- en: In order to proceed, it is essential to define a model. For our purposes, we
    utilize an Open AI model, although there exist numerous alternatives for consideration
    that are supported by LangChain. The standard model within the OpenAI class object
    is `**text-davinci-003**`. The `**temperature**` and `**max_tokens**` are parameters
    that control the output of the model. `**Temperature**` determines the randomness
    of the generated text, while `**max_tokens**` is the maximum number of tokens
    to generate shared between the prompt and completion.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了继续，定义一个模型是至关重要的。为了我们的目的，我们利用了一个Open AI模型，尽管LangChain支持许多其他备选方案。OpenAI类对象中的标准模型是`**text-davinci-003**`。`**temperature**`和`**max_tokens**`是控制模型输出的参数。`**Temperature**`确定生成文本的随机性，而`**max_tokens**`是在提示和完成之间生成的最大标记数。
- en: '`model = OpenAI(temperature=0.9, max_tokens=4000)`'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`model = OpenAI(temperature=0.9, max_tokens=4000)`'
- en: 'The next step is to pass the input to the model:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将输入传递给模型：
- en: '`_input = prompt.format(number_of_cases=”80”)`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`_input = prompt.format(number_of_cases=”80”)`'
- en: '`output = model(_input)`'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出 = 模型（_输入）`'
- en: 'Finally we are parsing the output in order to get a *List* object of civil
    cases processes:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们解析输出以获得民事案件流程的*列表*对象：
- en: '`list_of_processes = output_parser.parse(output)`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`processes_list = output_parser.parse(output)`'
- en: '`return list_of_processes`'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`return list_of_processes`'
- en: Now that we have a list of civil cases names, we will generate content.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一系列民事案件名称，我们将生成内容。
- en: '[Generating content of civil cases](toc.xhtml#s449a)'
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[生成民事案件内容](toc.xhtml#s449a)'
- en: 'First, let’s create a directory for our civil cases:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们为我们的民事案件创建一个目录：
- en: '`def generate_civil_cases(list_of_processes):`'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`def 生成民事案件（processes_list）：`'
- en: '`if not os.path.exists(“civil_cases”):`'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`if not os.path.exists(“civil_cases”):`'
- en: '`os.makedirs(“civil_cases”)`'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`os.makedirs(“civil_cases”)`'
- en: 'We want to generate each of the civil cases in a reproducible way, which is
    why we are again using a prompt template. We are using a Python f-string that
    includes the variable `**civil_case_name**`, which will be changed with each iteration:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望以可重复的方式生成每个民事案件，这就是为什么我们再次使用提示模板。我们使用了一个包含变量`**civil_case_name**`的Python
    f-string，它将在每次迭代中更改：
- en: '`generate_civil_case_template = “””`'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`generate_civil_case_template = “””`'
- en: '`You are a lawyer and a legal expert. Generate content of civil case: {civil_case_name}.
    Include title, full names of parties, background, claims, evidence, legal issues
    and procedural status.`'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`你是一名律师和法律专家。生成民事案件内容：{civil_case_name}。包括标题，当事人的全名，背景，主张，证据，法律问题和程序状态。`'
- en: '`“””`'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`“””`'
- en: When we’ve got simple use cases, using LLMs in isolation is fine. However, as
    the application gets more complex, chaining LLMs may come in handy. Let’s see
    how we can implement a simple `**LLMChain**` that takes in a prompt template,
    formats it with the user input, and returns a response from an LLM.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有简单的用例时，单独使用LLMs就可以了。但是，随着应用变得更加复杂，链式LLMs可能会派上用场。让我们看看如何实现一个简单的`**LLMChain**`，它接受一个提示模板，用用户输入进行格式化，并从LLM返回一个响应。
- en: '`llm = OpenAI(temperature=0.9, max_tokens=4000)`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`llm = OpenAI(温度=0.9，最大标记=4000)`'
- en: '`prompt = PromptTemplate(`'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`提示 = PromptTemplate(`'
- en: '`input_variables=[“civil_case_name”],`'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`input_variables=[“civil_case_name”],`'
- en: '`template=generate_civil_case_template,`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`template=generate_civil_case_template,`'
- en: '`)`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`)`'
- en: '`chain = LLMChain(llm=llm, prompt=prompt)`'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`chain = LLMChain(llm=llm, prompt=prompt)`'
- en: 'We are calling the chain in a loop in order to generate and save civil cases:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在循环中调用链，以生成并保存民事案件：
- en: '`for i, process in enumerate(list_of_processes, start=1):`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`对于 i, process in enumerate(list_of_processes, start=1):`'
- en: '`legal_process_content = chain.run(process)`'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`legal_process_content = chain.run(process)`'
- en: '`doc = Document()`'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`doc = Document()`'
- en: '`doc.add_paragraph(legal_process_content)`'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`doc.add_paragraph(legal_process_content)`'
- en: '`doc.save(f”civil_cases/civil_case_{i}.docx”)`'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`doc.save(f”civil_cases/civil_case_{i}.docx”)`'
- en: 'Here is the complete code for reference:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是完整的代码供参考：
- en: '`from LangChain import PromptTemplate`'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`从 LangChain 导入 PromptTemplate`'
- en: '`from LangChain.llms import OpenAI`'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`from LangChain.llms import OpenAI`'
- en: '`from LangChain.chains import LLMChain`'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`from LangChain.chains import LLMChain`'
- en: '`from LangChain.output_parsers import CommaSeparatedListOutputParser`'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`从 LangChain.output_parsers 导入 CommaSeparatedListOutputParser`'
- en: '`import os`'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '`导入 os`'
- en: '`from docx import Document`'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`from docx import Document`'
- en: '`from typing import List`'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`from typing import List`'
- en: '`def generate_civil_cases_names() -> List[str]:`'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`def 生成民事案件名称（）-> List[str]：`'
- en: '`generate_civil_cases_names_template = “””`'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`generate_civil_cases_names_template = “””`'
- en: '`You are a lawyer and a legal expert. Generate {number_of_cases} civil cases
    names.\n{format_instructions}`'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`你是一名律师和法律专家。生成 {number_of_cases} 民事案件名称。\n{format_instructions}`'
- en: '`“””`'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '`“””`'
- en: '``output_parser = CommaSeparatedListOutputParser()`'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '``output_parser = CommaSeparatedListOutputParser()`'
- en: '`format_instructions = output_parser.get_format_instructions()`'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '`format_instructions = output_parser.get_format_instructions()`'
- en: '`prompt = PromptTemplate(`'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`提示 = PromptTemplate(`'
- en: '`template=generate_civil_cases_names_template,`'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`template=generate_civil_cases_names_template,`'
- en: '`input_variables=[“number_of_cases”],`'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`input_variables=[“number_of_cases”],`'
- en: '`partial_variables={“format_instructions”: format_instructions},`'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`partial_variables={“format_instructions”: format_instructions},`'
- en: '`)`'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '`)`'
- en: '`model = OpenAI(temperature=0.9, max_tokens=4000)`'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`模型 = OpenAI(温度=0.9，最大标记=4000)`'
- en: '`_input = prompt.format(number_of_cases=”80”)`'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`_输入 = prompt.format(number_of_cases=”80”)`'
- en: '`output = model(_input)`'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出 = 模型（_输入）`'
- en: '`list_of_processes = output_parser.parse(output)`'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`processes_list = output_parser.parse(output)`'
- en: '``return list_of_processes`'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '``return list_of_processes`'
- en: '`def generate_civil_cases(list_of_processes):`'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`def 生成民事案件（processes_list）：`'
- en: '`if not os.path.exists(“civil_cases”):`'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`if not os.path.exists(“civil_cases”):`'
- en: '`os.makedirs(“civil_cases”)`'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`os.makedirs(“civil_cases”)`'
- en: '``generate_civil_case_template = “””`'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '``generate_civil_case_template = “””`'
- en: '`You are a lawyer and a legal expert. Generate content of civil case: {civil_case_name}.
    Include title, full names of parties, background, claims, evidence, legal issues
    and procedural status.`'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`你是一名律师和法律专家。生成民事案件内容：{civil_case_name}。包括标题，当事人的全名，背景，主张，证据，法律问题和程序状态。`'
- en: '`“””`'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`“””`'
- en: '`llm = OpenAI(temperature=0.9, max_tokens=4000)`'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`llm = OpenAI(温度=0.9，最大标记=4000)`'
- en: '`prompt = PromptTemplate(`'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`提示 = PromptTemplate(`'
- en: '`input_variables=[“civil_case_name”],`'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`input_variables=[“civil_case_name”],`'
- en: '`template=generate_civil_case_template,`'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`template=generate_civil_case_template,`'
- en: '`)`'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`)`'
- en: '`chain = LLMChain(llm=llm, prompt=prompt)`'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`chain = LLMChain(llm=llm, prompt=prompt)`'
- en: '`for i, process in enumerate(list_of_processes, start=1):`'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`对于 i, process in enumerate(list_of_processes, start=1):`'
- en: '`legal_process_content = chain.run(process)`'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`legal_process_content = chain.run(process)`'
- en: '`doc = Document()`'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '`doc = Document()`'
- en: '`doc.add_paragraph(legal_process_content)`'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`doc.add_paragraph(legal_process_content)`'
- en: '`doc.save(f”civil_cases/civil_case_{i}.docx”)`'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`doc.save(f”civil_cases/civil_case_{i}.docx”)`'
- en: '`if __name__ == “__main__”:`'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '`if __name__ == “__main__”:`'
- en: '`process_names = generate_civil_cases_names()`'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`process_names = generate_civil_cases_names()`'
- en: '`generate_civil_cases(process_names)`'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`generate_civil_cases(process_names)`'
- en: The code begins by introducing the *generate_civil_cases_names* function. This
    function utilizes LangChain’s `**PromptTemplate**` and the `**OpenAI**` model
    to generate a list of case names. To ensure proper formatting, the output is then
    processed with the `**output_parser**`. Following that, we have the `**generate_civil_cases**`
    function which takes the generated list of case names as a parameter. Within this
    function, `**LLMChain**` is employed to generate the content for each case individually
    through an iterative process. The resulting cases will be used as input for the
    subsequent section on semantic search.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 代码从介绍*generate_civil_cases_names*函数开始。这个函数利用LangChain的`**PromptTemplate**`和`**OpenAI**`模型生成一个案例名称列表。为了确保正确的格式，输出然后通过`**output_parser**`进行处理。接下来，我们有`**generate_civil_cases**`函数，它将生成的案例名称列表作为参数。在这个函数内部，`**LLMChain**`被用来通过迭代过程为每个案例单独生成内容。生成的案例将被用作后续语义搜索部分的输入。
- en: '[Semantic search](toc.xhtml#s450a)'
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[语义搜索](toc.xhtml#s450a)'
- en: Now that we have a database of civil cases, we want our application to be able
    to perform a semantic search in order to return all cases that are relevant for
    a lawyer that is preparing for a trial.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个民事案件数据库，我们希望我们的应用能够进行语义搜索，以返回所有对于一名准备审判的律师相关的案件。
- en: 'The idea is to take all the civil cases, and then use a model that converts
    them into a vector representation (embeds them) and stores them in a special vector
    database - vector store. For instance, once the lawyer inputs a query, for example,
    *I look for cases about financial loss*, the following steps are going to be performed:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是获取所有的民事案件，然后使用一个模型将它们转换为向量表示（嵌入）并将它们存储在一个特殊的向量数据库 - 向量存储中。例如，一旦律师输入一个查询，比如*我在寻找有关财务损失的案例*，接下来将执行以下步骤：
- en: Converting query into vector representation - embedding the query
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将查询转换为向量表示 - 嵌入查询
- en: Conducting similarity checks between embedded query and embedded civil cases
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在嵌入查询和嵌入民事案件之间进行相似性检查
- en: Retrieving the relevant vectors from the database
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从数据库中检索相关向量
- en: '[Embedding civil cases and storing them in Pinecone vector store](toc.xhtml#s451a)'
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[嵌入民事案件并将其存储在Pinecone向量存储中](toc.xhtml#s451a)'
- en: 'Let’s begin by importing the required libraries and modules:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始导入所需的库和模块：
- en: '`from LangChain.document_loaders import DirectoryLoader`'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '`from LangChain.document_loaders import DirectoryLoader`'
- en: '`from LangChain.vectorstores import Pinecone`'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '`from LangChain.vectorstores import Pinecone`'
- en: '`from LangChain.embeddings import OpenAIEmbeddings`'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '`from LangChain.embeddings import OpenAIEmbeddings`'
- en: '`import pinecone`'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '`import pinecone`'
- en: '`import os`'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '`import os`'
- en: We are going to embed the documents using an Open AI embeddings model and then
    store them in the Pinecone vector store.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Open AI嵌入模型嵌入文档，然后将它们存储在Pinecone向量存储中。
- en: 'The first thing to do is to load the civil cases. For this purpose the `**DirectoryLoader**`
    class object will load all files into a specified directory. Note that by default,
    the *DirectoryLoader* uses the `**UnstructuredLoader**` class to load the documents.
    If you need to load a different (structured) type of document, you can use a different
    loader class. For example, if you want to load Python source code files, you can
    use the *PythonLoader* class:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要做的是加载民事案件。为此，`**DirectoryLoader**`类对象将加载指定目录中的所有文件。请注意，默认情况下，*DirectoryLoader*使用`**UnstructuredLoader**`类来加载文档。如果您需要加载不同（结构化）类型的文档，可以使用不同的加载器类。例如，如果您想加载Python源代码文件，可以使用*PythonLoader*类：
- en: '`def embed_and_store_documents():`'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`def embed_and_store_documents():`'
- en: '`loader = DirectoryLoader(“civil_cases”, glob=”*.docx”)`'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '`loader = DirectoryLoader(“civil_cases”, glob=”*.docx”)`'
- en: '`documents = loader.load()`'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`documents = loader.load()`'
- en: 'Let’s define the embeddings model:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义嵌入模型：
- en: '`embeddings_model = OpenAIEmbeddings()`'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '`embeddings_model = OpenAIEmbeddings()`'
- en: Now, let’s delve into the functionality of Pinecone. Pinecone acts as a vector
    database, enabling us to store embedded data and send queries effectively. Before
    using Pinecone, we need to create an index, which serves as a container for our
    data.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入了解Pinecone的功能。Pinecone充当向量数据库，使我们能够有效地存储嵌入数据并发送查询。在使用Pinecone之前，我们需要创建一个索引，这个索引作为我们数据的容器。
- en: When creating a Pinecone index, we have to specify the similarity measure we
    want to use, such as cosine similarity. This measure helps in finding the most
    relevant vectors based on how close they are to the query in vector space. Additionally,
    we need to define the dimensions of the vectors we are working with. In the case
    of the OpenAI’s embedding model that we’ll use, *text-embedding-ada-002*, the
    dimension of output vectors is 1536.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建Pinecone索引时，我们必须指定我们想要使用的相似性度量，比如余弦相似度。这个度量有助于根据向量空间中它们与查询的接近程度来找到最相关的向量。此外，我们需要定义我们正在处理的向量的维度。在我们将使用的OpenAI的嵌入模型*text-embedding-ada-002*的情况下，输出向量的维度为1536。
- en: Once the index is created, we can proceed by adding our API key and environment
    name to the environmental variables. This step ensures that our Pinecone client
    has the necessary authorization to access the database. Furthermore, we need to
    provide the name of the index we created. This information enables us to initialize
    the Pinecone client effectively, providing seamless interactions with the vector
    store.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 创建索引后，我们可以通过将API密钥和环境名称添加到环境变量中来继续进行。这一步确保我们的Pinecone客户端具有必要的授权来访问数据库。此外，我们需要提供我们创建的索引的名称。这些信息使我们能够有效地初始化Pinecone客户端，提供与向量存储的无缝交互。
- en: '`pinecone.init(`'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`pinecone.init(`'
- en: '`api_key=os.environ[‘PINECONE_API_KEY’], environment=os.environ[‘PINECONE_ENV’]`'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '`api_key=os.environ[‘PINECONE_API_KEY’], environment=os.environ[‘PINECONE_ENV’]`'
- en: '`)`'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`)'
- en: '`index_name = “legal-documents-search”`'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '`index_name = “legal-documents-search”`'
- en: 'Finally, with this single line of code, all documents are going to be embedded
    and stored in Pinecone. These kinds of ready-to-use pieces are the great power
    of LangChain:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过这一行代码，所有文档都将被嵌入并存储在Pinecone中。这些 ready-to-use 的部分是LangChain的巨大力量：
- en: '`Pinecone.from_documents(`'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pinecone.from_documents(`'
- en: '`documents, embeddings_model, index_name=index_name`'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '`documents, embeddings_model, index_name=index_name`'
- en: '`)`'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '`)'
- en: 'Here is the complete code for reference:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这是完整的代码供参考：
- en: '`from LangChain.document_loaders import DirectoryLoader`'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '`from LangChain.document_loaders import DirectoryLoader`'
- en: '`from LangChain.vectorstores import Pinecone`'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '`from LangChain.vectorstores import Pinecone`'
- en: '`from LangChain.embeddings import OpenAIEmbeddings`'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '`from LangChain.embeddings import OpenAIEmbeddings`'
- en: '`import pinecone`'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '`import pinecone`'
- en: '`import os`'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '`import os`'
- en: '`def embed_and_store_documents():`'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '`def embed_and_store_documents():`'
- en: '`loader = DirectoryLoader(“civil_cases”, glob=”*.docx”)`'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '`loader = DirectoryLoader(“civil_cases”, glob=”*.docx”)`'
- en: '`documents = loader.load()`'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '`documents = loader.load()`'
- en: '`embeddings_model = OpenAIEmbeddings()`'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '`embeddings_model = OpenAIEmbeddings()`'
- en: '`pinecone.init(`'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '`pinecone.init(`'
- en: '`api_key=os.environ[‘PINECONE_API_KEY’], environment=os.environ[‘PINECONE_ENV’]`'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '`api_key=os.environ[‘PINECONE_API_KEY’], environment=os.environ[‘PINECONE_ENV’]`'
- en: '`)`'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '`)`'
- en: '`index_name = “legal-documents-search”`'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '`index_name = “legal-documents-search”`'
- en: '`Pinecone.from_documents(`'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pinecone.from_documents(`'
- en: '`documents, embeddings_model, index_name=index_name`'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '`documents, embeddings_model, index_name=index_name`'
- en: '`)`'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '`)`'
- en: '`if __name__ == “__main__”:`'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '`if __name__ == “__main__”:`'
- en: '`embed_and_store_documents()`'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '`embed_and_store_documents()`'
- en: The code snippet begins by loading the civil cases using the `**DirectoryLoader**`.
    This loader retrieves all files with a specified extension from the designated
    directory. Afterward, we define the embeddings model and proceed to initialize
    the Pinecone client. The final step entails embedding the loaded documents and
    storing them in the Pinecone vector store with the aid of the client.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 代码片段首先通过使用`**DirectoryLoader**`加载民事案件。这个加载器从指定的目录中检索所有具有指定扩展名的文件。然后，我们定义嵌入模型并初始化Pinecone客户端。最后一步是使用客户端将加载的文件进行嵌入并存储在Pinecone向量存储中。
- en: '[Similarity search and retrieving relevant data](toc.xhtml#s452a)'
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[相似性搜索和检索相关数据](toc.xhtml#s452a)'
- en: 'The last part is retrieving the relevant data. First, we want to embed the
    query that is asked by the lawyer and perform a similarity search. Once we find
    the closest results, we want to return the names of the civil case files to the
    user. Let’s start with necessary imports:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一部分是检索相关数据。首先，我们要嵌入律师提出的查询并进行相似性搜索。一旦找到最接近的结果，我们希望将民事案例文件的名称返回给用户。让我们从必要的导入开始：
- en: '`from LangChain.vectorstores import Pinecone`'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '`from LangChain.vectorstores import Pinecone`'
- en: '`from LangChain.embeddings import OpenAIEmbeddings`'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '`from LangChain.embeddings import OpenAIEmbeddings`'
- en: '`from LangChain.embeddings.openai import OpenAIEmbeddings`'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '`from LangChain.embeddings.openai import OpenAIEmbeddings`'
- en: '`import pinecone`'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '`import pinecone`'
- en: '`import os`'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '`import os`'
- en: 'In order to embed the lawyer’s query we will again use embeddings model from
    Open AI:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 为了嵌入律师的查询，我们将再次使用Open AI的嵌入模型：
- en: '`def retrieve_relevant_cases(query):`'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '`def retrieve_relevant_cases(query):`'
- en: '`embeddings_model = OpenAIEmbeddings()`'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '`embeddings_model = OpenAIEmbeddings()`'
- en: 'Now that we have an existing index with our civil cases represented as vectors,
    we can use the `**from_existing_index**` method to create a `**docsearch**` object
    that we’ll use for the similarity check:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个包含我们的民事案件的现有索引，我们可以使用`**from_existing_index**`方法创建一个`**docsearch**`对象，用于相似性检查：
- en: '`index_name = “legal-documents-search”`'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '`index_name = “legal-documents-search”`'
- en: '`pinecone.init(`'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '`pinecone.init(`'
- en: '`api_key=os.environ[“PINECONE_API_KEY”], environment=os.environ[“PINECONE_ENV”]`'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '`api_key=os.environ[“PINECONE_API_KEY”], environment=os.environ[“PINECONE_ENV”]`'
- en: '`)`'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '`)`'
- en: '`docsearch = Pinecone.from_existing_index(index_name, embeddings_model)`'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '`docsearch = Pinecone.from_existing_index(index_name, embeddings_model)`'
- en: 'If we pass a query like `“Cases about financial loss”` to the *docsearch* object
    calling *similarty_search* method, the following will happen: the query will be
    embedded with the default model defined in the *OpenAIEmbeddings* object, similarity
    search will be performed and all relevant documents will be returned:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将查询如“关于财务损失的案例”传递给*docsearch*对象调用*similarty_search*方法，将会发生以下情况：查询将使用*OpenAIEmbeddings*对象中定义的默认模型进行嵌入，执行相似性搜索并返回所有相关文件：
- en: '`docs = docsearch.similarity_search(query)`'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '`docs = docsearch.similarity_search(query)`'
- en: 'Finally, as we are looking for specific documents, we want to return the names
    of these documents that are stored in metadata:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由于我们正在寻找特定的文件，我们希望返回存储在元数据中的这些文件的名称：
- en: '`sources = [doc.metadata[“source”] for doc in docs]`'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '`sources = [doc.metadata[“source”] for doc in docs]`'
- en: 'Here is the complete code for retrieving relevant civil cases:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这是检索相关民事案例的完整代码：
- en: '`from LangChain.vectorstores import Pinecone`'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '`from LangChain.vectorstores import Pinecone`'
- en: '`from LangChain.embeddings import OpenAIEmbeddings`'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '`from LangChain.embeddings import OpenAIEmbeddings`'
- en: '`from LangChain.embeddings.openai import OpenAIEmbeddings`'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '`from LangChain.embeddings.openai import OpenAIEmbeddings`'
- en: '`import pinecone`'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '`import pinecone`'
- en: '`import os`'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '`import os`'
- en: '`def retrieve_relevant_cases(query):`'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '`def retrieve_relevant_cases(query):`'
- en: '`embeddings_model = OpenAIEmbeddings()`'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`embeddings_model = OpenAIEmbeddings()`'
- en: '`index_name = “legal-documents-search”`'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '`index_name = “legal-documents-search”`'
- en: '`pinecone.init(`'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '`pinecone.init(`'
- en: '`api_key=os.environ[“PINECONE_API_KEY”], environment=os.environ[“PINECONE_ENV”]`'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '`api_key=os.environ[“PINECONE_API_KEY”], environment=os.environ[“PINECONE_ENV”]`'
- en: '`)`'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '`)`'
- en: '`docsearch = Pinecone.from_existing_index(index_name, embeddings_model)`'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '`docsearch = Pinecone.from_existing_index(index_name, embeddings_model)`'
- en: '`docs = docsearch.similarity_search(query)`'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '`docs = docsearch.similarity_search(query)`'
- en: '`sources = [doc.metadata[“source”] for doc in docs]`'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '`sources = [doc.metadata[“source”] for doc in docs]`'
- en: '`return sources`'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '`return sources`'
- en: '`if __name__ == “__main__”:`'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '`if __name__ == “__main__”:`'
- en: '`print(retrieve_relevant_cases(“Cases about financial loss”))`'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '`print(retrieve_relevant_cases(“Cases about financial loss”))`'
- en: The code snippet includes a function called `**retrieve_relevant_cases**` that
    aims to get the appropriate cases. Initially, we define the embeddings model,
    index name, and initialize the Pinecone client. Next, we create a *docsearch*
    object for conducting similarity searches. Lastly, we extract the names of the
    cases from the metadata of the returned documents.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 代码片段包括一个名为`**retrieve_relevant_cases**`的函数，旨在获取适当的案例。首先，我们定义嵌入模型、索引名称并初始化Pinecone客户端。接下来，我们创建一个用于进行相似性搜索的*docsearch*对象。最后，我们从返回的文件的元数据中提取案例的名称。
- en: '[Example 2: Internal knowledge QA for automation specialists - chatting over
    a manual of a specific controller](toc.xhtml#s453a)'
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[示例2：自动化专家的内部知识QA-在特定控制器手册上聊天](toc.xhtml#s453a)'
- en: One common scenario for utilizing LLMs in a corporate environment involves extracting
    data from various types of text documents, such as product specifications, usage
    documentation, web pages, presentations, and internal documents. The main challenge
    in these cases is dealing with a large volume of data and the inconsistency in
    how different terms are used to refer to the same thing. A simple keyword search
    is often insufficient, and understanding the semantic meaning of user questions
    becomes crucial. To tackle this issue, LangChain offers a range of components
    with effective strategies to address semantic search.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在企业环境中利用LLM的一个常见场景是从各种类型的文本文档中提取数据，如产品规格、使用文档、网页、演示文稿和内部文档。在这些情况下的主要挑战是处理大量数据以及不同术语用于指代同一事物的不一致性。简单的关键词搜索通常是不够的，理解用户问题的语义含义变得至关重要。为了解决这个问题，LangChain提供了一系列组件，具有有效的策略来解决语义搜索。
- en: To illustrate the capabilities of LangChain, let’s consider an example where
    we develop a tool for automation specialists to expedite the process of searching
    through documentation. This tool will function as a chat mechanism, allowing the
    specialists to ask questions about a specific controller. As a reminder, a controller
    is a device that manages a process to achieve and maintain desired parameters.
    For instance, in a chocolate manufacturing plant, a programmable logic controller
    (PLC) acts as a controller by monitoring and controlling various equipment and
    processes to maintain specific production parameters such as temperature, pressure,
    and speed, ensuring optimal operation and product quality. The documentation for
    such controllers can be extensive and encompass a significant amount of content.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明LangChain的功能，让我们考虑一个例子，我们开发一个工具，用于自动化专家加快搜索文档的过程。这个工具将作为一个聊天机制，允许专家询问关于特定控制器的问题。提醒一下，控制器是管理过程以实现和维持期望参数的设备。例如，在巧克力制造工厂中，可编程逻辑控制器（PLC）通过监控和控制各种设备和过程来维持特定的生产参数，如温度、压力和速度，确保最佳运行和产品质量。这类控制器的文档可能非常广泛，包含大量内容。
- en: The objective is to provide automation specialists with a chat interface that
    enables them to quickly access the information they need for design and programming
    purposes.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是为自动化专家提供一个聊天界面，使他们能够快速访问设计和编程所需的信息。
- en: '[Chunking the manual and loading it to the Pinecone vector store](toc.xhtml#s454a)'
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[将手册分块并加载到Pinecone向量存储器](toc.xhtml#s454a)'
- en: 'Let’s explore the technical aspects of using LangChain for the implementation.
    The controller’s manual is a 40-page PDF document, so our initial task is to load
    the document. Since the entire document is too lengthy to input into our prompt,
    we must devise a strategy to divide the document into manageable parts. To begin,
    let’s consider some useful imports:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨使用LangChain进行实现的技术方面。控制器手册是一份40页的PDF文档，所以我们的初始任务是加载文档。由于整个文档太长而无法输入到我们的提示中，我们必须制定一种策略将文档分成可管理的部分。首先，让我们考虑一些有用的导入：
- en: '`from LangChain.text_splitter import RecursiveCharacterTextSplitter`'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '`from LangChain.text_splitter import RecursiveCharacterTextSplitter`'
- en: '`from LangChain.vectorstores import Pinecone`'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '`from LangChain.vectorstores import Pinecone`'
- en: '`from LangChain.embeddings import OpenAIEmbeddings`'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '`from LangChain.embeddings import OpenAIEmbeddings`'
- en: '`from LangChain.document_loaders import PyPDFLoader`'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '`from LangChain.document_loaders import PyPDFLoader`'
- en: '`import pinecone`'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '`import pinecone`'
- en: '`Import os`'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '`Import os`'
- en: 'Loading the manual is our next step. To accomplish this, we can utilize the
    `**PyPDFLoader**` class, which is readily available and allows us to easily load
    the document and save it as a *Document* object:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 加载手册是我们的下一步。为了实现这一目标，我们可以利用`**PyPDFLoader**`类，这个类可以方便地加载文档并将其保存为*Document*对象：
- en: '`def embed_and_store_documents():`'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '`def embed_and_store_documents():`'
- en: '`loader = PyPDFLoader(“Manual_MT655333.pdf”)`'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '`loader = PyPDFLoader(“Manual_MT655333.pdf”)`'
- en: '`manual = loader.load_and_split()`'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '`manual = loader.load_and_split()`'
- en: 'Now that we have loaded the PDF file, we need to divide it into smaller sections.
    In LangChain, there is a text splitter class called *RecursiveCharacterTextSplitter*
    that can help us with this task. The splitter splits the text based on specific
    characters, which are listed as follows: [“\n”, “\r”, “ “, „“]. The reason behind
    this approach is to keep entire paragraphs together for as long as possible. This
    increases the chances of getting strongly semantically related chunks of text.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经加载了PDF文件，我们需要将其分成较小的部分。在LangChain中，有一个名为*RecursiveCharacterTextSplitter*的文本分割器类，可以帮助我们完成这个任务。分割器根据特定字符拆分文本，这些字符列在下面：[“\n”,
    “\r”, “ “, „“]。采用这种方法的原因是尽可能保持整个段落在一起。这增加了获取强语义相关文本块的机会。
- en: The chunk size refers to the maximum size of each section, as measured by the
    length function. The overlap indicates how many tokens will be shared between
    each adjacent section. The length function determines how the length of the sections
    is calculated. By default, it counts the number of characters, but it is common
    to use a token counter instead. The `**add_start_index**` parameter determines
    whether the starting position of each section within the original document should
    be included in the metadata.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 分块大小指的是每个部分的最大大小，由长度函数测量。重叠指示了每个相邻部分之间共享多少标记。长度函数确定了如何计算各部分的长度。默认情况下，它计算字符数，但通常使用标记计数。`**add_start_index**`参数确定是否应在元数据中包括每个部分在原始文档中的起始位置。
- en: '`text_splitter = RecursiveCharacterTextSplitter(`'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '`text_splitter = RecursiveCharacterTextSplitter(`'
- en: '`chunk_size=500,`'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '`chunk_size=500,`'
- en: '`chunk_overlap=30,`'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '`chunk_overlap=30,`'
- en: '`length_function=len,`'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '`length_function=len,`'
- en: '`add_start_index=True,`'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '`add_start_index=True,`'
- en: '`)`'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '`)`'
- en: 'In order to split the document into smaller parts, we can utilize the `**text_splitter**`
    object and its `**split_documents**` method:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将文档分成较小的部分，我们可以利用`**text_splitter**`对象及其`**split_documents**`方法：
- en: '`documents = text_splitter.split_documents(documents=manual)`'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '`documents = text_splitter.split_documents(documents=manual)`'
- en: 'After completing this step, we proceed to embed these chunks and post them
    to the Pinecone vector store:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此步骤后，我们继续嵌入这些块并将它们发布到Pinecone向量存储中：
- en: '`embeddings_model = OpenAIEmbeddings()`'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: embeddings_model = OpenAIEmbeddings（）
- en: '`pinecone.init(`'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: pinecone.init（
- en: '`api_key=os.environ[‘PINECONE_API_KEY’], environment=os.environ[‘PINECONE_ENV’]`'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: api_key = os.environ [‘PINECONE_API_KEY’]，environment = os.environ [‘PINECONE_ENV’]
- en: '`)`'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ）
- en: '`index_name = ‘mt655333-manual’`'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: index_name = ‘mt655333-manual’
- en: '`Pinecone.from_documents(documents, embeddings_model, index_name=index_name)`'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: Pinecone.from_documents（documents，embeddings_model，index_name = index_name）
- en: 'Here is how the full code for chunking, embedding, and pushing vectors to Pinecone
    will appear:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是分块、嵌入和推送向Pinecone的完整代码：
- en: '`from LangChain.text_splitter import RecursiveCharacterTextSplitter`'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 从LangChain.text_splitter导入RecursiveCharacterTextSplitter
- en: '`from LangChain.vectorstores import Pinecone`'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 从LangChain.vectorstores导入Pinecone
- en: '`from LangChain.embeddings import OpenAIEmbeddings`'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 从LangChain.embeddings导入OpenAIEmbeddings
- en: '`from LangChain.document_loaders import PyPDFLoader`'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 从LangChain.document_loaders导入PyPDFLoader
- en: '`import pinecone`'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 导入pinecone
- en: '`import os`'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 导入os
- en: '`def embed_and_store_documents():`'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: def embed_and_store_documents（）：
- en: '`loader = PyPDFLoader(“Manual_MT655333.pdf”)`'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 加载器= PyPDFLoader（“Manual_MT655333.pdf”）
- en: '`manual = loader.load_and_split()`'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: manual = loader.load_and_split（）
- en: '`text_splitter = RecursiveCharacterTextSplitter(`'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: text_splitter = RecursiveCharacterTextSplitter（
- en: '`chunk_size=500,`'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: chunk_size = 500，
- en: '`chunk_overlap=30,`'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: chunk_overlap = 30，
- en: '`length_function=len,`'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: length_function = len，
- en: '`add_start_index=True,`'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: add_start_index = True，
- en: '`)`'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: ）
- en: '`documents = text_splitter.split_documents(documents=manual)`'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: documents = text_splitter.split_documents（documents = manual）
- en: '``embeddings_model = OpenAIEmbeddings()`'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '``embeddings_model = OpenAIEmbeddings（）'
- en: '`pinecone.init(`'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: pinecone.init（
- en: '`api_key=os.environ[‘PINECONE_API_KEY’], environment=os.environ[‘PINECONE_ENV’]`'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: api_key = os.environ [‘PINECONE_API_KEY’]，环境= os.environ [‘PINECONE_ENV’]
- en: '`)`'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: ）
- en: '`index_name = ‘mt655333-manual’`'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: index_name = ‘mt655333-manual’
- en: '`Pinecone.from_documents(documents, embeddings_model, index_name=index_name)`'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: Pinecone.from_documents（documents，embeddings_model，index_name = index_name）
- en: '`if __name__ == ‘__main__’:`'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: if __name__ == ‘__main__’：
- en: '`embed_and_store_documents()`'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: embed_and_store_documents（）
- en: The preceding code snippet uses `**PyPDFLoader**` to load the controller’s manual.
    After that, it utilizes `**RecursiveCharacterTextSplitter**` to split the documents
    into smaller chunks. Later on, it embeds these chunks and puts them in a vector
    store with the usage of the Pinecone client. This allows us to have the documents
    prepared for the next step, which involves performing a similarity search over
    the manual.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段使用**PyPDFLoader**加载控制器手册。之后，它利用**RecursiveCharacterTextSplitter**将文档分割成较小的块。随后，它嵌入这些块并使用Pinecone客户端将它们放入向量存储中。这使我们能够为下一步准备文档，该步骤涉及在手册上执行相似性搜索。
- en: '[Chatting over the controller’s manual - Conversational Retrieval Chain for
    similarity search](toc.xhtml#s455a)'
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[在控制器手册上聊天-用于相似性搜索的对话检索链]（toc.xhtml＃s455a）'
- en: Now let’s explore the implementation of a chat mechanism that allows automation
    specialists to ask for information about the controller. To ensure a smooth conversation
    flow, we must address the fact that subsequent queries may be related to previous
    queries. To handle this, we need to effectively manage the conversation’s memory.
    To retrieve relevant chunks of data and effectively handle chat history, we will
    employ the `**ConversationalRetrievalQAchain**`. With this class, we will process
    previous interactions and integrate them into the current chat.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探讨一种聊天机制的实现，该机制允许自动化专家询问有关控制器的信息。为了确保对话流畅，我们必须解决后续查询可能与先前查询相关的事实。为了处理这一点，我们需要有效地管理对话的记忆。为了检索相关数据块并有效处理聊天历史记录，我们将使用**ConversationalRetrievalQAchain**。使用这个类，我们将处理先前的交互并将它们整合到当前的聊天中。
- en: '`from LangChain.embeddings.openai import OpenAIEmbeddings`'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 从LangChain.embeddings.openai导入OpenAIEmbeddings
- en: '`from LangChain.llms import OpenAI`'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 从LangChain.llms导入OpenAI
- en: '`from LangChain.chat_models import ChatOpenAI`'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 从LangChain.chat_models导入ChatOpenAI
- en: '`from LangChain.chains import ConversationalRetrievalChain`'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 从LangChain.chains导入ConversationalRetrievalChain
- en: '`from LangChain.vectorstores import Pinecone`'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 从LangChain.vectorstores导入Pinecone
- en: '`import pinecone`'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 导入pinecone
- en: '`import os`'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 导入os
- en: '`pinecone.init(`'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: pinecone.init（
- en: '`api_key=os.environ[“PINECONE_API_KEY”], environment=os.environ[“PINECONE_ENV”]`'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: api_key = os.environ [“PINECONE_API_KEY”]，环境= os.environ [“PINECONE_ENV”]
- en: '`)`'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: ）
- en: '`def retrieve_controllers_info(query, chat_history):`'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: def retrieve_controllers_info（query，chat_history）：
- en: '`embeddings_model = OpenAIEmbeddings()`'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: embeddings_model = OpenAIEmbeddings（）
- en: '`index_name = “mt655333-manual”`'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: index_name = “mt655333-manual”
- en: '`docsearch = Pinecone.from_existing_index(index_name, embeddings_model)`'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: docsearch = Pinecone.from_existing_index（index_name，embeddings_model）
- en: '`The following chain is created by using the `**from_llm**` method on the `**ConversationalRetrievalChain**`
    class:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在**ConversationalRetrievalChain**类上使用**from_llm**方法创建以下链：
- en: '`qa = ConversationalRetrievalChain.from_llm(`'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: qa = ConversationalRetrievalChain.from_llm（
- en: '`OpenAI(temperature=0),`'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI（temperature = 0），
- en: '`docsearch.as_retriever(),`'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: docsearch.as_retriever（），
- en: '`return_source_documents=True,`'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: return_source_documents = True，
- en: '`condense_question_llm=ChatOpenAI(temperature=0, model=”gpt-3.5-turbo”),`'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: condense_question_llm = ChatOpenAI（temperature = 0，model =”gpt-3.5-turbo”），
- en: '`)`'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: ）
- en: '`return qa({“question”: query, “chat_history”: chat_history})`'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: return qa（{“question”：query，“chat_history”：chat_history}）
- en: To get information about the source documents that were used to generate the
    answer, we include an additional argument called `**return_source_documents**`
    and set it to `**True**`. The last parameter, `**condense_question_llm**`, is
    responsible for combining the chat history and query into a single query vector.
    This allows us to use different models for condensing the question and answering
    it. The models can be chosen based on their performance given a specific task
    or costs of usage. The chain produces a tuple that includes the answer and a chat
    history. The chat history is a list of tuples containing the queries and their
    corresponding answers.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取生成答案所使用的源文档的信息，我们包括一个名为“return_source_documents”的额外参数，并将其设置为“True”。最后一个参数“condense_question_llm”负责将聊天历史和查询合并为单个查询向量。这使我们能够使用不同的模型来压缩问题并回答它。可以根据特定任务的性能或使用成本选择模型。该链生成一个包含答案和聊天历史的元组。聊天历史是一个包含查询及其对应答案的元组列表。
- en: '[Memory handling](toc.xhtml#s456a)'
  id: totrans-386
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[内存处理](toc.xhtml#s456a)'
- en: 'The last step is to test the chain. It is important to note that in this case,
    we must manage the memory during the chat session. At the start of each session,
    we set the memory to an empty list. As each query is sent to the chat assistant,
    we add to it a tuple containing the query and its corresponding answer to the
    chat history list:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是测试链。重要的是要注意，在这种情况下，我们必须在聊天会话期间管理内存。在每个会话开始时，我们将内存设置为空列表。当每个查询发送到聊天助手时，我们将其添加到一个包含查询及其对应答案的元组中，添加到聊天历史列表中：
- en: '`if __name__ == “__main__”:`'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 如果__name__ ==“__main__”：
- en: '`chat_history = []`'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: chat_history=[]
- en: '`query = “Does MT65533 support autotuning?”`'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 查询=“MT65533支持自动调谐吗？”
- en: '``result = retrieve_controllers_info(query, chat_history)`'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 结果=检索控制器信息（查询，聊天记录）
- en: '``print(result[“answer”])`'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 打印（result[“answer”]）
- en: '`print(result[“source_documents”])`'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 打印（result[“source_documents”]）
- en: '``chat_history.append((query, result[“answer”]))`'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: chat_history.append（（query，result[“answer”]））
- en: '``query = “Which methods exactly?”`'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 查询=“到底是哪些方法？”
- en: '`result = retrieve_controllers_info(query, chat_history)`'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: result=检索控制器信息（query，chat_history）
- en: '``print(result[“answer”])`'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 打印（result[“answer”]）
- en: '`print(result[“source_documents”])`'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 打印（result[“source_documents”]）
- en: The following conversation is the outcome of the preceding backend application.
    Furthermore, depending on the design, one might want to add relevant fragments
    of the manual that are returned by the `**docsearch**` object.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 以下对话是前面的后端应用程序的结果。此外，根据设计，人们可能希望添加由“docsearch”对象返回的手册的相关片段。
- en: '*Automation Specialist: Does MT65533 support autotuning?*'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化专家：MT65533支持自动调谐吗？
- en: '*AI: Yes, the MT655333 controller supports auto-tuning techniques to determine
    optimal control parameters automatically.*'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '*AI：是的，MT655333控制器支持自动调谐技术，以自动确定最佳控制参数。*'
- en: '*Automation Specialist: Which methods exactly?*'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化专家：到底是哪些方法？
- en: '*AI: The MT65533 controller supports auto-tuning methods such as relay feedback,
    Ziegler-Nichols forward control, adaptive control, and fuzzy logic control.*'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '*AI：MT65533控制器支持自动调谐方法，如继电器反馈、Ziegler-Nichols前向控制、自适应控制和模糊逻辑控制。*'
- en: 'Here is the complete code for reference:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是完整的代码供参考：
- en: '`from LangChain.embeddings.openai import OpenAIEmbeddings`'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 从LangChain.embeddings.openai导入OpenAIEmbeddings
- en: '`from LangChain.llms import OpenAI`'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 从LangChain.llms导入OpenAI
- en: '`from LangChain.chat_models import ChatOpenAI`'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 从LangChain.chat_models导入ChatOpenAI
- en: '`from LangChain.chains import ConversationalRetrievalChain`'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 从LangChain.chains导入ConversationalRetrievalChain
- en: '`from LangChain.vectorstores import Pinecone`'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 从LangChain.vectorstores导入Pinecone
- en: '`import pinecone`'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 导入pinecone
- en: '`import os`'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 导入os
- en: '`pinecone.init(`'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: pinecone.init（
- en: '`api_key=os.environ[“PINECONE_API_KEY”], environment=os.environ[“PINECONE_ENV”]`'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: api_key=os.environ[“PINECONE_API_KEY”]，environment=os.environ[“PINECONE_ENV”]
- en: '`)`'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: ）
- en: '`def retrieve_controllers_info(query, chat_history):`'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: def retrieve_controllers_info（query，chat_history）：
- en: '`embeddings_model = OpenAIEmbeddings()`'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: embeddings_model=OpenAIEmbeddings（）
- en: '`index_name = “mt655333-manual”`'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: index_name=“mt655333-manual”
- en: '`docsearch = Pinecone.from_existing_index(index_name, embeddings_model)`'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: docsearch=Pinecone.from_existing_index（index_name，embeddings_model）
- en: '``qa = ConversationalRetrievalChain.from_llm(`'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: qa=从llm创建对话检索链（
- en: '`OpenAI(temperature=0),`'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI（temperature=0），
- en: '`docsearch.as_retriever(),`'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: docsearch.as_retriever（），
- en: '`return_source_documents=True,`'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: return_source_documents=True，
- en: '`condense_question_llm=ChatOpenAI(temperature=0, model=”gpt-3.5-turbo”),`'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: condense_question_llm=ChatOpenAI（temperature=0，model=”gpt-3.5-turbo”），
- en: '`)`'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: ）
- en: '`return qa({“question”: query, “chat_history”: chat_history})`'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 返回qa（{“question”：query，“chat_history”：chat_history}）
- en: '`if __name__ == “__main__”:`'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 如果__name__ ==“__main__”：
- en: '`chat_history = []`'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: chat_history=[]
- en: '`query = “Does MT65533 support autotuning?”`'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 查询=“MT65533支持自动调谐吗？”
- en: '``result = retrieve_controllers_info(query, chat_history)`'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: result=检索控制器信息（query，chat_history）
- en: '``print(result[“answer”])`'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 打印（result[“answer”]）
- en: '`print(result[“source_documents”])`'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 打印（result[“source_documents”]）
- en: '``chat_history.append((query, result[“answer”]))`'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: chat_history.append（（查询，result[“answer”]））
- en: '``query = “Which methods exactly?”`'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 查询=“到底是哪些方法？”
- en: '`result = retrieve_controllers_info(query, chat_history)`'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: result=检索控制器信息（query，chat_history）
- en: '``print(result[“answer”])`'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 打印（result[“answer”]）
- en: '`print(result[“source_documents”])`'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 打印（result[“source_documents”]）
- en: This code snippet begins by initializing the Pinecone client and then defines
    the `**retrieve_controllers_info**` function. This function accepts *query* and
    *chat_history* as input parameters and utilizes the `**ConversationalRetrievalChain**`
    to perform a similarity search over the vector store and generate an answer. In
    the main function, the solution is tested by passing the `**query**` and `**chat_history**`
    to the `**retrieve_controllers_info**` function. The management of conversation
    memory is left to the discretion of the developer.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码从初始化Pinecone客户端开始，然后定义了“retrieve_controllers_info”函数。此函数接受*query*和*chat_history*作为输入参数，并利用“ConversationalRetrievalChain”在向量存储中执行相似性搜索并生成答案。在主函数中，通过将*query*和*chat_history*传递给“retrieve_controllers_info”函数来测试解决方案。对话记忆的管理由开发人员自行决定。
- en: '[Example 3: Market research using LangChain Agent](toc.xhtml#s457a)'
  id: totrans-438
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[示例3：使用LangChain代理进行市场研究](toc.xhtml#s457a)'
- en: The following example illustrates how LangChain’s agents can be utilized to
    carry out market research. This process entails comprehending the task, strategizing
    execution, and implementing specific actions.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例说明了如何利用LangChain的代理进行市场调研。这个过程包括理解任务，制定执行策略和实施具体行动。
- en: For this example, we will employ LangChain’s agent component. We will empower
    our agent with the capability to utilize SERP API as a search engine. SERP API
    is essentially a tool that developers can utilize to access and extract data from
    search engine result pages. The agent will leverage an Open AI model to plan all
    necessary steps. It will identify the relevant data to search for, verify the
    accuracy of the API’s output, and compile the results to deliver an answer to
    the user.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用LangChain的代理组件。我们将赋予我们的代理能力，利用SERP API作为搜索引擎。SERP API本质上是开发人员可以利用的工具，用于访问和提取搜索引擎结果页面的数据。代理将利用Open
    AI模型来规划所有必要的步骤。它将确定要搜索的相关数据，验证API输出的准确性，并编译结果以向用户提供答案。
- en: 'Since LangChain provides all the necessary tools, the implementation of this
    feature will be straightforward and uncomplicated. Let us begin by importing the
    necessary modules and libraries:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LangChain提供了所有必要的工具，因此实现这个功能将是简单和不复杂的。让我们首先导入必要的模块和库：
- en: '`from LangChain.agents import load_tools`'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 从LangChain.agents导入load_tools
- en: '`from LangChain.agents import initialize_agent`'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 从LangChain.agents导入initialize_agent
- en: '`from LangChain.agents import AgentType`'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 从LangChain.agents导入AgentType
- en: '`from LangChain.llms import OpenAI`'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 从LangChain.llms导入OpenAI
- en: '`import os`'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 导入os
- en: 'In order to use SERP API, we need to register at and get `**serp_api_key**`:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用SERP API，我们需要注册并获取**serp_api_key**：
- en: '`serpapi_api_key = os.environ[‘SERPAPI_API_KEY’]`'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: serpapi_api_key = os.environ[‘SERPAPI_API_KEY’]
- en: '`llm = OpenAI(temperature=0)`'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: llm = OpenAI(temperature=0)
- en: 'We are equipping the agent with the SERP API and an LLM. These tools empower
    the agent to generate accurate and comprehensive answers:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在为代理配备SERP API和LLM。这些工具使代理能够生成准确和全面的答案：
- en: '`tools = load_tools([“serpapi”], llm=llm)`'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: tools = load_tools([“serpapi”], llm=llm)
- en: Now that we have the list of tools ready, let’s use the `**zero-shot-react-agent**`.
    This type of agent uses the *ReAct* [13] paradigm to determine the most suitable
    tool based on the tool’s description. The *ReAct* gets its name from the combination
    of the words *reasoning* and *acting*. This framework employs the reasoning component
    to identify the tasks that need to be performed and evaluate the observations.
    Then it utilizes the acting component to carry out tasks. *ReAct* is a general
    paradigm to combine reasoning and acting with language models for solving diverse
    language reasoning and decision-making tasks.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好工具列表，让我们使用**zero-shot-react-agent**。这种类型的代理使用*ReAct* [13]范式来确定基于工具描述的最合适的工具。*ReAct*得名于*reasoning*和*acting*这两个词的组合。这个框架利用推理组件来识别需要执行的任务并评估观察结果。然后它利用执行组件来执行任务。*ReAct*是一个将推理和行动与语言模型相结合，用于解决各种语言推理和决策任务的通用范式。
- en: '`agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True)`'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True)
- en: 'To get started, we first need to initialize the agent. We do this by providing
    it with a set of tools it can use, the reasoning LLM, and specifying the *ReAct*
    agent type. Additionally, we can set the verbose parameter to determine whether
    the agent should provide a step-by-step description of its actions. Once the agent
    is initialized, we can then request it to solve a problem:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，我们首先需要初始化代理。我们通过提供一组工具和推理LLM来实现这一点，并指定*ReAct*代理类型。此外，我们可以设置verbose参数，以确定代理是否应该提供其行动的逐步描述。一旦代理被初始化，我们就可以要求它解决一个问题：
- en: '`agent.run(“What is a price of current best Lamborghini model?”)`'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: agent.run(“当前最好的兰博基尼车型的价格是多少？”)
- en: 'Here is the complete code for reference:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 这是完整的代码供参考：
- en: '`from LangChain.agents import load_tools`'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 从LangChain.agents导入load_tools
- en: '`from LangChain.agents import initialize_agent`'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 从LangChain.agents导入initialize_agent
- en: '`from LangChain.agents import AgentType`'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 从LangChain.agents导入AgentType
- en: '`from LangChain.llms import OpenAI`'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 从LangChain.llms导入OpenAI
- en: '`import os`'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 导入os
- en: '`serpapi_api_key = os.environ[“SERPAPI_API_KEY”]`'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: serpapi_api_key = os.environ[“SERPAPI_API_KEY”]
- en: '`llm = OpenAI(temperature=0)`'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: llm = OpenAI(temperature=0)
- en: '`tools = load_tools([“serpapi”], llm=llm)`'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: tools = load_tools([“serpapi”], llm=llm)
- en: '`agent = initialize_agent(`'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: agent = initialize_agent(
- en: '`tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True`'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: tools，llm，agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION，verbose=True
- en: '`)`'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: ）
- en: '`agent.run(“What is a price of current best Lamborghini model?”)`'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: agent.run(“当前最好的兰博基尼车型的价格是多少？”)
- en: 'After initializing the agent and equipping it with tools we run it by using
    the query: *What is the price of the latest Lamborghini model?* The agent then
    begins its thought process and carries out a series of actions. These steps are
    displayed as output, thanks to the verbose parameter that we previously set during
    the initialization process:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化代理并装备它的工具后，我们通过使用查询*最新兰博基尼车型的价格是多少？*来运行它。代理然后开始思考并执行一系列动作。由于我们在初始化过程中先前设置了verbose参数，这些步骤将显示为输出：
- en: '`Entering new AgentExecutor chain…`'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 进入新的AgentExecutor链…
- en: '`I should look up the current best Lamborghini model`'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 我应该查找当前最好的兰博基尼车型
- en: '`Action: Search`'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 动作：搜索
- en: '`Action Input: “current best Lamborghini model”`'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 动作输入：“当前最好的兰博基尼车型”
- en: '`Observation: Best of the Current Lamborghini Model Lineup · Lamborghini Huracán
    Evo RWD / STO · Lamborghini Aventador SVJ · Lamborghini Urus · Lamborghini Sián
    · Awesome …`'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 观察：当前兰博基尼车型阵容中最好的是·兰博基尼Huracán Evo RWD / STO·兰博基尼Aventador SVJ·兰博基尼Urus·兰博基尼Sián·很棒…
- en: '`Thought: I should look up the price of the Lamborghini Huracán Evo RWD / STO`'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 思考：我应该查找兰博基尼Huracán Evo RWD / STO的价格
- en: '`Action: Search`'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 动作：搜索
- en: '`Action Input: “Lamborghini Huracán Evo RWD / STO price”`'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 动作输入：“兰博基尼Huracán Evo RWD / STO价格”
- en: '`Observation: When it was launched, the Lamborghini Huracan STO coupe’s price
    was $327,838 before taxes and delivery, and it’s up to around $335,000 for a 2023
    model. Should …`'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: '`Thought: I now know the final answer`'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: '`Final Answer: The price of the Lamborghini Huracán Evo RWD / STO is around
    $335,000 for a 2023 model.`'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: '`> Finished chain.`'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: Determining the price of the top Lamborghini model is the task to be performed
    by the agent. The agent starts with identifying the requested Lamborghini model,
    which was described as “best”.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: Once the model is identified, the agent uses the SERP API to conduct a thorough
    search for specific information about its price and gather relevant data. It continuously
    evaluates the output received from the API, carefully analyzing it to identify
    missing information. To arrive at the correct answer, the agent relies on a combination
    of reasoning and executing capabilities. This is what makes LangChain’s agents
    so powerful. They have the ability to use tools and are equipped with LLMs that
    allow them to make conclusions. By following this systematic approach, the agent
    is able to determine the price of the top Lamborghini model with precision and
    reliability. This showcases the impressive capabilities of LangChain’s agents,
    making them a valuable tool for various use cases. Some other examples include
    utilizing agents to create personalized dynamic pricing strategies, generating
    risk assessments for individual stocks based on historical and market data, or
    identifying market trends by analyzing news articles and social media posts relevant
    to specific industries.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: '[Conclusion](toc.xhtml#s458a)'
  id: totrans-484
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The LangChain framework is a powerful Python-based tool that unlocks the full
    potential of Large Language Models for application development. This framework
    offers a comprehensive set of components that empowers developers to create sophisticated
    applications. By leveraging the use of components, chains, and agents, the development
    process becomes more streamlined and efficient. This results in significant effort
    savings, improved quality, standardization, and readability, ultimately leading
    to increased productivity and faster time-to-market for organizations. LangChain
    is a game-changing tool that can revolutionize the way businesses and professionals
    harness all the capabilities of Large Language Models.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: As we conclude our chapter on LangChain, we have seen how this Python-based
    framework excels in developing GPT-based applications. As we embark on the next
    chapter, we will delve into `predictive-powers`, an exceptional, Java-based library
    specially designed for building solutions that harness the power of LLMs - a continuation
    of our exciting journey into the domain of generative AI.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: '[Key Points](toc.xhtml#s459a)'
  id: totrans-487
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LangChain is a Python-based framework that allows developers to create applications
    that maximize the potential of Large Language Models. It works with LLMs from
    multiple vendors, including OpenAI, as well as custom-made models.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using LangChain contributes to improved quality, standardization and readability.
    With its ready-to-use components, the process of developing comprehensive applications
    is simplified and streamlined.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By combining pre-learned knowledge with real-time or domain information, LangChain
    enhances the capabilities and reliability of working with LLMs.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The framework consists of components like Schema, Models, Data processing, Chains,
    Memory Handling, and Agents, which empower developers to work faster.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chains are ready-to-use classes that allow stacking commands. They contribute
    to increased efficiency for developers.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agents are highly valuable in tasks that require independent reasoning, planning,
    and execution actions, making it possible to create autonomous and powerful applications.[PRE0]`
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
