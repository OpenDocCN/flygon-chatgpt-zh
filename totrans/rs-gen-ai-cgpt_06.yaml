- en: '[CHAPTER 7](toc.xhtml#c07)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[ChatGPT Technical Overview: Introduction](toc.xhtml#c07)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Introduction](toc.xhtml#s63a)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Artificial Intelligence or Machine learning, provides automated both supervised
    and unsupervised learning across many modalities, be it textual, or imagery, or
    vocal, maybe across different types such as numerical data, contextual data, feature-based
    data, pattern-based data. **Natural language processing** (**NLP**) has been one
    of the subdomains in the arena of Artificial Intelligence which only captures
    almost 1/5th market share and number of solutions, focusing on the interaction
    between computers and human language.
  prefs: []
  type: TYPE_NORMAL
- en: '[Introduction to Natural Language Processing](toc.xhtml#s64a)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NLP uses computational techniques to enable computers to understand, interpret,
    and generate human language. It is one of crucial segments of AI which deals with
    the linguistic tasks and automates the process of analyzing and getting meaningful
    context out of any phrase. The tasks involve sentiment analysis, context-mapping,
    chatbots, content predictions, captioning, answer generation, machine translation,
    content classification etc and are used across different industries like banking,
    finance, customer service, health and medical, educational and almost in every
    other entity. NLP has made significant advancements in recent years, thanks to
    the availability of large datasets, powerful computing resources, and advanced
    machine learning algorithms. With its ability to process and understand human
    language, NLP is helping to bridge the gap between humans and machines and making
    our interactions with technology more intuitive and natural.
  prefs: []
  type: TYPE_NORMAL
- en: '[Evolution of NLP](toc.xhtml#s65a)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: According to Stanford university, the first need towards NLP began during World
    war II where urgency translation was reflected. Back to the 1950s when researchers
    began to explore the possibility of using computers to understand and generate
    human language. In 1950, Alan Turing proposed the “Turing Test,” a benchmark for
    machine intelligence that involved a computer’s ability to carry on a conversation
    that was indistinguishable from a human. This led to the development of early
    NLP systems, such as the “ELIZA” program developed in the 1960s, which simulated
    a conversation between a computer and a human therapist.
  prefs: []
  type: TYPE_NORMAL
- en: In the *1970s*, researchers began to develop more advanced NLP algorithms, such
    as the “SHRDLU” program, which could understand natural language commands and
    manipulate virtual objects in a simulated environment. In the 1980s and 1990s,
    researchers focused on developing statistical models for language processing,
    which allowed computers to learn from large datasets of human language.
  prefs: []
  type: TYPE_NORMAL
- en: In the *2000s and 2010s*, NLP made significant advancements with the development
    of deep learning algorithms and the availability of large datasets, such as Wikipedia
    and social media data. These advancements have led to the development of more
    sophisticated NLP applications, such as voice assistants, chatbots, and machine
    translation.
  prefs: []
  type: TYPE_NORMAL
- en: In the latter part of the last decade, Natural Language Processing (NLP) has
    continued to advance, with researchers making significant progress in areas such
    as deep learning, transfer learning, and pre-training.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most significant developments in NLP has been the emergence of large
    pre-trained language models such as **BERT** (**Bidirectional Encoder Representations
    from Transformers**), GPT-2 (Generative Pre-trained Transformer 2), and GPT-3\.
    These models are trained on massive amounts of text data and can perform a wide
    range of NLP tasks, including text classification, question answering, and language
    generation. They have enabled researchers to achieve state-of-the-art results
    on a variety of NLP benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: Another important development in NLP has been the use of transfer learning,
    where models are first pre-trained on a large dataset and then fine-tuned for
    a specific task. This approach has been used to achieve high performance on a
    variety of NLP tasks, including sentiment analysis, named entity recognition,
    and text classification.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to these advancements, researchers have also focused on improving
    the robustness and fairness of NLP models. This includes developing methods to
    detect and mitigate bias in language data and models and to ensure that NLP applications
    are accessible to people from diverse linguistic and cultural backgrounds.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, these advancements in NLP have opened up new possibilities for developing
    more sophisticated and accurate language-based applications, from chatbots to
    virtual assistants, and are likely to have far-reaching implications for many
    industries in the years to come. From then, LUNAR- scientific qualitative data,
    ELIZA - the first chatbot, from the complex models and use cases of today’s date
    such as smart Alexa, conversational bots is Siri with high-level complex neural
    networks at backend. In the context of ChatGPT, it’s one of the modern advanced
    NLP architectures developed, which is able to perform very high level tasks with
    more quantitative and qualitative accuracy and precision, closer to human perceptions
    and interpretations. In between, there has been a gradual yet constant development
    of the process of improvement from Word2Vec model to today’s ChatGPT through neural
    networks, LSTM models, encoder-decoder, Attention models, Transformer model, Google’
    BERT, imageBERT.
  prefs: []
  type: TYPE_NORMAL
- en: '[GPT and ChatGPT](toc.xhtml#s66a)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Talking about the **Generative Pre-trained Transformer** (GPT), it is a sophisticated
    neural network architecture that underpins ChatGPT with their version 3.5 of the
    GPT series( known as InstructGPT), being their most recent development. The Transformers
    model, created by Google in 2017, is the foundation and the preliminary element
    for this GPT model. It is based on the intuition of the attention-based model
    that was first presented in the paper “**Attention is all you need**.”
  prefs: []
  type: TYPE_NORMAL
- en: '[GPT series by OpenAI](toc.xhtml#s67a)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Between 2019 and 2022, the whole GPT series had numerous technical model and
    hyper-parameter adjustments by openAI and they have been improvising on many micro-level
    changes. The entire GPT-3 consists of approximately 175B parameters in its entire
    model which is around 50x higher than the language model that Google introduced
    in 2018, BERT; though there are some heavily loaded language models available
    in the research of NLP - like Megatron-NLG, by NVIDIA, with 530B parameters which
    is composed of 560 DGX A100 servers, each containing eight A100 80GB GPUs, capable
    of auto-completing phrases and statements. Google’s PaLM scaled to 540B parameters
    is another example of such a highly multi-tasking NLP model, trained on the largest
    TPU of the world with 6144 chips. Google also introduced LaMDA; in contrast to
    the task-based replies that conventional models frequently provide, the model
    may produce conversational chat in a free-form manner, which also has around 137B
    parameters. The following bubble chart by *Dr Alan D. Thompson* blog series explains
    about the estimation on recent developments of heavy load models with large parameters
    in language model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/Figure-7.1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 7.1:** *Leading NLP models with large parameters [Source: Lifearchitect.ai]*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Points to remember](toc.xhtml#s68a)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Natural language processing (NLP) has been one of the subdomains in the arena
    of Artificial Intelligence which only captures almost 1/5th market share and number
    of solutions, focusing on the interaction between computers and human language.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NLP uses computational techniques to enable computers to understand, interpret,
    and generate human language.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NLP has made significant advancements in recent years, thanks to the availability
    of large datasets, powerful computing resources, and advanced machine learning
    algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With its ability to process and understand human language, NLP is helping to
    bridge the gap between humans and machines and making our interactions with technology
    more intuitive and natural.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the 2000s and 2010s, NLP made significant advancements with the development
    of deep learning algorithms and the availability of large datasets, such as Wikipedia
    and social media data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the latter part of the last decade, Natural Language Processing (NLP) has
    continued to advance, with researchers making significant progress in areas such
    as deep learning, transfer learning, and pre-training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These models are trained on massive amounts of text data and can perform a wide
    range of NLP tasks, including text classification, question answering, and language
    generation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to these advancements, researchers have also focused on improving
    the robustness and fairness of NLP models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This includes developing methods to detect and mitigate bias in language data
    and models and to ensure that NLP applications are accessible to people from diverse
    linguistic and cultural backgrounds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, these advancements in NLP have opened up new possibilities for developing
    more sophisticated and accurate language-based applications, from chatbots to
    virtual assistants, and are likely to have far-reaching implications for many
    industries in the years to come.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our book's Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book''s Discord Workspace for Latest updates, Offers, Tech happenings
    around the world, New Release and Sessions with the Authors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**https://discord.bpbonline.com**](https://discord.bpbonline.com)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/dis.jpg)'
  prefs: []
  type: TYPE_IMG
