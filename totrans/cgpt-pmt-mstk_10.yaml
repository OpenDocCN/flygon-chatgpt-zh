- en: '| ![image](d2d_images/chapter_title_corner_decoration_left.png) |  | ![image](d2d_images/chapter_title_corner_decoration_right.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '![image](d2d_images/chapter_title_above.png)'
  prefs: []
  type: TYPE_IMG
- en: Tokenization and Tokens
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![image](d2d_images/chapter_title_below.png)'
  prefs: []
  type: TYPE_IMG
- en: Tokenization is the process of breaking down text into smaller units called
    tokens, usually words or sub-words. These tokens serve as the input for the neural
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Granularity, or the level of detail in tokens, affects how the model processes
    text and learns language patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples to illustrate the different levels of granularity in tokenization:'
  prefs: []
  type: TYPE_NORMAL
- en: Character-level tokenization (finer granularity)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Input text: "ChatGPT is amazing!"'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tokens: [''C'', ''h'', ''a'', ''t'', ''G'', ''P'', ''T'', '' '', ''i'', ''s'',
    '' '', ''a'', ''m'', ''a'', ''z'', ''i'', ''n'', ''g'', ''!'']'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, each character, including spaces and punctuation marks, is treated as
    an individual token.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word-level tokenization (coarser granularity)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Input text: "ChatGPT is amazing!"'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tokens: [''ChatGPT'', ''is'', ''amazing'', ''!'']'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, each word is treated as a separate token, including punctuation
    marks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sub-word-level tokenization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Input text: "ChatGPT is amazing!"'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tokens: [''Chat'', ''G'', ''PT'', '' is'', '' amaz'', ''ing'', ''!'']'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sub-word tokenization breaks the text into smaller units that are larger than
    characters but smaller than full words. These units can be parts of words or even
    combinations of words and spaces, depending on the specific tokenization method
    used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finer granularity aids the model in learning shared patterns among words and
    generalizing better, while coarser granularity may limit its ability to do so.
  prefs: []
  type: TYPE_NORMAL
