- en: 7 LLMs for Data Science
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7种用于数据科学的LLMs
- en: Join our book community on Discord
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们在Discord上的书籍社区
- en: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
- en: '![Qr code Description automatically generated](../media/file49.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的二维码描述](../media/file49.png)'
- en: 'This chapter is about how generative AI can automate data science. Generative
    AI, in particular large language models (LLMs) have the potential to accelerate
    scientific progress across various domains, especially by providing efficient
    analysis of research data and aiding in literature review processes. A lot of
    current approaches that fall within the domain of AutoML can help data scientists
    increase their productivity and help make the data science more repeatable. I’ll
    first give an overview over automation in data science and then we’ll discuss
    how data science is affected by generative AI.Next, we’ll discuss how we can use
    code generation and tools in different ways to answer questions related to data
    science. This can come in the form of doing a simulation or of enriching out dataset
    with additional information. Finally, we put the focus on exploratory analysis
    of structured datasets. We can set up agents to run SQL or tabular data in Pandas.
    We’ll see how we can ask questions about the dataset, statistical questions about
    the data, or ask for visualizations.Throughout the chapter, we’ll work on different
    approaches to doing data science with LLMs, which you can find in the `data_science`
    directory in the Github repository for the book at [https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain)The
    main sections are:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了生成式人工智能如何自动化数据科学。生成式人工智能，特别是大型语言模型（LLMs），有潜力加速各个领域的科学进展，尤其是通过提供对研究数据的高效分析和协助文献回顾过程。许多当前属于AutoML领域的方法可以帮助数据科学家提高生产力，并帮助使数据科学更具可重复性。我将首先概述数据科学中的自动化，然后我们将讨论生成式人工智能对数据科学的影响。接下来，我们将讨论如何以不同方式使用代码生成和工具来回答与数据科学相关的问题。这可以以模拟的形式进行，或者通过为数据集添加额外信息来丰富数据集。最后，我们将重点放在对结构化数据集的探索性分析上。我们可以设置代理来运行SQL或Pandas中的表格数据。我们将看到如何提出关于数据集的问题、关于数据的统计问题，或者要求可视化。在整个章节中，我们将使用LLMs进行数据科学的不同方法，您可以在书籍的Github存储库中的`data_science`目录中找到。主要章节包括：
- en: Automated data science
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化数据科学
- en: Agents can answer data science questions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理可以回答数据科学问题
- en: Data exploration with LLMs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LLMs进行数据探索
- en: Let’s start by discussing how data science can be automated and which parts
    of it, and how generative AI will impact data science.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从讨论数据科学如何自动化以及其中的哪些部分开始，以及生成式人工智能将如何影响数据科学。
- en: Automated data science
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动化数据科学
- en: 'Data science is a field that combines computer science, statistics, and business
    analytics to extract knowledge and insights from data. Data scientists use a variety
    of tools and techniques to collect, clean, analyze, and visualize data. They then
    use this information to help businesses make better decisions.The work of a data
    scientist can vary depending on the specific role and industry. However, some
    common tasks that data scientists might perform include:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学是一个结合了计算机科学、统计学和商业分析的领域，从数据中提取知识和见解。数据科学家使用各种工具和技术来收集、清洗、分析和可视化数据。然后他们利用这些信息帮助企业做出更好的决策。数据科学家的工作可能因具体角色和行业而异。然而，数据科学家可能执行的一些常见任务包括：
- en: Collecting data: Data scientists need to collect data from a variety of sources,
    such as databases, social media, and sensors.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集数据：数据科学家需要从各种来源收集数据，如数据库、社交媒体和传感器。
- en: Cleaning data: Data scientists need to clean data to remove errors and inconsistencies.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清洗数据：数据科学家需要清洗数据以消除错误和不一致性。
- en: Analyzing data: Data scientists use a variety of statistical and machine learning
    techniques to analyze data.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析数据：数据科学家使用各种统计和机器学习技术来分析数据。
- en: Visualizing data: Data scientists use data visualizations to communicate insights
    to stakeholders.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可视化：数据科学家使用数据可视化向利益相关者传达见解。
- en: Building models: Data scientists build models to predict future outcomes or
    make recommendations.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建模型：数据科学家构建模型以预测未来结果或提出建议。
- en: 'Data analysis is a subset of data science that focuses on extracting insights
    from data. Data analysts use a variety of tools and techniques to analyze data,
    but they typically do not build models.The overlap between data science and data
    analysis is that both fields involve working with data to extract insights. However,
    data scientists typically have a more technical skillset than data analysts. Data
    scientists are also more likely to build models and sometimes deploy models into
    production. Data scientists sometimes deploy models into production so that they
    can be used to make decisions in real time, however, we’ll avoid automatic deployment
    of models in this discussion.Here is a table that summarizes the key differences
    between data science and data analysis:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析是数据科学的一个子集，专注于从数据中提取见解。数据分析师使用各种工具和技术来分析数据，但他们通常不构建模型。数据科学和数据分析之间的重叠之处在于两个领域都涉及与数据一起工作以提取见解。然而，数据科学家通常具有比数据分析师更丰富的技术技能。数据科学家也更有可能构建模型，并有时将模型部署到生产环境中。数据科学家有时将模型部署到生产环境中，以便实时做出决策，但在本讨论中，我们将避免自动部署模型。以下是总结数据科学和数据分析之间关键差异的表格：
- en: '| **Feature** | **Data Science** | **Data Analysis** |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| **特点** | **数据科学** | **数据分析** |'
- en: '| Technical skillset | More technical | Less technical |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 技术技能 | 更丰富 | 较少 |'
- en: '| Machine learning | Yes | No |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 机器学习 | 是 | 否 |'
- en: '| Model deployment | Sometimes | No |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 模型部署 | 有时 | 否 |'
- en: '| Focus | Extracting insights and building models | Extracting insights |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 焦点 | 提取见解和构建模型 | 提取见解 |'
- en: 'Figure 7.1: Comparison of Data Science and Data Analysis.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1：数据科学和数据分析的比较。
- en: 'The common denominator between the two is collecting data, cleaning data, analyzing
    data, visualizing data, all of which fall into the category of extracting insights.
    Data science, additionally is about training machine learning models and usually
    it has a stronger focus on statistics. In some cases, depending on the setup in
    the company and industry practices, deploying models and writing software can
    be added to the list for data science. Automatic data analysis and data science
    aims to automate many of the tedious, repetitive tasks involved in working with
    data. This includes data cleaning, feature engineering, model training, tuning,
    and deployment. The goal is to make data scientists and analysts more productive
    by enabling faster iterations and less manual coding for common workflows.A lot
    of these tasks can be automated to some degree. Some of the tasks for data science
    are similar to those of a software developer that we talked about in chapter 6,
    *Developing Software*, namely writing and deploying software although with a narrower
    focus, on models.Data science platforms such as Weka, H2O, KNIME, RapidMiner,
    and Alteryx are unified machine learning and analytics engines that can be used
    for a variety of tasks, including preprocessing of large volumes of data and feature
    extraction. All of these come with a graphical user interface (GUI), have the
    capability to integrate 3^(rd) party data source, and write custom plug-ins. KNIME
    is mostly open-source, however also offers a commercial product called KNIME Server. Apache
    Spark is a versatile tool that can be used for a variety of tasks involved in
    data science. It can be used to to clean, transform, extract features, and prepare
    high-volume data for analysis and also to train and deploy machine learning models,
    both in streaming scenarios, when it’s about real-time decisions or monitoring
    events.Further, at its most fundamental, libraries for scientific computing such
    as NumPy can be serve for all tasks involved in automated data science. Deep learning
    and machine learning libraries such as TensorFlow, Pytorch, and Scikit-Learn can
    be used for a variety of tasks beyond creating complex machine learning models,
    including data preprocessing and feature extraction. Orchestration tools such
    as Airflow, Kedro, or others can help in all these tasks, and include a lot of
    integrations with specific tools related to all steps in data science.Several
    data science tools have generative AI support. In *Chapter 6*, *Developing Software*,
    we’ve already mentioned GitHub Copilot, but there are others such as the PyCharm
    AI Assistant, and even more to the point, Jupyter AI, which is a subproject of
    Project Jupyter that brings generative artificial intelligence to Jupyter notebooks.
    Jupyter AI allows users to generate code, fix errors, summarize content, and even
    create entire notebooks using natural language prompts. The tool connects Jupyter
    with LLMs from various providers, allowing users to choose their preferred model
    and embedding.Jupyter AI prioritizes responsible AI and data privacy. The underlying
    prompts, chains, and components are open source, ensuring transparency. It saves
    metadata about model-generated content, making it easy to track AI-generated code
    within the workflow. Jupyter AI respects user data privacy and only contacts LLMs
    when explicitly requested, which is done through LangChain integrations.To use
    Jupyter AI, users can install the appropriate version for JupyterLab and access
    it through a chat UI or the magic command interface. The chat interface features
    Jupyternaut, an AI assistant that can answer questions, explain code, modify code,
    and identify errors. Users can also generate entire notebooks from text prompts.The
    software allows users to teach Jupyternaut about local files and interact with
    LLMs using magic commands in notebook environments. It supports multiple providers
    and offers customization options for the output format. This screenshot from the
    documentation shows the chat feature, the Jupyternaut chat:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 两者之间的共同点是收集数据、清洗数据、分析数据、可视化数据，所有这些都属于提取见解的范畴。数据科学另外还涉及训练机器学习模型，通常更加注重统计学。在某些情况下，根据公司的设置和行业惯例，部署模型和编写软件可能会被添加到数据科学的任务列表中。自动数据分析和数据科学旨在自动化许多与处理数据相关的繁琐、重复的任务。这包括数据清洗、特征工程、模型训练、调优和部署。目标是通过实现更快的迭代和减少常见工作流程的手动编码，使数据科学家和分析师更加高效。许多这些任务可以在一定程度上自动化。数据科学的一些任务与我们在第6章“开发软件”中讨论的软件开发人员的任务相似，即编写和部署软件，尽管焦点更窄，主要集中在模型上。数据科学平台如Weka、H2O、KNIME、RapidMiner和Alteryx是统一的机器学习和分析引擎，可用于各种任务，包括预处理大量数据和特征提取。所有这些都配备了图形用户界面（GUI），具有集成第三方数据源和编写自定义插件的能力。KNIME主要是开源的，但也提供了一个名为KNIME
    Server的商业产品。Apache Spark是一种多功能工具，可用于数据科学中涉及的各种任务。它可用于清洁、转换、提取特征和准备大容量数据进行分析，还可用于训练和部署机器学习模型，无论是在流式场景中，当涉及实时决策或监控事件时。此外，在其最基本的层面上，用于科学计算的库，如NumPy，可以用于自动化数据科学中涉及的所有任务。深度学习和机器学习库，如TensorFlow、Pytorch和Scikit-Learn，可用于各种任务，包括创建复杂的机器学习模型以外的数据预处理和特征提取。编排工具，如Airflow、Kedro或其他工具，可以帮助完成所有这些任务，并包含许多与数据科学各个步骤相关的特定工具的集成。几个数据科学工具支持生成式人工智能。在第6章“开发软件”中，我们已经提到了GitHub
    Copilot，但还有其他工具，如PyCharm AI助手，甚至更为重要的Jupyter AI，它是Project Jupyter的一个子项目，为Jupyter笔记本带来了生成式人工智能。Jupyter
    AI允许用户生成代码、修复错误、总结内容，甚至使用自然语言提示创建整个笔记本。该工具将Jupyter与各种提供商的LLM连接起来，使用户可以选择其首选模型和嵌入。Jupyter
    AI优先考虑负责任的人工智能和数据隐私。底层提示、链和组件是开源的，确保透明度。它保存有关模型生成内容的元数据，使跟踪工作流中的AI生成代码变得容易。Jupyter
    AI尊重用户数据隐私，仅在明确请求时才与LLMs联系，这是通过LangChain集成完成的。要使用Jupyter AI，用户可以安装适用于JupyterLab的适当版本，并通过聊天界面或魔术命令界面访问它。聊天界面具有Jupyternaut，一个可以回答问题、解释代码、修改代码和识别错误的AI助手。用户还可以从文本提示中生成整个笔记本。该软件允许用户教导Jupyternaut有关本地文件，并在笔记本环境中使用魔术命令与LLMs进行交互。它支持多个提供商，并为输出格式提供定制选项。文档中的此截图显示了聊天功能，Jupyternaut聊天：
- en: '![Figure 7.2: Jupyter AI – Jupyternaut chat.](../media/file50.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2：Jupyter AI – Jupyternaut 聊天。](../media/file50.png)'
- en: 'Figure 7.2: Jupyter AI – Jupyternaut chat.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2：Jupyter AI – Jupyternaut 聊天。
- en: It should be plain to see that having a chat like that at your fingertips to
    ask questions, create simple functions, or change existing functions can be a
    boon to data scientists. The benefits of using these tools include improved efficiency,
    reduced manual effort in tasks like model building or feature selection, enhanced
    interpretability of models, identification and fixing of data quality issues,
    integration with other scikit-learn pipelines (pandas_dq), and overall improvement
    in the reliability of results.Overall, automated data science can greatly accelerate
    analytics and ML application development. It allows data scientists to focus on
    higher value and creative aspects of the process. Democratizing data science for
    business analysts is also a key motivation behind automating these workflows.
    In the following sections, we’ll look into these steps in turn, and we’ll discuss
    automating them, and we’ll highlight how generative AI can make a contribution
    to improving the workflow and create efficiency gains.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，像这样随时可以提问、创建简单函数或更改现有函数的聊天工具对数据科学家来说是一个福音。使用这些工具的好处包括提高效率，在模型构建或特征选择等任务中减少手动工作量，增强模型的可解释性，识别和解决数据质量问题，与其他
    scikit-learn 管道（pandas_dq）集成，以及结果可靠性的整体改善。总的来说，自动化数据科学可以极大加速分析和机器学习应用程序的开发。它使数据科学家能够专注于过程的更高价值和创造性方面。为业务分析师民主化数据科学也是自动化这些工作流程的一个关键动机。在接下来的章节中，我们将依次研究这些步骤，并讨论如何自动化它们，以及我们将强调生成式人工智能如何为改善工作流程和创造效率带来贡献。
- en: Data collection
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据收集
- en: 'Automated data collection is the process of collecting data without human intervention.
    Automatic data collection can be a valuable tool for businesses. It can help businesses
    to collect data more quickly and efficiently, and it can free up human resources
    to focus on other tasks. Generally, in the context of data science or analytics
    we refer to ETL (Extract, Transform, and Load) as the process that not only takes
    data from one or more sources (the data collection), but also prepares it for
    specific use cases. The ETL process typically follows these steps:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化数据收集是在没有人为干预的情况下收集数据的过程。自动数据收集可以成为企业的有价值工具。它可以帮助企业更快速、更高效地收集数据，并可以释放人力资源专注于其他任务。通常，在数据科学或分析的背景下，我们将
    ETL（提取、转换和加载）称为不仅从一个或多个来源获取数据（数据收集），还为特定用例准备数据的过程。ETL 过程通常遵循以下步骤：
- en: Extract: The data is extracted from the source systems. This can be done using
    a variety of methods, such as web scraping, API integration, or database queries.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Extract：数据从源系统中提取出来。可以使用各种方法进行提取，例如网页抓取，API 集成或数据库查询。
- en: Transform: The data is transformed into a format that can be used by the data
    warehouse or data lake. This may involve cleaning the data, removing duplicates,
    and standardizing the data format.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Transform：数据被转换为数据仓库或数据湖可以使用的格式。这可能涉及清洗数据，去重和标准化数据格式。
- en: Load: The data is loaded into the data warehouse or data lake. This can be done
    using a variety of methods, such as bulk loading or incremental loading.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Load：数据被加载到数据仓库或数据湖中。可以使用各种方法进行加载，例如批量加载或增量加载。
- en: 'ETL and data collection can be done using a variety of tools and techniques,
    such as:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ETL 和数据收集可以使用各种工具和技术完成，例如：
- en: Web scraping: Web scraping is the process of extracting data from websites.
    This can be done using a variety of tools, such as Beautiful Soup, Scrapy, Octoparse,
    .
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网页抓取：网页抓取是从网站提取数据的过程。可以使用各种工具进行此操作，例如 Beautiful Soup、Scrapy、Octoparse 等。
- en: 'APIs (Application Programming Interfaces): These are a way for software applications
    to talk to each other. Businesses can use APIs to collect data from other companies
    without having to build their own systems.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API（应用程序编程接口）：这是软件应用程序相互通信的一种方式。企业可以使用 API 从其他公司收集数据，而无需构建自己的系统。
- en: 'Query languages: Any database can serve as data source including of the SQL
    (Structured Query Language) or the no-SQL variety.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询语言：任何数据库都可以作为数据源，包括 SQL（结构化查询语言）或无 SQL 类型。
- en: Machine learning: Machine learning can be used to automate the process of data
    collection. For example, businesses can use machine learning to identify patterns
    in data and then collect data based on those patterns.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习：机器学习可以用于自动化数据收集过程。例如，企业可以使用机器学习识别数据中的模式，然后根据这些模式收集数据。
- en: 'Once the data has been collected, it can be processed to prepare it for use
    in a data warehouse or data lake. The ETL process will typically clean the data,
    remove duplicates, and standardize the data format. The data will then be loaded
    into the data warehouse or data lake, where it can be used by data analysts or
    data scientists to gain insights into the business.There are many ETL tools including
    commercial ones such as AWS glue, Google Dataflow, Amazon Simple Workflow Service
    (SWF), dbt, Fivetran, Microsoft SSIS, IBM InfoSphere DataStage, Talend Open Studio
    or open-source tools such as Airflow, Kafka, and Spark. In Python are many more
    tools, too many to list all, such as Pandas for data extraction and processing,
    and even celery and joblib, which can serve as ETL orchestration tools. In LangChain,
    there’s an integration with Zapier, which is an automation tool that can be used
    to connect different applications and services. This can be used to automate the
    process of data collection from a variety of sources.Here are some of the benefits
    of using automated ETL tools:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据被收集，就可以对其进行处理，以准备在数据仓库或数据湖中使用。ETL过程通常会清理数据，删除重复项，并标准化数据格式。然后数据将被加载到数据仓库或数据湖中，数据分析师或数据科学家可以利用这些数据来获取对业务的洞察。有许多ETL工具，包括商业工具如AWS
    Glue、Google Dataflow、Amazon Simple Workflow Service（SWF）、dbt、Fivetran、Microsoft
    SSIS、IBM InfoSphere DataStage、Talend Open Studio或开源工具如Airflow、Kafka和Spark。在Python中还有许多其他工具，太多了无法列出所有，例如用于数据提取和处理的Pandas，甚至celery和joblib，可以作为ETL编排工具。在LangChain中，与Zapier集成，这是一种可以用于连接不同应用程序和服务的自动化工具。这可以用于自动化从各种来源收集数据的过程。使用自动化ETL工具的一些好处包括：
- en: Increased accuracy: Automated ETL tools can help to improve the accuracy of
    the data extraction, transformation, and loading process. This is because the
    tools can be programmed to follow a set of rules and procedures, which can help
    to reduce human error.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高准确性：自动化的ETL工具可以帮助提高数据提取、转换和加载过程的准确性。这是因为工具可以编程遵循一套规则和程序，可以帮助减少人为错误。
- en: Reduced time to market: Automated ETL tools can help to reduce the time it takes
    to get data into a data warehouse or data lake. This is because the tools can
    automate the repetitive tasks involved in the ETL process, such as data extraction
    and loading.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩短上市时间：自动化的ETL工具可以帮助缩短将数据导入数据仓库或数据湖所需的时间。这是因为工具可以自动化ETL过程中涉及的重复任务，如数据提取和加载。
- en: Improved scalability: Automated ETL tools can help to improve the scalability
    of the ETL process. This is because the tools can be used to process large volumes
    of data, and they can be easily scaled up or down to meet the needs of the business.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高可扩展性：自动化的ETL工具可以帮助提高ETL过程的可扩展性。这是因为工具可以用于处理大量数据，并且可以轻松地按需扩展或缩减以满足业务需求。
- en: Improved compliance: Automated ETL tools can help to improve compliance with
    regulations such as GDPR and CCPA. This is because the tools can be programmed
    to follow a set of rules and procedures, which can help to ensure that data is
    processed in a compliant manner.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高合规性：自动化的ETL工具可以帮助提高符合GDPR和CCPA等法规的合规性。这是因为工具可以编程遵循一套规则和程序，可以帮助确保数据以符合法规的方式处理。
- en: The best tool for automatic data collection will depend on the specific needs
    of the business. Businesses should consider the type of data they need to collect,
    the volume of data they need to collect, and the budget they have available.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 自动数据收集的最佳工具将取决于企业的具体需求。企业应考虑他们需要收集的数据类型、需要收集的数据量以及他们可用的预算。
- en: Visualization and EDA
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化和探索性数据分析
- en: 'Automated EDA (Exploratory Data Analysis) and visualization refer to the process
    of using software tools and algorithms to automatically analyze and visualize
    data, without significant manual intervention. Traditional EDA involves manually
    exploring and summarizing data to understand its various aspects before performing
    machine learning or deep learning tasks. It helps in identifying patterns, detecting
    inconsistencies, testing assumptions, and gaining insights. However, with the
    advent of large datasets and the need for efficient analysis, automated EDA has
    become important.Automated EDA and visualization tools provide several benefits.
    They can speed up the data analysis process, reducing the time spent on tasks
    like data cleaning, handling missing values, outlier detection, and feature engineering.
    These tools also enable a more efficient exploration of complex datasets by generating
    interactive visualizations that provide a comprehensive overview of the data.Several
    tools are available for automated EDA and visualization, including:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化探索性数据分析（EDA）和可视化是指使用软件工具和算法自动分析和可视化数据，无需重大手动干预的过程。传统的EDA涉及手动探索和总结数据，以了解其各个方面，然后再执行机器学习或深度学习任务。它有助于识别模式、检测不一致性、测试假设并获得洞察。然而，随着大型数据集的出现和对高效分析的需求，自动化EDA变得重要。自动化EDA和可视化工具提供了几个好处。它们可以加快数据分析过程，减少在数据清洗、处理缺失值、异常值检测和特征工程等任务上花费的时间。这些工具还通过生成交互式可视化，提供对数据的全面概述，实现对复杂数据集的更有效探索。有许多用于自动化EDA和可视化的工具，包括：
- en: 'D-Tale: A library that facilitates easy visualization of pandas data frames.
    It supports interactive plots, 3D plots, heatmaps, correlation analysis, custom
    column creation.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: D-Tale：一个库，方便地可视化pandas数据框。它支持交互式图表、3D图表、热图、相关性分析、自定义列创建。
- en: 'ydata-profiling (previously pandas profiling): An open-source library that
    generates interactive HTML reports (`ProfileReport`) summarizing different aspects
    of the dataset such as missing values statistics, variable types distribution
    profiles, correlations between variables. It works with Pandas as well as Spark
    DataFrames.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ydata-profiling（之前称为pandas profiling）：这是一个开源库，生成交互式HTML报告（`ProfileReport`），总结数据集的不同方面，如缺失值统计、变量类型分布概况、变量之间的相关性。它适用于Pandas和Spark
    DataFrames。
- en: 'Sweetviz: A Python library that provides visualization capabilities for exploratory
    data analysis with minimal code required. It allows for comparisons between variables
    or datasets.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sweetviz：一个Python库，提供探索性数据分析的可视化功能，只需很少的代码。它允许在变量或数据集之间进行比较。
- en: 'Autoviz: This library automatically generates visualizations for datasets regardless
    of their size with just a few lines of code.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Autoviz：这个库可以自动生成数据集的可视化，无论其大小，只需几行代码。
- en: 'DataPrep: With just a few lines you can collect data from common data sources
    do EDA and data cleaning such as standardization of column names or entries.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataPrep：只需几行代码，您就可以从常见数据源中收集数据，进行探索性数据分析和数据清洗，比如标准化列名或条目。
- en: 'Lux: Displays a set of visualizations with interesting trends and patterns
    in the dataset displayed via an interactive widget that users can quickly browse
    in order to gain insights.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lux：通过交互式小部件显示数据集中有趣的趋势和模式的一组可视化结果，用户可以快速浏览以获取洞察。
- en: The use of generative AI in data visualization adds another dimension to automated
    EDA by allowing algorithms to generate new visualizations based on existing ones
    or specific user prompts. Generative AI has the potential to enhance creativity
    by automating part of the design process while maintaining human control over
    the final output.Overall, automated EDA and visualization tools offer significant
    advantages in terms of time efficiency, comprehensive analysis, and the generation
    of meaningful visual representations of data. Generative AI has the potential
    to revolutionize data visualization in a number of ways. For example, it can be
    used to create more realistic and engaging visualizations, which can help in business
    communication and to communicate data more effectively to stakeholders to provide
    each user with the information they need to gain insights and make informed decisions.Generative
    AI can enhance and extend the creation that traditional tools are capable of by
    making personalized visualizations tailored to the individual needs of each user.
    Further, Generative AI can be used to create interactive visualizations that allow
    users to explore data in new and innovative ways.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据可视化中使用生成式人工智能为自动探索性数据分析增加了另一个维度，使算法能够基于现有的可视化或特定用户提示生成新的可视化。生成式人工智能有潜力通过自动化设计过程的一部分来增强创造力，同时保持人类对最终输出的控制。总的来说，自动化的探索性数据分析和可视化工具在时间效率、全面分析以及生成有意义的数据可视化方面提供了显著优势。生成式人工智能有潜力以多种方式革新数据可视化。例如，它可以用于创建更真实和引人入胜的可视化，有助于商业沟通并更有效地向利益相关者传达数据，以为每个用户提供他们需要获取洞察并做出明智决策所需的信息。生成式人工智能可以通过制作针对每个用户个性化需求的可视化来增强和扩展传统工具的创作能力。此外，生成式人工智能可以用于创建交互式可视化，让用户以新颖创新的方式探索数据。
- en: Pre-processing and feature extraction
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理和特征提取
- en: Automated data preprocessing is the process of automating the tasks involved
    in data preprocessing. This can include tasks such as data cleaning, data integration,
    data transformation, and feature extraction. It is related to the transform step
    in ETL, so there’s a lot of overlap in tools and techniques.Data preprocessing
    is important because it ensures that data is in a format that can be used by data
    analysts and machine learning models. This includes removing errors and inconsistencies
    from the data, as well as converting it into a format that is compatible with
    the analytical tools that will be used.Manually engineering features can be tedious
    and time consuming, so automating this process is valuable. Recently, several
    open source Python libraries have emerged to help auto-generate useful features
    from raw data as we’ll see.Featuretools offers a general-purpose framework that
    can synthesize many new features from transactional and relational data. It integrates
    across multiple ML frameworks making it flexible. Feature Engine provides a simpler
    set of transformers focused on common data transformations like handling missing
    data. For optimizing feature engineering specifically for tree-based models, ta
    from Microsoft shows strong performance through techniques like automatic crossing.AutoGluon
    Features applies neural network style automatic feature generation and selection
    to boost model accuracy. It is tightly integrated with the AutoGluon autoML capabilities.
    Finally, TensorFlow Transform operates directly on Tensorflow pipelines to prepare
    data for models during training. It has progressed rapidly with diverse open source
    options now available. Featuretools provides the most automation and flexibility
    while integrating across ML frameworks. For tabular data, ta and Feature Engine
    offer easy-to-use transformers optimized for different models. Tf.transform is
    ideal for TensorFlow users, while AutoGluon specializes in the Apache MXNet deep
    learning software framework.As for time series data, Tsfel is a library that extracts
    features from time series data. It allows users to specify the window size for
    feature extraction and can analyze the temporal complexity of the features. It
    computes statistical, spectral, and temporal features.On the other hand, tsflex
    is a time series feature extraction toolkit that is flexible and efficient for
    sequence data. It makes few assumptions about the data structure and can handle
    missing data and unequal lengths. It also computes rolling features.Both libraries
    offer more modern options for automated time series feature engineering compared
    to tsfresh. Tsfel is more full-featured, while tsflex emphasizes flexibility on
    complex sequence data.There are a few tools that focus on data quality for machine
    learning and data science that come with data profiling and automatic data transformations.
    For example, the pandas-dq library, which can be integrated with scikit-learn
    pipelines, offers a range of useful features for data profiling, train-test comparison,
    data cleaning, data imputation (filling missing values), and data transformation
    (e.g., skewness correction). It helps improve the quality of data analysis by
    addressing potential issues before modeling.More focused on improved reliability
    through early identification of potential issues or errors are tools like Great
    Expectations and Deequ. Great Expectations is a tool for validating, documenting,
    and profiling data to maintain quality and improve communication between teams.
    It allows users to assert expectations on the data, catch issues quickly through
    unit tests for data, create documentation and reports based on expectations. Deequ
    is built on top of Apache Spark for defining unit tests for data quality in large
    datasets. It lets users explicitly state assumptions about the dataset and verifies
    them through checks or constraints on attributes. By ensuring adherence to these
    assumptions, it prevents crashes or wrong outputs in downstream applications.All
    these libraries allow data scientists to shorten feature preparation and expand
    the feature space to improve model quality. Automated feature engineering is becoming
    essential to leveraging the full power of ML algorithms on complex real-world
    data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化数据预处理是自动化数据预处理中涉及的任务的过程。这可能包括数据清洗、数据集成、数据转换和特征提取等任务。它与ETL中的转换步骤相关，因此在工具和技术上存在很多重叠。数据预处理很重要，因为它确保数据以数据分析师和机器学习模型可以使用的格式存在。这包括从数据中删除错误和不一致性，以及将其转换为与将要使用的分析工具兼容的格式。手动工程特征可能会很繁琐和耗时，因此自动化这个过程是有价值的。最近，出现了几个开源的Python库，可以帮助从原始数据中自动生成有用的特征，正如我们将看到的那样。Featuretools提供了一个通用框架，可以从事务性和关系数据中合成许多新特征。它集成了多个ML框架，使其灵活。Feature
    Engine提供了一组更简单的转换器，专注于处理缺失数据等常见数据转换。为了专门为基于树的模型优化特征工程，来自Microsoft的ta通过自动交叉等技术展现出强大的性能。AutoGluon
    Features应用神经网络风格的自动特征生成和选择来提高模型准确性。它与AutoGluon自动ML功能紧密集成。最后，TensorFlow Transform直接在Tensorflow管道上运行，为模型在训练期间准备数据。随着现在有多种开源选项，它已经迅速发展。Featuretools提供了最多的自动化和灵活性，同时集成了多个ML框架。对于表格数据，ta和Feature
    Engine提供了易于使用的转换器，针对不同的模型进行了优化。Tf.transform非常适合TensorFlow用户，而AutoGluon专门针对Apache
    MXNet深度学习软件框架。至于时间序列数据，Tsfel是一个从时间序列数据中提取特征的库。它允许用户指定特征提取的窗口大小，并可以分析特征的时间复杂性。它计算统计、频谱和时间特征。另一方面，tsflex是一个灵活高效的时间序列特征提取工具包，适用于序列数据。它对数据结构做出了很少的假设，可以处理缺失数据和不等长度。它还计算滚动特征。与tsfresh相比，这两个库提供了更现代的自动化时间序列特征工程选项。Tsfel功能更全面，而tsflex强调对复杂序列数据的灵活性。一些工具专注于机器学习和数据科学的数据质量，具有数据概要和自动数据转换。例如，pandas-dq库可以与scikit-learn管道集成，为数据概要、训练-测试比较、数据清理、数据填充（填补缺失值）和数据转换（例如，偏度校正）提供一系列有用的功能。它通过在建模之前解决潜在��题来改善数据分析的质量。更专注于通过早期识别潜在问题或错误来提高可靠性的工具包括Great
    Expectations和Deequ。Great Expectations是一个用于验证、记录和概要数据以保持质量并改善团队间沟通的工具。它允许用户对数据提出期望，通过数据的单元测试快速捕捉问题，基于期望创建文档和报告。Deequ建立在Apache
    Spark之上，用于在大型数据集中定义数据质量的单元测试。它允许用户明确陈述关于数据集的假设，并通过对属性进行检查或约束来验证这些假设。通过确保遵守这些假设，它可以防止下游应用程序中的崩溃或错误输出。所有这些库都允许数据科学家缩短特征准备时间，并扩展特征空间以提高模型质量。自动特征工程正变得越来越重要，以充分利用ML算法在复杂现实世界数据上的全部潜力。
- en: AutoML
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AutoML
- en: 'Automated Machine Learning (AutoML) frameworks are tools that automate the
    process of machine learning model development. They can be used to automate tasks
    such as data cleaning, feature selection, model training, and hyperparameter tuning.
    This can save data scientists a lot of time and effort, and it can also help to
    improve the quality of machine learning models.The basic idea of AutoML is illustrated
    in this diagram from the Github repo of the mljar autoML library (source: https://github.com/mljar/mljar-supervised):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化机器学习（AutoML）框架是一种自动化机器学习模型开发过程的工具。它们可以用于自动化任务，如数据清洗、特征选择、模型训练和超参数调整。这可以节省数据科学家大量的时间和精力，也有助于提高机器学习模型的质量。AutoML
    的基本思想在 mljar autoML 库的 Github 仓库中有所体现（来源：https://github.com/mljar/mljar-supervised）：
- en: '![Figure 7.3: How AutoML works.](../media/file51.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3：AutoML 的工作原理。](../media/file51.png)'
- en: 'Figure 7.3: How AutoML works.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3：AutoML 的工作原理。
- en: 'Load some data, try different combinations of preprocessing methods, ML algorithms,
    training and model parameters, create explanations, compare results in a leaderboard
    together with visualizations. The main value proposition of an AutoML framework
    is the ease-of-use of and an increased developer productivity in finding a machine
    learning model, understanding it, and getting it to production. AutoML tools have
    been around for a long time. One of the first broader frameworks was AutoWeka,
    written in Java, and it was designed to automate the process of machine learning
    model development for tabular data in the Weka (Waikato Environment for Knowledge
    Analysis) machine learning suite, which is developed at the University of Waikato.In
    the years since AutoWeka was released, there have been many other AutoML frameworks
    developed. Some of the most popular AutoML frameworks today include auto-sklearn,
    autokeras, NASLib, Auto-Pytorch, tpot, optuna, autogluon, and ray (tune). These
    frameworks are written in a variety of programming languages, and they support
    a variety of machine learning tasks.Recent advances in autoML and neural architecture
    search have allowed tools to automate large parts of the machine learning pipeline.
    Leading solutions like Google AutoML, Azure AutoML, and H2O AutoML/Driverless
    AI can automatically handle data prep, feature engineering, model selection, hyperparameter
    tuning, and deployment based on the dataset and problem type. These make machine
    learning more accessible to non-experts as well.Current autoML solutions can handle
    structured data like tables and time series data very effectively. They can automatically
    generate relevant features, select algorithms like tree ensembles, neural networks
    or SVMs, and tune hyperparameters. Performance is often on par or better than
    manual process due to massive hyperparameter search. AutoML for unstructured data
    like images, video and audio is also advancing rapidly with neural architecture
    search techniques.Open source libraries like AutoKeras, AutoGluon, and AutoSklearn
    provide accessible autoML capabilities as well. However, most autoML tools still
    require some coding and data science expertise. Fully automating data science
    remains challenging and autoML does have limitations in flexibility and controllability.
    But rapid progress is being made with more user-friendly and performant solutions
    coming to market.Here’s a tabular summary of frameworks:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 加载一些数据，尝试不同的预处理方法、机器学习算法、训练和模型参数的组合，创建解释，将结果与可视化一起在排行榜上进行比较。AutoML 框架的主要价值主张在于易用性和提高开发者在找到机器学习模型、理解它并将其投入生产中的生产力。AutoML
    工具已经存在很长时间了。其中一个最早的广泛框架是 AutoWeka，用 Java 编写，旨在自动化 Weka（Waikato 知识分析环境）机器学习套件中表格数据的机器学习模型开发过程，该套件是在
    Waikato 大学开发的。自 AutoWeka 发布以来，已经开发了许多其他 AutoML 框架。如今一些最受欢迎的 AutoML 框架包括 auto-sklearn、autokeras、NASLib、Auto-Pytorch、tpot、optuna、autogluon
    和 ray（tune）。这些框架用各种编程语言编写，支持各种机器学习任务。最近在 autoML 和神经架构搜索方面取得的进展使工具能够自动化机器学习流程的大部分。像
    Google AutoML、Azure AutoML 和 H2O AutoML/Driverless AI 这样的领先解决方案可以根据数据集和问题类型自动处理数据准备、特征工程、模型选择、超参数调整和部署。这使得机器学习对非专家更加易于接触。当前的
    autoML 解决方案可以非常有效地处理结构化数据，如表格和时间序列数据。它们可以自动生成相关特征，选择算法如树集成、神经网络或 SVM，并调整超参数。由于大规模超参数搜索，性能通常与手动流程相当甚至更好。对于像图像、视频和音频这样的非结构化数据，autoML
    也在快速发展，采用神经架构搜索技术。像 AutoKeras、AutoGluon 和 AutoSklearn 这样的开源库也提供了易于访问的 autoML 能力。然而，大多数
    autoML 工具仍需要一些编码和数据科学专业知识。完全自动化数据科学仍然具有挑战性，autoML 在灵活性和可控性方面存在局限性。但随着更加用户友好和高性能的解决方案进入市场，进展迅速。以下是框架的表格总结：
- en: '| **Framework** | **Language** | **ML Frameworks** | **First Release** | **Key
    Features** | **Data Types** | **Maintainer** | **Github stars** |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| **框架** | **语言** | **机器学习框架** | **首次发布** | **关键特点** | **数据类型** | **维护者** |
    **Github 星数** |'
- en: '| Auto-Keras | Python | Keras | 2017 | Neural architecture search, easy to
    use | Images, text, tabular | Keras Team (DATA Lab, Texas A&M) | 8896 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| Auto-Keras | Python | Keras | 2017 | 神经架构搜索，易于使用 | 图像、文本、表格 | Keras 团队（DATA
    实验室，德克萨斯 A&M 大学） | 8896 |'
- en: '| Auto-PyTorch | Python | PyTorch | 2019 | Neural architecture search, hyperparameter
    tuning | Tabular, text, image, time series | AutoML Group, Univ. of Freiburg |
    2105 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| Auto-PyTorch | Python | PyTorch | 2019 | 神经架构搜索，超参数调整 | 表格、文本、图像、时间序列 | AutoML
    Group, 弗莱堡大学 | 2105 |'
- en: '| Auto-Sklearn | Python | Scikit-learn | 2015 | Automated scikit-learn workflows
    | Tabular | AutoML Group, Univ. of Freiburg | 7077 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| Auto-Sklearn | Python | Scikit-learn | 2015 | 自动化scikit-learn工作流程 | 表格 |
    AutoML Group，弗莱堡大学 | 7077 |'
- en: '| Auto-WEKA | Java* | WEKA | 2012 | Bayesian optimization | Tabular | University
    of British Columbia | 315 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| Auto-WEKA | Java* | WEKA | 2012 | 贝叶斯优化 | 表格 | 不列颠哥伦比亚大学 | 315 |'
- en: '| AutoGluon | Python | MXNet, PyTorch | 2019 | Optimized for deep learning
    | Text, image, tabular | Amazon | 6032 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| AutoGluon | Python | MXNet, PyTorch | 2019 | 优化用于深度学习 | 文本，图像，表格 | 亚马逊 |
    6032 |'
- en: '| AWS SageMaker Autopilot | Python | XGBoost, sklearn | 2020 | Cloud-based,
    simple | Tabular | Amazon | - |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| AWS SageMaker Autopilot | Python | XGBoost, sklearn | 2020 | 云端，简单 | 表格 |
    亚马逊 | - |'
- en: '| Azure AutoML | Python | Scikit-learn, PyTorch | 2018 | Explainable models
    | Tabular | Microsoft | - |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| Azure AutoML | Python | Scikit-learn, PyTorch | 2018 | 可解释模型 | 表格 | 微软 |
    - |'
- en: '| DataRobot | Python, R | Multiple | 2012 | Monitoring, explainability | Text,
    image, tabular | DataRobot | - |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| DataRobot | Python, R | 多种 | 2012 | 监控，可解释性 | 文本，图像，表格 | DataRobot | - |'
- en: '| Google AutoML | Python | TensorFlow | 2018 | Easy to use, cloud-based | Text,
    image, video, tabular | Google | - |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Google AutoML | Python | TensorFlow | 2018 | 易于使用，云端 | 文本，图像，视频，表格 | 谷歌 |
    - |'
- en: '| H2O AutoML | Python, R | XGBoost, GBMs | 2017 | Automatic workflow, ensembling
    | Tabular, time series, images | h2o.ai | 6430 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| H2O AutoML | Python, R | XGBoost, GBMs | 2017 | 自动化工作流程，集成 | 表格，时间序列，图像 |
    h2o.ai | 6430 |'
- en: '| hyperopt-sklearn | Python | Scikit-learn | 2014 | Hyperparameter tuning |
    Tabular | Hyperopt team | 1451 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| hyperopt-sklearn | Python | Scikit-learn | 2014 | 超参数调整 | 表格 | Hyperopt团队
    | 1451 |'
- en: '| Ludwig | Python | Transformers/Pytorch | 2019 | Low-code framework for building
    and tuning custom LLMs and deep neural networks | Multiple | Linux Foundation
    | 9083 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| Ludwig | Python | Transformers/Pytorch | 2019 | 用于构建和调整自定义LLMs和深度神经网络的低代码框架
    | 多种 | Linux基金会 | 9083 |'
- en: '| MLJar | Python | Multiple | 2019 | Explainable, customizable | Tabular |
    MLJar | 2714 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| MLJar | Python | 多种 | 2019 | 可解释，可定制 | 表格 | MLJar | 2714 |'
- en: '| NASLib | Python | PyTorch, TensorFlow/Keras | 2020 | Neural architecture
    search | Images, text | AutoML Group, Univ. of Freiburg | 421 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| NASLib | Python | PyTorch, TensorFlow/Keras | 2020 | 神经架构搜索 | 图像，文本 | AutoML
    Group，弗莱堡大学 | 421 |'
- en: '| Optuna | Python | Agnostic | 2019 | Hyperparameter tuning | Agnostic | Preferred
    Networks Inc | 8456 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Optuna | Python | 通用 | 2019 | 超参数调整 | 通用 | Preferred Networks Inc | 8456
    |'
- en: '| Ray (Tune) | Python | Agnostic | 2018 | Distributed hyperparameter tuning;
    Accelerating ML workloads | Agnostic | University of California, Berkeley | 26906
    |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Ray (Tune) | Python | 通用 | 2018 | 分布式超参数调整；加速ML工作负载 | 通用 | 加州大学伯克利分校 | 26906
    |'
- en: '| TPOT | Python | Scikit-learn, XGBoost | 2016 | Genetic programming, pipelines
    | Tabular | Epistasis Lab, Penn State | 9180 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| TPOT | Python | Scikit-learn, XGBoost | 2016 | 遗传编程，管道 | 表格 | Epistasis Lab,
    宾夕法尼亚州立大学 | 9180 |'
- en: '| TransmogrifAI | Scala | Spark ML | 2018 | AutoML on Spark | Text, tabular
    | Salesforce | 2203 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| TransmogrifAI | Scala | Spark ML | 2018 | Spark上的AutoML | 文本，表格 | Salesforce
    | 2203 |'
- en: 'Figure 7.4: Comparison of open-source AutoML frameworks. Weka can be accessed
    from Python as pyautoweka. Stars for Ray Tune and H2O concern the whole project
    rather than only the automl part. The H2O commercial product related to AutoML
    is Driverless AI. Most projects are maintained by a community of contributors
    not affiliated with just one company'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4：开源AutoML框架的比较。Weka可以通过pyautoweka从Python访问。Ray Tune和H2O的星号涉及整个项目而不仅仅是自动化部分。与AutoML相关的H2O商业产品是Driverless
    AI。大多数项目由一群与任何公司无关的贡献者维护
- en: 'I have only included the biggest frameworks, libraries or products – omitting
    a few. Although the focus is on open-source frameworks in Python, I’ve included
    a few big commercial products. Github stars aim to show the popularity of the
    framework – they are not relevant to proprietary products. Pycaret is another
    big project (7562 stars) that gives the option to train several models simultaneously
    and compare them with relatively low amounts of code. Projects like Nixtla’s Statsforecast
    and MLForecast, or Darts have similar functionality specific to time series data.Libraries
    like Auto-ViML and deep-autoviml handle various types of variables and are built
    on scikit-learn and keras, respectively. They aim to make it easy for both novices
    and experts to experiment with different kinds of models and deep learning. However,
    users are advised to exercise their own judgement for accurate and explainable
    results.Important features of AutoML frameworks include the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我只包括了最大的框架、库或产品 - 省略了一些。虽然重点是在Python中的开源框架上，我也包括了一些大型商业产品。Github 星标旨在展示框架的流行程度
    - 它们与专有产品无关。Pycaret 是另一个大型项目（7562星），它提供了同时训练多个模型并用相对较少的代码进行比较的选项。像Nixtla的Statsforecast和MLForecast，或者Darts这样的项目具有针对时间序列数据的类似功能。像Auto-ViML和deep-autoviml这样的库处理各种类型的变量，并分别构建在scikit-learn和keras之上。它们旨在使新手和专家都能轻松尝试不同类型的模型和深度学习。然而，建议用户行使自己的判断以获得准确和可解释的结果。AutoML框架的重要特性包括以下内容：
- en: 'Deployment: Some solutions, especially those in the cloud, can be directly
    deployed to production. Others export to tensorflow or other formats.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署：一些解决方案，特别是云中的解决方案，可以直接部署到生产环境。其他则导出到tensorflow或其他格式。
- en: 'Types of data: Most solutions focus on tabular datasets; deep learning automl
    frameworks often work with different types of data. For example, autogluon facilitates
    rapid comparison and prototyping of ml solutions for images, text, time series
    in addition to tabular data. A few that focus on hyperparameter optimization such
    as optuna and ray tune, are totally agnostic to the format.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据类型：大多数解决方案侧重于表格数据集；深度学习自动化框架通常处理不同类型的数据。例如，autogluon促进了图像、文本、时间序列以及表格数据的快速比较和原型设计。一些专注于超参数优化的解决方案，如optuna和ray
    tune，对格式完全不可知。
- en: 'Explainability: This can be very important depending on the industry, related
    to regulation (for example, healthcare or insurance) or reliability (finance).
    For a few solutions, this is a unique selling point.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可解释性：这在某些行业中可能非常重要，与监管（例如，医疗保健或保险）或可靠性（金融）相关。对于一些解决方案，这是一个独特的卖点。
- en: 'Monitoring: After deployment, the model performance can deteriorate (drift).
    A few providers provide monitoring of performance.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控：部署后，模型性能可能会下降（漂移）。一些提供商提供性能监控。
- en: 'Accessibility: Some providers require coding or at least basic data science
    understanding, others are turnkey solutions that require very little to no code.
    Typically, low and no-code solutions are less customizable.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可访问性：一些提供商需要编码或至少基本的数据科学理解，而其他人则是即插即用的解决方案，几乎不需要编码。通常，低代码和无代码解决方案的可定制性较低。
- en: 'Open Source: The advantage of open-source platforms is that they are fully
    transparent about the implementation and the availability of methods and their
    parameters, and that they are fully extensible.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开源：开源平台的优势在于完全透明地展示实现和方法及其参数的可用性，并且它们是完全可扩展的。
- en: 'Transfer Learning: This capability means being able to extend or customize
    existing foundation models.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移学习：这种能力意味着能够扩展或定制现有的基础模型。
- en: 'There is a lot more to cover here that would go beyond the scope of this chapter
    such as the number of available methods. Less-well supported are features such
    as self-supervised learning, reinforcement learning, or generative image and audio
    models. For deep learning, a few libraries focus on the backend being specialized
    in Tensorflow, Pytorch, or MXNet. Auto-Keras, NASLib, and Ludwig have broader
    support, especially because they work with Keras. Starting with version 3.0, which
    is scheduled for release in fall 2023, Keras supports the three major backends
    TensorFlow, JAX, and PyTorch. Sklearn has its own hyperparameter optimization
    tools such as grid search, random search, successive halving. More specialized
    libraries such as auto-sklearn and hyperopt-sklearn go beyond this by offering
    methods for Bayesian Optimization. Optuna can integrate with a broad variety of
    ML frameworks such as AllenNLP, Catalyst, Catboost, Chainer, FastAI, Keras, LightGBM,
    MXNet, PyTorch, PyTorch, Ignite, PyTorch, Lightning, TensorFlow, and XGBoost.
    Ray Tune comes with its own integrations among which is optuna. Both of them come
    with cutting edge parameter optimization algorithms and mechanisms for scaling
    (distributed training).In addition to the features listed above, some of these
    frameworks can automatically perform feature engineering tasks, such as data cleaning
    and feature selection, for example removing highly correlated features, and generating
    performance results graphically. Each of the listed tools has their own implementations
    for each step of the process such as feature selection and feature transformations
    – what differs is the extent to which this is automated. More specifically, the
    advantages of using AutoML frameworks include:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有很多内容需要涵盖，超出了本章的范围，比如可用方法的数量。支持较少的功能包括自监督学习、强化学习，或生成图像和音频模型。对于深度学习，一些库专注于后端，专门针对Tensorflow、Pytorch或MXNet。Auto-Keras、NASLib和Ludwig具有更广泛的支持，特别是因为它们与Keras一起工作。从计划于2023年秋季发布的版本3.0开始，Keras将支持三个主要后端TensorFlow、JAX和PyTorch。Sklearn拥有自己的超参数优化工具，如网格搜索、随机搜索、逐步减半。更专业的库，如auto-sklearn和hyperopt-sklearn，通过提供贝叶斯优化方法，超越了这一点。Optuna可以与各种ML框架集成，如AllenNLP、Catalyst、Catboost、Chainer、FastAI、Keras、LightGBM、MXNet、PyTorch、PyTorch
    Ignite、PyTorch Lightning、TensorFlow和XGBoost。Ray Tune具有自己的集成，其中包括optuna。它们都具有最先进的参数优化算法和用于扩展（分布式训练）的机制。除了上述列出的功能外，其中一些框架可以自动执行特征工程任务，例如数据清洗和特征选择，例如删除高度相关的特征，并以图形方式生成性能结果。列出的每个工具都有各自的实现，例如特征选择和特征转换
    - 不同之处在于这种自动化程度。更具体地说，使用AutoML框架的优势包括：
- en: Time savings: AutoML frameworks can save data scientists a lot of time by automating
    the process of machine learning model development.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节省时间：AutoML框架可以通过自动化机器学习模型开发过程来节省数据科学家大量时间。
- en: Improved accuracy: AutoML frameworks can help to improve the accuracy of machine
    learning models by automating the process of hyperparameter tuning.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高准确性：AutoML框架可以通过自动化超参数调整过程来帮助提高机器学习模型的准确性。
- en: Increased accessibility: AutoML frameworks make machine learning more accessible
    to people who do not have a lot of experience with machine learning.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高可访问性：AutoML框架使得那些没有太多机器学习经验的人更容易接触机器学习。
- en: 'However, there are also some disadvantages to using AutoML frameworks:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用AutoML框架也存在一些缺点：
- en: Black box: AutoML frameworks can be "black boxes," meaning that it can be difficult
    to understand how they work. This can make it difficult to debug problems with
    AutoML models.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黑盒子：AutoML框架可能是“黑盒子”，意味着很难理解它们的工作原理。这可能会导致难以调试AutoML模型的问题。
- en: Limited flexibility: AutoML frameworks can be limited in terms of the types
    of machine learning tasks that they can automate.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 灵活性有限：AutoML框架在可以自动化的机器学习任务类型方面可能受到限制。
- en: A lot of the above tools have at least some kind of automatic feature engineering
    or preprocessing functionality, however, there are a few more specialized tools
    for this.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 上述工具中很多都至少具有某种自动特征工程或预处理功能，然而，也有一些更专业的工具。
- en: The impact of generative models
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成模型的影响
- en: 'Generative AI and LLMs like GPT-3 have brought about significant changes to
    the field of data science and analysis. These models, particularly LLMs have the
    potential to revolutionize all steps involved in data science in a number of ways
    offering exciting opportunities for researchers and analysts. Generative AI models,
    such as ChatGPT, have the ability to understand and generate human-like responses,
    making them valuable tools for enhancing research productivity.Generative AI can
    play a crucial role in analyzing and interpreting research data. These models
    can assist in data exploration, uncover hidden patterns or correlations, and provide
    insights that may not be apparent through traditional methods. By automating certain
    aspects of data analysis, generative AI saves time and resources, allowing researchers
    to focus on higher-level tasks.Another area where generative AI can benefit researchers
    is in performing literature reviews and identifying research gaps. ChatGPT and
    similar models can summarize vast amounts of information from academic papers
    or articles, providing a concise overview of existing knowledge. This helps researchers
    identify gaps in the literature and guide their own investigations more efficiently.
    We’ve looked at this aspect of using generative AI models in *Chapter 4*, *Question
    Answering*.Other use cases for generative AI can be:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能和像GPT-3这样的LLM已经给数据科学和分析领域带来了重大变革。这些模型，特别是LLMs，有潜力以多种方式彻底改变数据科学中涉及的所有步骤，为研究人员和分析师提供令人兴奋的机会。生成式人工智能模型，如ChatGPT，能够理解和生成类似人类的回应，使它们成为增强研究生产力的有价值工具。生成式人工智能可以在分析和解释研究数据方面发挥关键作用。这些模型可以协助进行数据探索，发现隐藏的模式或相关性，并提供通过传统方法可能不明显的见解。通过自动化数据分析的某些方面，生成式人工智能节省时间和资源，使研究人员能够专注于更高级别的任务。生成式人工智能可以在帮助研究人员进行文献综述和识别研究空白方面发挥作用。ChatGPT和类似模型可以总结大量来自学术论文或文章的信息，提供现有知识的简明概述。这有助于研究人员更有效地识别文献中的空白，并指导他们自己的调查。我们已经在*第4章*，*问答*中探讨了使用生成式人工智能模型的这一方面。生成式人工智能的其他用例可能包括：
- en: Automatically generate synthetic data: Generative AI can be used to automatically
    generate synthetic data that can be used to train machine learning models. This
    can be helpful for businesses that do not have access to large amounts of real-world
    data.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动生成合成数据：生成式人工智能可以用于自动生成合成数据，用于训练机器学习模型。对于没有大量真实世界数据的企业来说，这可能会有所帮助。
- en: Identify patterns in data: Generative AI can be used to identify patterns in
    data that would not be visible to human analysts. This can be helpful for businesses
    that are looking to gain new insights from their data.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别数据中的模式：生成式人工智能可以用于识别数据中人类分析师无法看到的模式。这对于希望从数据中获得新见解的企业可能有所帮助。
- en: Create new features from existing data: Generative AI can be used to create
    new features from existing data. This can be helpful for businesses that are looking
    to improve the accuracy of their machine learning models.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从现有数据中创建新特征：生成式人工智能可以用于从现有数据中创建新特征。这对于希望提高其机器学习模型准确性的企业可能有所帮助。
- en: 'According to recent reports by the likes of McKinsey and KPMG, the consequences
    of AI relate to what data scientists will work on, how they will work, and who
    can work on data science tasks. The main areas of key impact include:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 根据麦肯锡和毕马威等机构最近的报告，AI的后果涉及数据科学家将从事的工作、他们将如何工作以及谁可以从事数据科学任务。主要影响领域包括：
- en: 'Democratization of AI: Generative models allow many more people to leverage
    AI by generating text, code, and data from simple prompts. This expands the use
    of AI beyond data scientists.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能的民主化：生成模型使更多人能够利用人工智能，通过简单提示生成文本、代码和数据。这将AI的使用扩展到数据科学家以外的领域。
- en: 'Increased productivity: By auto-generating code, data, and text, generative
    AI can accelerate development and analysis workflows. This allows data scientists
    and analysts to focus on higher-value tasks.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高生产力：通过自动生成代码、数据和文本，生成式人工智能可以加速开发和分析工作流程。这使数据科学家和分析师能够专注于更高价值的任务。
- en: 'Innovation in data science: Generative AI is bringing about is the ability
    to explore data in new and more creative ways, and generate new hypotheses and
    insights that would not have been possible with traditional methods'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学的创新：生成式人工智能正在带来的是以新的更有创意的方式探索数据，并生成新的假设和见解，这是传统方法所无法实现的。
- en: 'Disruption of industries: New applications of generative AI could disrupt industries
    by automating tasks or enhancing products and services. Data teams will need to
    identify high-impact use cases.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行业的颠覆：生成式人工智能的新应用可能通过自动化任务或增强产品和服务来颠覆行业。数据团队需要确定高影响的使用案例。
- en: 'Limitations remain: Current models still have accuracy limitations, bias issues,
    and lack of controllability. Data experts are needed to oversee responsible development.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仍存在限制：当前模型仍存在准确性限制、偏见问题和缺乏可控性。需要数据专家监督负责任的开发。
- en: 'Importance of governance: Rigorous governance over development and ethical
    use of generative AI models will be critical to maintaining stakeholder trust.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 治理的重要性：对生成式人工智能模型的开发和道德使用进行严格的治理将对维护利益相关者的信任至关重要。
- en: Need for partnerships - Companies will need to build ecosystems with partners,
    communities and platform providers to effectively leverage generative AI capabilities.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合作伙伴关系的必要性 - 公司将需要与合作伙伴、社区和平台提供商建立生态系统，以有效利用生成式人工智能的能力。
- en: Changes to data science skills - Demand may shift from coding expertise to abilities
    in data governance, ethics, translating business problems, and overseeing AI systems.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学技能的变化 - 需求可能从编码专业知识转向数据治理、道德、翻译业务问题和监督AI系统的能力。
- en: 'Regarding democratization and innovation of data science, more specifically,
    generative AI is also having an impact on the way that data is visualized. In
    the past, data visualizations were often static and two-dimensional. However,
    generative AI can be used to create interactive and three-dimensional visualizations
    that can help to make data more accessible and understandable. This is making
    it easier for people to understand and interpret data, which can lead to better
    decision-making.Again, one of the biggest changes that generative AI is bringing
    about is the democratization of data science. In the past, data science was a
    very specialized field that required a deep understanding of statistics and machine
    learning. However, generative AI is making it possible for people with less technical
    expertise to create and use data models. This is opening up the field of data
    science to a much wider range of people.LLMs and generative AI can play a crucial
    role in automated data science by offering several benefits:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据科学的民主化和创新，更具体地说，生成式人工智能也对数据可视化方式产生影响。过去，数据可视化通常是静态的和二维的。然而，生成式人工智能可以用于创建交互式和三维的可视化，有助于使数据更易访问和理解。这使得人们更容易理解和解释数据，从而促进更好的决策。再次强调，生成式人工智能带来的最大变革之一是数据科学的民主化。过去，数据科学是一个需要深入了解统计学和机器学习的非常专业领域。然而，生成式人工智能使得技术专业知识较少的人能够创建和使用数据模型。这将数据科学领域开放给更广泛的人群。LLMs和生成式人工智能可以在自动化数据科学中发挥关键作用，提供多种好处：
- en: 'Natural Language Interaction: LLMs allow for natural language interaction,
    enabling users to communicate with the model using plain English or other languages.
    This makes it easier for non-technical users to interact with and explore the
    data using everyday language, without requiring expertise in coding or data analysis.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言交互：LLMs允许进行自然语言交互，使用户能够使用普通英语或其他语言与模型进行交流。这使得非技术用户能够使用日常语言与数据进行交互和探索，而无需专业的编码或数据分析技能。
- en: 'Code Generation: Generative AI can automatically generate code snippets to
    perform specific analysis tasks during EDA. For example, it can generate code
    to retrieve data (for example, SQL) clean data, handle missing values, or create
    visualizations (for example in Python). This feature saves time and reduces the
    need for manual coding.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码生成：生成式人工智能可以自动生成代码片段，执行EDA期间的特定分析任务。例如，它可以生成检索数据的代码（例如SQL）、清理数据、处理缺失值或创建可视化（例如Python）的代码。这一功能节省时间，减少了手动编码的需求。
- en: 'Automated Report Generation: LLMs can generate automated reports summarizing
    the key findings of EDA. These reports provide insights into various aspects of
    the dataset such as statistical summary, correlation analysis, feature importance,
    etc., making it easier for users to understand and present their findings.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动报告生成：LLMs可以生成自动报告，总结EDA的关键发现。这些报告提供了关于数据集各个方面的见解，如统计摘要、相关性分析、特征重要性等，使用户更容易理解和展示他们的发现。
- en: 'Data Exploration and Visualization: Generative AI algorithms can explore large
    datasets comprehensively and generate visualizations that reveal underlying patterns,
    relationships between variables, outliers or anomalies in the data automatically.
    This helps users gain a holistic understanding of the dataset without manually
    creating each visualization.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据探索和可视化：生成式人工智能算法可以全面探索大型数据集，并自动生成可视化图表，自动揭示数据中的潜在模式、变量之间的关系、异常值或异常情况。这有助于用户在不需要手动创建每个可视化图表的情况下全面了解数据集。
- en: Further, we could think that generative AI algorithms should be able to learn
    from user interactions and adapt their recommendations based on individual preferences
    or past behaviors. They improve over time through continuous adaptive learning
    and user feedback, providing more personalized and useful insights during automated
    EDA.Finally, generative AI models can identify errors or anomalies in the data
    during EDA by learning patterns from existing datasets (Intelligent Error Identification).
    They can detect inconsistencies and highlight potential issues quickly and accurately.Overall,
    LLMs and generative AI can enhance automated EDA by simplifying user interaction,
    generating code snippets, identifying errors/ anomalies efficiently, automating
    report generation, facilitating comprehensive data exploration, visualization
    creation, and adapting to user preferences for more effective analysis of large
    and complex datasets.However, while these models offer immense potential to enhance
    research and aiding in literature review processes, they should not be treated
    as infallible sources. As we’ve seen earlier, LLMs work by analogy and struggle
    with reasoning and math. Their strength is creativity, not accuracy, and therefore,
    researchers must exercise critical thinking and ensure that the outputs generated
    by these models are accurate, unbiased, and aligned with rigorous scientific standards.One
    notable example is Microsoft's Fabric, which incorporates a chat interface powered
    by generative AI. This allows users to ask data-related questions using natural
    language and receive instant answers without having to wait in a data request
    queue. By leveraging LLMs like OpenAI models, Fabric enables real-time access
    to valuable insights.Fabric stands out among other analytics products due to its
    comprehensive approach. It addresses various aspects of an organization's analytics
    needs and provides role-specific experiences for different teams involved in the
    analytics process, such as data engineers, warehousing professionals, scientists,
    analysts, and business users.With the integration of Azure OpenAI Service at every
    layer, Fabric harnesses generative AI's power to unlock the full potential of
    data. Features like Copilot in Microsoft Fabric provide conversational language
    experiences, allowing users to create dataflows, generate code or entire functions,
    build machine learning models, visualize results, and even develop custom conversational
    language experiences.Anecdotally, ChatGPT is (and Fabric in extension) often produces
    incorrect SQL queries. This is fine when used by analysts who can check the validity
    of the output, but a total disaster as a self-service analytics tool for non-technical
    business users. Therefore, organizations must ensure that they have reliable data
    pipelines in place and employ data quality management practices while using Fabric
    for analysis.While the possibilities of generative AI in data analytics are promising,
    caution must be exercised. The reliability and accuracy of LLMs should be verified
    using first-principled reasoning and rigorous analysis. While these models have
    shown their potential in ad-hoc analysis, idea generation during research, and
    summarizing complex analyses, they may not always be suitable for self-service
    analytical tools for non-technical users due to the need for validation by domain
    experts.Let’s start to use agents to run code or call other tools to answer questions!
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以认为生成式人工智能算法应该能够从用户互动中学习，并根据个人偏好或过去行为调整推荐。它们通过持续的自适应学习和用户反馈不断改进，提供更个性化和有用的见解，在自动化的探索性数据分析过程中提供更多个性化和有用的见解。最后，生成式人工智能模型可以通过从现有数据集中学习模式（智能错误识别）在探索性数据分析过程中识别数据中的错误或异常。它们可以快速准确地检测不一致之处并快速准确地突出潜在问题。总的来说，大型和复杂数据集的更有效分析。然而，尽管这些模型具有增强研究和辅助文献审阅过程的巨大潜力，但它们不应被视为不可靠的信息源。正如我们之前所看到的，LLMs是通过类比工作的，而且在推理和数学方面有困难。它们的优势在于创造力，而不是准确性，因此，研究人员必须运用批判性思维，并确保这些模型生成的输出准确、无偏见，并符合严格的科学标准。一个显著的例子是微软的Fabric，它集成了由生成式人工智能驱动的聊天界面。这使用户可以使用自然语言提出与数据相关的问题，并在不必等待数据请求队列的情况下立即获得答案。通过利用像OpenAI模型这样的LLMs，Fabric实现了对有价值见解的实时访问。Fabric在其他分析产品中脱颖而出，因为它采用了全面的方法。它解决了组织分析需求的各个方面，并为参与分析过程的不同团队提供了特定角色的体验，如数据工程师、仓储专业人员、科学家、分析师和业务用户。通过在每一层集成Azure
    OpenAI服务，Fabric利用生成式人工智能的力量来释放数据的全部潜力。像Microsoft Fabric中的Copilot这样的功能提供了对话式语言体验，允许用户创建数据流、生成代码或整个函数、构建机器学习模型、可视化结果，甚至开发定制的对话式语言体验。据传闻，ChatGPT（以及Fabric扩展）经常生成不正确的SQL查询。当分析人员可以检查输出的有效性时，这对于使用者来说是可以接受的，但对于非技术业务用户来说，作为自助式分析工具则是一场灾难。因此，组织在使用Fabric进行分析时必须确保他们有可靠的数据管道，并采用数据质量管理实践。虽然生成式人工智能在数据分析中的可能性令人兴奋，但必须谨慎行事。LLMs的可靠性和准确性应通过首创性推理和严格分析进行验证。虽然这些模型在临时分析、研究过程中的创意生成和总结复杂分析方面显示出潜力，但由于需要领域专家验证，它们并不总是适���非技术用户的自助式分析工具。让我们开始使用代理来运行代码或调用其他工具来回答问题！
- en: Agents can answer data science questions
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理可以回答数据科学问题
- en: 'As we’ve seen with Jupyter AI (Jupyternaut chat) – and in chapter 6, *Developing
    Software* – there’s a lot of potential to increase efficiency creating and creating
    software with generative AI (code LLMs). This is a good starting point for the
    practical part of this chapter as we look into the use of generative AI in data
    science.We’ve already seen different agents with tools before. For example, the
    LLMMathChain can execute Python to answer math questions as illustrated here:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在Jupyter AI（Jupyternaut chat）中看到的 - 以及在第6章*开发软件*中看到的 - 利用生成式AI（代码LLMs）提高创建和编写软件的效率的潜力很大。这是本章实践部分的一个很好的起点，因为我们将探讨在数据科学中使用生成式AI的用途。我们之前已经看到了不同的带有工具的代理。例如，LLMMathChain可以执行Python来回答数学问题，如下所示：
- en: '[PRE0]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'While this is useful to extract information and feed it back, it’s less obvious
    to see how to plug this into a traditional EDA process. Similarly, the CPAL (`CPALChain`)
    and PAL (`PALChain`) chains can answer more complex reasoning questions while
    keeping hallucinations in check, but it’s hard to come up with real-life use cases
    for them.With the `PythonREPLTool` we can create simple visualizations of toy
    data or train with synthetic data, which can be nice for illustration or bootstrapping
    of a project. This is an example from the LangChain documentation:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这对提取信息并反馈信息很有用，但如何将其插入传统的EDA过程并不那么明显。同样，CPAL（`CPALChain`）和PAL（`PALChain`）链可以回答更复杂的推理问题，同时保持幻觉受控，但很难想出它们的真实用例。通过`PythonREPLTool`，我们可以创建玩具数据的简单可视化或使用合成数据进行训练，这对于项目的说明或引导可能很好。这是LangChain文档中的一个示例：
- en: '[PRE1]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Please note that this should be executed with caution since the Python code
    is executed directly on the machine without any safeguards in place. This actually
    works, and creates a dataset, trains a model, and we get a prediction back:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这应该谨慎执行，因为Python代码直接在机器上执行而没有任何保障。这实际上是有效的，可以创建数据集，训练模型，然后我们得到一个预测：
- en: '[PRE2]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Again, this is very cool, but it’s hard to see how that would scale up without
    more serious engineering similar to what we did in chapter 6, *Developing Software*.LLMs
    and tools can be useful if we want to enrich our data with category or geographic
    information. For example, if our company offers flights from Tokyo, and we want
    to know the distances of our customers from Tokyo, we can use Wolfram Alpha as
    a tool. Here’s a simplistic example:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这非常酷，但很难看到如何在没有更严格的工程支持的情况下扩展，类似于我们在第6章*开发软件*中所做的。如果我们想要用类别或地理信息丰富我们的数据，LLMs和工具可能会很有用。例如，如果我们的公司从东京提供航班，并且我们想知道客户距离东京的距离，我们可以使用Wolfram
    Alpha作为工具。这是一个简单的例子：
- en: '[PRE3]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Please make sure you’ve set the OPENAI_API_KEY and WOLFRAM_ALPHA_APPID environment
    variables as discussed in chapter 3, *Getting Started with LangChain*. Here’s
    the output:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保您已经设置了OPENAI_API_KEY和WOLFRAM_ALPHA_APPID环境变量，如第3章*开始使用LangChain*中所讨论的。这是输出：
- en: '[PRE4]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, a lot of these questions are very simple. However, we can give agents datasets
    to work with and here’s where it can get very powerful when we connect more tools.
    Let’s start with asking and answering questions about structured datasets!
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，很多这些问题都非常简单。然而，我们可以给代理提供数据集来处理，这就是当我们连接更多工具时可以变得非常强大的地方。让我们开始问答关于结构化数据集的问题！
- en: Data exploration with LLMs
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用LLMs进行数据探索
- en: 'Data exploration is a crucial and foundational step in data analysis, allowing
    researchers to gain a comprehensive understanding of their datasets and uncover
    significant insights. With the emergence of LLMs like ChatGPT, researchers can
    harness the power of natural language processing to facilitate data exploration.As
    we’ve mentioned earlier Generative AI models, such as ChatGPT, have the ability
    to understand and generate human-like responses, making them valuable tools for
    enhancing research productivity. Asking our questions in natural language and
    getting responses in digestible pieces and shape can be a great boost to analysis.LLMs
    can assist in exploring not only textual data but also other forms of data such
    as numerical datasets or multimedia content. Researchers can leverage ChatGPT''s
    capabilities to ask questions about statistical trends in numerical datasets or
    even query visualizations for image classification tasks.Let’s load up a dataset
    and work with that. We can quickly get a dataset from scikit-learn:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 数据探索是数据分析中至关重要且基础的一步，使研究人员能够全面了解其数据集并发现重要见解。随着像ChatGPT这样的LLM的出现，研究人员可以利用自然语言处理的力量促进数据探索。正如我们之前提到的生成式AI模型，如ChatGPT，具有理解和生成类似人类响应的能力，使它们成为增强研究生产力的宝贵工具。用自然语言提出问题并获得易消化的回答可以极大地促进分析。LLM不仅可以帮助探索文本数据，还可以帮助探索其他形式的数据，如数值数据集或多媒体内容。研究人员可以利用ChatGPT的能力提出关于数值数据集中统计趋势的问题，甚至查询图像分类任务的可视化。让我们加载一个数据集并开始处理。我们可以从scikit-learn快速获取一个数据集：
- en: '[PRE5]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The Iris dataset is well-known – it’s a toy dataset, but it will help us illustrate
    the capabilities of using generative AI for data exploration. We’ll use the DataFrame
    in the following. We can create a Pandas dataframe agent now and we’ll see how
    easy it is to get simple stuff done!
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 鸢尾花数据集是众所周知的 - 它是一个玩具数据集，但它将帮助我们展示使用生成式人工智能进行数据探索的能力。我们将在接下来使用DataFrame。我们现在可以创建一个Pandas
    dataframe代理，看看完成简单任务有多容易！
- en: '[PRE6]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'I’ve put the instruction for the model to say it doesn’t know when in doubt
    and thinking step by step, both to reduce hallucinations. Now we can query our
    agent against the DataFrame:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经告诉模型在怀疑和逐步思考时说它不知道的指令，这样可以减少幻觉。现在我们可以针对DataFrame查询我们的代理：
- en: '[PRE7]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We get the answer `''This dataset is about the measurements of some type of
    flower.`'' which is correct. Let’s show how to get a visualization:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了答案“'这个数据集是关于某种花的测量。”'这是正确的。让我们展示如何获得一个可视化：
- en: '[PRE8]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'It’s not perfect, but we are getting a nice-looking plot:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 它并不完美，但我们得到了一个看起来不错的图表：
- en: '![Figure 7.5: Iris dataset barplots.](../media/file52.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5：鸢尾花数据集条形图。](../media/file52.png)'
- en: 'Figure 7.5: Iris dataset barplots.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5：鸢尾花数据集条形图。
- en: 'We can also ask to see the distributions of the columns visually, which will
    give us this neat plot:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以要求以可视化方式查看列的分布，这将给我们这个整洁的图表：
- en: '![Figure 7.6: Iris dataset boxplots.](../media/file53.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图7.6：鸢尾花数据集箱线图。](../media/file53.png)'
- en: 'Figure 7.6: Iris dataset boxplots.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6：鸢尾花数据集箱线图。
- en: 'We can request the plot to use other plotting backends such as seaborn, however,
    please note that these have to be installed. We can also ask more questions about
    the dataset like which row has the biggest difference between petal length and
    petal width. We get the answer with the intermediate steps as follows (shortened):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以要求图使用其他绘图后端，如seaborn，但请注意这些必须已安装。我们还可以询问有关数据集的更多问题，比如哪一行的花瓣长度和花瓣宽度之间的差异最大。我们得到了带有中间步骤的答案如下（缩短）：
- en: '[PRE9]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: I think that’s worth a pat on the back, LLM!Next steps could be adding more
    instructions to the prompt about plotting such about the sizes of plots. It’s
    a bit harder to implement the same plotting logic in a streamlit app, because
    we need to use the plotting functionality in corresponding streamlit functions,
    for example, `st.bar_chart()`, however, this can be done as well. You can find
    explanations for this on the Streamlit blog (“Building a Streamlit and scikit-learn
    app with ChatGPT”).What about statistical tests?
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这值得表扬，LLM！下一步可能是在提示中添加更多关于绘图的指令，比如关于绘图大小的指令。在streamlit应用程序中实现相同的绘图逻辑有点困难，因为我们需要在相应的streamlit函数中使用绘图功能，例如，`st.bar_chart()`，但是这也是可以做到的。您可以在Streamlit博客中找到关于此的解释（“使用ChatGPT构建Streamlit和scikit-learn应用程序”）。统计测试呢？
- en: '[PRE10]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We get this response:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了这个回应：
- en: '[PRE11]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '''The p-value of 6.639808432803654e-32 indicates that the two variables come
    from different distributions.''That’s check for statistical test! That’s cool.
    We can ask fairly complex questions about the dataset with simple prompts in plain
    English. There’s also the pandas-ai library, which uses LangChain under the hood
    and provides similar functionality. Here’s an example from the documentation with
    an example dataset:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '''6.639808432803654e-32的p值表明这两个变量来自不同的分布。''这是统计检验！这很酷。我们可以用简单的提示用普通英语提出相当复杂的关于数据集的问题。还有pandas-ai库，它在内部使用LangChain并提供类似的功能。以下是文档中的一个示例数据集：'
- en: '[PRE12]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will give us the requested result similarly to before when we were using
    LangChain directly. Please note that pandas-ai is not part of the setup for the
    book, so you’ll have to install it separately if you want to use it.For data in
    SQL-databases, we can connect with a `SQLDatabaseChain`. The LangChain documentation
    shows this example:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们提供类似于直接使用LangChain时的请求结果。请注意，pandas-ai不是本书的设置的一部分，所以如果你想使用它，你需要单独安装它。对于SQL数据库中的数据，我们可以连接一个`SQLDatabaseChain`。LangChain文档展示了这个例子：
- en: '[PRE13]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We are connecting to a database first. Then we can ask questions about the data
    in natural language. This can also be quite powerful. An LLM will create the queries
    for us. I would expect this to be particularly useful when we don’t know about
    the schema of the database.The `SQLDatabaseChain` can also check queries and autocorrect
    them if the `use_query_checker` option is set.Let’s summarize!
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先连接到数据库。然后我们可以用自然语言提问关于数据的问题。这也是非常强大的。一个LLM会为我们创建查询。我期望这在我们不了解数据库架构时特别有用。如果设置了`use_query_checker`选项，`SQLDatabaseChain`还可以检查查询并自动更正它们。让我们总结一下！
- en: Summary
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we’ve explored the state-of-the-art in automated data analysis
    and data science. There are quite a few areas, where LLMs can benefit data science,
    mostly as coding assistants or in data exploration.We’ve started off with an overview
    over frameworks that cover each step in the data science process such as AutoML
    methods, and we’ve discussed how LLMs can help us further increasing productivity
    and making data science and data analysis more accessible, both to stakeholders
    and to developers or users. We’ve then investigated how code generation and tools,
    similar to code LLMs *Chapter 6*, *Developing Software*, can help in data science
    tasks by creating functions or models that we can query, or how we can enrich
    data using LLMs or third-party tools like Wolfram Alpha.We then had a look at
    using LLMs in data exploration. In *Chapter 4*, *Question Answering*, we looked
    at ingesting large amounts of textual data for analysis. In this chapter, we focused
    on exploratory analysis of structured datasets in SQL or tabular form. In conclusion,
    AI technology has the potential to revolutionize the way we can analyze data,
    and ChatGPT plugins or Microsoft Fabric are examples of this. However, at the
    current state-of-affairs, AI can’t replace data scientists, only help enable them.Let’s
    see if you remember some of the key takeaways from this chapter!
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了自动化数据分析和数据科学的最新技术。有很多领域可以从LLMs中受益，主要是作为编码助手或数据探索。我们从涵盖数据科学过程中每个步骤的框架概述开始，比如AutoML方法，讨论了LLMs如何帮助我们进一步提高生产力，使数据科学和数据分析更容易访问，无论是对利益相关者还是开发人员或用户。然后，我们研究了代码生成和工具，类似于代码LLMs
    *第6章*，*开发软件*，可以通过创建我们可以查询的函数或模型来帮助数据科学任务，或者我们如何使用LLMs或第三方工具（如Wolfram Alpha）来丰富数据。然后，我们看了一下在数据探索中使用LLMs。在*第4章*，*问答*中，我们研究了摄入大量文本数据进行分析。在本章中，我们专注于SQL或表格形式的结构化数据集的探索性分析。总之，人工智能技术有潜力彻底改变我们分析数据的方式，ChatGPT插件或Microsoft
    Fabric就是这方面的例子。然而，在当前的情况下，人工智能不能取代数据科学家，只能帮助他们。让我们看看你是否记得本章的一些关键要点！
- en: Questions
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: 'Please have a look to see if you can come up with the answers to these questions
    from memory. I’d recommend you go back to the corresponding sections of this chapter,
    if you are unsure about any of them:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 请看看你是否能够从记忆中回答这些问题。如果你对任何问题不确定，我建议你回到本章的相应部分：
- en: What’s the difference between data science and data analysis?
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据科学和数据分析有什么区别？
- en: What steps are involved in data science?
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据科学涉及哪些步骤？
- en: Why would we want to automate data science/analysis?
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们想要自动化数据科学/分析？
- en: What frameworks exist for automating data science tasks and what can they do?
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有哪些用于自动化数据科学任务的框架，它们能做什么？
- en: How can generative AI help data scientists?
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成式人工智能如何帮助数据科学家？
- en: What kind of agents and tools can we use to answer simple questions?
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用什么样的代理和工具来回答简单问题？
- en: How can we get an LLM to work with data?
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何让LLM与数据一起工作？
