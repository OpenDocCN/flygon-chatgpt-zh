- en: 8 Customizing LLMs and their output
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 自定义LLMs及其输出
- en: Join our book community on Discord
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们在Discord上的书籍社区
- en: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
- en: '![Qr code Description automatically generated](../media/file54.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的二维码描述](../media/file54.png)'
- en: 'This chapter is about techniques and best practices to improve the reliability
    and performance of large language models (LLMs) in certain scenarios such as on
    complex reasoning and problem-solving tasks. Generally, this process of adapting
    a model for a certain task or making sure that our model output corresponds to
    what we expect is called conditioning. In this chapter, we’ll discuss fine-tuning
    and prompting as methods for conditioning.Fine-tuning involves training the pre-trained
    base model on specific tasks or datasets relevant to the desired application.
    This process allows the model to adapt and become more accurate and contextually
    aligned for the intended use case. Similarly, by providing additional input or
    context at inference time, large language models (LLMs) can generate text tailored
    to a particular tasks or style.Prompt design is highly significant for unlocking
    LLM reasoning capabilities, the potential for future advancements in models and
    prompting techniques, and these principles and techniques form a valuable toolkit
    for researchers and practitioners working with large language models. Understanding
    how LLMs generate text token-by-token helps create better reasoning prompts.Prompting
    is still an empirical art - trying variations to see what works is often needed.
    But some prompt engineering insights can transfer across models and tasks. We’ll
    discuss the tools in LangChain to enable advanced prompt engineering strategies
    like few-shot learning, dynamic example selection, and chained reasoning.Throughout
    the chapter, we’ll work on fine-tuning and prompting with LLMs, which you can
    find in the `notebooks` directory in the Github repository for the book at [https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain)The
    main sections are:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了改进大型语言模型（LLMs）在复杂推理和问题解决任务等特定场景中的可靠性和性能的技术和最佳实践。通常，将模型调整到特定任务或确保模型输出符合我们期望的过程称为调节。在本章中，我们将讨论微调和提示作为调节方法。微调涉及在特定任务或数据集上对预训练基础模型进行训练，这些任务或数据集与所需应用相关。这个过程使模型能够适应并变得更准确和上下文对齐以用于预期的用例。同样，在推断时提供额外的输入或上下文，大型语言模型（LLMs）可以生成适合特定任务或风格的文本。提示设计对于释放LLM推理能力、未来模型和提示技术的潜力以及这些原则和技术对于与大型语言模型一起工作的研究人员和从业者来说是非常重要的工具包。了解LLMs如何逐个标记生成文本有助于创建更好的推理提示。提示仍然是一门经验艺术
    - 经常需要尝试各种变化来看看哪种方法有效。但是一些提示工程的见解可以在模型和任务之间转移。我们将讨论LangChain中的工具，以实现高级提示工程策略，如少样本学习、动态示例选择和链式推理。在整个章节中，我们将使用LLMs进行微调和提示，您可以在书籍的Github存储库的`notebooks`目录中找到。主要章节包括：
- en: Conditioning and Alignment
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调节和对齐
- en: Fine-Tuning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调
- en: Prompt Engineering
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示工程
- en: Let’s start by discussing conditioning and alignment, why it’s important, and
    how we can achieve it.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从讨论调节和对齐开始，为什么它很重要，以及我们如何实现它。
- en: Conditioning and alignment
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调节和对齐
- en: Alignment, in the context of generative AI models, refers to ensuring that the
    outputs of these models are consistent with human values, intentions, or desired
    outcomes. It involves guiding the model's behavior to be in line with what is
    considered ethical, appropriate, or relevant within a given context. The concept
    of alignment is crucial to avoid generating outputs that may be biased, harmful,
    or deviate from the intended purpose. Addressing alignment involves careful attention
    to the biases present in training data, iterative feedback loops involving human
    reviewers, refining objective functions during training/fine-tuning stages, leveraging
    user feedback, and continuous monitoring during deployment to ensure ongoing alignment.There
    are several reasons one may want to condition a large language model. The first
    is to control the content and style of the outputs. For example, conditioning
    on certain keywords or attributes like formality level can produce more relevant
    and high-quality text. Conditioning also encompasses safety measures to prevent
    the generation of malicious or harmful content. For example, avoiding generating
    misleading information, inappropriate suggestions, or potentially dangerous instructions,
    or – more generally aligning the model with certain values.The potential benefits
    of conditioning large language models are numerous. By providing more specific
    and relevant input, we can achieve outputs that are tailored to our needs. For
    instance, in a customer support chatbot, conditioning the model with user queries
    allows it to generate responses that address their concerns accurately. Conditioning
    also helps control biased or inappropriate outputs by constraining the model's
    creativity within specific boundaries.Furthermore, by conditioning large language
    models, we can make them more controllable and adaptable. We can fine-tune and
    shape their behavior according to our requirements and create AI systems that
    are reliable in specific domains such as legal advice or technical writing.However,
    there are potential downsides to consider as well. Conditioning models too heavily
    might result in overfitting, where they become excessively reliant on specific
    inputs and struggle with generating creative or diverse outputs in different contexts.
    Moreover, conditioning should be utilized responsibly since large language models
    have the tendency to amplify biases present in the training data. Care must be
    taken not to exacerbate issues related to bias or controversial topics when conditioning
    these models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成式人工智能模型的背景下，对齐是指确保这些模型的输出与人类价值观、意图或期望结果一致。它涉及引导模型的行为与在特定背景下被认为是道德、适当或相关的内容保持一致。对齐的概念对于避免生成可能带有偏见、有害或偏离预期目的的输出至关重要。解决对齐问题需要仔细关注训练数据中存在的偏见，通过涉及人类审阅者的迭代反馈循环，在训练/微调阶段完善客观函数，利用用户反馈，并在部署过程中进行持续监控以确保持续对齐。有几个原因可能会希望对大型语言模型进行条件化。首先是控制输出的内容和风格。例如，根据特定关键词或属性（如正式程度）进行条件化可以产生更相关和高质量的文本。条件化还包括安全措施，以防止生成恶意或有害内容。例如，避免生成误导性信息、不当建议或潜在危险的指令，或者更一般地将模型与某些价值观保持一致。对大型语言模型进行条件化的潜在好处是多方面的。通过提供更具体和相关的输入，我们可以获得符合我们需求的输出。例如，在客户支持聊天机器人中，通过将用户查询作为条件，使其生成能够准确解决他们关注的回复。条件化还有助于通过将模型的创造力限制在特定范围内来控制有偏见或不当的输出。此外，通过对大型语言模型进行条件化，我们可以使它们更易控制和适应。我们可以根据我们的要求对其进行微调和塑造其行为，并创建在特定领域（如法律咨询或技术写作）可靠的人工智能系统。然而，也需要考虑潜在的缺点。过度对模型进行条件化可能导致过拟合，使其过于依赖特定输入，并在不同情境下难以生成创造性或多样化的输出。此外，应当负责任地利用条件化，因为大型语言模型有放大训练数据中存在偏见的倾向。在对这些模型进行条件化时，必须小心不要加剧与偏见或有争议话题相关的问题。
- en: '**Benefits of alignment** include:'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**对齐的好处** 包括：'
- en: 'Enhanced User Experience: Aligned models generate outputs that are relevant
    to user queries or prompts.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强用户体验：对齐模型生成与用户查询或提示相关的输出。
- en: 'Trust-building: Ensuring ethical behavior helps build trust among users/customers.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立信任：确保道德行为有助于在用户/客户之间建立信任。
- en: 'Brand Reputation: By aligning with business goals regarding branding consistency
    and desired tone/style guidelines.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 品牌声誉：通过与关于品牌一致性和所需语气/风格指南的业务目标保持一致。
- en: 'Mitigating Harmful Effects: Alignment with safety, security, and privacy considerations
    helps prevent the generation of harmful or malicious content.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓解有害影响：与安全、保密和隐私考虑的对齐有助于防止生成有害或恶意内容。
- en: '**Potential downsides** include:'
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**潜在的缺点** 包括：'
- en: 'Challenging Balance: Striking a balance between extreme alignment (overly conservative)
    and creative freedom (overly permissive) can be difficult.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 挑战平衡：在极端对齐（过于保守）和创造自由（过于宽松）之间取得平衡可能很困难。
- en: 'Limitations of Automated Metrics: Quantitative evaluation metrics might not
    capture alignment nuances fully.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化指标的局限性：定量评估指标可能无法完全捕捉对齐细微差别。
- en: 'Subjectivity: Alignment judgments are often subjective, requiring careful consideration
    and consensus building on desired values and guidelines.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主观性：对齐判断往往是主观的，需要仔细考虑并就期望的价值观和指导方针达成共识。
- en: Pre-training a large model on diverse data to learn patterns and language understanding
    results in a base model that has a broad understanding of various topics but lacks
    specificity or alignment to any particular context. While base models such as
    GPT-4 are capable of generating impressive text on a wide range of topics, conditioning
    them can enhance their capabilities in terms of task relevance, specificity, and
    coherence, and make their outputs more relevant and on-topic. Without conditioning,
    these models tend to generate text that may not always align perfectly with the
    desired context. By conditioning them, we can guide the language models to produce
    outputs that are more closely related to the given input or instructions. The
    major advantage of conditioning is that it allows guiding the model without extensive
    retraining. It also enables interactive control and switching between modes. Conditioning
    can happen at different stages of the model development cycle—from fine-tuning
    to output generation in various contexts. There are several options for achieving
    alignment of large language models. One approach is to condition during fine-tuning,
    by training the model on a dataset reflective of the desired outputs. This allows
    the model to specialize, but requires access to relevant training data. Another
    option is to dynamically condition the model at inference time by providing conditional
    input along with the main prompt. This is more flexible but introduces some complexity
    during deployment.In the next section, I will summarize key methods for alignment
    such as fine-tuning and prompt engineering, discuss the rationale, and examine
    their relative pros and cons.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在多样化数据上预训练大型模型以学习模式和语言理解，结果是得到一个具有广泛理解各种主题但缺乏特定性或与任何特定背景相关性的基础模型。虽然像GPT-4这样的基础模型能够在各种主题上生成令人印象深刻的文本，但对其进行调节可以增强其在任务相关性、特定性和连贯性方面的能力，并使其输出更相关和主题相关。没有调节，这些模型往往会生成与期望背景不完全吻合的文本。通过调节它们，我们可以引导语言模型生成更与给定输入或指令相关的输出。调节的主要优势在于它允许引导模型而无需进行大量的重新训练。它还能够实现交互式控制和在不同模式之间切换。调节可以发生在模型开发周期的不同阶段——从微调到在各种背景下生成输出。有几种实现大型语言模型对齐的选项。一种方法是在微调过程中进行调节，通过在反映所需输出的数据集上训练模型。这使模型能够专门化，但需要访问相关的训练数据。另一种选择是在推断时动态调节模型，通过提供条件输入以及主要提示。这更加灵活，但在部署过程中引入了一些复杂性。在接下来的部分中，我将总结关于对齐的关键方法，如微调和提示工程，讨论其基本原理，并检查它们的相对优缺点。
- en: Methods for alignment
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对齐方法
- en: 'With the advent of large pre-trained language models like GPT-3, there has
    been growing interest in techniques to adapt these models for downstream tasks.
    This process is known as fine-tuning. Fine-tuning allows pre-trained models to
    be customized for specific applications while leveraging the vast linguistic knowledge
    acquired during pre-training. The idea of adapting pre-trained neural networks
    originated in computer vision research in the early 2010s. In NLP, Howard and
    Ruder (2018) demonstrated the effectiveness of fine-tuning pre-trained contextual
    representations like ELMo and ULMFit on downstream tasks. The seminal BERT model
    (Devlin and others., 2019) established fine-tuning of pre-trained transformers
    as the de facto approach in NLP.The need for fine-tuning arises because pre-trained
    LMs are designed to model general linguistic knowledge, not specific downstream
    tasks. Their capabilities manifest only when adapted to particular applications.
    Fine-tuning allows pre-trained weights to be updated for target datasets and objectives.
    This enables knowledge transfer from the general model while customizing it for
    specialized tasks. Several approaches have been proposed for alignment, with trade-offs
    in efficacy and efficiency and it’s worth to delve a bit more into the details
    of each of these alignment methods. **Full Fine-Tuning** involves updating all
    the parameters of the pre-trained language model during fine-tuning. The model
    is trained end-to-end on the downstream tasks, allowing the weights to be updated
    globally to maximize performance on the target objectives. FFT consistently achieves
    strong results across tasks but requires extensive computational resources and
    large datasets to avoid overfitting or forgetting.In **Adapter Tuning** additional
    trainable adapter layers are inserted, usually bottleneck layers, into the pre-trained
    model while keeping the original weights frozen. Only the newly added adapter
    layers are trained on the downstream tasks. This makes tuning parameter-efficient
    as only a small fraction of weights are updated. However, as the pre-trained weights
    remain fixed, adapter tuning has a risk of underfitting to the tasks. The insertion
    points and capacity of the adapters impact overall efficacy.**Prefix Tuning**:
    This prepends trainable vectors to each layer of the LM, which are optimized during
    fine-tuning while the base weights remain frozen. The prefixes allow injection
    of inductive biases into the model. Prefix tuning has a smaller memory footprint
    compared to adapters but has not been found to be as effective. The length and
    initialization of prefixes impact efficacy.In **Prompt Tuning**, the input text
    is appended with trainable prompt tokens which provide a soft prompt to induce
    the desired behavior from the LM. For example, a task description can be provided
    as a prompt to steer the model. Only the added prompt tokens are updated during
    training while the pre-trained weights are frozen. Performance is heavily influenced
    by prompt engineering. Automated prompting methods are being explored.**Low-Rank
    Adaptation (LoRA)** adds pairs of low-rank trainable weight matrices to the frozen
    LM weights. For example, to each weight W, low-rank matrices B and A are added
    such that the forward pass uses W + BA. Only B and A are trained, keeping the
    base W frozen. LoRA achieves reasonable efficacy with greater parameter efficiency
    than full tuning. The choice of rank r impacts tradeoffs. LoRA enables tuning
    giant LMs on limited hardware.Another way to ensure proper alignment of outputs
    is through **human oversight** methods like human-in-the-loop systems. These systems
    involve human reviewers who provide feedback and make corrections if necessary.
    Human involvement helps align generated outputs with desired values or guidelines
    set by humans.Here is a table summarizing the different techniques for steering
    generative AI outputs:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 随着像GPT-3这样的大型预训练语言模型的出现，人们对将这些模型调整为下游任务的技术越来越感兴趣。这个过程被称为微调。微调允许预训练模型根据预训练期间获得的广泛语言知识定制特定应用程序。调整预训练神经网络的想法起源于2010年代初的计算机视觉研究。在自然语言处理领域，Howard和Ruder（2018）展示了微调预训练的上下文表示（如ELMo和ULMFit）在下游任务上的有效性。开创性的BERT模型（Devlin等人，2019）将微调预训练的transformers确立为自然语言处理中的事实标准。微调的必要性源于预训练语言模型的设计目的是对一般语言知识进行建模，而不是特定的下游任务。只有在适应特定应用程序时，它们的能力才会显现出来。微调允许更新预训练权重以适应目标数据集和目标。这使得可以从一般模型中进行知识转移，同时为专门任务定制它。已经提出了几种用于对齐的方法，效率和效率之间存在权衡，值得更深入地研究每种对齐方法的细节。**全面微调**在微调期间更新预训练语言模型的所有参数。该模型在下游任务上端到端地进行训练，允许全局更新权重以最大化目标性能。FFT在各种任务中始终取得强大的结果，但需要大量的计算资源和大型数据集以避免过拟合或遗忘。在**适配器调整**中，额外的可训练适配器层通常是瓶颈层插入到预训练模型中，同时保持原始权重冻结。只有新添加的适配器层在下游任务上进行训练。这使得调整参数高效，因为只有一小部分权重被更新。然而，由于预训练权重保持不变，适配器调整存在低拟合任务的风险。适配器的插入点和容量影响整体有效性。**前缀调整**：这在LM的每一层前面添加可训练向量，在微调期间进行优化，而基本权重保持冻结。前缀允许向模型注入归纳偏见。与适配器相比，前缀调整的内存占用较小，但效果没有那么好。前缀的长度和初始化影响有效性。在**提示调整**中，输入文本附加了可训练提示标记，这些标记提供软提示以诱导LM产生所需的行为。例如，可以提供任务描述作为提示来引导模型。只有添加的提示标记在训练期间进行更新，而预训练权重保持冻结。性能受提示工程的影响很大。正在探索自动提示方法。**低秩适应（LoRA）**向冻结的LM权重添加成对的低秩可训练权重矩阵。例如，对于每个权重W，添加低秩矩阵B和A，使得前向传���使用W
    + BA。只有B和A被训练，基本W保持冻结。LoRA以更高的参数效率实现了合理的效果。秩r的选择影响权衡。LoRA使得在有限硬件上调整巨大LM成为可能。确保输出正确对齐的另一种方法是通过**人工监督**方法，如人机协同系统。这些系统涉及提供反馈并在必要时进行更正的人类审阅者。人类参与有助于使生成的输出与人类设定的期望值或指导原则保持一致。以下是总结引导生成AI输出的不同技术的表格：
- en: '| **Stage** | **Technique** | **Examples** |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| **阶段** | **技术** | **示例** |'
- en: '| Training | Pre-training | Training on diverse data |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 训练 | 预训练 | 在多样数据上训练 |'
- en: '|  | Objective Function | Careful design of training objective |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  | 目标函数 | 训练目标的精心设计 |'
- en: '|  | Architecture and Training Process | Optimizing model structure and training
    |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  | 架构和训练过程 | 优化模型结构和训练 |'
- en: '| Fine-Tuning | Specialization | Training on specific datasets/tasks |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | 专业化 | 在特定数据集/任务上训练 |'
- en: '| Inference-Time Conditioning | Dynamic Inputs | Prefixes, control codes, context
    examples |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 推理时条件 | 动态输入 | 前缀，控制代码，上下文示例 |'
- en: '| Human Oversight | Human-in-the-Loop | Human review and feedback |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 人类监督 | 人类参与 | 人类审查和反馈 |'
- en: 'Figure 8.1: Steering generative AI outputs.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1：引导生成式人工智能输出。
- en: Combining these techniques provides developers with more control over the behavior
    and outputs of generative AI systems. The ultimate goal is to ensure that human
    values are incorporated at all stages, from training to deployment, in order to
    create responsible and aligned AI systems.Furthermore, careful design choices
    in the pre-training objective function also impact what behaviors and patterns
    the language model learns initially. By incorporating ethical considerations into
    these objective functions, developers can influence the initial learning process
    of large language models.We can distinguish a few more approaches in fine-tuning
    such as online and offline. InstructGPT was considered a game-changer because
    it demonstrated the potential to significantly improve language models, such as
    GPT-3, by incorporating reinforcement learning from human feedback (RLHF). Let’s
    talk about the reasons why InstructGPT had such a transformative impact.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 结合这些技术为开发人员提供了更多对生成式人工智能系统行为和输出的控制。最终目标是确保人类价值在训练到部署的所有阶段都被纳入，以创建负责任和一致的人工智能系统。此外，在预训练目标函数中的精心设计选择也会影响语言模型最初学习的行为和模式。通过将道德考虑因素纳入这些目标函数中，开发人员可以影响大型语言模型的初始学习过程。我们可以区分一些微调方法，如在线和离线。InstructGPT被认为是一个改变游戏规则的因素，因为它展示了通过引入强化学习从人类反馈（RLHF）可以显著改进语言模型，如GPT-3。让我们谈谈InstructGPT为何具有如此变革性影响的原因。
- en: Reinforcement learning with human feedback
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 强化学习与人类反馈
- en: 'In their March 2022 paper, Ouyang and others from OpenAI demonstrated using
    reinforcement learning from human feedback (RLHF) with proximal policy optimization
    (PPO) to align large language models like GPT-3 with human preferences. Reinforcement
    learning from human feedback (RLHF) is an online approach that fine-tunes LMs
    using human preferences. It has three main steps:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们2022年3月的论文中，OpenAI的欧阳等人展示了使用强化学习从人类反馈（RLHF）与近端策略优化（PPO）来调整大型语言模型如GPT-3与人类偏好一致。强化学习从人类反馈（RLHF）是一种在线方法，通过人类偏好对语言模型进行微调。它有三个主要步骤：
- en: 'Supervised pre-training: The LM is first trained via standard supervised learning
    on human demonstrations.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 监督预训练：首先通过标准监督学习对语言模型进行训练，使用人类演示。
- en: 'Reward model training: A reward model is trained on human ratings of LM outputs
    to estimate reward.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 奖励模型训练：通过人类对语言模型输出的评分来训练奖励模型以估计奖励。
- en: 'RL fine-tuning: The LM is fine-tuned via reinforcement learning to maximize
    expected reward from the reward model using an algorithm like PPO.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RL微调：通过强化学习对语言模型进行微调，以最大化来自奖励模型的预期奖励，使用类似PPO的算法。
- en: 'The main change, RLHF, allows incorporating nuanced human judgments into language
    model training through a learned reward model. As a result, human feedback can
    steer and improve language model capabilities beyond standard supervised fine-tuning.
    This new model can be used to follow instructions that are given in natural language,
    and it can answer questions in a way that’s more accurate and relevant than GPT-3\.
    InstructGPT outperformed GPT-3 on user preference, truthfulness, and harm reduction,
    despite having 100x fewer parameters.Starting in March 2022, OpenAI started releasing
    the GPT-3.5 series models, upgraded versions of GPT-3, which include fine-tuning
    with RLHF.There are three advantages of fine-tuning that were immediately obvious
    to users of these models:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 主要变化RLHF允许通过学习奖励模型将微妙的人类判断纳入语言模型训练中。因此，人类反馈可以引导和改进语言模型的能力，超越标准监督微调。这种新模型可以用于遵循以自然语言给出的指令，并且可以以比GPT-3更准确和相关的方式回答问题。尽管参数少100倍，InstructGPT在用户偏好、真实性和减少伤害方面优于GPT-3。从2022年3月开始，OpenAI开始发布GPT-3.5系列模型，这是GPT-3的升级版本，包括与RLHF微调。这些模型的用户立即注意到了微调的三个优点：
- en: 'Steerability: the capability of models to follow instructions (instruction-tuning)'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可操纵性：模型遵循指令的能力（指令微调）
- en: 'Reliable output-formatting: this became important, for example, for API calls/function
    calling)'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可靠的输出格式化：这对于API调用/函数调用等方面变得重要。
- en: 'Custom tone: this makes it possible to adapt the output style as appropriate
    to task and audience.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自定义语调：这使得可以根据任务和受众适当地调整输出风格。
- en: InstructGPT opened up new avenues for improving language models by incorporating
    reinforcement learning from human feedback methods beyond traditional fine-tuning
    approaches. RL training can be unstable and computationally expensive, notwithstanding,
    its success inspired further research into refining RLHF techniques, reducing
    data requirements for alignment, and developing more powerful and accessible models
    for a wide range of applications.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: InstructGPT通过将人类反馈的强化学习方法纳入传统微调方法之外的新途径，开辟了改进语言模型的新途径。尽管RL训练可能不稳定且计算成本高昂，但其成功激发了进一步研究，以完善RLHF技术，减少对齐的数据需求，并为各种应用开发更强大和更易访问的模型。
- en: Offline Approaches
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 离线方法
- en: 'Offline methods circumvent the complexity of online RL by directly utilizing
    human feedback. We can distinguish between ranking-based on language-based approaches:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 离线方法通过直接利用人类反馈来规避在线RL的复杂性。我们可以区分基于排名和基于语言的方法：
- en: 'Ranking-based: Human rankings of LM outputs are used to define optimization
    objectives for fine-tuning, avoiding RL entirely. This includes methods like Preference
    Ranking Optimization (PRO; Song et al., 2023) and Direct Preference Optimization
    (DPO; Rafailov et al., 2023).'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于排名：人类对LM输出的排名用于定义微调的优化目标，避免完全使用RL。这包括Preference Ranking Optimization（PRO；宋等，2023）和Direct
    Preference Optimization（DPO；拉法洛夫等，2023）等方法。
- en: 'Language-based: Human feedback is provided in natural language and utilized
    via standard supervised learning. For example, Chain of Hindsight (CoH; Liu et
    al., 2023) converts all types of feedback into sentences and uses them to fine-tune
    the model, taking advantage of the language comprehension capabilities of language
    models.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于语言：人类反馈以自然语言形式提供，并通过标准监督学习利用。例如，Hindsight链（CoH；刘等，2023）将所有类型的反馈转换为句子，并用于微调模型，利用语言模型的语言理解能力。
- en: 'Direct Preference Optimization (DPO) is a simple and effective method for training
    language models to adhere to human preferences without needing to explicitly learn
    a reward model or use reinforcement learning. While it optimizes the same objective
    as existing RLHF methods, it is much simpler to implement, more stable, and achieves
    strong empirical performance.Researchers from Meta, in the paper “*LIMA: Less
    Is More for Alignment*”, simplified alignment by minimizing a supervised loss
    on only 1,000 carefully curated prompts in fine-training the LLaMa model. Based
    on the favorable human preferences when comparing their outputs to DaVinci003
    (GPT-3.5), they conclude that fine-training has only minimal importance. This
    they refer to as the superficial alignment hypothesis.Offline approaches offer
    more stable and efficient tuning. However, they are limited by static human feedback.
    Recent methods try to combine offline and online learning.While both DPO and RLHF
    with PPO aim to align LLMs with human preferences, they differ in terms of complexity,
    data requirements, and implementation details. DPO offers simplicity but achieves
    strong performance by directly optimizing probability ratios. On the other hand,
    RLHF with PPO in InstructGPT introduces more complexity but allows for nuanced
    alignment through reward modeling and reinforcement learning optimization.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 直接偏好优化（DPO）是一种简单而有效的方法，用于训练语言模型以符合人类偏好，无需明确学习奖励模型或使用强化学习。虽然它优化的目标与现有的RLHF方法相同，但实现起来要简单得多，更稳定，并且取得了强大的实证表现。Meta的研究人员在论文“*LIMA：对齐的少即是多*”中，通过在精细训练LLaMa模型时最小化仅有1,000个精心策划的提示的监督损失，简化了对齐。基于与DaVinci003（GPT-3.5）的输出相比较时的有利人类偏好，他们得出结论，精细训练只有极小的重要性。他们将此称为表面对齐假设。离线方法提供更稳定和高效的调整。然而，它们受到静态人类反馈的限制。最近的方法尝试将离线和在线学习结合起来。虽然DPO和带有PPO的RLHF旨在将LLM与人类偏好对齐，但它们在复杂性、数据需求和实现细节方面存在差异。DPO提供简单性，但通过直接优化概率比实现强大的性能。另一方面，在InstructGPT中，带有PPO的RLHF引入了更多复杂性，但通过奖励建模和强化学习优化允许通过微妙的对齐。
- en: Low-Rank Adaptation
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 低秩适应
- en: LLMs have achieved impressive results in Natural Language Processing and are
    now being used in other domains such as Computer Vision and Audio. However, as
    these models become larger, it becomes difficult to train them on consumer hardware
    and deploying them for each specific task becomes expensive. There are a few methods
    that reduce computational, memory, and storage costs, while improving performance
    in low-data and out-of-domain scenarios.Low-Rank Adaptation (LoRA) freezes the
    pre-trained model weights and introduces trainable rank decomposition matrices
    into each layer of the Transformer architecture to reduce the number of trainable
    parameters. LoRA achieves comparable or better model quality compared to fine-tuning
    on various language models (RoBERTa, DeBERTa, GPT-2, and GPT-3) while having fewer
    trainable parameters and higher training throughput.The QLORA method is an extension
    of LoRA, which enables efficient fine-tuning of large models by backpropagating
    gradients through a frozen 4-bit quantized model into learnable low-rank adapters.
    This allows fine-tuning a 65B parameter model on a single GPU. QLORA models achieve
    99% of ChatGPT performance on Vicuna using innovations like new data types and
    optimizers. In particular, QLORA reduces the memory requirements for fine-tuning
    a 65B parameter model from >780GB to <48GB without affecting runtime or predictive
    performance.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: LLM在自然语言处理领域取得了令人印象深刻的成果，现在也被用于其他领域，如计算机视觉和音频。然而，随着这些模型变得更大，训练它们在消费者硬件上变得困难，并且为每个特定任务部署它们变得昂贵。有一些方法可以降低计算、内存和存储成本，同时提高在低数据和领域外情况下的性能。低秩适应（LoRA）冻结了预训练模型的权重，并在Transformer架构的每一层中引入可训练的秩分解矩阵，以减少可训练参数的数量。LoRA在各种语言模型（RoBERTa、DeBERTa、GPT-2和GPT-3）上实现了与微调相当或更好的模型质量，同时具有更少的可训练参数和更高的训练吞吐量。QLORA方法是LoRA的扩展，通过将梯度反向传播通过冻结的4位量化模型到可学习的低秩适配器，实现了对大型模型的高效微调。这使得可以在单个GPU上微调一个65B参数模型。QLORA模型通过引入新的数据类型和优化器等创新，实现了在Vicuna上ChatGPT性能的99%。特别是，QLORA将微调一个65B参数模型的内存需求从>780GB降低到<48GB，而不影响运行时或预测性能。
- en: '**Quantization** refers to techniques for reducing the numerical precision
    of weights and activations in neural networks like large language models (LLMs).
    The main purpose of quantization is to reduce the memory footprint and computational
    requirements of large models.'
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**量化**是指减少神经网络中权重和激活的数值精度的技术，如大型语言模型（LLMs）。量化的主要目的是减少大型模型的内存占用和计算需求。'
- en: ''
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Some key points about quantization of LLMs:'
  id: totrans-51
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 有关LLM量化的一些关键要点：
- en: It involves representing weights and activations using fewer bits than standard
    single-precision floating point (FP32). For example, weights could be quantized
    to 8-bit integers.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它涉及使用比标准单精度浮点（FP32）更少的位数来表示权重和激活。例如，权重可以量化为8位整数。
- en: This allows shrinking model size by up to 4x and improving throughput on specialized
    hardware.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这可以将模型大小缩小多达4倍，并提高专用硬件的吞吐量。
- en: Quantization typically has a minor impact on model accuracy, especially with
    re-training.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 量化通常对模型准确性影响较小，尤其是在重新训练时。
- en: Common quantization methods include scalar, vector, and product quantization
    which quantize weights separately or in groups.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见的量化方法包括标量、向量和乘积量化，它们将权重分别或分组量化。
- en: Activations can also be quantized by estimating their distribution and binning
    appropriately.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过估计激活的分布并适当分组，激活也可以被量化。
- en: Quantization-aware training adjusts weights during training to minimize quantization
    loss.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 量化感知训练在训练过程中调整权重以最小化量化损失。
- en: LLMs like BERT and GPT-3 have been shown to work well with 4-8 bit quantization
    via fine-tuning.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像BERT和GPT-3这样的LLMs已经证明通过微调可以很好地使用4-8位量化。
- en: Parameter-Efficient Fine-tuning (PEFT) methods enable the use of small checkpoints
    for each task, making the models more portable. These small trained weights can
    be added on top of the LLM, allowing the same model to be used for multiple tasks
    without replacing the entire model.In the next section, we’ll discuss methods
    for conditioning large language models (LLMs) at inference time.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 参数高效微调（PEFT）方法使每个任务可以使用小的检查点，使模型更具可移植性。这些小的训练权重可以添加到LLM之上，使得同一模型可以用于多个任务而无需替换整个模型。在下一节中，我们将讨论在推理时对大型语言模型（LLMs）进行条件化的方法。
- en: Inference-Time conditioning
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 推理时的条件化
- en: 'One commonly used approach is **conditioning at inference time** (output generation
    phase) where specific inputs or conditions are provided dynamically to guide the
    output generation process. LLM fine-tuning may not always be feasible or beneficial
    in certain scenarios:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常用的方法是**推理时的条件化**（输出生成阶段），在这里特定的输入或条件会动态提供以指导输出生成过程。在某些情况下，LLM微调可能并不总是可行或有益的：
- en: 'Limited Fine-Tuning Services: Some models are only accessible through APIs
    that lack or have restricted fine-tuning capabilities.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有限的微调服务：一些模型只能通过缺乏或受限的微调能力的API访问。
- en: 'Insufficient Data: In cases where there is a lack of data for fine-tuning,
    either for the specific downstream task or relevant application domain.'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据不足：在某些情况下，针对特定下游任务或相关应用领域缺乏微调数据。
- en: 'Dynamic Data: Applications with frequently changing data, such as news-related
    platforms, may struggle to fine-tune models frequently, leading to potential drawbacks.'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 动态数据：应用程序中频繁更改数据的情况，例如新闻相关平台，可能难以频繁微调模型，导致潜在的缺点。
- en: 'Context-Sensitive Applications: Dynamic and context-specific applications like
    personalized chatbots cannot perform fine-tuning based on individual user data.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上下文敏感应用：动态和特定上下文的应用，如个性化聊天机器人，无法根据个人用户数据进行微调。
- en: 'For conditioning at inference time, most commonly, we provide a textual prompt
    or instruction at the beginning of the text generation process. This prompt can
    be a few sentences or even a single word, acting as an explicit indication of
    the desired output. Some common techniques for dynamic inference-time conditioning
    include:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于推理时的条件化，通常我们在文本生成过程的开头提供一个文本提示或指示。这个提示可以是几句话甚至一个单词，作为所需输出的明确指示。一些常见的动态推理时条件化技术包括：
- en: 'Prompt tuning: Providing natural language guidance for intended behavior. Sensitive
    to prompt design.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示调整：为预期行为提供自然语言指导。对提示设计敏感。
- en: 'Prefix tuning: Prepending trainable vectors to LLM layers.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前缀调整：在LLM层前添加可训练向量。
- en: 'Constraining tokens: Forcing inclusion/exclusion of certain words'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制标记：强制包含/排除某些词语。
- en: 'Metadata: Providing high-level info like genre, target audience, etc.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元数据：提供高级信息，如流派、目标受众等。
- en: Prompts can facilitate generating text that adheres to specific themes, styles,
    or even mimics a particular author's writing style. These techniques involve providing
    contextual information during inference time such as for in-context learning or
    retrieval augmentation.An example of prompt tuning is prefixing prompts, where
    instructions like "Write a child-friendly story about..." are prepended to the
    prompt. For example, in chatbot applications, conditioning the model with user
    messages helps it generate responses that are personalized and pertinent to the
    ongoing conversation. Further examples include prepending relevant documents to
    prompts to assist LLMs with writing tasks (e.g., news reports, Wikipedia pages,
    company documents), or retrieving and prepending user-specific data (financial
    records, health data, emails) before prompting an LLM to ensure personalized answers.
    By conditioning LLM outputs on contextual information at runtime, these methods
    can guide models without relying on traditional fine-tuning processes.Often demonstrations
    are part of the instructions for reasoning tasks, where few-shot examples are
    provided to induce desired behavior. Powerful LLMs, such as GPT-3, can solve tasks
    without further training through prompting techniques. In this approach, the problem
    to be solved is presented to the model as a text prompt, possibly with some text
    examples of similar problems and their solutions. The model must provide a completion
    of the prompt via inference. **Zero-shot prompting** involves no solved examples,
    while few-shot prompting includes a small number of examples of similar (problem,
    solution) pairs. It has shown that prompting provides easy control over large
    frozen models like GPT-3 and allows steering model behavior without extensive
    fine-tuning. Prompting enables conditioning models on new knowledge with low overhead,
    but careful prompt engineering is needed for best results. This is what we’ll
    discuss as part of this chapter.In prefix tuning, continuous task-specific vectors
    are trained and supplied to models at inference time. continuous task-specific
    vectors. Similar ideas have been proposed for adapter-approaches such as parameter
    efficient transfer learning (PELT) or Ladder Side-Tuning (LST).Conditioning at
    inference time can also happen during sampling such as grammar-based sampling,
    where the output can be constrained to be compatible with certain well-defined
    patterns, such as a programming language syntax.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 提示可以促进生成符合特定主题、风格甚至模仿特定作者写作风格的文本。这些技术包括在推理时提供上下文信息，例如在上下文学习或检索增强中。提示调整的一个例子是在提示前加上前缀，比如“写一个适合儿童的故事...”这样的指示。例如，在聊天机器人应用中，通过使用用户消息来调节模型，帮助其生成个性化且与当前对话相关的回复。更多的例子包括在提示前加上相关文档以帮助LLMs完成写作任务（例如新闻报道、维基百科页面、公司文件），或者在提示LLM之前检索并加上用户特定数据（财务记录、健康数据、电子邮件）以确保个性化答案。通过在运行时将LLM输出与上下文信息相结合，这些方法可以引导模型而不依赖于传统的微调过程。通常演示是推理任务指令的一部分，其中提供少量示例以诱导期望的行为。强大的LLMs，如GPT-3，可以通过提示技术解决任务而无需进一步训练。在这种方法中，要解决的问题被呈现给模型作为文本提示，可能还包括一些类似问题及其解决方案的文本示例。模型必须通过推理提供提示的完成。**零样本提示**不涉及已解决的示例，而少量样本提示包括少量类似（问题，解决方案）对的示例。已经证明提示可以轻松控制像GPT-3这样的大型冻结模型，并且可以在不进行大量微调的情况下引导模型行为。提示使模型能够在新知识上进行条件化，但需要精心设计提示以获得最佳结果。这将是我们在本章讨论的内容之一。在前缀调整中，连续的任务特定向量在推理时被训练并提供给模型。类似的想法已经被提出用于适配器方法，如参数高效的迁移学习（PELT）或梯子侧调整（LST）。在推理时进行条件化也可以发生在采样过程中，例如基于语法的采样，其中输出可以被限制为与某些明确定义的模式兼容，比如编程语言语法。
- en: Conclusions
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结论
- en: Full fine-tuning consistently achieves strong results but often requires extensive
    resources, and trade-offs exist between efficacy and efficiency. Methods like
    adapters, prompts, and LoRA reduce this burden via sparsity or freezing, but can
    be less effective. The optimal approach depends on constraints and objectives.
    Future work on improved techniques tailor-made for large LMs could push the boundaries
    of both efficacy and efficiency. Recent work blends offline and online learning
    for improved stability. Integrating world knowledge and controllable generation
    remain open challenges.Prompt-based techniques allow flexible conditioning of
    LLMs to induce desired behaviors without intensive training. Careful prompt design,
    optimization, and evaluation is key to effectively controlling LLMs. Prompt-based
    techniques allow conditioning LLMs on specific behaviors or knowledge in a flexible,
    low-resource manner.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 完全微调始终能取得强大的结果，但通常需要大量资源，并且在功效和效率之间存在权衡。像适配器、提示和LoRA这样的方法通过稀疏性或冻结减轻了这种负担，但可能效果较差。最佳方法取决于约束和目标。未来改进的技术针对大型语言模型量身定制，可以推动功效和效率的边界。最近的工作将离线和在线学习相结合以提高稳定性。整合世界知识和可控生成仍然是开放挑战。基于提示的技术允许对LLM进行灵活的条件设定，以诱导所需行为而无需进行密集训练。谨慎的提示设计、优化和评估是有效控制LLM的关键。基于提示的技术允许以灵活、低资源的方式对LLM进行特定行为或知识的条件设定。
- en: Evaluations
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估
- en: 'Alignment is evaluated on alignment benchmarks like HUMAN and generalization
    tests like FLAN. There are a few core benchmarks with high differentiability to
    accurately assess model strengths and weaknesses such as these:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐是通过像HUMAN这样的对齐基准和像FLAN这样的泛化测试来评估的。有一些核心基准具有很高的可区分性，可以准确评估模型的优势和劣势，例如：
- en: 'English knowledge: MMLU'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 英语知识：MMLU
- en: 'Chinese knowledge: C-Eval'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中文知识：C-Eval
- en: 'Reasoning: GSM8k / BBH (Algorithmic)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推理：GSM8k / BBH（算法）
- en: 'Coding: HumanEval / MBPP'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码：HumanEval / MBPP
- en: After balancing these directions, additional benchmarks like MATH (high-difficulty
    reasoning) and Dialog could be pursued.A particularly interesting evaluation is
    in math or reasoning, where generalization abilities would be expected to be very
    strong. The MATH benchmark demonstrates high-level difficulty, and GPT-4 achieves
    varying scores based on prompting methods. Results range from naive prompting
    via few-shot evaluations to PPO + process-based reward modeling. If fine-tuning
    involves dialog data only, it might negatively affect existing capabilities such
    as MMLU or BBH. Prompt engineering is essential, as biases and query difficulty
    impact evaluations.There are quantitative metrics like perplexity (measuring how
    well a model predicts data) or BLEU score (capturing similarity between generated
    text and reference text). These metrics provide rough estimates but may not fully
    capture semantic meaning or alignment with higher-level goalsOther metrics include
    user preferences ratings through human evaluation, pairwise preference, utilizing
    pre-trained reward model for online small/medium models or automated LLM-based
    assessments (for example GPT-4). Human evaluations can sometimes be problematic
    since humans can be swayed by subjective criteria such as an authoritative tone
    in the response rather than the actual accuracy. Conducting evaluations where
    users assess the quality, relevance, appropriateness of generated text against
    specific criteria set beforehand provides more nuanced insights into alignment.
    Fine-tuning is not intended to solely improve user preferences on a given set
    of prompts. Its primary purpose is to address AI safety concerns by reducing the
    occurrence of undesirable outputs such as illegal, harmful, abusive, false, or
    deceptive content. This focus on mitigating risky behavior is crucial in ensuring
    the safety and reliability of AI systems. Evaluating and comparing models based
    purely on user preferences without considering the potential harm they may cause
    can be misleading and prioritize suboptimal models over safer alternatives. In
    summary, evaluating LLM alignment requires careful benchmark selection, consideration
    of differentiability, and a mix of automatic evaluation methods and human judgments.
    Attention to prompt engineering and specific evaluation aspects is necessary to
    ensure accurate assessment of model performance.In the next section, we’ll fine-tune
    a small open-source LLM (OpenLLaMa) for a question answering with PEFT and quantization,
    and we’ll deploy it on HuggingFace.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在平衡这些方向之后，可以追求像MATH（高难度推理）和对话等额外的基准。特别有趣的评估在数学或推理方面，预计泛化能力会非常强。 MATH基准展示了高水平的难度，而GPT-4根据提示方法实现了不同的得分。
    结果范围从通过少量评估进行天真提示到PPO + 基于过程的奖励建模。 如果微调仅涉及对话数据，可能会对现有能力（如MMLU或BBH）产生负面影响。 提示工程至关重要，因为偏见和查询难度会影响评估。
    还有像困惑度（衡量模型预测数据的能力）或BLEU分数（捕捉生成文本与参考文本之间的相似性）这样的定量指标。 这些指标提供了粗略的估计，但可能无法完全捕捉语义含义或与更高级目标的对齐。其他指标包括通过人类评估的用户偏好评分，成对偏好，利用预训练奖励模型进行在线小/中型模型或自动化LLM评估（例如GPT-4）。
    人类评估有时可能存在问题，因为人类可能会受到主观标准的影响，例如回应中的权威语气而不是实际准确性。 进行评估，让用户根据事先设定的具体标准评估生成文本的质量、相关性和适当性，可以提供更细致入微的见解。
    微调的目的不仅仅是改善给定提示集上的用户偏好。 其主要目的是通过减少不良输出（如非法、有害、滥用、虚假或欺骗性内容）的发生来解决AI安全问题。 关注减轻风险行为对确保AI系统的安全性和可靠性至关重要。
    仅基于用户偏好评估和比较模型，而不考虑它们可能造成的潜在危害，可能会误导，并优先考虑比较安全的替代方案。 总之，评估LLM的对齐性需要仔细选择基准，考虑可区分性，并结合自动评估方法和人类判断。
    注意提示工程和特定评估方面的关注是必要的，以确保对模型性能进行准确评估。在接下来的部分中，我们将使用PEFT和量化对一个小型开源LLM（OpenLLaMa）进行微调，用于问答，并将其部署在HuggingFace上。
- en: Fine-Tuning
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调
- en: 'As we’ve discussed in the first section of this chapter, the goal of model
    fine-tuning for LLMs is to optimize a model to generate outputs that are more
    specific to a task and context than the original foundation model. Amongst the
    multitude of tasks and scenarios, where we might want to apply this approach are
    these:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章第一节中讨论的那样，LLM的模型微调的目标是优化模型，使其生成的输出比原始基础模型更具体于任务和上下文。 在我们可能想要应用这种方法的众多任务和场景中，包括以下几种：
- en: Software Development
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 软件开发
- en: Document classification
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档分类
- en: Question-Answering
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问答
- en: Information Retrieval
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信息检索
- en: Customer Support
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户支持
- en: In this section, we’ll fine-tune a model for question answering. This recipe
    is not specific to LangChain, but we’ll point out a few customizations, where
    LangChain could come in. For performance reasons, we'll run this on Google Colab
    instead of the usual local environment.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将为问答模型进行微调。这个步骤不是特定于 LangChain 的，但我们将指出一些自定义内容，LangChain 可以参与其中。出于性能原因，我们将在
    Google Colab 上运行这个步骤，而不是通常的本地环境。
- en: '**Google Colab** is a computation environment that provides different means
    for hardware acceleration of computation tasks such as Tensor Processing Units
    (TPUs) and Graphical Processing Units (GPUs). These are available both in free
    and professional tiers. For the purpose of the task in this section, the free
    tier is completely sufficient. You can sign into a Colab environment at this url:
    [https://colab.research.google.com/](https://colab.research.google.com/)'
  id: totrans-89
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**Google Colab** 是一个计算环境，提供不同的硬件加速计算任务的手段，如张量处理单元（TPUs）和图形处理单元（GPUs）。这些在免费和专业版中都可用。对于本节任务的目的，免费版完全足够。您可以在此网址登��到
    Colab 环境：[https://colab.research.google.com/](https://colab.research.google.com/)'
- en: 'Please make sure you set your google colab machine settings in the top menu
    to TPU or GPU in order to make sure you have sufficient resources to run this
    and that the training doesn''t take too long. We’ll install all required libraries
    in the Google Colab environment – I am adding the versions of these libraries
    that I’ve used in order to make our fine-tuning repeatable:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保您在 Google Colab 机器的顶部菜单中设置为 TPU 或 GPU，以确保您有足够的资源来运行此操作，并且训练不会花费太长时间。我们将在
    Google Colab 环境中安装所有所需的库 - 我添加了我使用的这些库的版本，以便使我们的微调可重复进行：
- en: 'peft: Parameter-Efficient Fine-Tuning (PEFT; version 0.5.0)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: peft：参数高效微调（PEFT；版本 0.5.0）
- en: 'trl: Proximal Policy Optimization (0.6.0)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: trl：近端策略优化（0.6.0）
- en: 'bitsandbytes: k-bit optimizers and matrix multiplication routines, needed for
    quantization (0.41.1)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: bitsandbytes：k 位优化器和矩阵乘法例程，用于量化（0.41.1）
- en: 'accelerate: train and use PyTorch models with multi-GPU, TPU, mixed-precision
    (0.22.0)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: accelerate：使用多 GPU、TPU、混合精度训练和使用 PyTorch 模型（0.22.0）
- en: 'transformers: HuggingFace transformers library with backends in JAX, PyTorch
    and TensorFlow (4.32.0)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: transformers：HuggingFace transformers 库，支持 JAX、PyTorch 和 TensorFlow（4.32.0）
- en: 'datasets: community-driven open-source library of datasets (2.14.4)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: datasets：社区驱动的开源数据集库（2.14.4）
- en: 'sentencepiece: Python wrapper for fast tokenization (0.1.99)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: sentencepiece：用于快速标记化的 Python 封装（0.1.99）
- en: 'wandb: for monitoring the training progress on Weights and Biases (0.15.8)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: wandb：用于监控 Weights and Biases 上训练进度的工具（0.15.8）
- en: langchain for loading the model back as a langchain llm after training (0.0.273)
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: langchain 用于在训练后将模型加载回作为 langchain llm（0.0.273）
- en: 'We can install these libraries from the Colab notebook as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过 Colab 笔记本安装这些库，如下所示：
- en: '[PRE0]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In order to download and train models from HuggingFace, we need to authenticate
    with the platform. Please note that if you want to push your model to HuggingFace
    later, you need to generate a new API token with write permissions on HuggingFace:
    [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从 HuggingFace 下载和训练模型，我们需要在平台上进行身份验证。请注意，如果您想稍后将您的模型推送到 HuggingFace，您需要在 HuggingFace
    上生成一个具有写入权限的新 API 令牌：[https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)
- en: '![Figure 8.3: Creating a new API token on HuggingFace write permissions.](../media/file55.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.3：在 HuggingFace 上创建一个新的具有写入权限的 API 令牌。](../media/file55.png)'
- en: 'Figure 8.3: Creating a new API token on HuggingFace write permissions.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3：在 HuggingFace 上创建一个新的具有写入权限的 API 令牌。
- en: 'We can authenticate from the notebook like this:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像这样从笔记本进行身份验证：
- en: '[PRE1]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When prompted, paste your HuggingFace access token.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在提示时，粘贴您的 HuggingFace 访问令牌。
- en: 'A note of caution before, we start: when executing the code, you need to log
    into different services so make sure you pay attention when running the notebook!'
  id: totrans-108
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在我们开始之前，请注意：在执行代码时，您需要登录不同的服务，因此请确保在运行笔记本时要注意！
- en: 'Weights and Biases (W&B) is an MLOps platform that can help developers to monitor
    and document ML training workflows from end to end. As mentioned earlier, we will
    use W&B to get an idea of how well the training is working, in particular if the
    model is improving over time.For W&B, we need to name the project; alternatively,
    we can use wandb''s `init()` method:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Weights and Biases（W&B）是一个 MLOps 平台，可以帮助开发人员从头到尾监控和记录 ML 训练工作流程。正如前面提到的，我们将使用
    W&B 来了解训练的效果如何，特别是模型是否随着时间的推移而改进。对于 W&B，我们需要为项目命名；或者，我们可以使用 wandb 的 `init()` 方法：
- en: '[PRE2]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In order to authenticate with W&B, you need to create a free account with them
    for this at [https://www.wandb.ai](https://www.wandb.ai) You can find your API
    key on the Authorize page: [https://wandb.ai/authorize](https://wandb.ai/authorize)Again,
    we need to paste in our API token. If the previous training run is still active
    – this could be from a previous execution of the notebook if you are running a
    second time –, let''s make sure we start a new one! This will ensure that we get
    new reports and dashboard on W&B:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与W&B进行身份验证，您需要在他们这里创建一个免费帐户：[https://www.wandb.ai](https://www.wandb.ai) 您可以在授权页面上找到您的API密钥：[https://wandb.ai/authorize](https://wandb.ai/authorize)同样，我们需要粘贴我们的API令牌。如果之前的训练仍然处于活动状态
    - 这可能是从笔记本的上一次执行中，如果您再次运行第二次 - 让我们确保我们开始一个新的！这将确保我们在W&B上获得新的报告和仪表板：
- en: '[PRE3]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we’ll need to choose a dataset against which we want to optimize. We
    can use lots of different datasets here that are appropriate for coding, storytelling,
    tool use, SQL generation, grade-school math questions (GSM8k), or many other tasks.
    HuggingFace provides a wealth of datasets, which can be viewed at this url: [https://huggingface.co/datasets](https://huggingface.co/datasets)These
    cover a lot of different, even the most niche tasks. We can also customize our
    own dataset. For example, we can use langchain to set up training data. There
    are quite a few methods available for filtering that could help reduce redundancy
    in the dataset. It would have been appealing to show data collection as a practical
    recipe in this chapter. However, because of the complexity I am leaving it out
    of scope for the book.It might be harder to filter for quality from web data,
    but there are a lot of possibilities. For code models, we could apply code validation
    techniques to score segments as a quality filter. If the code comes from Github,
    we can filter by stars or by stars by repo owner. For texts in natural language,
    quality filtering is not trivial. Search engine placement could serve as a popularity
    filter since it''s often based on user engagement with the content. Further, knowledge
    distillation techniques could be tweaked as a filter by fact density and accuracy.In
    this recipe, we are fine-tuning for question answering performance with the Squad
    V2 dataset. You can see a detailed dataset description on HuggingFace: [https://huggingface.co/spaces/evaluate-metric/squad_v2](https://huggingface.co/spaces/evaluate-metric/squad_v2)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要选择一个数据集进行优化。我们可以使用许多不同的数据集，适用于编码、叙述、工具使用、SQL生成、小学数学问题（GSM8k）或许多其他任务。HuggingFace提供了丰富的数据集，可以在此网址查看：[https://huggingface.co/datasets](https://huggingface.co/datasets)这些数据集涵盖了许多不同的，甚至是最为专业的任务。我们也可以自定义我们自己的数据集。例如，我们可以使用langchain来设置训练数据。有许多可用的过滤方法可以帮助减少数据集中的冗余。在本章中展示数据收集作为一个实用的步骤可能会很吸引人。然而，由于复杂性，我将其排除在本书的范围之外。从网络数据中过滤出质量可能会更加困难，但有许多可能性。对于代码模型，我们可以应用代码验证技术来对段落进行评分作为质量过滤器。如果代码来自Github，我们可以按星级或按存储库所有者的星级进行过滤。对于自然语言文本，质量过滤并不是一件简单的事情。搜索引擎排名可以作为一个流行度过滤器，因为它通常基于用户与内容的互动。此外，知识蒸馏技术可以通过事实密度和准确性进行调整作为一个过滤器。在这个步骤中，我们正在使用Squad
    V2数据集进行问答性能的微调。您可以在HuggingFace上查看详细的数据集描述：[https://huggingface.co/spaces/evaluate-metric/squad_v2](https://huggingface.co/spaces/evaluate-metric/squad_v2)
- en: '[PRE4]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We are taking both training and validation splits. The Squad V2 dataset bas
    a part that’s supposed to be used in training and another one in validation as
    we can see in the output of `load_dataset(dataset_name)`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们同时采用训练和验证拆分。Squad V2数据集有一个部分应该用于训练，另一个部分用于验证，正如我们可以在`load_dataset(dataset_name)`的输出中看到的：
- en: '[PRE5]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We’ll use the validation splits for early stopping. Early stopping will allow
    us to stop training when the validation error begins to degrade.The Squad V2 dataset
    is composed of various features, which we can see here:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用验证拆分进行早停。早停将允许我们在验证错误开始恶化时停止训练。Squad V2数据集由各种特征组成，我们可以在这里看到：
- en: '[PRE6]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The basic idea in training is prompting the model with a question and comparing
    the answer to the dataset.We want a small model that we can run locally at a decent
    token rate. LLaMa-2 models require signing a license agreement with your email
    address and to get confirmed (which, to be fair, can be very fast) as it comes
    with restrictions to commercial use. LLaMa derivates such as OpenLLaMa have been
    performing quite well as can be evidenced on the HF leaderboard: [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)OpenLLaMa
    version 1 cannot be used for coding tasks, because of the tokenizer. Therefore,
    let''s use v2! We’ll use a 3 billion parameter model, which we’ll be able to use
    even on older hardware:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 训练中的基本思想是用一个问题提示模型，并将答案与数据集进行比较。我们希望有一个小型模型，可以在本地以合理的标记速率运行。LLaMa-2模型需要使用您的电子邮件地址签署许可协议并得到确认（公平地说，这可能非常快），因为它受到商业使用的限制。像OpenLLaMa这样的LLaMa衍生产品在HF排行榜上表现相当不错：[https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)OpenLLaMa版本1不能用于编码任务，因为分词器的原因。因此，让我们使用v2！我们将使用一个30亿参数的模型，即使在较旧的硬件上也能使用：
- en: '[PRE7]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can use even smaller models such as `EleutherAI/gpt-neo-125m` which can
    also give a very good compromise between resource use and performance.Let’s load
    the model:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以使用更小的模型，比如`EleutherAI/gpt-neo-125m`，这样可以在资源使用和性能之间取得很好的折衷。让我们加载模型：
- en: '[PRE8]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The Bits and Bytes configuration makes it possible to quantize our model in
    8, 4, 3 or even 2 bits with a much-accelerated inference and lower memory footprint
    without a incurring a big cost in terms of performance. We are going to store
    model checkpoints on Google Drive; you need to confirm your login to your google
    account:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Bits and Bytes配置使我们能够将模型量化为8、4、3甚至2位，从而加速推理并减少内存占用，而不会在性能方面产生大的成本。我们将在谷歌云盘上存储模型检查点；您需要确认登录到您的谷歌账户：
- en: '[PRE9]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We’ll need to authenticate with Google for this to work. We can set our output
    directory for model checkpoints and logs to our Google Drive:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要通过谷歌进行身份验证才能使其工作。我们可以将模型检查点和日志的输出目录设置为我们的谷歌云盘：
- en: '[PRE10]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If you don''t want to use google drive, just set this to a directory on your
    computer.For training, we need to set up a tokenizer:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不想使用谷歌云盘，只需将其设置为计算机上的一个目录。对于训练，我们需要设置一个分词器：
- en: '[PRE11]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now we’ll define our training configuration. We’ll set up LORA and other training
    arguments:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将定义我们的训练配置。我们将设置LORA和其他训练参数：
- en: '[PRE12]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'A few comments to explain some of these parameters are in order. The `push_to_hub`
    argument means that we can push the model checkpoints to the HuggingSpace Hub
    regularly during training. For this to work you need to set up the HuggingSpace
    authentication (with write permissions as mentioned). If we opt for this, as `output_dir`
    we can use `new_model_name`. This will be the repository name under which the
    model will be available here on HuggingFace: [https://huggingface.co/models](https://huggingface.co/models)Alternatively,
    as I’ve done here, we can save your model locally or to the cloud, for example
    google drive to a directory.I’ve set `max_steps` and `num_train_epochs` very high,
    because I’ve noticed that training can still improve after many steps. We are
    using early stepping together with a high number of maximum training steps to
    get the model to converge to higher performance. For early stopping, we need to
    set the `evaluation_strategy` as `"steps"` and `load_best_model_at_end=True`.`eval_steps`
    is the number of update steps between two evaluations. `save_total_limit=5` means
    that only last 5 models are saved. Finally, `report_to="wandb"` means that we’ll
    send training stats, some model metadata, and hardware information to W&B, where
    we can look at graphs and dashboards for each run. The training can then use our
    configuration:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些参数需要解释一下。`push_to_hub`参数意味着我们可以在训练过程中定期将模型检查点推送到HuggingSpace Hub。为了使其工作，你需要设置HuggingSpace身份验证（如上所述，需要写入权限）。如果我们选择这个选项，作为`output_dir`我们可以使用`new_model_name`。这将是模型在HuggingFace上可用的存储库名称：[https://huggingface.co/models](https://huggingface.co/models)或者，如我在这里所做的，我们可以将模型保存在本地或云端，例如谷歌云盘的一个目录。我将`max_steps`和`num_train_epochs`设置得非常高，因为我注意到训练仍然可以在许多步骤后改进。我们使用早停和较高数量的最大训练步数使模型收敛到更高的性能。对于早停，我们需要将`evaluation_strategy`设置为`"steps"`，并且`load_best_model_at_end=True`。`eval_steps`是两次评估之间的更新步数。`save_total_limit=5`意味着只保存最后5个模型。最后，`report_to="wandb"`意味着我们将发送训练统计数据、一些模型元数据和硬件信息到W&B，我们可以在那里查看每次运行的图表和仪表板。然后训���可以使用我们的配置：
- en: '[PRE13]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The training can take quite a while, even running on TPU device. The evaluating
    and early stopping slows the training down by a lot. If you disable the early
    stopping, you can make this much faster.We should see some statistics as the training
    progresses, but it’s nicer to show the graph of performance as we can see it on
    W&B:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 训练可能需要相当长的时间，即使在TPU设备上运行。评估和早停会大大减慢训练速度。如果禁用早停，可以使训练速度更快。我们应该在训练过程中看到一些统计数据，但最好显示性能图表，这样我们可以在W&B上看到：
- en: '![Figure 8.4: Fine-tuning training loss over time (steps).](../media/file56.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图8.4：随时间（步数）的微调训练损失。](../media/file56.png)'
- en: 'Figure 8.4: Fine-tuning training loss over time (steps).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4：随时间（步数）的微调训练损失。
- en: 'After training is done, we can save the final checkpoint on disk for re-loading:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们可以将最终检查点保存在磁盘上以便重新加载：
- en: '[PRE14]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can now share our final model with friends in order to brag about the performance
    we''ve achieved by manually pushing to HuggingFace:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以与朋友分享我们的最终模型，以炫耀我们通过手动推送到HuggingFace所取得的性能：
- en: '[PRE15]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can now load the model back using the combination of our HuggingFace username
    and the repository name (new model name).Let’s quickly show how to use this model
    in LangChain. usually, the peft model is stored as an adapter, not as a full model,
    therefore the loading is a bit different:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用我们的HuggingFace用户名和存储库名称（新模型名称）的组合来加载模型。让我们快速展示如何在LangChain中使用这个模型。通常，peft模型存储为适配器，而不是完整模型，因此加载方式有点不同：
- en: '[PRE16]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We’ve done everything so far on Google Colab, but we can equally execute this
    locally, just note that you need to have the huggingface peft library installed!So
    far, we’ve shown how to fine-tune and deploy an open-source LLM. Some commercial
    models can be fine-tuned on custom data as well. For example, both OpenAI’s GPT-3.5
    and Google’s PaLM model offer this capability. This has been integrated with a
    few Python libraries. With the Scikit-LLM library, this is only a few lines of
    code in either case:Fine-tuning a PaLM model for text classification can be done
    like this:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们在Google Colab上完成了所有工作，但我们也可以在本地执行，只需注意你需要安装huggingface peft库！到目前为止，我们已经展示了如何对开源LLM进行微调和部署。一些商业模型也可以根据自定义数据进行微调。例如，OpenAI的GPT-3.5和Google的PaLM模型都提供了这种能力。这已经与一些Python库集成。使用Scikit-LLM库，在任何情况下这只是几行代码：对文本分类进行PaLM模型的微调可以这样完成：
- en: '[PRE17]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Similarly, you can fine-tune the GPT-3.5 model for text classification like
    this:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，你可以这样对GPT-3.5模型进行文本分类的微调：
- en: '[PRE18]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Interestingly, in the fine-tuning available on OpenAI, all inputs are passed
    through a moderation system to make sure that the inputs are compatible with safety
    standards.This concludes fine-tuning. On the extreme end, LLMs can be deployed
    and queried without any task-specific tuning. By prompting, we can accomplish
    few-shot learning or even zero-shot learning as we’ll discuss in the next section.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，在OpenAI提供的微调中，所有输入都会通过一个调节系统，以确保输入与安全标准兼容。这结束了微调。在极端情况下，LLMs可以在没有任何任务特定调整的情况下部署和查询。通过提示，我们可以实现少样本学习甚至零样本学习，正如我们将在下一节讨论的那样。
- en: Prompt Engineering
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示工程
- en: 'Prompts are important for steering the behavior of large language models (LLMs)
    because they allow aligning the model outputs to human intentions without expensive
    retraining. Carefully engineered prompts can make LLMs suitable for a wide variety
    of tasks beyond what they were originally trained for. Prompts act as instructions
    that demonstrate to the LLM what is the desired input-output mapping. The picture
    below shows a few examples for prompting different language models (source: “Pre-train,
    Prompt, and Predict - A Systematic Survey of Prompting Methods in Natural Language
    Processing” by Liu and colleagues, 2021):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 提示对于引导大型语言模型（LLMs）的行为非常重要，因为它们可以在不进行昂贵的重新训练的情况下将模型输出与人类意图对齐。精心设计的提示可以使LLMs适用于原始训练之外的各种任务。提示充当指示，向LLM展示所需的输入-输出映射。下图显示了提示不同语言模型的几个示例（来源：“预训练、提示和预测
    - 自然语言处理中提示方法的系统调查”由刘等人，2021年）：
- en: '![Figure 8.5: Prompt examples, particularly knowledge probing in cloze form,
    and summarization.](../media/file57.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图8.5：提示示例，特别是填空形式的知识探测和摘要。](../media/file57.png)'
- en: 'Figure 8.5: Prompt examples, particularly knowledge probing in cloze form,
    and summarization.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5：提示示例，特别是填空形式的知识探测和摘要。
- en: Prompt engineering, also known as in-context learning, refers to techniques
    for steering large language model (LLM) behavior through carefully designed prompts,
    without changing the model weights. The goal is to align the model outputs with
    human intentions for a given task. By designing good prompt templates, models
    can achieve strong results, sometimes comparable to fine-tuning. But how do good
    prompts look like?
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程，也称为上下文学习，是指通过精心设计的提示技术引导大型语言模型（LLM）的行为，而无需更改模型权重。其目标是使模型输出与给定任务的人类意图对齐。通过设计良好的提示模板，模型可以取得强大的结果，有时可与微调相媲美。但是好的提示看起来是什么样的呢？
- en: Structure of Prompts
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示的结构
- en: 'Prompts consist of three main components:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 提示由三个主要组成部分组成：
- en: Instructions that describe the task requirements, goals and format of inputs/outputs
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述任务要求、目标和输入/输出格式的指令
- en: Examples that demonstrate the desired input-output pairs
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展示所需输入-输出对的示例
- en: The input that the model must act on to generate the output
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型必须对其进行操作以生成输出的输入
- en: 'Instructions explain the task to the model unambiguously. Examples provide
    diverse demonstrations of how different inputs should map to outputs. The input
    is what the model must generalize to.Basic prompting methods include zero-shot
    prompting with just the input text, and few-shot prompting with a few demonstration
    examples showing desired input-output pairs. Researchers have identified biases
    like majority label bias and recency bias that contribute to variability in few-shot
    performance. Careful prompt design through example selection, ordering, and formatting
    can help mitigate these issues.More advanced prompting techniques include instruction
    prompting, where the task requirements are described explicitly rather than just
    demonstrated. Self-consistency sampling generates multiple outputs and selects
    the one that aligns best with the examples. Chain-of-thought (CoT) prompting generates
    explicit reasoning steps leading to the final output. This is especially beneficial
    for complex reasoning tasks. CoT prompts can be manually written or generated
    automatically via methods like augment-prune-select.This table gives a brief overview
    of a few methods of prompting compared to fine-tuning:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '| **Technique** | **Method** | **Key Idea** | **Results** |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| Fine-tuning | Fine-tune on explanation dataset generated via prompting |
    Improves model''s reasoning abilities | 73% accuracy on commonsense QA dataset
    |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot prompting | Simply feeding the task text to the model and asking
    for results. | Text: "i''ll bet the video game is a lot more fun than the film."<br>-
    Sentiment: |  |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: '| Chain-of-Thought (CoT) | Prefix responses with "Let''s think step by step"
    | Gives model space to reason before answering | Quadrupled accuracy on math dataset
    |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: '| Few-shot prompting | Provide few demos consisting of input and desired output
    to help the model understand | Shows desired reasoning format | Tripled accuracy
    on grade school math |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: '| Least-to-most prompting | Prompt model for simpler subtasks to solve incrementally.
    "To solve {question}, we need to first solve: " | Decomposes problems into smaller
    pieces | Boosted accuracy from 16% to 99.7% on some tasks |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
- en: '| Selection-inference prompting | Alternate selection and inference prompts
    | Guides model through reasoning steps | Lifts performance on long reasoning tasks
    |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: '| Self-consistency | Pick most frequent answer from multiple samples | Increases
    redundancy | Gained 1-24 percentage points across benchmarks |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: '| Verifiers | Train separate model to evaluate responses | Filters out incorrect
    responses | Lifted grade school math accuracy ~20 percentage points |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
- en: 'Figure 8.6: Pronpting techniques for LLMs.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'Some prompting techniques incorporate external information retrieval to provide
    missing context to the LLM before generating the output. For open-domain QA, relevant
    paragraphs can be retrieved via search engines and incorporated into the prompt.
    For closed-book QA, few-shot examples with evidence-question-answer format work
    better than question-answer format.There are different techniques to improve the
    reliability of large language models (LLMs) in complex reasoning tasks:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompting and Explanation: prompting the model to explain its reasoning step-by-step
    before answering using prompts like "Let''s think step by step" (as in CoT) significantly
    improves accuracy in reasoning tasks.'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Providing few-shot examples of reasoning chains helps demonstrate the desired
    format and guides LLMs in generating coherent explanations.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Alternate Selection and Inference Prompts: Utilizing a combination of specialized
    selection prompts (narrow down the answer space) and inference prompts (generate
    the final response) leads to better results compared to generic reasoning prompts
    alone.'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Problem Decomposition: Breaking down complex problems into smaller subtasks
    or components using a least-to-most prompting approach helps improve reliability,
    as it allows for a more structured and manageable problem-solving process.'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sampling Multiple Responses: Sampling multiple responses from LLMs during generation
    and picking the most common answer increases consistency, reducing reliance on
    a single output. In particular, training separate verifier models that evaluate
    candidate responses generated by LLMs helps filter out incorrect or unreliable
    answers, improving overall reliability.'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, fine-tuning LLMs on explanation datasets generated through prompting
    enhances their performance and reliability in reasoning tasks
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '**Few-shot learning** presents the LLM with just a few input-output examples
    relevant to the task, without explicit instructions. This allows the model to
    infer the intentions and goals purely from demonstrations. Carefully selected,
    ordered and formatted examples can greatly improve the model''s inference abilities.
    However, few shot learning can be prone to biases and variability across trials.
    Adding explicit instructions can make the intentions more transparent to the model
    and improve robustness. Overall, prompts combine the strengths of instructions
    and examples to maximize steering of the LLM for the task at hand.'
  id: totrans-175
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Instead of hand-engineering prompts, methods like automatic prompt tuning learn
    optimal prompts by directly optimizing prefix tokens on the embedding space. The
    goal is to increase the likelihood of desired outputs given inputs. Overall, prompt
    engineering is an active area of research for aligning large pre-trained LLMs
    with human intentions for a wide variety of tasks. Careful prompt design can steer
    models without expensive retraining.In this section, we’ll go through a few (but
    not all) of the techniques mentioned beforehand. Let’s discuss the tools that
    LangChain provides tools to create prompt templates in Python!
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 自动提示调整等方法不是手工设计提示，而是通过直接优化嵌入空间上的前缀标记来学习最佳提示。目标是在给定输入的情况下增加期望输出的可能性。总的来说，提示工程是一个活跃的研究领域，用于使大型预训练LLM与人类意图在各种任务上保持一致。精心设计的提示可以引导模型而无需昂贵的重新训练。在本节中，我们将介绍之前提到的一些（但不是全部）技术。让我们讨论LangChain提供的用Python创建提示模板的工具！
- en: Templating
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模板化
- en: 'Prompts are the instructions and examples we provide to language models to
    steer their behavior. Prompt templating refers to creating reusable templates
    for prompts that can be configured with different parameters.LangChain provides
    tools to create prompt templates in Python. Templates allow prompts to be dynamically
    generated with variable input. We can create a basic prompt template like this:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 提示是我们提供给语言模型的指令和示例，以引导它们的行为。提示模板化指的是创建可配置不同参数的提示的可重用模板。LangChain提供了用Python创建提示模板的工具。模板允许使用变量输入动态生成提示。我们可以创建一个基本的提示模板如下：
- en: '[PRE19]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This template has two input variables - {adjective} and {topic}. We can format
    these with values:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模板有两个输入变量 - {形容词} 和 {主题}。我们可以用值格式化这些变量：
- en: '[PRE20]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The template format defaults to Python f-strings, but Jinja2 is also supported.Prompt
    templates can be composed into pipelines, where the output of one template is
    passed as input to the next. This allows modular reuse.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 模板格式默认为Python f-strings，但也支持Jinja2。提示模板可以组合成管道，其中一个模板的输出作为下一个模板的输入。这样可以实现模块化重用。
- en: Chat Prompt Templates
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 聊天提示模板
- en: 'For conversational agents, we need chat prompt templates:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对于对话代理，我们需要聊天提示模板：
- en: '[PRE21]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This formats a list of chat messages instead of a string. This can be useful
    for taking the history of a conversation into account. We’ve looked at different
    memory methods in Chapter 5\. These are similarly relevant in this context to
    make sure model outputs are relevant and on point.Prompt templating enables reusable,
    configurable prompts. LangChain provides a Python API for conveniently creating
    templates and formatting them dynamically. Templates can be composed into pipelines
    for modularity. Advanced prompt engineering can further optimize prompting.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这将格式化一系列聊天消息而不是一个字符串。这对考虑对话历史非常有用。我们在第5章中看过不同的记忆方法。在这种情况下，这些方法同样相关，以确保模型输出相关且切题。提示模板化可以实现可重用、可配置的提示。LangChain提供了一个Python
    API，方便地创建模板并动态格式化它们。模板可以组合成管道以实现模块化。高级提示工程可以进一步优化提示。
- en: Advanced Prompt Engineering
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高级提示工程
- en: LangChain provides tools to enable advanced prompt engineering strategies like
    few-shot learning, dynamic example selection, and chained reasoning.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain提供了工具，以实现像少样本学习、动态示例选择和链式推理等高级提示工程策略。
- en: Few-Shot Learning
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 少样本学习
- en: 'The `FewShotPromptTemplate` allows showing the model just a few demonstration
    examples of the task to prime it, without explicit instructions. For instance:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`FewShotPromptTemplate`允许向模型展示任务的几个示例以引导它，而无需明确的指令。例如：'
- en: '[PRE22]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The model must infer what to do from the examples alone.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 模型必须仅从示例中推断出该做什么。
- en: Dynamic Example Selection
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 动态示例选择
- en: 'To choose examples tailored to each input, `FewShotPromptTemplate` can accept
    an `ExampleSelector` rather than hardcoded examples:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 为了选择针对每个输入量身定制的示例，`FewShotPromptTemplate`可以接受一个`ExampleSelector`而不是硬编码的示例：
- en: '[PRE23]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '`ExampleSelector` implementations like `SemanticSimilarityExampleSelector`
    automatically find the most relevant examples for each input.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`SemanticSimilarityExampleSelector`等`ExampleSelector`实现可以自动找到每个输入最相关的示例。'
- en: Chained Reasoning
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 链式推理
- en: 'When asking an LLM to reason through a problem, it is often more effective
    to have it explain its reasoning before stating the final answer. For example:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 当要求LLM通过一个问题进行推理时，通常更有效的做法是在陈述最终答案之前让它解释其推理过程。例如：
- en: '[PRE24]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This encourages the LLM to logically think through the problem first, rather
    than just guessing the answer and trying to justify it after. This is called **Zero-Shot
    Chain of Thought**. Asking an LLM to explain its thought process aligns well with
    its core capabilities.**Few-Shot Chain of Thought** prompting is a few-shot prompt,
    where the reasoning is explained as part of the example solutions, with the idea
    to encourage an LLM to explain its reasoning before making a decision. It has
    been shown that this kind of prompting can lead to more accurate results, however,
    this performance boost was found to be proportional to the size of the model,
    and the improvements seemed to be negligible or even negative in smaller models.In
    **Tree of Thoughts (ToT)** prompting, we are generating multiple problem-solving
    steps or approaches for a given prompt and then using the AI model to critique
    these steps. The critique will be based on the model’s judgment of the solution’s
    suitability to the problem. Let''s walk through a more detailed example of implementing
    ToT using LangChain.First, we''ll define our 4 chain components with `PromptTemplates`.
    We need a solution template, an evaluation template, a reasoning template, and
    a ranking template. Let’s first generate solutions:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这鼓励LLM首先通过逻辑方式思考问题，而不是仅仅猜测答案，然后试图在之后证明。这被称为**零射链思维**。要求LLM解释其思维过程与其核心能力非常契合。**少射链思维**提示是一个少射提示，其中推理作为示例解决方案的一部分进行解释，旨在鼓励LLM在做出决定之前解释其推理。已经证明这种提示可以导致更准确的结果，然而，这种性能提升发现与模型的大小成正比，而在较小的模型中，改进似乎是微不足道的，甚至是负面的。在**思维树（ToT）**提示中，我们生成多个解决问题的步骤或方法，然后使用AI模型对这些步骤进行批判。批判将基于模型对解决方案适应问题的判断。让我们通过使用LangChain实现ToT的更详细示例来走一遍。首先，我们将使用`PromptTemplates`定义我们的4个链组件。我们需要一个解决方案模板，一个评估模板，一个推理模板和一个排名模板。让我们首先生成解决方案：
- en: '[PRE25]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let’s ask the LLM to evaluate these solutions:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们要求LLM评估这些解决方案：
- en: '[PRE26]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now we’ll reason a bit more about them:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将对它们进行更多的推理：
- en: '[PRE27]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Finally, we can rank these solutions given our reasoning so far:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，根据我们迄今为止的推理，我们可以对这些解决方案进行排名：
- en: '[PRE28]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, we create chains from these templates before we’ll put it all together:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在将所有内容放在一起之前从这些模板中创建链：
- en: '[PRE29]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Finally. we connect these chains into a `SequentialChain`:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将这些链连接成一个`SequentialChain`：
- en: '[PRE30]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This allows us to leverage the LLM at each stage of the reasoning process. The
    ToT approach helps avoid dead-ends by fostering exploration.These techniques collectively
    enhance the accuracy, consistency, and reliability of large language models' reasoning
    capabilities on complex tasks by providing clearer instructions, fine-tuning with
    targeted data, employing problem breakdown strategies, incorporating diverse sampling
    approaches, integrating verification mechanisms, and adopting probabilistic modeling
    frameworks.Prompt design is highly significant for unlocking LLM reasoning capabilities,
    the potential for future advancements in models and prompting techniques, and
    these principles and techniques form a valuable toolkit for researchers and practitioners
    working with large language models.Let’s summarize!
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够在推理过程的每个阶段利用LLM。ToT方法有助于通过促进探索来避免死胡同。这些技术共同增强了大型语言模型在复杂任务上的推理能力的准确性、一致性和可靠性，提供更清晰的指导，通过有针对性的数据微调，采用问题分解策略，融合多样的抽样方法，整合验证机制，并采用概率建模框架。提示设计对于释放LLM的推理能力、模型和提示技术未来进展的潜力以及这些原则和技术对于与大型语言模型一起工作的研究人员和从业者来说都是非常重要的。让我们总结一下！
- en: Summary
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In Chapter 1, we discussed the basic principles of generative models, particularly
    LLMs, and their training. We focused mostly on the pre-training step, which is
    – generally speaking – adjusting the models to the correlations within words and
    wider segments of texts. Alignment it the assessment of model outputs against
    expectations and conditioning is the process of making sure the output is according
    to expectations. Conditioning allows steering generative AI to improve safety
    and quality, but it is not a complete solution. In this chapter, the focus is
    on conditioning, in particular through fine-tuning and prompting. In fine-tuning
    the language model is trained on many examples of tasks formulated as natural
    language instructions, along with appropriate responses. Often this is done through
    reinforcement learning with human feedback (RLHF), which involves training on
    a dataset of human-generated (prompt, response) pairs, followed by reinforcement
    learning from human feedback, however, other techniques have been developed that
    have been shown to produce competitive results with lower resource footprints.
    In the first recipe of this chapter, we’ve implemented a fine-tuning of a small
    open-source model for question answering.There are many techniques to improve
    the reliability of LLMs in complex reasoning tasks including step-by-step prompting,
    alternate selection, and inference prompts, problem decomposition, sampling multiple
    responses, and employing separate verifier models. These methods have shown to
    enhance accuracy and consistency in reasoning tasks. We’ve discussed and compared
    several techniques. LangChain provides building blocks to unlock advanced prompting
    strategies like few-shot learning, dynamic example selection, and chained reasoning
    decomposition as we’ve shown in the examples.Careful prompt engineering is key
    to aligning language models with complex objectives. Reliability in reasoning
    can be improved by breaking down problems and adding redundancy. The principles
    and techniques that we’ve discussed in this chapter provide a toolkit for experts
    working with LLMs. We can expect future advancements in both model training and
    prompting techniques. As these methods and LLMs continue to develop, they will
    likely become even more effective and useful for a broader range of applications.Let’s
    see if you remember some more key points from this chapter!
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Please have a look to see if you can come up with the answers to these questions
    from memory. I’d recommend you go back to the corresponding sections of this chapter,
    if you are unsure about any of them:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: What’s alignment in the context of LLMs?
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are different methods of conditioning and how can we distinguish them?
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How’s moderation related to conditioning?
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is instruction tuning and what’s its importance?
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is quantization?
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are a few methods for fine-tuning?
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is few-shot learning?
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is Chain of Thought prompting?
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain Tree of Thought prompting!
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
