# （一）



Mascha Kurpicz-Briki

# 不仅仅是一个聊天机器人

## 语言模型揭秘

![](img/604345_1_En_BookFrontmatter_Figa_HTML.png)

Springer 标志。

Mascha Kurpicz-Briki 应用机器智能，伯尔尼应用科学大学，比尔/宾讷，瑞士 ISBN 978-3-031-37689-4e-ISBN 978-3-031-37690-0[`doi.org/10.1007/978-3-031-37690-0`](https://doi.org/10.1007/978-3-031-37690-0)© 编者（如适用）和作者，独家许可给 Springer Nature Switzerland AG 2023 本作品受版权保护。所有权利仅由出版商独家许可，无论涉及全部还是部分材料，特别是翻译权、再版、插图重复使用、朗诵、广播、微缩胶片复制或以任何其他物理方式、传输或信息存储和检索、电子适应、计算机软件，或通过现在已知或今后开发的类似或不同方法。本出版物中使用的一般描述性名称、注册名称、商标、服务标记等，并不意味着，即使在没有明确声明的情况下，这些名称免受相关保护法律和法规的约束，因此可供普遍使用。出版商、作者和编辑可以安全地假设本书中的建议和信息在出版日期被认为是真实和准确的。出版商、作者或编辑对本书中包含的材料或可能存在的任何错误或遗漏不作任何明示或暗示的保证。出版商在已发表的地图和机构隶属关系方面保持中立。

封面插图：© metamorworks/stock.adobe.com

这本 Springer 印记由注册公司 Springer Nature Switzerland AG 出版

注册公司地址为：瑞士，6330 Cham，Gewerbestrasse 11

致谢

我要感谢激励我写这本书的跨学科项目合作伙伴，特别是“BIAS：减轻劳动力市场 AI 多样性偏见”地平线欧洲项目，以及我的研究小组和其他研究同事，对本书主题的启发性讨论和建议。

特别感谢审阅者对书稿和手稿提出的有益评论、想法和建议：Souhir Ben Souissi，Sarah Dégallier Rochat，Ralf Kurpicz，Martin Spätig，Roger A. Søraa，Jürgen Vogel，以及三位匿名审阅者。特别感谢 Alexandre R. Puttick 在整个项目期间进行的广泛讨论和详细审查。也要特别感谢 Ralf Gerstner 和 Springer 团队在整个过程中对我的支持。

最后，我要感谢我的家人，感谢他们持续的支持和启发性的讨论和建议，使我能够完成这本书的写作。

内容 1 介绍 1 欢迎来到未来 1 媒体中的聊天机器人 2 人工智能和普通大众 3 这本书是为谁准备的？3 章节概述 52 机器学习简介 7 概述 7 传统计算机编程和机器学习 7 监督式机器学习 9 无监督机器学习 10 强化学习 11 算法和人工智能 12 文本处理即将开始！14 总结 153 处理书面语言 17 概述 17 自然语言处理应用 18 搜索引擎和写作建议 18 文本分类 18 情感分析 20 文本生成 21 信息提取 21 从应用到方法 22 自然语言处理方法简介 23 词性标注 24 依存句法分析 25 命名实体识别 26 深入机器学习 27 处理训练数据 28 特征选择 30 从词到数字 31 机器学习分类器 32 损失函数 33 训练和测试数据 35 从机器学习到深度学习 35 线性模型的局限性 36 神经网络 37 神经元：受人脑启发 39 从错误中学习 40 使用神经网络进行情感分析 41 深入词嵌入 42 词之间的关系 43 创建词嵌入 44Word2Vec 词嵌入 45 现成的词嵌入 49 使用库 50 语言模型的高层���图 51 总结 514 聊天机器人有情感吗？53 概述 53 机器和情感 53 图灵测试 53 有情感的聊天机器人？54 神秘谷现象 56 进入语言模型的世界 57 整个句子的向量 57 记住一些词 59 带有循环的神经网络 60 它们就在这里，语言模型！63 走向变压器模型 67 编码器和解码器 67 预训练和微调 69 变压器模型架构概述 71 自注意力 73 交叉注意力和位置编码 75 静态和上下文化的词嵌入 76BERT 模型 78 聊天机器人的架构 79 总结 815 真与假 83 概述 83 如果语言模型错了怎么办？83 你知道白松露吗？83 幻觉 85 训练假新闻？87 错误答案的不同原因 87 聊天机器人会撒谎吗？89 聊天机器人的简史 90ELIZA91 基于规则的方法 92 带有句子嵌入的简单聊天机器人 94 什么是好模型？95 总结 996 语言模型中的刻板印象 101 概述 101 自然语言处理中的偏见 101 词嵌入中的性别偏见 101 变压器模型中的性别偏见 104 变压器模型中的种族偏见 104 机器翻译中的偏见 105ChatGPT 和偏见 106 偏见缓解 108 其他风险和局限性 109 危险的使用案例 109 幕后的工作者 110 环境成本 113 总结 1137 人类和语言模型的未来 115 概述 115 人类的未来 115 负责任人工智能的未来 118 工作的未来 120 教育的未来 121 结论：塑造未来 123 参考文献 125 关于作者 Mascha Kurpicz-Briki

是瑞士比尔应用科学伯尔尼大学数据工程教授，也是应用机器智能研究小组的共同领导者。在她的研究中，她正在探讨如何将数字技术，特别是机器学习和自然语言处理（NLP），应用于社会和社区挑战。这包括医疗保健应用，以及在 NLP 公平领域的研究，她研究如何衡量和减轻词嵌入和语言模型中的偏见。在这个背景下，她领导了“BIAS：减轻劳动力市场 AI 多样性偏见”Horizon Europe 项目的技术工作包。她对以人为中心的应用感兴趣，遵循增强智能的原则 - 支持人类而不是取代他们。


© 作者，独家许可给 Springer Nature Switzerland AG 2023M. Kurpicz-Briki 超越聊天机器人[`doi.org/10.1007/978-3-031-37690-0_1`](https://doi.org/10.1007/978-3-031-37690-0_1)

# 1. 介绍

Mascha Kurpicz-Briki^(1  )(1)应用机器智能，伯尔尼应用科学大学，比尔/比尔，瑞士

## 欢迎来到未来

在*人工智能*（AI）领域的最新成就给社会带来了许多挑战。对于广大公众，包括来自 AI 领域以外的研究人员和专业人士，技术上可能和科幻之间的边界变得更加模糊。

人工智能这个术语是一个相当通用的术语。它有许多不同的解释，这些解释的混淆导致对人工智能技术运作方式的夸大期望和假设。

如今被称为人工智能技术背后的主要方法是*机器学习*（*ML*）。机器学习使计算机能够从示例中学习，以解决它以前没有见过的类似数据的任务。尽管机器学习的基础可以追溯到上世纪 60 年代，但高计算和数据需求使其在计算机科学领域更多地成为一个次要的好奇心数十年。这在新千年发生了变化，因为计算机硬件的强大和大数据的出现（计算机学习的示例）达到了这样一个程度，使得机器学习开始取得令人印象深刻的结果。机器学习可以应用于不同领域，计算机学习的示例可以是不同数据类型，如图像、文本、视频或传统数据表。

*自然语言处理（NLP）*领域研究计算机与人类语言之间的交互，包括开发用于自动分析或生成文本的方法。过去已经采用了不同的方法来实现这一点，而如今，这种处理的好处在很大程度上依赖于机器学习的进展。机器学习和 NLP 的结合催生了语言模型，如图 1.1 所示。这些是旨在描述书面语言模式的统计模型，可以用于文本生成等用途。语言模型是一些最新炒作产品背后的幕后工作者，包括 ChatGPT。![](img/604345_1_En_1_Fig1_HTML.png)

一个维恩图表示为 2 个相交的块，标记为机器学习和自然语言处理。相交区域被指示为语言模型。

图 1.1

机器学习（ML）和自然语言处理（NLP）背景下的语言模型

## 媒体中的聊天机器人

过去几年来，人工智能的话题在广泛的媒体中随处可见。特别是*生成式人工智能*作为机器学习软件生成（虚假）图像能力的一个例子，受到了极大关注（以及一些臭名）。语言模型（可以看作是文本生成式人工智能）直到 2022 年底 OpenAI 发布 ChatGPT 才被更详细地讨论。

ChatGPT 是一个对用户编写的提示提供类似人类回答并可以用多种语言进行的对话聊天机器人。它的回答听起来似乎合理且雄辩，并且表面上非常可信，但仔细检查后发现，答案可能包含不正确或误导性信息。例如，它被知道会制造对虚构法律文本或书籍的引用。

从专家技术角度来看，在 2022 年底发布时，ChatGPT 的能力似乎令人印象深刻，但并不令人惊讶，广大公众却感到印象深刻 - 甚至感到恐惧。反应从在 ChatGPT 的回答中感受到真实情感和意识到担心失业和学生作弊。也许是因为这是第一次将语言模型轻松地提供给广大公众？

## 人工智能与广大公众

当代历史正在数字转型的背景下书写。为了积极参与这一过程，广大公众需要参与讨论，并与政治家、科学家、工程师和从业者一起引导未来数字社会朝着对人工智能新技术的负责任和合理使用方向发展。为了能够参与这一讨论，公众需要了解这些技术背后的技术基础。只有这样的理解才能对其实际可能性、限制和仅仅是科幻进行合理评估。这就是为什么我决定写这本书。作为一名数据工程师和应用机器智能研究员，我对如何实现人工智能技术的负责任使用很感兴趣。这需要与广大公众进行对话。在这本书中，我希望让非技术读者更好地了解语言模型（比如 ChatGPT 所基于的模型）的工作原理、其惊人的能力以及其局限性。

## 这本书是为谁写的？

尽管这些技术在研究、工作环境和社会中变得无处不在，但广大公众对它们的识别力仍然太低。在我作为技术合作伙伴参与的跨学科研究项目中，非技术项目合作伙伴对技术概念的基本理解被证明是至关重要的。达成这样的理解可能是耗时的，有时候也很困难，因为不同的工作文化相遇并交流。然而，我发现用非技术性的语言解释新技术的技术解释受到了很高的赞赏。我的项目合作伙伴发现我的解释有价值的事实激励我写这本书。

我写这本书是为了向广大公众提供关于文本分析和文本生成的必要背景信息，以便能够理性地参与关于 AI 技术的使用何时以及在哪些条件下是可接受的公共话语。这项技术有潜力在长期内从根本上影响我们的生活、工作和教育。例如，论文、文章和广告口号都可以生成和定制为特定的目标受众。将技术概念翻译成视觉幻灯片和全面解释给我的计算机科学学生已经具有挑战性，而为广大公众解释这些内容则更具挑战性。然而，在今天的世界中，提供易于理解的解释非常重要，我认为这是我们作为技术研究人员的任务。

在本书中，我将在不同章节中深入探讨一些相当技术性的主题。本书面向广大读者，根据需要解释数学或技术背景。因此，当然本书将无法完全涵盖机器学习或自然语言处理的全部内容。然而，我将引用更多技术计算机科学教材和文献，这可能会引起一些读者的兴趣。

最后但并非最不重要的是，我将为你提供对数据工程师工作的更好理解^(1)。对于其他领域的人来说，往往很难想象数据工程师实际上处理的任务和主题是什么。

本书特别针对以下非穷尽读者群体：

+   广大公众中对学习更多关于人工智能，特别是语言模型的非技术读者。

+   其他领域的研究人员，如医学、社会科学、人文学或经济学，有兴趣应用人工智能技术。

+   刚接触机器学习和自然语言处理的技术学生，寻找一般性的软入门。

阅读完本书后，我希望您能够自信地参与公开讨论关于这一新一代语言模型将如何影响社会的话题。您将意识到这些技术可能带来的风险和陷阱，以及在使用从 AI 技术构建的工具时如何负责任地处理。我希望您具备必要的技术基础理解，以便能够至少与技术人员在高层次上讨论 AI 及其社会影响，从而将 AI 技术的技术发展开放给将最受影响的人们的特定思想、需求和专业知识。

## 章节概述

在第二章中，我将介绍机器学习的概念。通常情况下，当我们谈论人工智能时，我们实际上是在谈论机器学习方法。特别是在自然语言处理领域的最新突破中，我们将在后续章节中讨论的语言模型。因此，这个机器学习的介绍将为您奠定基础。我将解释机器学习的不同子领域，以及它与传统软件的区别。

然后，在第三章中，我将深入探讨自然语言处理的主题。书面语言可以表达比句子中包含的单词内容更多的信息。已经开发了不同的方法来处理书面语言或生成类似人类的文本。本章概述了这些方法论，并指出了从这些技术中受益的用例。您还将进一步了解机器学习知识。逐个示例，您将更近距离地了解学习过程实际发生的情况，我将介绍神经网络和深度学习的概念。最后，这个基础将支持引入词嵌入的概念，这使得计算机（更喜欢数字而不是单词）能够处理来自人类语言的单词。

在第四章中，我将从词嵌入转向构建最新技术进步基础的最先进语言模型。随着语言模型变得更加复杂，它们创造了一个具有真正类似人类理解的系统的幻觉。它们真的有一个有意识的思维过程吗？在围绕 ChatGPT 的公开讨论中，这是一个常见问题。因此，我将详细探讨基于 transformer 的语言模型的内部运作，并揭示它们与人类情感相比更多地与统计概率相关。此外，我将讨论对话的人类一面。例如，我将解释为什么人们对聊天机器人提供的答案与搜索引擎的结果列表有不同的反应。

第五章涵盖了新型文本生成技术生成的文本虽然乍一看大多看起来合理，但可能包含错误信息的观点。本章讨论了为什么会发生这种情况，以及文本生成技术的主要目的不一定是在其生成的内容上完全正确。连接到第三章和第四章的技术描述，本章说明了这种错误信息是如何生成的，例如以幻觉的形式，技术上解决这个问题有多困难，以及由此产生的挑战。此外，还提供了对聊天机器人历史的简短介绍，并介绍了用于评估机器学习模型的一些常见性能指标。

在第六章中，我将描述社会刻板印象如何被编码到语言模型中。连接到第三章的词嵌入的技术解释和第四章的基于变压器的模型，并使用具体示例，本章向您介绍自然语言处理中偏见问题。此外，还讨论了语言模型中编码的这种偏见如何导致生成歧视性建议和文本。最后，我们将探讨大型语言模型的其他局限性和风险，例如用例和生态方面。

最后，在第七章中，我将进一步讨论所有这些的社会后果。人工智能技术的崛起引发了许多关于未来人类与智能技术合作的问题，可能影响我们学习、教育和工作的方式。特别是，本章的重点在于未来人类与由语言模型驱动的机器人之间合作的需求。鉴于其他章节中讨论的局限性，我将介绍增强智能的概念，赋予人类权力而不是取代他们。这改变了一些工作的执行方式，但使人类能够控制基于语言模型的应用潜在的偏见或错误结果。我认为，我们不应完全排除人工智能技术，而是必须以负责任的方式将其整合到我们的工作和学习环境中。


© 作者，独家许可给 Springer Nature Switzerland AG 2023M. Kurpicz-Briki 超越聊天机器人[`doi.org/10.1007/978-3-031-37690-0_2`](https://doi.org/10.1007/978-3-031-37690-0_2)

# 2. 机器学习简介

Mascha Kurpicz-Briki^(1 )(1)应用机器智能，伯尔尼应用科学大学，比尔/宾讷，瑞士

## 概述

尽管历史上使用了不同的方法来构建人工智能系统，但如今人工智能这个术语几乎已经成为*机器学习*的同义词。在本章中，我们将深入探讨这个主题，更好地理解机器如何以及在什么意义上能够学习。为此，我们将介绍一些类比和术语，这些类比和术语将贯穿整本书，当我们讨论最终将讨论的文本处理技术的技术方面时。

我们从传统计算机编程和机器学习之间的关键区别开始。接下来，我们将看一下机器学习的不同子领域，即监督学习、无监督学习和强化学习。

## 传统计算机编程和机器学习

首先，让我们讨论*机器学习*如何使人工智能技术与其他软件不同。一般来说，在生成软件时，程序员会用*编程语言*（一组关键字，可以转换为机器可读格式，因此可以轻松地被计算机处理和理解）编写*代码*。^(1) 传统上，这包括一组指令，按顺序执行，导致特定结果。让我们看一个从人类生活而不是软件借鉴的例子：

拿 10 个草莓，1 个苹果和 1 个香蕉

把草莓切成小块放入碗中

把苹果切成片加入碗中

剥香蕉，切成小块，也加入碗中

在碗里加入 1 勺糖和 3 勺柠檬汁

混合所有的配料

把碗放入冰箱至少 30 分钟

如果一切顺利并且遵循指示，每次执行时都会产生相同的结果 - 一份美味的水果沙拉。这类似于传统软件程序。一组指令 - 通常称为*算法* - 在编程语言中实现。在这种软件中，我们有特定的*输入数据*（在我们的例子中，草莓、苹果、柠檬汁等）和产生的*输出*（水果沙拉）。

另一方面，机器学习的工作方式有所不同。基于给定的数据（例如，我们水果沙拉的成分），机器学习或检测模式。这就是我们机器学习过程中所谓的*训练数据*。简化来说，传统算法中的问题是“你如何逐步从成分中获得水果沙拉？”，而在机器学习中，我们问的是“你能用成分做什么或找出什么？”对于第一个问题，我们会得到水果沙拉。而在机器学习的情况下，结果更加开放。如果在机器学习的情况下，我们想要获得水果沙拉，我们会向机器展示成分和成品，但是，至关重要的是，不展示从一个到另一个的步骤。在看到许多成分和水果沙拉的例子之后，机器学习过程构建自己的一套规则来获得水果沙拉。我们无法控制机器最终如何完成任务。

## 监督机器学习

让我们首先看一下监督学习，留在我们比喻的厨房里，我们想要训练一个监督机器学习程序来区分覆盆子和草莓的图片。选择一张图片是否包含覆盆子或草莓是一种所谓的分类任务。我们向系统提供了几个例子（是的，通常我们需要*许多*这样的例子，取决于我们讨论的应用，我们可能需要数千到数百万的例子）的草莓和覆盆子的图片。每张图片旁边，我们提供一个文本，说明这是草莓还是覆盆子。这是分类任务的*正确解决方案*，基于此，机器可以学习这两种浆果的外观，更重要的是，如何区分它们。这种训练的结果是一个*模型*。模型是一个能够对新图片进行分类的软件组件（这些图片不是训练数据的一部分），确定它是否属于训练过程中的一个类别。在我们的情况下，模型对新图片是否显示草莓或覆盆子进行预测。细心的读者可能已经注意到了术语*预测*的使用。预测是一种估计，根据系统在训练过程中识别的模式指示最有可能的选项。虽然根据模型提出的结果是最有可能的选项，但我们必须明白，它也可能是*错误的*。这是本书其余部分需要牢记的关键要点之一。图 2.1 说明了基于草莓和覆盆子图片的监督学习过程！[](../images/604345_1_En_2_Chapter/604345_1_En_2_Fig1_HTML.png)

一幅插图代表了一组草莓和覆盆子经过训练数据，然后是一个机器学习模型识别草莓图像的过程。

图 2.1

用于分类任务的监督机器学习：图片上是草莓还是覆盆子？

## 无监督机器学习

在第二种学习策略中，无监督学习，我们也有训练数据；然而，我们没有*标签*。标签是我们在前一段中看到的*正确解决方案*，指示训练数据中的每张图片上是否有覆盆子或草莓。这意味着我们的训练数据包含一堆图片，有些是草莓，有些是覆盆子，没有任何指示显示图片上显示的是什么。无监督机器学习的一个应用是解决*聚类任务*。在聚类任务中，机器学习算法分析数据并尝试识别相似样本的组。没有标签，我们不知道每个组包含什么，但我们知道单个组内的图像彼此更相似，而不同组的图像至少根据模型来说更相似。理想情况下，我们会有两个簇，一个包括所有草莓的图片，另一个包括所有覆盆子的图片。与前面的例子一样，聚类可能不完美。可能有一些棘手的覆盆子图片，与典型的草莓图片有共同之处，被放入错误的组中（例如，你可能最熟悉杂货店里的驯化大草莓，但野生草莓要小得多，对于不熟悉的人来说，可能会被误认为是覆盆子）。图 2.2 说明了这个无监督机器学习示例！[](../images/604345_1_En_2_Chapter/604345_1_En_2_Fig2_HTML.png)

一幅插图展示了被分为 2 个簇的草莓和覆盆子的插图。簇 1 包括草莓的图像，簇 2 包含覆盆子。训练数据在底部。

图 2.2

将草莓和覆盆子的图片分组：一项无监督机器学习任务

## 强化学习

最后，最后一种机器学习类型是强化学习。这种学习类型类似于监督方法，在这种方法中，我们在每个图像旁边提供了系统信息（例如，我们看到给定图片中有一个草莓）。在强化学习中，机器与（可能是虚拟的）环境之间存在互动，在学习过程中提供反馈。你可以将其想象为一种*试错*方法。

让我们通过一个现实世界的例子来说明这一点。想象一下，有人让你把草莓切成大致相同大小的薄片。提高技能的一种方法可能是接受已经切好的草莓薄片的反馈。例如，旁边的人可能会说，“这片比其他的大很多！”有了这个反馈，你会调整你的切片方法来处理接下来的薄片。一步一步，结果会变得更好。在某个时候，结果会足够好以停止训练过程。这个过程在图 2.3 中有所说明。![](img/604345_1_En_2_Fig3_HTML.png)

一个插图代表了一组切成薄片的草莓的插图，分组在第 1 轮和 N 轮下。第 1 轮表示一个切片，并给出了切片相当大的反馈。第 N 轮表示切片相当均匀，是训练过程的结论。

图 2.3

强化学习：通过反馈改进

同样，在机器学习中也可以提供这种外部反馈。特定结果可以得到奖励，因此这种行为将在最终模型中得到强化。强化学习中的一个关键因素是我们如何向机器提供反馈。例如，如果反馈没有考虑到切片的数量，你可以利用缺乏具体性，干脆不切草莓，导致一个完全均匀的切片，然后得到主管的赞赏。

## 算法和人工智能

那么人工智能（AI）在哪里呢？有一些事情需要澄清。在媒体上，通常会在 AI 的背景下使用术语*算法*。我想在这里给你一些关于这个词的背景以及它的起源与当前在 AI 背景下使用方式略有不同的上下文。一般来说，*算法*这个词（源自九世纪波斯数学家 M. al-Khwarizmi 的名字）意味着一组实现特定目标的指令，例如，正如我们之前所见，如何根据一份食材清单准备水果沙拉。算法也可以是我们想要传达给计算机的一组数据处理步骤。在传统软件中，这种处理就像一道菜谱。例如，要找到一个单词中字母“a”的第一次出现，我们可以使用以下算法：

输入：用英语书写的单词

进行的步骤：

读取第一个字母：是“a”吗？那么结果是 1。

如果不是，读取第二个字母：是“a”吗？那么结果是 2。

等。

输出：输入单词中第一个“a”的位置。

在当代人工智能的背景下，算法一词用来指代整个软件。例如，个性化的音乐或电影推荐通常被认为是基于机器学习的算法。

从技术上讲，当谈论人工智能或机器学习并使用这种措辞时，实际上有两个独立的算法在起作用。在本例（以及大多数未来的例子）的范围内，我们将考虑监督学习设置，其中模型是在带标签的训练数据上训练的。为了说明这一点，我想借用 Katharina Zweig 的定义，她将机器学习定义为“通过示例自动学习，在其中搜索决策规则，然后将其存储在统计模型中”（Zweig 2022，第 95 页）。一方面，我们有用于根据训练数据训练模型的算法，这是“搜索决策规则”的过程。我们将这个过程称为*训练阶段*，如图 2.4 所示。这个训练算法的输入是训练数据和一个未经训练的模型，例如，一个随机猜测的模型。输出是一个经过训练的模型。这是实际的机器学习发生的地方。![](img/604345_1_En_2_Fig4_HTML.png)

一幅插图代表了训练阶段和预测阶段的场景。算法 1 使用训练数据中的草莓和覆盆子的图像来训练模型。在生产阶段，算法 2 识别草莓的图像。

图 2.4

在训练阶段，机器学习模型使用训练数据进行训练。这是实际的机器学习过程发生的地方。然后，得到的模型在预测阶段应用于新数据。

另一方面，我们有统计或*训练*模型，这是训练过程的结果，本身是一个算法，接受一个数据实例作为输入，并根据在训练阶段学到的规则提供预测作为输出。这是*预测阶段*，在这个阶段，训练过的机器学习模型应用于新数据，通常是在模型被训练的任何生产环境中部署后。例如，我们可以输入一张新图片，与训练数据中的图片不同，并询问它是否描绘了一个草莓或覆盆子。我们将照片输入模型，并得到它对这个问题的答案，如图 2.4 右侧所示。

## 文本处理进行中！

到目前为止，我们已经讨论了基于图像的机器学习示例。训练数据包括草莓或覆盆子的图片。然而，我们所看到的学习策略可以以相同或类似的方式应用于不同类型的数据，例如文本。我们现在将从图像转移到文本，并探索本书剩余部分如何将这些机器学习技术应用于自然语言处理。尽管我们将专注于文本处理和生成，但我们描述的概念和限制在一定程度上也适用于处理其他数据类型的模型，如图像、视频或多模态系统。^(2)

## 总结

在本章中，我们看了三种不同类型的机器学习。在监督机器学习中，训练数据是有标签的（例如，指示我们在给定图片中是否看到草莓）。我们研究了一个简单的分类任务，训练一个模型来区分草莓和覆盆子，并对新的、未见过的图片进行预测。对于无监督机器学习，我们看了聚类的例子，最后，我们学习了如何使用强化学习在学习过程中提供反馈并改善结果。

在这些例子中，我们了解到机器学习模型可能会犯错。虽然它们通过提出根据模型最有可能的解决方案来进行预测，但这个解决方案很容易是错误的。在使用人工智能技术时，这是一个需要考虑的关键点。

我们还研究了传统算法和机器学习之间的区别。监督机器学习算法接受以输入和期望输出（标签）形式的数据，并输出从一个到另一个的规则，而传统算法仅包含从输入到期望输出的规则。

现在让我们继续深入探讨迷人的文本处理世界吧！


© 作者，独家许可给 Springer Nature Switzerland AG 2023M. Kurpicz-Briki 不仅仅是一个聊天机器人[`doi.org/10.1007/978-3-031-37690-0_3`](https://doi.org/10.1007/978-3-031-37690-0_3)

# 3. 处理书面语言

Mascha Kurpicz-Briki^(1  )(1)应用机器智能，瑞士比尔/宾讷伯恩应用科学大学

## 概述

从技术角度来看，人类语言长期以来一直吸引着研究人员的兴趣。自然语言处理的第一个应用是在 1948 年在伦敦的伯贝克学院进行的，这是一个字典查找系统（Hancox 1996）。虽然 1948 年在其他领域可能相对较近，但在计算机科学领域，这已经非常早了。从那时起，发生了很多事情。如今，自然语言处理技术在我们的日常生活中得到应用，有时是显性的（例如，与聊天机器人交互时），有时是在幕后（例如，在使用在线搜索时）。

在本章中，我们将更多地了解文本如何被自动处理。自然语言处理是指对语音和文本的自动处理（包括生成）。在本书中，我们将文本处理和自然语言处理这两个术语互换使用。我们将看一些常见的自然语言处理应用程序，我相信您在最近与这些系统互动时会认出其中一些。然后，我们将看一些来自自然语言处理领域的常见方法。接着，我们将加深我们的机器学习知识，介绍和理解深度学习和神经网络的优缺点。最后，我们将了解人类语言如何被表示为数学向量以及为什么这对机器学习有益。

## 自然语言处理应用

文本的自动处理在许多场景中都是有用的。受 Lane 等人（2019）对自然语言处理应用的分类启发，我们一起探讨一些用例，揭示这些应用中潜在的潜力。我们中的许多人每天都在与这些技术无意识地互动。

### 搜索引擎和写作建议

网络和文档搜索应用程序在评估搜索查询（您在搜索框中输入的句子或单词）和可用文档（例如，已收集和索引的互联网上的现有网站）以及识别最佳匹配结果方面严重依赖自然语言处理技术。所有这些通常在一秒钟内完成，您可能并不知道背景中正在进行的极其高效的处理。此外，一些搜索引擎会提出对搜索查询的更正（*您是不是想要*：）或通过评估用户输入的搜索查询提供*自动完成*功能。

自然语言处理技术也可以在我们撰写文本时提供支持。许多文本处理程序会标出拼写错误或提出如何调整语法和风格的建议。这听起来熟悉吗？那么这是另一个用例，您在其中积极与我们在本书中探讨的自然语言处理技术类似的技术进行交互。

### 文本分类

让我们看另一个例子。您是否曾想过您电子邮件收件箱中的*垃圾邮件过滤器*是如何工作的？文本处理技术驱动着对传入电子邮件的分析，以及它们是否对您感兴趣或是垃圾邮件的决定。一些电子邮件提供商使用更先进的过滤，例如提供垃圾邮件、广告或社交媒体通知等类别。但是电子邮件的文本处理并不止于此。进一步的用例可能是按优先级自动排名或提出电子邮件答复，这些功能最近已被不同提供商引入。

但让我们现在继续讨论垃圾邮件检测。根据给定软件产品所使用的底层技术的复杂性，检测垃圾邮件的机制可能会有所不同，也会导致不同质量的结果。在最简单的情况下，可以基于预定义句子或发件人地址进行关键词分析。当然，这可能仍然会让大量垃圾邮件进入您的收件箱。因此，很可能（也希望）没有垃圾邮件检测系统仅依赖于预定义列表。我们还应该记住，垃圾邮件制作者在应对更好的过滤器时变得越来越好。自动文本生成器现在能够编写非常可信的文本，这些文本是专门为不同用途定制的。这是一场关于改进技术的武器竞赛，用于开发看起来非常逼真的垃圾邮件和检测垃圾邮件的技术，两者都使用自然语言处理中最新的机器学习驱动进展。

将电子邮件分类到不同类别是我们在自然语言处理领域称之为*文本分类任务*的一个例子。当使用机器学习方法时，这与我们在上一章中看到的将图像分类为草莓或覆盆子的过程非常相似。假设我们有大量垃圾邮件、广告邮件、社交媒体通知和其他邮件的示例。我们训练数据集中来自这四个类别的所有示例都被标记，这意味着它们被分配了一个*标签*，指示它们属于哪个类别。在训练阶段，机器然后识别区分这些文本组的特点。最后，模型可以将新的文本样本分类到这四个类别中的一个，如图 3.1 所示。![](img/604345_1_En_3_Fig1_HTML.png)

一幅插图描述了使用机器学习模型将电子邮件分类为垃圾邮件或正常邮件的过程。该模型在训练数据上进行训练，并在决定电子邮件是否为垃圾邮件时区分广告和社交媒体类别。

图 3.1

使用监督机器学习将电子邮件分类为不同类别

回顾前一章节，这些预测的类别是基于概率得出的最佳猜测，结果的质量取决于训练数据的完整性和质量。即使在最好的情况下，也可能出现错误。牢记这一点，下次收件箱中的电子邮件被错误分类时你也不会感到惊讶。

这种文本分类也可以用于其他用例。例如，可以对来自两个或更多不同作者的大量文本进行机器学习模型训练，从而获得一个能够识别文本作者的分类器。这种技术已被用于确定莎士比亚剧作《亨利八世》中可能由他人撰写的部分，这一问题长期以来一直受到学者们的争论（Plecháč 2021）。作者归属也在*抄袭检测*中发挥着重要作用，这种软件通常应用于科学出版的背景下，以及检查学生论文。该领域的其他工作探讨了如何利用机器学习来识别互联网上的仇恨言论或虚假新闻。

### 情感分析

文本分类也应用于*情感分析*领域。这些方法旨在从文本中提取表明情感或观点的信息，例如，一篇评论文本中作者对产品持有积极还是消极态度。同样，这个任务可以通过不同的方式解决，从非常简单的方法开始。如果我们发现表达如“质量差”，“设计可怕”，或“界面丑陋”，或者相反，“产品精彩”，“设计惊人”，“我喜欢它”等，我们可以很容易地了解评论作者对产品的喜好或厌恶。然而，情感可以以不可数的方式表达，人类的创造力几乎是无限的，因此当我们创建一个包含所有可能的积极和消极表达的词汇表时，我们很快就会达到极限。因此，另一种（除其他方法外）是使用监督机器学习的方法，就像我们讨论过的电子邮件分类的情况一样。基于积极文本和消极文本的示例，训练一个分类器，能够预测新的文本样本是积极还是消极。每当我们有一个问题，想要将文本分类为两个或更多类别，并且对于不同类别有足够多的示例可用时，我们可以利用这种监督学习方法。

一般来说，在文本分类任务的背景下，当我们有两个可能的组别需要区分时，我们称之为*二元分类*，例如，正面和负面文本，否则称为*多类分类*。多类情感分类的一个常见例子是将评论文本映射到它们的 1 到 5 星评级，通常在用户提供反馈的网站上看到，例如电影或餐厅。我们会尝试使用机器学习来预测星级评分（即标签/*正确答案*），基于用户撰写的评论文本。这将是一个包含五个类别的多类分类。

### 文本生成

与文本分类任务相反，模型将文本作为输入并预测指定类别之一作为输出，其他任务将文本作为输入并且输出也是文本。我们称这些任务为*文本生成*。这种任务的一个例子是*文本摘要*。当处理数百页的文本时，通常，内容也可以在几页甚至更少的表面层次上进行聚合。语言模型可以自动支持这项任务。*聊天机器人*是文本生成的另一个应用。基于文本作为输入，它们提供类似于人类的文本回答。聊天机器人可以有不同的目标，例如回答客户查询并提供客户正在寻找的信息。在这里，信息的正确性非常重要。正如我们最近看到的 ChatGPT 的崛起一样，语言模型和特别是聊天机器人产生的内容的正确性并不总是确定的（这自然可能会让某些读者感到惊讶）。我们将在后面的章节中进一步探讨这个问题。目前，我想指出一些聊天机器人的目的也可以是提供尽可能接近人类选择的措辞的最合理文本。文本生成的其他应用包括机器学习模型自动生成电影剧本、歌词或诗歌，文本生成的另一个重要用例是*机器翻译*，其中文本从一种语言翻译为另一种语言，例如从英语到德语。文本生成也可以是非文本输入的结果，例如自动生成图像标题。

### 信息提取

我们可以在文本上进行的另一个任务是*信息提取*。我们希望基于文本获取结构化信息，而这种信息通常被认为是*非结构化数据*（这也适用于图像）。非结构化是因为数据没有预定义的组织原则，与按行和列标识结构化的值表相对。在书面语言的上下文中，术语*非结构化*可能有点令人困惑。在处理给定语言的文本时，当然存在外部约束，比如语言的语法可能会在一定程度上限制句子中单词或对象的顺序。然而，仍然存在灵活性，并非所有句子或文本具有相同的结构。因此，文本有时也被称为*半结构化*数据，可以通过附加信息进行扩展，例如标识句子的主语或为单词类型添加标记（例如，将*to be*标记为动词）。

原始文本数据本身并不等同于以表格形式排列的数据，其中列对应于主语、动词和宾语。我们可以从文本中提取这样的结构化数据，但从结构化表格中我们无法仅仅恢复原始文本。

在信息提取过程中，我们希望从文本中获取非常具体的数据。因此，我们定义了一个数据结构，意味着我们描述了我们希望从计划处理的所有不同文本中获取的结构化数据。这个数据结构可能会因应用程序的用例而有很大不同。例如，在处理法律文本时，提取文本中提到的段落和法律可能是感兴趣的。在医生的笔记中，我们可能希望从文本中提取诊断，并最好将其转换为标准化格式，例如诊断代码。

让我们通过一个例子来看这个问题。图 3.2 的左侧包含非结构化文本。我们希望从文本中提取两件事（数据结构准备在右侧的表格中）：

+   提到了哪些种类的浆果

+   是否提到了人物（例如，通过使用他们的名字）

![](img/604345_1_En_3_Fig2_HTML.png)

一幅插图展示了将非结构化文本转化为结构化数据的过程，其中突出显示了每个文本中提到的浆果和人物，文本 1 提到草莓、覆盆子和安娜，而文本 2 提到蓝莓和安娜。

图 3.2

从非结构化文本中提取结构化信息

信息提取的目的是提供关于文本的结构化信息，这些信息可以进一步由人类或另一个软件组件处理。例如，生成的数据表格可以轻松地用于快速获取所有提到安娜或包含有关草莓信息的文本。

### 从应用到方法

在本节中我们介绍的用例示例并不是自然语言处理的所有可能应用的完整列表。所选用例旨在让您感受到这些技术的强大之处，广泛的用途范围，以及它们如何潜在地支持我们的日常工作和生活。我还希望您从本节中了解到，根据问题的复杂性和预期结果，可以应用非常不同的技术。这可以从简单方法到更复杂的方法，如深度学习和大型语言模型。数据工程师的艺术和技巧之一是在当今工具包和研究成果中提供的广泛可能性中为给定应用程序识别正确的方法。在接下来的章节中，我们将更仔细地研究其中一些技术。

## 自然语言处理方法简介

尽管自然语言处理中的最新发展严重依赖于机器学习，但一般情况下并非总是如此。一个简单的文本处理任务可能是根据程序提供的单词列表识别文本中出现的浆果。在这种情况下，不需要训练数据或训练阶段，而是采用类似于之前看到的传统算法。我们将逐步处理给定的句子，将每个单词与列表中的单词进行比较。这些简单方法有一些局限性，例如，我们必须在列表中添加*草莓*和复数形式*草莓*以确保全部获取。图 3.3 给出了这种情况的一个示例！[](../images/604345_1_En_3_Chapter/604345_1_En_3_Fig3_HTML.png)

给定句子中关键词的插图，句子为太阳照耀着，天气很好。一个大田地里种满了草莓，然而在这个地区没有种植覆盆子。关键词列表包括草莓、覆盆子和蓝莓。

图 3.3

保持简单：根据关键词列表在文本中识别特定单词

从计算机的角度来看，文本只是一串字母和符号。我们称将额外结构添加到这些字符串的过程为*parsing*。当人类看文本时，我们当然看到更多的结构 - 比如分开的单词或语法结构。问题在于计算机更喜欢处理数字和进行数学运算，并且通常没有我们在头脑中自动和立即构建这样的文本所需的多年训练。在解析文本时，我们指示计算机，例如进行单词或句子的*tokenization*：我们分隔单词和句子，指定在每个空格后，一个新单词开始。或者在句号后，一个新句子开始。我们通过用编程语言编写指令来指示计算机这样做。

更高级的文本标记可以通过使用*词性标注*、*依存句法分析*或*命名实体识别*来完成。

### 词性标注

词性标注是指用*标签*注释每个单词的类型的过程。例如，strawberry 是一个*名词*。

正如你所想象的那样，这些是需要在不同应用程序中一遍又一遍地完成的常规任务。数据工程师有现有的软件组件（称为库）可供重复使用，而不是每次都重新发明轮子。

在图 3.4 中，我们看到一个例子，说明了这些信息是如何由一个名为 SpaCy 的库自动提供的^(1)，这个库通常用于自然语言处理。![](img/604345_1_En_3_Fig4_HTML.png)

一个文本 read anna is eating the strawberry. 这个句子在词性标注中被分为代词、动词和名词。

图 3.4

词性标注的一个例子：提取有关单词类型的信息。例如，“.” 被视为标点符号（PUNCT），“eating” 被分类为动词，而名字“Anna” 是专有名词（PROPN）。

### 依存句法分析

现在让我们看一个依存句法分析的例子。依存句法分析提供了关于句子中单词如何相互关联的信息。我们中的一些人，特别是那些具有更多技术背景的人，在处理这种类型的分析时可能需要查阅我们的语法书。

为了让你有一些直觉，我们将看一段使用编程语言 Python 和库 SpaCy 的编程代码的简短片段，如图 3.5 所示。![](img/604345_1_En_3_Fig5_HTML.png)

该代码利用 s p a C y 处理文本 there was a large strawberry field，并使用 dep 样式可视化其依存解析树。

图 3.5

使用 SpaCy 库可视化依存句法分析的代码片段

第一行加载了这个现有库中英语语言的模型。处理另一种语言时会有所不同。例如，为了考虑语法概念来解析我们的文本，我们需要有给定语言的具体信息。在第二行，我们定义了要解析的句子。最后一行启动了依赖解析并启用了可视化。如果你不理解上述代码片段中每个单词的含义，不要担心；主要目标是理解这些工作原理。

基于这三行编程代码，生成了关于我们句子的图示，如图 3.6 所示。![](img/604345_1_En_3_Fig6_HTML.png)

一幅插图展示了对句子“there was a large strawberry field”进行句法分析，使用依赖标签，单词按其词性分类，连接由属性如 a t t r、d e t、a m o d、e x p l 和 compound 定义。

图 3.6

使用 SpaCy 库可视化一个示例句子的依赖解析

在这幅插图中我们有两种不同的信息：一方面，我们可以看到底部的单词类型，我们之前称之为词性标签（*was* 是动词，*large* 是形容词）。箭头指示了句子中单词之间的关系，句法依赖。例如，单词 *strawberry* 和 *field* 被识别为复合词。单词 *large* 被标记为形容词修饰语（*amod*）到单词 *field*（我们在这一点上不会深入讨论语法，但你明白了）。

### 命名实体识别

现在我们将考虑一个 *命名实体识别* 任务的示例，这涉及从文本中提取相关信息，例如通过将单词分类为组织、地缘政治实体或数字。

要查看这个示例，我们使用了与之前类似的代码示例，但是我们改变了句子和最后一行的一个单词（将简单的 *dep* 替换为 *ent*，如 *dependency* 替换为 *entity*），如图 3.7 所示。![](img/604345_1_En_3_Fig7_HTML.png)

一段代码加载了 SpaCy 的英文模型，处理了给定的文本，然后使用实体样式可视化了命名实体。

图 3.7

用 SpaCy 库可视化命名实体识别的代码片段

在我们的句子中已经识别出了不同的命名实体，如图 3.8 所示：2023 年被识别为日期，欧盟被识别为组织（ORG 代表公司、机构、机构），美国被识别为地缘政治实体（GPE），*数千*被分类为数字（CARDINAL）。还有一些更多的类别（比如 MONEY 代表货币值），但这些是最常见的。在我们的文本中自动识别这样的信息可以作为更高级文本处理的入口点。我们注意到，在这种情况下，欧盟可能被视为地缘政治实体而不是组织。这提醒我们这样的模型可能在所有情况下都不完美地工作。![](img/604345_1_En_3_Fig8_HTML.png)

一段文本中提到，在 2023 年，欧盟和美国等国家和地区吃掉了数千个草莓。

图 3.8

使用 SpaCy 库进行命名实体识别示例的结果

## 机器学习内部

在上一章中，我们看了机器学习的基本过程。我们当时没有完全回答的有趣问题是学习实际上是如何发生的。^(2) 我们将首先定义经典机器学习，并看看这与深度学习和神经网络有何不同。听起来困难吗？别担心，本部分所需的所有数学背景将逐步介绍。所以，让我们一步一步地进行。

在这一部分，我们将集中讨论监督机器学习。我们以情感分析任务为例。这意味着我们考虑一组积极的文本，即表达作者积极态度的文本，以及另一组消极的文本。正如你可能记得的那样，*监督* 意味着每个文本片段都标有关于它是积极还是消极的信息。这是一个分类任务，因为我们希望训练一个能够将新的、未见过的文本分类为*积极*或*消极*的模型。图 3.9 显示了我们的机器学习分类器的设置，就像我们之前在其他示例中看到的那样。![](img/604345_1_En_3_Fig9_HTML.png)

一幅插图代表了一个简单的二元分类任务的训练数据场景，其中数据样本被标记为积极或消极。

图 3.9

情感分析：使用监督机器学习将文本分类为积极或消极

现在，这里有趣的事情是在训练阶段发生了什么。模型是如何创建的，然后能够对未见过的文本进行分类的呢？

### 预处理训练数据

在图 3.9 中显示的训练数据可以以表格形式结构化，第一行包含文本样本，第二行包含标签，如表 3.1 所示。表 3.1

训练数据可以以表格形式结构化，其中一列是文本，另一列是标签

| 文本 | 标签 |
| --- | --- |
| 哇，这是一个多么酷的��站！ | 正面 |
| 哦，我真的觉得这很糟糕。 | 负面 |
| … |   |

在我们开始训练之前，需要进行文本*预处理*。这意味着我们必须清理我们考虑作为输入的文本（即训练数据）。在我们进行情感分析的情况下，这些是包含正面或负面文本的文本样本。我们希望排除不相关的信息。在这个阶段需要做什么取决于我们有什么样的文本。例如，一个典型的操作是删除额外信息，比如链接或将所有字母转换为小写并删除标点符号：

+   “哇，这真的是一个很酷的网站！[`​www.​springernature.​com`](http://www.springernature.com)”

在这个阶段之后，可能会被调整为以下形式：

+   “哇，这真是一个很酷的网站”

我们还将句子拆分为可以单独处理的单词（所谓的标记化）：

+   “哇，这真是一个很酷的网站”

变成：

+   [“哇”,”这”,”是”,”真的”,”一个”,”酷”,”网站”]

在某些情况下，我们可能会进一步删除被认为对训练无关紧要的单词，即所谓的停用词。Lane 等人（2019，第 51 页）将停用词定义为“任何语言中频繁出现但携带较少实质信息的常见词语。”常见的停用词包括*the*、*a*或*on*。

有时，使用*词干提取*或*词形还原*也是有用的。这两种方法的目标相同：通过这些方法修改单词，使它们以一种形式出现，可以确定两个单词是否属于同一个词，即它们是不同形式的同一个词。词干提取是一种通过截取单词的部分来识别词干的技术。例如，*houses*的词干是*house*，*runs*的词干是*run*。然而，这种方法存在一些局限性，通常过于简化，我们可能想要区分的单词被分配到相同的词干。例如，单词*meetings*被改为*meet*，但也许*meeting*会是更好的词干（Hagiwara 2021）。此外，这种方法通常无法处理不规则的单词，比如*caught*，我们可能更希望得到*catch*而不是词干。另一方面，通过词形还原，与使用词干不同，识别单词的原始形式。这可能看起来与词干提取类似，但实际上有所不同：这里不仅仅是截取单词的部分，而是考虑语言的结构，例如对于动词，是在变位前的基本形式。对于我们的例子*meetings*和*caught*，词形还原后的形式分别为*meeting*和*catch*。

### 特征选择

我们将*特征*定义为数据的特征或可测量属性。机器学习中一个重要且常常棘手的部分是*特征选择*，决定哪些是适当的特征以及它们对于训练在其他未见数据上表现最佳的分类器有多重要。在人类决策中，根据上下文，某些方面比其他方面更相关。在对患者进行诊断时，血压或症状描述可能比患者穿的 T 恤的颜色更相关。因此，血压可能是机器学习训练的一个好特征，而 T 恤的颜色可能不是最佳选择。

在文本处理的情况下，特征选择可以是文本中的所有单词。在其他情况下，一个好的特征选择可能是考虑最常出现的单词，或者仅考虑形容词，具体取决于用例。假设我们想要根据*词频*进行特征选择。因此，我们需要找出在我们的训练数据中出现最频繁的单词。更具体地说，我们想找出对正面文本和负面文本都*典型*的单词。一些在一般情况下经常出现的单词可能会出现在两组中的前列，例如，像*the*或*a*这样的单词（如果我们之前已经去除了停用词，这可能不是一个大问题）。我们可以通过删除同时出现在正面和负面组中的任何单词来解决这个问题。这样做的直觉是这样的单词对于区分这两组没有任何用处。假设为了这个例子，我们取每组中出现频率最高的三个单词，最终得到以下结果：^(3)

+   积极 = [“好”, “快乐”, “棒极了”]

+   负面 = [“丑陋”, “可怕”, “糟糕”]

### 从文字到数字

正如前面提到的，计算机在处理数字方面比处理人类语言更擅长。那么，对于那些单词该怎么办？

在深入探讨这个问题之前，让我们先看看数学中*向量*的概念。一般来说，我们每天都在使用数字。向量包含的信息比单个数字更多。它们由多个数字的列表组成，其长度被称为向量的维度。因此，例如，向量(0, −2, 5.1)是一个三维向量。对于给定维度*n*，所有可能的 n 维向量的集合是所谓的向量空间的一个示例。向量通常被描绘为箭头，具有相同长度、相同方向且平行的箭头被视为相同的向量。

在二维向量空间中，向量有两个坐标来指定它们的位置。第一个坐标表示它们相对于 x 轴的位置，第二个坐标表示它们相对于 y 轴的位置，如图 3.10 所示。对于三维空间，我们可以想象一个具有三个坐标的立方体。从四维开始（向量空间可以具有非常高的维度），想象起来会变得困难，但工作方式类似：在一个具有 20 维度的向量空间中，生活在该空间中的向量有 20 个数字组成它们的坐标。![](img/604345_1_En_3_Fig10_HTML.png)

一个插图描述了在二维中 y 轴与 x 轴的向量 V 向量，其中 V 向量=矩阵 2, 1。

图 3.10

一个具有两个维度的向量的示例

我们还需要知道，在这样的向量之间，可以执行数学运算，如加法或减法，类似于数字。

我们需要将我们的单词转换为向量，以便机器学习分类器可以处理它们。每个单词将被转换为一个单一的向量。这样做的最简单方法是使用*one-hot 向量*。每个向量的维度等于我们词汇表中可能的单词数。在实际情况下，这些可能是数十万个可能的单词，但让我们保持简单，假设我们的语言具有以下由四个单词组成的词汇表：

+   [“awesome”, “bad”, “good”, “horrible”]

向量空间（维度）将是 4。因此，每个向量有四个条目（比图 3.10 中的示例向量多两个）。然后，每个单词将通过具有*0*的向量表示，并且仅在词汇表中的位置上有*1*。我们所说的位置是什么意思？在上面的词汇表中，单词*awesome*位于位置 1，*bad*位于位置 2，依此类推。^(4) 因此，*bad*的向量表示将在位置 2 处有一个 1，其他位置为 0：^(5)

+   vector[“bad”] = [0,1,0,0]

就是这样。我们已经创建了我们的单词的向量表示。显然，这种简单的方法存在一些限制。在四维空间中，一些零是可以原谅的，但如果我们有数十万维度的向量（以涵盖英语语言的全部词汇），那么就会有许多需要存储的零。我们将在后面的部分看到更高级的方法来使用更少的维度对单词进行向量化。

### 机器学习分类器

在*向量化*这一步之后，我们得到了一组典型于正面文本的向量列表，以及另一组典型于负面文本的向量列表。因此，我们准备训练我们的机器学习分类器。首先要做的是选择我们想要使用的分类器类型。存在不同的数学模型，我们根据数据和需求选择其中之一。通常情况下，最佳表现的分类器很难提前预测。因此，有时尝试不同的分类器以找到适合给定数据集或问题的正确分类器是非常有益的。

一种这样的机器学习分类器被称为*逻辑回归分类器*。这种类型的分类器是*判别式分类器*（Jurafsky 和 Martin 2023）的一种，旨在学会根据统计差异区分两组。评估不同特征在区分两组方面的重要性。这是通过分配权重来实现的，这意味着为每个特征分配一个值，衡量其对结果的重要性。这意味着它试图弄清楚我们提供的输入中哪些词是最相关的。这是通过逐个查看我们提供的所有特征并根据需要调整*权重*来实现的。

为了更好地理解我们所说的权重，让我们考虑以下例子。我们考虑一个长满美味草莓的草莓植物，大小不一。有一些水果，使得植物倾向于向右侧弯曲。我们想研究是什么导致草莓植物弯曲。可能，巨大的草莓对此的贡献比微小的草莓更大。因此，巨大的草莓比微小的草莓有更大的权重，如图 3.11 所示。![](img/604345_1_En_3_Fig11_HTML.png)

一幅插图描绘了一个带有标有巨大草莓导致植物弯曲和微小草莓影响较小的盆栽。

图 3.11

巨大的草莓导致植物弯曲；微小的草莓影响较小

这类似于我们特征的权重。在学习过程的开始，所有单词可能具有相等的权重。随着时间的推移，模型会学习到单词*good*表示积极，因此在积极类别中给予单词*good*更多的权重。因此，在训练之后，当单词*good*出现时，模型的决策将更多地向积极方向*倾斜*，类似于具有较大权重的草莓对草莓植物向右弯曲产生重大影响。

### 损失函数

让我们回到我们在早前章节中看到的将草莓切成均匀片的例子。随着每次迭代，我们都会变得更好一点。但是我们如何衡量我们的改进呢？假设期望的片大小是 4 毫米。我们希望我们切的每一片都尽可能接近这个参考值，因此我们希望能够熟练地将草莓切成这个特定大小的片。每次我们切一片，我们可以将其大小与期望的片大小进行比较。我们大了 2 毫米？让我们试着切小一点。这个过程在图 3.12 中展示。我们改进我们的方法，以尽可能接近期望的草莓片大小。在数学术语中，我们希望减少实际片大小和期望片大小之间的差异。在这种情况下，我们谈论的是片大小，但在机器学习的一般术语中，我们谈论的是*损失函数*。学习过程的目标是最小化这个损失函数。在图 3.12 中，第 1 轮开始时的损失为 2 毫米，在第 N 轮为 0 毫米。损失越接近零，机器学习模型的性能就越好。在学习过程中，机器学习模型的值（例如权重）会被调整以最小化这个损失。![](img/604345_1_En_3_Fig12_HTML.png)

一幅插图代表了在多轮中进行的训练过程，最终通过减小实际片大小和期望片大小之间的差异来最小化损失函数，最终实现实际片大小为 4 毫米时的 0 毫米损失。

图 3.12

为了训练模型，我们需要最小化损失。我们越接近期望的片大小，损失就越低。

关于损失函数的一个重要观点是，它明确定义了我们*训练模型*要做什么。整个训练的目的是修改模型以最小化损失。这就是我们所说的*学习*。

现在让我们从切草莓片回到我们先前的情感分析任务。我们想要将文本分类为积极或消极。根据我们选择训练的分类器（例如，之前提到的逻辑回归），损失函数被定义。在最简单的情况下，它是预测值和期望值（训练数据的标签）之间的差异。在其他情况下，可以使用稍微复杂一些的函数，但通常以某种方式量化预测值和期望值之间的差异。

### 训练和测试数据

现在我们有一个模型，可以将文本分类为正面或负面两个类别之一。但是我们如何知道分类器实际表现如何？为了更好地理解这一点，我们引入了*测试数据*的概念。假设我们有 100 个文本样本标记为正面和 100 个标记为负面。最好的做法是只使用每个类别的 80 个样本进行训练过程。将 20 个正面和 20 个负面的例子保留下来，以便稍后在这些数据上*验证*我们的机器学习分类器。这意味着我们挑战分类器，让其预测这些片段是正面还是负面，而不包括它们在训练过程中。这就是我们所说的*未见数据*。由于我们知道这些样本的正确分类，我们可以利用这些知识来验证分类器的正确性。如果它能够正确分类 40 个中的 38 个（20 个正面和 20 个负面），我们可能对结果感到满意。如果只有 40 个中的 10 个被正确分类，我们可能需要重新考虑之前采取的步骤，或者尝试其他特征或调整我们的预处理步骤。^(6)请记住，这只是一个预测，几乎不可能构建一个在所有情况下都正确的系统。

图 3.13 概述了我们一直在进行的文本预处理、特征选择以及最终训练和验证分类器的不同步骤。通常需要这些步骤来准备机器学习训练。![](img/604345_1_En_3_Fig13_HTML.png)

流程图包括预处理、特征选择、从词到数字、机器学习分类器、损失函数以及训练和测试数据。

图 3.13

准备机器学习训练所需的不同步骤概述

## 从机器学习到深度学习

在上一节中，我们看到了机器学习模型是如何训练的：特征是预先定义的，系统通过最小化损失函数来学习。让我们称之为*经典机器学习*。

### 线性模型的局限性

此外，逻辑回归和一些其他经典机器学习是所谓*线性模型*的例子。对于分类的情况，这意味着它们只能分离可以通过直线（或高维空间中的超平面）分离的类别。在其他情况下，这是不可能的，数据需要非线性处理才能产生良好的结果。这两个例子在图 3.14 中有所说明。![](img/604345_1_En_3_Fig14_HTML.png)

2 个比较图表显示数据可以用线性分类器分离和数据无法用线性分类器分离。

图 3.14

在某些情况下，数据可以用线性分类器分离。在其他情况下，这是不可能的。

在我们知道要寻找什么并且能够识别和提取对于区分类别最有用的特征的情况下，经典机器学习分类器如逻辑回归可能会表现良好。一些其他经典机器学习分类器甚至允许处理非线性可分数据。然而，在许多情况下，可能不容易识别相关特征。例如，回想一下我们之前看到的想要区分草莓和覆盆子图片的分类器的情况。对于人眼来说，很容易看出图片上是覆盆子还是草莓，但很难形式化我们所看到的并定义机器学习算法能理解的具体特征。这里的潜在特征可能是，例如，浆果是否在外面有种子。然而，明确指导计算机如何从像素值转换为这些特征将会非常困难。将相同的推理应用于文本，人类可以看出文字之间的含义，并根据电子邮件消息了解某人的情绪。但是，如果要求你根据帮助你注意到这一点的特征给出清晰明确的指示，将很难用言语表达。你可能会说这与*语气*有关，但如何从文本中提取*语气*特征以输入到机器学习算法中呢？在这些任务中，很难从数据中识别/提取相关特征的地方，*深度学习*的力量就派上用场了。深度学习是机器学习的一个子集，涉及一种特定技术：*神经网络*。

### 神经网络

Masato Hagiwara 将神经网络描述为：

> 一个将向量转换为另一个向量的通用数学模型。就是这样。与您在流行媒体中读到和听到的不同，它的本质很简单。（Hagiwara 2021，第 37 页）

我喜欢这个定义，尽管它有点揭示了魔法，因为它把事情说到了点上：最终，这只是数学。

与之前使用逻辑回归的示例一样，我们需要进行预处理，最终将我们的单词转换为向量。暂时，让我们假设我们已经有了文本中每个单词的向量形式的数值表示。

像之前看到的经典机器学习一样，神经网络也依赖于反馈循环来改进预测。非常简化地说，我们可以将神经网络看作是图 3.15 中所示结构的一种。![](img/604345_1_En_3_Fig15_HTML.png)

流程图示输入、处理、估计，基于结果，调整处理以改进估计。

图 3.15

与我们之前看到的经典机器学习类似，神经网络也有一个反馈机制来改进预测

一个神经网络可以被看作是一个更复杂的结构，在最后一部分包括一个经典的机器学习模型，类似于之前描述的模型，如图 3.16 所示。同样，在这里，学习是通过最小化损失来进行的。神经网络可以由多个层组成。第一层接收输入数据并输出一组新的特征，作为下一层的输入。在所有层中重复此过程后，最终一层提取出经典模型可以处理的特征。在训练过程中，每一层提取的特征都可以进行修改，直到找到*正确的处理步骤序列*。当神经网络涉及多个层时，我们称之为*深度学习*。根据神经网络的确切设置，更复杂的数学运算是可能的；此外，特征提取可以自动进行，通常比经典机器学习更高效。![](img/604345_1_En_3_Fig16_HTML.png)

流程图表示了一个神经网络架构，最后一层的功能类似于传统的机器学习分类器，旨在最小化损失函数。

图 3.16

神经网络一般类似于经典机器学习，但允许更复杂的数学运算

现在我们将深入探讨神经网络的架构，但仍然保持在一个相当高的层次。^(7)

### 神经元：受人脑启发

神经网络在某种程度上受到人脑中的神经元的启发。然而，我们必须明确，这样的系统不能直接与人脑相比较。它们非常简化，人脑的许多方面仍然不为人所理解。考虑以下类比（Rashid 2017）：人类大约有 1000 亿个神经元，而其他较小的动物或昆虫只有几十万个神经元。尽管我们能够建模比这些动物的大脑更复杂得多的计算机系统，但这些动物可以完成一些对计算机来说难以解决的有用任务。值得一提的是，最新的语言模型的神经元数量与人类大脑相同数量级。因此，仅仅根据神经元数量来比较人类或动物智能和机器智能是困难的。人类或动物智能似乎还有更多的东西。

回到我们的技术系统，神经网络是一组所谓的神经元相互连接在一起。信号从神经网络的一侧输入，经过不同的神经元，最终产生结果。

一个层通常由多个神经元组成。如图 3.17 所示，单个神经元具有多个输入值和分配给连接的权重。在神经元内部，输入被处理，权重指示了每个输入对于神经元输出的重要性。当思考权重的含义时，请记住我们之前看到的因为有一个巨大的草莓而向右弯曲的草莓植物。![](img/604345_1_En_3_Fig17_HTML.png)

一个流程插图展示了多个权重输入，接着是神经元和输出。

图 3.17

神经网络中的权重在训练阶段逐渐调整，以减少损失并提高系统性能

好处在于我们不必指定这些权重。这些权重在训练阶段逐渐调整，以减少损失并改善整个系统。

一个神经网络由几个神经元组成，组织在不同的层中。

在图 3.18 的示例中，我们有三层，每层有两个神经元。来自第一层神经元的输出是第二层两个神经元的输入。第一层称为*输入层*，第二层是*隐藏层*（可能不止一个），第三层是*输出层*。在每个神经元中，基于输入值和权重进行计算，生成一个输出。这种数学运算依赖于向量和特别是*矩阵*数学，因此重要的是输入和输出是向量而不是人类词语。即使对于高维神经网络，计算机也能够非常有效地执行这些操作。![](img/604345_1_En_3_Fig18_HTML.png)

一个插图展示了一个神经网络，其中包含六个输入层中的神经元实例，接着是三个隐藏层，标记为层 1、层 2 和层 3，最终导致一个输出层。

图 3.18

一个由三层组成的神经网络示例

### 从错误中学习

那么我们如何从错误中学习并提高神经网络的性能呢？就像之前一样，我们希望调整权重以最小化损失。如果我们只调整最后一层的权重，就像我们为逻辑回归的例子所做的那样（只有一层），这只能部分解决问题。最后一层的输出取决于它从前一层接收到的输入。如果我们不调整那里的权重，那么我们又陷入了低复杂度线性模型的情况，因此我们还需要弄清楚如何调整倒数第二层的权重，以此类推。因此，我们必须调整神经网络中每一层的权重。这个过程称为*反向传播*。图 3.19 展示了我们之前讨论过的神经网络中反向传播的过程。![](img/604345_1_En_3_Fig19_HTML.png)

一幅插图显示了一个神经网络处理术语神经元在 3 层，接着是一个输入和输出，后面可能是反向传播，表明一个训练过程或学习机制，

图 3.19

神经网络的所有层中都会调整权重。这个过程被称为反向传播。

### 使用神经网络进行情感分析

现在让我们回到情感分类的例子，并在神经网络中描述这个问题。在情感分析中，我们希望将文本分类为消极或积极。对于输入，我们将再次需要我们单词的向量表示。如图 3.20 所示，最后一层具有特殊的格式。由于我们的目标是做出二元决策，最后一个神经元被调整为产生这两个输出选项之一（消极或积极）。

一个流程图表示了一个过程，我们的文本被转换为向量，然后通过多个层处理，最终在第 3 层产生一个被分类为积极或消极的输出。

图 3.20

在文本情感分析的背景下，我们希望提供文本作为输入，并获得关于它们是积极还是消极的预测。

与线性分类器（比如我们之前看到的逻辑回归）相比，神经网络通常提供重要的优势。例如，更复杂的数学运算可能导致文本分类的更好性能。有时，在文本中表示积极或消极情感可能会很棘手。讽刺可能是消极情感的一个强烈指标。然而，就像在逻辑回归中给单词分配权重一样，永远无法捕捉到这个复杂的概念。另一方面，可以想象文本中存在或不存在的单词组合的某种复杂函数可能导致文本中讽刺的良好度量。如果是这样，那么神经网络可能能够做到这一点。

我们现在对神经网络的工作原理有了基本的了解，并准备进一步深入。在下一节中，我们将更仔细地研究词嵌入，这些向量编码了单词的含义。

## 深入研究词嵌入

让我们回到之前遇到的问题：机器学习方法需要数值表示（以向量形式）而无法处理人类可读的文本。为了使我们的文本处理和生成能够利用机器学习方法的优势，提高不同任务的性能，我们需要将单词转换为数学向量。

那么我们如何实现这一点呢？一个解决方案被称为*词嵌入*（有时也被称为*词向量*）。

人类语言中的每个单词，比如英语单词*草莓*或*覆盆子*，都有一个数值表示，一个特定的词嵌入，如图 3.21 所示。暂时假设它们如下：

+   “草莓” = [1,6]

+   “覆盆子” = [1,7]

+   “大象” = [2,1]

![](img/604345_1_En_3_Fig21_HTML.png)

一个插图。文字写着草莓，编号为 12、11、3 和 4。

图 3.21

单词“草莓”从人类语言映射到一个向量

举例来说，我们假设我们的向量存在于一个二维空间中（因此，上面括号中有两个数字）。这意味着我们可以轻松地在纸张上绘制它们，通过在二维坐标系上使用点。

### 单词之间的关系

我们从图 3.22 中注意到，单词*草莓*和*覆盆子*的向量彼此之间比与单词*大象*的向量更接近。如果两个单词具有相似的含义，它们的词嵌入将在向量空间中更接近。由于草莓和覆盆子都是浆果，而大象是动物，它们的词嵌入更接近。这种特性使我们能够使用数学运算处理单词的含义。例如，考虑以下谜题（Bolukbasi 等人 2016）：

> 男人对国王，女人对 X

![](img/604345_1_En_3_Fig22_HTML.png)

一个图示呈现了一系列单词，包括草莓、覆盆子和大象在 x 和 y 轴上以不同角度。

图 3.22

含义相似的单词具有更接近的词向量

这可以通过使用减法和加法来解决。虽然我们经常使用数字进行加法或减法（我们都熟悉类似 1 + 2 - 1 = 2 的操作），但也可以用向量进行相同的操作，从而解决谜题^(11)：

+   向量(“女王”) = 向量(“国王”) - 向量(“男人”) + 向量(“女人”)

通过从单词*man*和单词*woman*的向量之和中减去单词*king*的向量，我们可以得到结果单词*queen*。是不是很迷人？

同样，类比问题可以解决，甚至跨越不同领域，例如从科学到音乐（Lane 等人 2019）：

> 谁是音乐界的玛丽·居里？

这将被翻译为向量数学如下，类似于上面提到的*king*和*queen*的例子：

+   向量(“解决方案”) = 向量(“玛丽·居里”) - 向量(“科学”) + 向量(“音乐”)

自然地，我们非常兴奋，想要探索这些单词嵌入中更多的关系，并了解它们如何作为机器学习训练的输入。但这些单词嵌入实际上是从哪里来的呢？

### 创建单词嵌入

我们已经看到的词嵌入示例与实际词嵌入之间的主要区别是维度。^(12) 虽然我们在示例中使用了两个维度以便能够直观地查看词嵌入及其彼此之间的关系，但通常它们有大约 300 个维度。^(13) 难以想象吧？我也有同感。但到目前为止我们看到的原理是相同的：基于向量的属性并使用数学运算（幸运的是，它们适用于不同维度的向量），我们仍然能够获得相同的见解。你可能会问，如果我们只能使用 2 个维度，为什么要使用 300 个维度？这背后的直觉是，我们有更多的维度，我们可以考虑单词相似的更多方面。更高维度的向量有助于捕获单词的不同属性，从而提高词嵌入之间关系的质量。

因此，让我们看看意义是如何被编码到这些词嵌入中的，以及我们如何获得人类语言中特定单词的词嵌入。我们希望有一个包含所有单词的字典，将每个单词翻译成相应的向量。类似于语言词典将单词从英语翻译成，比如西班牙语，在我们的情况下，字典将单词从英语翻译成词嵌入。要创建这样一个字典，可以使用机器学习，或者更准确地说，可以使用神经网络。

等等，什么？我们使用神经网络生成词嵌入，然后使用它们来编码我们要馈送给神经网络的单词？让我详细解释一下。

我们想要将文本的单词转换为向量，以便在机器学习任务中处理它们，例如，在情感分析的上下文中训练一个二元分类任务的模型。这是一个监督机器学习任务，因为我们训练数据中的文本被标记为正面或负面。词嵌入字典是在实际训练过程之前独立创建的。那些词嵌入可以被训练一次，然后在不同的任务中重复使用。就像你的语言词典在书架上一样，每当你需要它时，例如写信或从外语文本中翻译一个单词，你只需拿起它并查找你需要的单词。设置如图 3.23 所示。![](img/604345_1_En_3_Fig23_HTML.png)

将训练数据中的单词转换为向量表示的过程，可能用于情感分析。讨论了词嵌入的利用、监督二元情感分析以及通过对文本语料库进行无监督训练生成词嵌入的过程。

图 3.23

训练情感分析分类器的示例流程：文本训练数据使用已经单独训练过的现有词嵌入进行向量化。

### Word2Vec 词嵌入

单词嵌入的词典是使用无监督机器学习方法训练的。在本节中，我们将探讨 2013 年提出的 word2vec 方法（Mikolov 等人 2013），该方法为一系列自然语言处理任务提供了重大改进。这标志着自然语言处理与单词嵌入的新时代的开始，后来更进一步地改进了基于 transformer 的模型，我们将在后面的章节中探讨。

单词嵌入背后的理念可以用语言学家 J.R. Firth 的以下引用来概括：

> 你可以通过一个词的周围词了解它（Firth 1962，第 11 页）

例如，单词*草莓*和单词*覆盆子*可能与*浆果*、*领域*、*红色*、*美味*等词一起出现。这些共同出现的词使得这两个词在意义上更有可能相似。因此，这两个词应该有相似的向量。

训练单词嵌入的机器学习方法是*无监督*的。这意味着数据不需要被标记。在这种情况下，这是一个重大优势，因为在自然语言的情况下标记会很复杂。有许多关系和背景知识，对于我们人类来说，这些知识是多年学习而来的，很难在标记数据集中表达出来（在之前的示例中使用监督学习时会需要）。例如，草莓是浆果，浆果是植物，植物被人类和动物吃（等等）。

然而，仔细观察后，文本中的单词确实有一种标签。与监督学习场景相比的区别在于，标签是隐含的，不需要在训练之前添加。这些算法不是学习每个单词的*含义*，而是学习与所提到的单词一起出现的常见单词。对于这个任务，文本中每个单词的标签就是紧挨着该单词出现的单词。^(14)

为了让你对这意思有所直觉，让我们看一下图 3.24 中显示的例子。单词*领域*和*与*刚好在*草莓*之前，而*和*和*覆盆子*则紧随其后。在 skip-gram 方法中（word2vec 单词嵌入训练中的一种方法^(15 )），我们会尝试预测每个单词周围的单词。在这种情况下，我们会使用例如围绕*草莓*的单词进行训练。由于我们知道正确答案，我们可以利用这些信息来改进学习并减少错误。![](img/604345_1_En_3_Fig24_HTML.png)

文本中提到有一个有草莓和覆盆子的领域。

图 3.24

单词“草莓”以及它之前和之后出现的两个单词

要回到神经网络，单词*草莓*将是我们神经网络的输入。作为输出，我们希望神经网络预测周围的单词。为此，神经网络的结构如图 3.25 所示。我们认识到我们在前一节中看到的架构：有神经元和层。特别是，输入层由单词*草莓*的独热向量组成。使用独热向量，所有字段都是 0，只有词汇表中特定单词位置的字段设置为 1。这个输入层中的条目数对应于我们在词汇表中有的单词数。在中间，我们有一个隐藏层。隐藏层有特定数量的神经元。神经元的数量对应于生成的单词嵌入应该具有的维度数。例如，如果我们想要生成我们之前看到的两维样本单词嵌入，那么我们在这一层中会有两个神经元。在一个更现实的情况下，我们想要生成 300 维的单词嵌入，那么我们在这里会有 300 个神经元。最后，输出层输出一个值，该值对应于词汇表中每个单词的概率。在图 3.25 中，我们看到训练对“草莓+和”这对的示例（许多训练步骤中的一个）。

一个神经网络通过一系列层处理单词草莓，包括输入层、隐藏层和输出层。网络根据训练数据学到的关联为序列中各种单词分配潜在的下一个单词的概率。

图 3.25

单词嵌入训练的示例（基于 Lane 等人(2019，第 193 页)）

谈到概率时，我们使用 0 到 1 之间的值。有些人可能更熟悉使用百分比，但从 0 到 1 的概率值转换为百分比值很简单。例如，概率为 0.5 是 50%，0.2 是 20%。基于此，以 98.8%的概率，单词*和*很可能跟在单词*草莓*后面。

在训练过程中，“草莓+和”这对使*和*的分数上升，而训练示例“草莓+带”会使*带*的分数上升。训练是在迭代中进行的，取决于我们考虑的周围单词数。例如，在图 3.24 中所示的情况下，我们有四个周围单词，因此在到达下一个单词之前需要进行四次迭代。因此，我们可能不会看到*和*和*草莓*在一般情况下非常相关；然而，在这个训练步骤中这是正确的答案。

这个过程重复了很多次。首先，我们遍历句子中的所有单词，对每个周围的单词进行一次训练步骤。然后，我们不仅对几个句子这样做，而是对包括数百万页文本的大型文本语料库这样做。

令人惊讶的是，一旦训练完成，输出层可以被忽略。我们实际上要找的是隐藏层。它为每个单词生成一个向量（单词嵌入），编码了它与词汇表中其他单词在语义上的关系，它们经常一起出现。语义相似的单词将具有相似的向量。这意味着我们可以从隐藏层中提取我们需要用来将英语单词映射到单词嵌入的单词向量，一旦训练结束。^(16)

这只是生成单词嵌入的一个过程。除了这里介绍的 skip-gram 方法，Mikolov 等人（2013）还提出了一种稍有不同的替代方法，该方法反转了任务，根据一组周围的单词预测一个单词。其他方法包括 GloVe（Pennington 等人 2014）嵌入或 fasttext（Mikolov 等人 2018）。

### 现成的词嵌入

如我们在图 3.23 中看到的，单词嵌入可以创建一次，然后重复使用。这是幸运的，因为训练单词嵌入需要大量资源。一方面，您需要强大的硬件来执行计算。另一方面，您需要耐心，因为这样的训练可能需要几个小时或几天（甚至在我们将在后面章节中看到的大型语言模型的情况下可能需要更长时间）。此外，训练需要大量文本数据（所谓的语料库）。幸运的是，单词嵌入通常是公开可用的，并且可以被文本处理应用程序下载并直接使用。那么，何时值得生成自己的单词嵌入？由于单词嵌入是与语言相关的，您可能需要为特定语言训练自己的单词嵌入。然而，fasttext（Mikolov 等人 2018）提供了 157 种语言的单词嵌入（Grave 等人 2018），所以现在很少出现这种情况。在其他情况下，当您需要特定领域的词汇时，您可能需要训练自己的嵌入。现成的词嵌入（如 word2vec 或 fasttext）依赖于涵盖广泛主题的文本，试图对“通用”语言进行建模。但假设您只处理法律文件。这些文件可能包含许多领域特定的词汇，您可能特别感兴趣的是在单词嵌入中编码了这些词之间的关系。

### 使用库

类似地，为数据工程师提供了实现机器学习或数据准备任务常见方法的*库*。这意味着数据工程师可以在软件中包含预定义组件，而不是每次都重新编写编程语言中的所有指令。通过使用库，数据工程师可以提供输入数据集，进行特定设置，然后，例如，重复使用他人事先实现的逻辑回归算法。这简化了这些技术的应用视角，并将所需的知识转移到了了解可用的库、它们的用途以及如何应用它们。图 3.26 说明了数据工程师用于模型训练的典型组件！[](../images/604345_1_En_3_Chapter/604345_1_En_3_Fig26_HTML.png)

一幅插图展示了词嵌入的概念，其中诸如 good 和 morning 这样的词被表示为数值向量，如 12、6、8 和 1、4、190。通过库提供的公开可用的词嵌入，具有在数据工程和建模任务中潜在应用的可能性。

图 3.26

数据工程师可以依赖现有库和公开可用的词嵌入

到目前为止，我们所见过的词嵌入将一个词编码为一个数学向量。这些编码可以在不同应用中共享和重复使用。一旦训练完成，更复杂的语言模型也可以与社区共享。

### 语言模型的高层视图

展望即将遇到的这些语言模型，我想在这一点上给你一个高层次的概述，我们正在处理什么。让我们暂时将*语言模型*定义为将概率与文本片段联系起来的统计模型。通常，它们被，以非常简化的方式陈述，用于预测（部分）句子中的下一个词，旨在产生最佳的类人文本。让我们考虑以下例子：

+   *安娜去田野收集……*

有了语言模型，我们可以预测这个句子中接下来可能是什么词。可能基于语言模型在训练数据中包含的其他文本，像草莓、胡萝卜或番茄这样的词比猫或狗更有可能成为下一个词。训练是通过隐藏原始文本中的一些词然后预测它们来进行的。这在某种程度上类似于我们在本章中看到的词嵌入训练算法，因此我们已经准备好迈向大型语言模型了！

## 摘要

在本章中，我们了解了自然语言处理的应用和方法。我们看到了如何将不同复杂程度的解决方案应用于同一问题。挑战在于确定给定任务最合适的方法。

我们看到了神经网络如何有益于非线性可分数据，通过允许更复杂的数学运算和自动化特征提取。经典机器学习和深度学习都依赖于损失函数，并调整模型权重以最小化损失并改进模型的预测。

在自然语言处理的背景下，向量化对于将人类语言映射到数学向量非常重要，这样计算机可以更轻松地处理。我们已经看到了不同的方法，用于训练词嵌入以及语义上相似的单词对应到更接近的向量。

最后，我们看到了公开可用的词嵌入和库是如何整合到数据工程师的工作流程中的。


© 作者，独家许可给 Springer Nature Switzerland AG 2023M. Kurpicz-Briki[更多内容请查看](https://doi.org/10.1007/978-3-031-37690-0_4)

# 4. 聊天机器人是否有情感？

Mascha Kurpicz-Briki^(1  )(1)应用机器智能，伯尔尼应用科学大学，比尔/比尔，瑞士

## 概述

创造*智能*机器的想法一直是人类的迷恋，导致了大量涉及这类情景的书籍和电影的出现。随着先进语言模型的崛起，我们再次思考什么是*智能*。在本章中，我们将讨论这个问题，并看看它是如何受人类感知影响的。在本章的第二部分，我们将深入研究技术技能，并查看最先进的语言模型。最后，我将描述一个典型的聊天机器人架构。

## 机器与情感

### 图灵测试

早在计算机科学的早期，即上世纪 50 年代，就讨论了衡量机器是否具有与人类相同思维能力的挑战。当今经常在讨论这个话题时提到的一个测试被称为*图灵测试*（最初称为*模仿游戏*（图灵 1950））。在这个实验中，一个人与两个对话伙伴（基本上就像现在的聊天机器人对话）进行交互，其中一个是人类，另一个是机器。参与者无法看到或听到这两个对话伙伴。接下来，参与者会询问这两个对话伙伴，试图找出哪个是机器，哪个是人类。图 4.1 描述了这个设置。如果经过一番深入的对话后，人类无法确定哪个是机器，哪个是人类，那么测试假定机器具有与人类相同的思维能力。![](img/604345_1_En_4_Fig1_HTML.png)

人类和机器互动的图示，文本内容为“你好，嗨，谁是人类，谁是机器？”

图 4.1

图灵测试的设置：谁是人类，谁是机器？

当与 ChatGPT 或基于最先进语言模型的类似聊天机器人进行对话时，你可能会有这样的印象，即这个标准可能会被满足。这是否意味着我们已经达到了机器具有与人类相同的思维能力的程度？

图灵测试因其测试人的轻信而不是系统的实际智能而受到批评（Tegmark 2018）。在上述场景中测试的是写作*像*人类的能力，而不是执行人类的思考或理解。我们可以通过更具体地研究我们之前看到的词向量化示例来更深入地探讨这一点。基于数学运算，我们能够根据单词*king, man*和*woman*的词向量获得单词*queen*。这是否意味着系统*理解*了做国王的概念或男人和女人之间的区别？可能不是，它所做的只是对向量应用数学运算，而向量在向量空间中的排序是由训练数据中看到的所有示例决定的。^(1)

### 情感聊天机器人？

那么聊天机器人表达的情感呢？让我们考虑与一款最先进的聊天机器人（基于 GPT-3 模型）的以下对话：

人类：

“你好，你好吗？”

聊天机器人：

“嗨，我过得很好。谢谢你的关心。你呢？今天感觉怎么样？”

人类：

“我还好。因为外面阴天，有点低落。你最近有什么好事吗？”

聊天机器人：

“很抱歉听到你今天有点低落。在天气阴沉的时候感到这样完全可以理解。至于我，我感到感激生活中支持我并让我快乐的人。我也感到满足，因为我有能力识别和处理自己的情绪，这有助于我保持心理和情感平衡。”

不错吧？这个聊天机器人能通过图灵测试吗？也许可以。这个聊天机器人是否*理解*它所产生的文本，就像人类一样理解吗？不，它不会。

这种难以置信的事实根源于人类沟通方式的本质。这在一篇关键的科学论文中被详细指出，该论文于 2021 年发表，警告大型语言模型背后的危险和风险（Bender 等人 2021）。它指出（除了我们稍后将讨论的许多其他有趣观点之外），大型语言模型生成的文本缺乏两个人之间通常沟通模式的不同关键方面，这两个人共享一个共同基础：交际意图、世界模型和读者心理状态模型。这导致了一个问题，即即使沟通的一方（聊天机器人）在其产出中没有更深层含义，人类也会假设*隐含含义*，这创造了我们对语言的独特人类理解的错觉。基本上，我们被聊天机器人欺骗，以模拟像人类一样写作。

机器人、人工智能，尤其是语言模型处理信息并向人类呈现答案或结果的方式可以被视为对人类行为或活动的*模拟*：

> 我们必须意识到*思考*、*计算*、*反应*、*决策*的机器人只是对思考、计算、反应、决策的*模拟*，而不是以人类意义上的真实思维过程。（Nida-Rümelin 和 Weidenfeld2022，p.59）

这一事实在关于类人机器人的一次采访中也被德国哲学家托马斯·梅辛格（Honert 2017）称为*社会幻觉*。他指出，人类有能力想象自己在与有意识的对等体打交道，即使事实并非如此。这不仅适用于看起来像人类的机器人，还可能适用于聊天机器人。在其他情况下，甚至可能发生我们将人类特征赋予更为被动的物体，如汽车。例如，一项研究表明，人们会将个性特征或面部表情赋予汽车（Windhager 等人 2008）。在这项研究中，40 名参与者被展示了来自不同制造商的不同汽车的 3D 计算机模型。参与者被要求说出他们是否将汽车的前部与人类（或动物）的脸部联系起来，识别出对应于嘴巴、眼睛或脸部其他部分的部位。此外，他们还被要求评价汽车在多大程度上看起来快乐、愤怒、惊讶、支配或悲伤等。有趣的是，人们在他们的评价中普遍达成一致，因此作者认为必须存在一些一致的信息被人类感知在汽车前部。参与者因此用生物学术语评估汽车，尽管它们是无生命的结构。

### 神秘的山谷

在机器人和头像的情况下，^(2)过于类人会引起人类的负面情绪。观察表明，那些明显人造且与真实人类有所区别的物体（例如明显与人类不同的类人机器人或填充动物）更容易被接受。*神秘的山谷*假设（最初由日本机器人学教授森正洋（Mori 1970）于 1970 年提出）预测，一个几乎看起来像人类的实体会在人类中引起冷漠和毛骨悚然的感觉。

这种神秘的山谷效应也可以应用于聊天机器人。一项研究（Ciechanowski 等人 2019）观察了两组不同的人类参与者如何与两组聊天机器人互动：第一个聊天机器人是一个非常简单的基于文本的聊天机器人，而第二个则还有一个阅读答案的头像，而不仅仅是在屏幕上显示答案。在结果中，作者得出结论，参与者在处理简单的聊天机器人时体验到的神秘效应和负面情绪要少于与动画聊天机器人交互时。

看起来最先进的语言模型不仅在自然语言处理领域引发了革命，使多项任务更加高效。它们似乎还引发了有关人类与聊天机器人互动的新问题，导致我们将它们优雅而措辞得当的文本解释为情感，或者在与它们互动时让我们感到怪异。是否已经达到了某种语言模型的怪异谷？

你仍在思考过于情绪化的聊天机器人对话，不确定它是否只是一种幻觉？现在让我们回到技术部分，更仔细地看看聊天机器人是如何工作以及它们的答案是如何生成的。

## 进入语言模型的世界

### 整个句子的向量

到目前为止，在查看单词嵌入时，我们考虑了一个单词对应一个向量的情况。我们示例中的每个单词都有一个数值表示，对于玩具示例是两个维度，对于真实世界的单词嵌入则是更大的维度。在某些情况下，可能需要或更好地将几个单词聚合在一起。在进行文本情感分析时，通常我们希望了解整个句子（或文本块）是积极还是消极，而不仅仅是单个单词。当在句子上执行此类操作并使用将整个句子作为输入的机器学习模型时，我们有一个*句子分类任务*。类似任务的其他示例包括垃圾邮件过滤器或识别给定文本是否涉及特定主题。

考虑以下句子，我们希望将其提供给一个机器学习模型，并估计它是积极还是消极:^(3)

+   *安娜吃了一个美味的草莓。*

基于之前讨论的单词嵌入，我们现在可以轻松地将每个单词编码为一个向量:^(4)

+   Vector(“安娜”) = (0, 2)

+   Vector (“吃”) = (1, 3)

+   …

现在我们想要将这些单词嵌入聚合成所谓的句子嵌入，一个代表不仅是单个单词而是整个句子的向量。一个简单的方法是只需取句子中所有单词向量的平均值。基于向量的数学属性，我们可以应用数学运算，就像我们在计算平均数一样：

+   平均值 = [Vector(“安娜”)+Vector(“吃”)+…+Vector(“草莓”)]/5

我们将所有向量相加，然后除以向量的数量（=单词的数量）。结果是一个向量（与单词向量相同维度的向量），代表整个句子：

+   Vector(“安娜吃了一个美味的草莓。”)

这些方法还有助于处理*可变长度的序列*。在处理语言时，单词（可以看作是字符的*序列*）可以更长（如单词*strawberry*）或更短（如单词*the*）。同样，我们可以将句子视为单词的*序列*，每个句子中的单词数量可能不同。因此，我们可以将要提供给机器学习模型的句子视为*可变长度输入*。根据我们想要分类的句子，我们可能有更少或更多的单词要处理。大多数情况下，我们事先无法知道这些信息，因此需要在软件中准备好处理非常短的句子以及长句子。使用平均值是一个简单而有效的解决方案：无论句子中有多少单词，我们最终都会得到一个单一向量（固定维度）。

尽管这种解决方案暂时解决了我们的问题，但它也带来了一些主要限制。例如，当使用这种方法时，不考虑单词在句子中出现的顺序。这导致以下句子与我们上面的示例具有相同的句子向量：

+   *一个美味的草莓吃安娜。*

这不完全相同，对吧？在某些情况下，这种差异对于分类任务可能非常重要，因此，我们需要更先进的方法，可以考虑句子中单词的顺序。此外，句子越长，平均向量包含的信息就越少。

### 记住一些单词

为了克服这一限制，我们需要一种不同的方法，允许我们按照从左到右逐步阅读单词的顺序，就像人类读者所做的那样。让我们通过一个示例来考虑这个问题^(5)，如图 4.2 所示。![](img/604345_1_En_4_Fig2_HTML.png)

人类和机器交互的图解，文本内容为“anna eats a tasty strawberry”。它读取、理解并存储在记忆中。

图 4.2

人类逐字阅读句子的示例（基于 Hagiwara 的解释（2021））

我们考虑之前相同的示例句子。人类读者逐字阅读句子。^(6) 作为第一步，单词*Anna*被阅读并存储在短期记忆中。然后，阅读下一个单词，本例中为*eats*。它与之前已经阅读的单词放在上下文中，并且也存储在短期记忆中。在这里，短期记忆被更新为“Anna eats”，记录了两个单词之间的关系，意味着 Anna 执行了一个动作。这种短期记忆也可以称为*心理状态*，根据逐字阅读句子时阅读的新单词逐步更新（Hagiwara 2021）。

### 具有循环的神经网络

-   现在我们想要将阅读句子的机制与前一章中看到的神经网络结合起来。这些神经网络被称为*循环神经网络*（*RNNs*）。我们可以将前面示例中的人类阅读过程转化为更技术化的设置，如下所示：我们引入神经网络的*状态*的概念，作为读者的短期记忆的类比，在每个单词向量后更新它，如图 4.3 所示。![](img/604345_1_En_4_Fig3_HTML.png)

-   一个图解人类和机器互动的示例，文本显示 anna 在 4 个状态下吃了一个美味的草莓。

-   图 4.3

-   一个神经网络逐个向量处理句子（基于 Hagiwara 的解释（2021））

-   神经网络首先接收单词*Anna*的向量。然后接收第二个向量：单词*eats*的向量。这个向量与上一步隐藏层的输出结合在一起，并传递到下一步。换句话说，神经网络的状态被更新，反映了新单词对到目前为止已读句子带来的额外信息。这个操作重复进行直到句子结束。到那时，整个句子的内容被存储在这个*最终状态*中。

-   在前一章中，我们已经看到了神经网络的架构，包括一个输入层，几个隐藏层和一个输出层。每一层都由几个神经元组成，每个神经元都处理一个输入和一些权重，并将输出提供给下一层。你可能记得信号从输入层进入我们的神经网络，逐层到达输出层。由于逐字阅读句子时，我们一遍又一遍地执行相同的操作，相同的神经网络结构被重复使用。处理中存在*循环*。循环是计算机编程中常见的结构。每当需要多次执行相同操作时，可以使用特定的结构仅编写一次指令，并告诉计算机执行多次。这类似于以下情景：如果你有十碗草莓，并想指示某人把它们从厨房带到客厅的桌子上，你可以说：

+   -   *请去厨房，把第一碗草莓端到桌子上。*

+   -   *然后，请再次去厨房，把第二碗草莓端到桌子上。*

+   -   *等等。*

-   但是，你可能不会这样做。你可能会说：

+   -   *对于这十碗草莓，每碗，请去厨房，把碗端到桌子上。*

-   因此，你刚刚创建了一个有十次迭代的循环。

在循环结构中重复使用神经网络的想法引出了所谓的循环神经网络的概念。相同的结构可以在下一个单词到来时被重复使用。在每一步中，我们将上一次迭代的隐藏层输出（状态）以及当前步骤的单词向量作为输入传入网络。这使得神经网络能够将输入的单词与句子的其余部分连接起来。回到厨房里的草莓碗，拿起碗的人可能在厨房里有一个笔记本和一支笔，每次记下哪些碗已经被拿到桌子上以及它们是如何摆放的。

因此，我们的循环神经网络的架构如图 4.4 所示。*状态*对应于前面提到的短期记忆，并存储了关于每个单词上下文的信息。![](img/604345_1_En_4_Fig4_HTML.png)

流程图说明了一个由输入、神经网络状态和输出组成的循环神经网络。

图 4.4

循环神经网络的架构

这有什么用？假设我们想通过预测部分句子的下一个单词来生成文本：

步骤 1：

安娜吃…

步骤 2：

安娜吃了一个…

步骤 3：

安娜吃了一个草莓…

每一步生成的单词由神经网络对下一个单词的预测给出。在每一步中，使用相同的神经网络，并且状态是隐藏层的输出，其中存储了迄今为止所见的所有信息。

因此，让我们回到我们最初的问题。我们想找到一种比平均方法更好的方法，以从可变长度的输入中生成固定长度的向量。通过循环神经网络，我们找到了一个解决方案。如图 4.5 所示，我们可以输入一个句子（每个单词编码为一个向量），并通过使用这种类型的神经网络得到一个句子嵌入。句子嵌入是神经网络的最终状态。由于在神经网络内部使用循环可能导致的重复，我们可以用它来处理非常短的句子以及包含许多单词的句子。此外，与平均方法相比，这里考虑了句子中单词的顺序。![](img/604345_1_En_4_Fig5_HTML.png)

在标有 M State 的神经网络示例中，句子安娜吃了一个美味的草莓通过向量进行处理，包括安娜向量、吃向量和草莓向量。句子的结果向量表示为句子安娜吃了一个美味的草莓的向量，其值为 2.3、55.6 和 33.4。

图 4.5

一个循环神经网络为完整句子生成一个向量

通常，我们会发现一个如图 4.6 所示的可视化表示来说明相同的网络。与循环不同，递归神经网络被*展开*并显示为几个相同的层。这是有益的，因为它可以展示单词序列（句子）是如何逐步处理的。神经网络的每个副本状态表示到目前为止已经引入的单词的信息。![](img/604345_1_En_4_Fig6_HTML.png)

一个神经网络的插图接收句子“Anna 吃了一个美味的草莓”并将其处理为向量，最终达到具有向量值的最终状态。在到达最终结果之前，网络在过程中经过各种状态的转换。

图 4.6

递归神经网络的展开表示

### 这就是，语言模型！

我们现在正在接近本书的核心，介绍语言模型的概念。让我们看看以下定义：

> 形式上，语言模型是一个给文本赋予概率的统计模型。[...] 句子越符合语法，越有“意义”，概率就越高。（Hagiwara 2021，第 131 页）

让我们考虑以下两个句子：

+   *有一个满是草莓的地方。*

+   *有一个满是猫的地方。*

对于一个人来说，很容易确定第一个句子比第二个句子更有可能。这种*可能性*的感觉在语言模型中以概率的形式表达。对于上面的句子，我们期望语言模型给予第一个句子��高的概率，而给第��个句子较低的概率。

语言模型是许多自然语言处理任务的基础。例子包括机器翻译、语音识别和聊天机器人中的答案生成。

简单来说，为了得到句子的概率，考虑上下文（之前和/或之后的单词）下每个单词的概率被结合起来。例如，考虑以下句子：

+   *有一个满是草莓的地方。*

我们会考虑这样的问题：一个句子以*There*开头的可能性有多大？单词*is*在单词*There*之后出现的可能性有多大？所有这些问题的答案将为我们提供一个总体概率，然后可以将其与另一个句子的总体概率进行比较，以决定哪个句子更有可能是一个*真实*、*好的*句子（一个人类可能使用的单词组合）。

当看到这些问题时，你可能会发现与之前创建句子嵌入时的任务有些相似。为了生成句子嵌入，我们逐词处理句子，考虑上下文并将含义存储在我们的*状态*中。为了回答上一段提出的关于*相似性*的问题，我们也需要逐词处理句子，并考虑单词在句子中出现的概率。

因此，我们可以使用循环神经网络的架构并训练我们的第一个语言模型。

首先，我们回到之前看到的展开的循环神经网络。特别是，我们现在对*状态*的中间值感兴趣。当在图 4.7 中没有输出的情况下看这个时，我们意识到我们实际上在做以下事情：基于一系列输入（以向量形式的单词序列），我们生成一系列输出（*状态*的中间值）。这样的架构被称为*序列到序列编码器*。![](img/604345_1_En_4_Fig7_HTML.png)

在神经网络处理流程中，输入序列以向量 Anna 开始，然后是向量 eats 和向量 strawberry。这些输入通过各种状态处理，最终导致输出序列。

图 4.7

序列到序列编码器的架构

虽然*状态*的中间值很有用，但这不是我们语言模型的输出。我们想要的是根据我们迄今为止读取的部分序列（部分句子）来指示潜在下一个单词的概率。

在图 4.8 中，我们考虑阅读句子第一个单词*Anna*的例子。这一步的输出是下一个单词的概率。虽然单词*Anna*后面跟着单词*eats*的可能性很高，但跟着*strawberry*或*tasty*的可能性相对较低。与上一章类似，我们将概率表示为从 0.0 到 1.0 的数字，可以轻松映射到 0%到 100%的百分比。![](img/604345_1_En_4_Fig8_HTML.png)

神经网络预测序列中的下一个单词。给定输入 Anna，它将概率最高的 0.9 分配给 eats，其次是 tasty，概率为 0.3，而 strawberry 的概率为 0.2。

图 4.8

预测句子中跟在“Anna”后面的单词。虽然“eats”这个词似乎是一个不错的选择，但“strawberry”和“tasty”这两个词的概率较低。

如前所述，我们想要使用这种架构来*训练*语言模型。因此，我们将上述原则应用于大量文本，*隐藏*特定单词并预测它们，以便根据需要控制和调整结果。在这里减少损失意味着更接近正确预测隐藏单词。

在上面的例子中，我们知道单词*eats*在文本中跟在单词*Anna*后面。因此，*eats*是正确答案，对于一个完全训练好的系统（在预测阶段），我们期望它有最高的概率。然而，在训练阶段的开始，也许答案还不正确，模型可能会给其他单词估计更高的概率。这就是学习发生的地方：基于这一步的估计值和正确答案，对系统进行*改进*以做出更好的预测（正如我们之前看到的通过调整神经网络内部的权重来最小化损失），逐句、逐文本地进行，数以百万计或十亿计的单词。

一旦语言模型训练完成，我们就可以用它来构建新的句子！这发生在*预测阶段*。通过提供句子的第一部分，我们可以预测下一个单词，如图 4.9 所示。我们要求语言模型预测句子“Anna eats a tasty …”中的下一个单词，（希望）我们的语言模型的答案是*strawberry*。在幕后，词汇表中每个单词都有一个导致这个答案的概率分数。![](img/604345_1_En_4_Fig9_HTML.png)

一幅插图，一个标记为神经网络的语言模型处理输入 Anna eats a tasty 并预测下一个单词，这个单词是 strawberry。模型使用其内部状态 M 进行这个预测。

图 4.9

使用语言模型预测句子中的下一个单词

早期的词嵌入训练和这个过程之间的主要区别是输入。在词嵌入训练中，我们计算一个单词作为上下文单词到目标单词（被编码的单词）的概率。唯一的输入是目标单词。在本节的神经网络情况下，用于预测下一个单词的输入不仅考虑一个单词，还考虑了前一个状态，其中包含有关多个单词的信息。

现在我们已经看到如何通过使用循环神经网络构建语言模型作为序列到序列编码器的第一个示例。这是一个成就，恭喜！

## 走向 Transformer 模型

现在只剩下最后一步，将我们带到最新的最先进语言模型。当前模型如谷歌的 BERT（Devlin 等人 2019）或 OpenAI 的 GPT-3 或 GPT-4 都是所谓的基于 transformer 的模型（Vaswani 等人 2017）。在本节中，我们将发现这种架构是什么样的，以及为什么它如此强大。

我们在上一节中看到了如何创建一个语言模型，通过预测部分句子中的下一个单词来生成文本。在这个例子中，我们观察了从一系列单词（一个句子）到一系列概率（最可能的下一个单词是什么？）的转换。现在，我们想稍微调整用例，并将一系列单词转换为*另一系列*单词。这可以是一个英文句子作为输入，被翻译为另一个西班牙文句子，或者用户向聊天机器人提出的问题，被转换为匹配的答案。为了实现这一点，我们将连接两个模型，就像之前看到的那样，并将一系列文本转换为另一系列文本。特别是，在本节中我们将处理一个问答任务。

### 编码器和解码器

为了说明这个想法，让我们考虑用户可能向聊天机器人提出的以下查询：

+   *草莓在哪些国家生长？*

要生成一个答案，我们需要处理这个用户输入并找到最合适的答案。这假设模型的训练过程已经结束，我们现在处于预测阶段。让我们假设对于我们的例子，聊天机器人将生成以下（不太有用的）答案：

+   *草莓在不同的国家生长。*

要做到这一点，我们使用一个模型来创建第一句的句子嵌入，然后将其传递给另一个模型，并使用这第二个模型生成答案。第一个模型称为*编码器*，因为它将输入句子的含义编码为句子嵌入（整个句子的向量表示）。第二个模型称为*解码器*，因为它解码这个嵌入并生成答案句子。图 4.10 显示了这种架构的高层视图。![](img/604345_1_En_4_Fig10_HTML.png)

在这个示例中，编码器处理问题“草莓在哪些国家生长？”并生成一个数字化的句子嵌入。解码器使用这个嵌入来生成答案“草莓在不同的国家生长”。

图 4.10

编码器-解码器架构的高层视图

根据我们之前看到的内容，现在我们可以更仔细地看看这两个组件内部发生了什么 - 编码器和解码器。这种序列到序列模型的实现方式可能会有所不同：根据特定的自然语言处理任务，编码器或解码器可以以不同的方式实现。

让我们创建一个基于我们之前看到的递归神经网络架构的编码器，以生成句子嵌入。句子嵌入对应于最终迭代后的隐藏状态。这个网络实际上已经在执行编码器的任务，所以我们可以直接从上一节中获取它，并将其包含在这个新系统中作为编码器。^(7) 哦，第一步完成了！

现在让我们看看解码器。在前面的例子中，下一个单词的估计是基于到目前为止已生成的单词的上下文，而对于解码器，模型从编码器接收的输入也被考虑在内。基于此，逐字生成输出句子，如图 4.11 所示。![](img/604345_1_En_4_Fig11_HTML.png)

一个插图，编码器将问题“草莓在哪些国家生长？”处理成一个数字化的句子嵌入。解码器然后使用这个嵌入来生成响应“草莓在国家生长”。

图 4.11

对编码器-解码器架构的两个部分进行更详细的观察。编码器的输出是解码器的输入。

### 预训练和微调

现在让我们更详细地了解我们问答任务的训练过程。这个例子的训练阶段有两部分。我们假设我们有一个像之前看到的那样已经*预训练*的循环神经网络。这意味着它已经用大量文本语料库进行了训练，因此权重已经配置得相当不错。一般来说，它已经准备好预测句子中的下一个单词。我们拿这个模型的两个实例：一个用来编码我们的句子（编码器），另一个用来生成答案文本（解码器）。我们准备了一个数据集来*微调*这个模型。在额外的训练阶段，使用额外的训练数据，进一步优化权重。特别是，由于模型通常是为文本生成而训练的，微调允许专门针对特定任务进行优化，在我们的情况下是问答。

表 4.1 展示了这种微调训练数据可能的示例。表 4.1

微调任务的训练数据示例

| 问题 | 答案 |
| --- | --- |
| <START> 草莓在哪些国家生长？ <END> | <START> 草莓在不同的国家生长。 <END> |
| <START> 草莓是什么颜色？ <END> | <START> 除了例外，草莓大多是红色的。 <END> |
| … | … |

让我们将*标记*定义为在文本中找到的常见字符序列。^(8) 在以下示例中，我们将整个单词作为输入标记。实际上，并非每个单词都一定完全匹配一个标记。例如，单词*sequences*被分成两个标记：*sequ*和*ences*。

在我们的编码器-解码器架构的背景下，需要考虑的一个方面是输入和输出中单词的长度。用户向聊天机器人提出的问题和聊天机器人的回答可能包含不同数量的单词。特殊单词，即所谓的特殊标记，用于标记输入或输出序列的开始（<START>）和结束（<END>）。

具有这些特殊标记的问题和答案显示在表 4.1 中。当然，我们需要一长串这些问题和答案对来微调语言模型。

在微调过程中，第一个问题被输入到编码器中，生成句子嵌入。解码器接收编码器的输出（句子嵌入）。由于我们处于微调阶段，这是训练阶段的一部分，我们知道这个问题的答句来自我们的训练数据。因此，第一个单词被生成，学习的目标是最大化我们正在寻找的单词的概率。然而，在开始阶段，其他单词将被提出。系统仍然需要学习。在生成答句的第二个单词时，我们将从编码器接收到的句子嵌入和*正确*答案的第一个单词作为输入。特意提到正确答案，而不是学习过程中可能错误的第一步生成的单词，因为这可能会在估计后续单词时产生错误信息。对于每个生成的单词，考虑该单词与正确答案之间的差异以改进系统。同样，我们希望调整权重以减少损失。与以前一样，系统通过许多迭代和许多训练示例进行改进，重复本段中提到的步骤。

一旦系统经过调优并应用于我们不知道正确答案的新用户查询时，我们就处于预测阶段（或在这种情况下，实际上更多是*生成*阶段）。解码器现在的工作方式与训练阶段的解释略有不同。再次，编码器的输出和<START>标记启动整个过程。与训练阶段相比，现在考虑以前位置的预测单词以生成下一个单词（因为我们假设这个输入是在训练期间没有见过的新问题，所以没有正确答案可以采取）。

因此，现在我们已经启动并了解了编码器和解码器的工作方式的序列到序列模型，我们终于可以开始使用变压器模型了。

### 变压器模型架构概述

最初在 2017 年提出的变压器模型（Vaswani 等人 2017）是一种新型的编码器-解码器神经网络，在自然语言处理领域开启了“变压器时代”（Hagiwara 2021，第 185 页），为不同的文本处理和文本生成任务提供了显著的改进。

与我们详细了解的编码器-解码器架构相比，基于 Transformer 的模型*更多相同*。在这些系统中，不同层的编码器和解码器被组合在一起。因此，多个编码器层被传递以进一步改善结果。而在原始论文中介绍这种架构时使用了六层，实际上可以使用更多层（Hagiwara 2021）。同样，解码器由不同层组成。该架构在图 4.12 中有所说明。![](img/604345_1_En_4_Fig12_HTML.png)

输入句子到多个编码器和解码器以及输出句子的流程图。

图 4.12

Transformer 架构的高层视图

图 4.13 更详细地展示了 Transformer 模型中每个编码器和解码器组件的内部情况。![](img/604345_1_En_4_Fig13_HTML.png)

该插图描述了具有前馈神经网络、自注意力机制和交叉注意力机制的编码器和解码器。

图 4.13

Transformer 架构中编码器和解码器的内部结构（灵感来自 Alammar 的 Transformer 模型出色的可视化(2018)）

与我们之前详细了解的示例相反，Transformer 模型不使用循环神经网络。随着句子变得越来越长，循环神经网络及其状态在远离彼此的标记之间的依赖性问题逐渐增多（Hagiwara 2021）。换句话说，状态开始*遗忘*关于序列中较早出现的单词的信息。当使用称为*自注意力*的机制时，这些长距离依赖性可以得到更好的覆盖。Transformer 多次应用这种自注意力，结合*前馈神经网络*。前馈神经网络没有循环，如循环神经网络，因此大致对应于我们所见过的最初类型的神经网络。通过不同的层，输入从神经网络的左侧经过到右侧，最终产生输出。

这种*注意力机制*可能是使基于 Transformer 的架构与以前的自然语言处理方法有所不同的最重要组成部分。我们将在接下来的章节中更详细地探讨它。

### 自注意力

当看到序列到序列模型的编码器-解码器架构时，我们注意到句子嵌入的高相关性，将编码器与解码器联系起来。这个向量包含了从用户查询中可用的所有信息，以生成聊天机器人的答案。然而，这个向量相当有限。即使它包含了一些百维度，它是*固定长度*的。而固定长度可能不适合我们可能遇到的所有用例。无论输入句子是 3 个单词还是 20 个单词，用于表示所有这些信息的可用空间始终是相同的。事实上，在我们之前看到的架构中，当输入句子非常长时，性能会受到影响。如果有一些机制只考虑长序列中最重要的部分（而不仅仅是最近的，如 RNN 的情况），这个问题就可以得到缓解。因此，总结一下，我们可以得出结论，解码器需要更多信息来参考来自编码器的输入的不同方面。这就是*self-attention*提供的。

直觉上：我们希望它集中在最相关的信息上。想象一下，你正在看一张文字页面，并被要求找出文字*草莓*在文本中出现的位置。自动地，我们会扫描文本，略过其他信息，试图*专注*于对我们任务有关的单词。同样，我们希望变压器模型专注于每个给定单词的相关信息。

与具有固定长度向量的句子嵌入以包含与输入句子相关的所有信息不同，我们回到了单词嵌入。输入句子中的每个单词都将有一个嵌入。然而，我们之前看到的单词嵌入和这里的单词嵌入之间有一个重要的区别。这里的单词嵌入包含了单词的整个上下文的信息。

生成这些*基于上下文的摘要*对于每个单词来说是自注意力机制的任务。可以说，基于表示单词的词嵌入形式的向量，创建了一个更高级的词嵌入，代表了单词*和*它的上下文。

在使用自注意力计算给定单词的这些高级词嵌入时，同一句子中的每个其他单词都被赋予一个权重。权重决定了它对给定单词的相关性，就像巨大草莓对使草莓植物弯曲的相关性一样，总是同一个故事。

让我们考虑以下例子：

+   *有一片草莓地，而* ***它*** *是如此美丽！*

当使用自注意力计算单词*it*的基于上下文的摘要时，预计单词*field*的权重会比其他单词更高，因为*it*指的是草莓地。使用这种自注意力机制，我们不会忽略指向先前出现在句子中的其他单词的单词。这是不使用自注意力系统的典型问题，特别是当句子变长时，单词和指向该单词的引用之间有许多单词时。

例如，可以简化地将从单词*it*到上下文单词的注意力权重和依赖关系可视化，如图 4.14 所示。![](img/604345_1_En_4_Fig14_HTML.png)

一个草莓的插图，文字写着有一片草莓地，非常美丽。

图 4.14

从单词“it”反映出的依赖关系体现在不同单词所赋予的权重上

与单词*it*直接相关的单词，如它所指的*field*或描述它的*beautiful*，对于其基于上下文的摘要更为重要。其他单词，如*of strawberries*或动词*is*指向单词*it*，是相关的但可能比其他单词不那么重要。因此，它们的权重较小。

基于上下文的摘要是在编码器中创建和处理的。如图 4.15 所示，单词*在*在考虑上下文的情况下产生一个向量，从编码器传递到解码器。这里的上下文是指，例如，单词*在*后面跟着单词*哪些*，再后面跟着单词*国家*。第二个向量将是单词*哪些*的上下文感知表示，输入句子的第二个单词，以此类推处理输入句子中的其他单词。![](img/604345_1_En_4_Fig15_HTML.png)

一个流程图。输入句子“在哪些国家种植草莓？”经过编码器和解码器处理。在此过程中生成基于上下文的值。输出句子是“在哪里”，作为“在”这个词的摘要。

图 4.15

基于上下文的摘要是在编码器中创建和处理的

### 交叉注意力和位置编码

在解码器中，使用另一种形式的注意力：*交叉注意力*。这种机制类似于编码器和解码器内部使用的自注意力，用于生成上下文感知的词嵌入。交叉注意力在解码器中用于总结从编码器获得的信息（*交叉*是指它跨越编码器和解码器之间的边界）。这里的想法是再次在考虑上下文的情况下获得最相关的信息。

当阅读关于自注意力机制的内容时，主要关注单词之间的关系，你可能会想知道我们是否会遇到与本书早期相同的问题，即我们会错过单词顺序的信息（记得例子，*安娜吃草莓*和*草莓吃安娜*被表示为相同）。实际上，在描述的方法中可能会遇到这个问题。然而，transformer 编码器通过额外的计算来解决这个问题：*位置编码*。

位置编码是一个额外的组件，添加到单词的词嵌入中，包含有关位置的信息。在*安娜吃草莓*中，*安娜*的位置编码（安娜在位置 1）会与*草莓吃安娜*中*安娜*的位置编码（安娜在位置 4）不同。

在计算上下文化词嵌入时包含有关单词位置的信息，这些信息可以被保留，我们就不会遇到像草莓吃安娜这样的问题。

### 静态和上下文化词嵌入

我们在本书中之前看到的词嵌入，不考虑单词的上下文，也被称为*静态*或*传统词嵌入*。在本节中看到的基于 transformer 模型的更高级的嵌入被称为*上下文化词嵌入*。在静态词嵌入中，单词*橙色*指代颜色和同样的单词*橙色*指代水果会被编码为相同的词嵌入。在上下文化词嵌入中，情况会有所不同，因为会考虑这些单词在训练数据中出现的上下文。例如，水果可能会与草莓、苹果或香蕉一起出现，而颜色可能会与紫色、蓝色或绿色等其他颜色一起出现。在训练基于 transformer 模型时，这种差异会被考虑并反映在嵌入中。因此，橙色（颜色）的上下文化词嵌入会与橙色（水果）的嵌入不同。

我们使用变压器架构训练的这些语言模型可以作为预测不同情况下的句子的基础，比如聊天机器人或下一个句子的预测。这些语言模型，或者更准确地说是由这些训练过程产生的*上下文化词嵌入*，也可以应用于不同的任务，比如分类。这让我们想起了我们在本书中早些时候看到的将人类语言中的单词映射到向量表示的字典。至于这个字典，基于变压器的语言模型可以训练一次，然后在不同的用例中共享和使用，如图 4.16 所示。这是个好消息，因为训练一款最先进的基于变压器的模型需要强大（因此昂贵）的硬件，大量的文本语料库（我们谈论的是数以亿计的单词），以及（取决于您的硬件）对训练的耐心，可能需要数天、数周或数月的时间才能执行完毕。![](img/604345_1_En_4_Fig16_HTML.png)

一个流程图。基于变压器的语言模型，训练一次并且公开可访问，利用大量的训练数据使用变压器架构高效有效地执行特定的机器学习任务。

图 4.16

基于变压器的模型可以训练并提供给其他应用程序使用。

在这些词嵌入和语言模型出现在自然语言处理领域之前，机器学习模型通常是针对一个非常具体的任务进行训练的。正如在这里和之前提到的，这些*预训练组件*（语言模型或词嵌入）可以被提供并立即使用，这改变了最近构建自然语言处理应用程序的方式。这是一个优势，因为不同的自然语言处理任务可能会发现知道覆盆子和草莓有些相似是有益的，因为它们都是浆果。描述这种设置的一个很好的类比已经由（Hagiwara 2021）提供：

> [这相当于]教一个婴儿（=NLP 模型）如何跳舞。通过让婴儿先学会稳定地行走（=训练词嵌入），舞蹈老师（=特定任务的数据集和训练目标）可以专注于教授具体的舞蹈动作，而不必担心婴儿是否能够站立和行走得当。（Hagiwara 2021，第 220 页）

使用预训练组件并根据实际任务的需求进行调整（称为*适应*或*微调*的过程）通常可以称为*迁移学习*。第一学习阶段中存储在语言模型或词嵌入中的见解可以被*转移*到不同类型的任务中。这避免了当可以从已经存在的语言模型中受益时，一遍又一遍地从头开始训练模型。

这通常导致数据工程师设置如图 4.17 所述。它依赖于公开可用的单词嵌入或在大量数据上训练过的语言模型，并且可以通过使用库轻松地包含在数据工程师的项目中。基于这些组件和一个较小的训练数据集，微调过程对实际所需任务进行机器学习训练，例如前面描述的情感分析任务。在权重方面，从头开始训练意味着我们从随机权重开始训练阶段。在微调的情况下，权重已经从不同任务中学习过，并且只是从那里调整过来。![](img/604345_1_En_4_Fig17_HTML.png)

一个流程图。公开可用的语言模型是在大型文本语料库上训练的，使它们可以用于各种任务。或者，您可以使用较小的数据集对这些模型进行微调，以在训练阶段为特定任务定制它们。

图 4.17

预训练语言模型可以为其他机器学习任务进行微调

拥有语言模型和单词嵌入是方便的；然而，它们也存在一些限制（例如，偏见），这些限制很容易通过这种方式传播。我们将在后面的章节中更多地讨论这个问题。

### BERT 模型

举个例子，让我们更仔细地看一下 Google 的语言模型 BERT 是如何训练的（Devlin 等人 2019）。BERT 模型已经在两个任务上进行了训练：*遮蔽语言建模*和*下一个句子预测*。让我们看看这意味着什么。对于遮蔽语言建模，训练数据中有 15%的标记被隐藏（一个大型文本语料库）。然后，变压器模型必须预测这些被遮蔽的标记。这类似于我们之前看到的例子。标记<MASK>表示被覆盖的单词，需要进行预测。

+   *有一片充满<MASK>的地方。*

在第二个任务中，重点是不同句子之间的关系。这在第一个任务中没有涵盖，仅依赖于一个句子内的单词。系统必须预测两个句子 A 和 B，句子 B 是否是原始文本中跟在句子 A 后面的句子。在 50%的情况下，这是正确的，在另外 50%的情况下，这不是情况。考虑以下示例，系统应该预测句子 B 跟在句子 A 后面：

句子 A：

安娜去草莓地。

句子 B：

她采集了许多浆果并把它们带回家。

在以下示例中，系统应该更倾向于预测这不是情况：

句子 A：

安娜去草莓地。

句子 B：

猫喜欢睡觉。

BERT 的名称代表双向编码器来自变压器的表示。这个基于变压器架构的强大系统在 2019 年为自然语言处理领域带来了重要进展。

### 聊天机器人的架构

根据我们迄今所见，我们现在可以绘制一流的基于变压器的聊天机器人的架构。如图 4.18 所示，涉及不同的软件组件。特别是，我们发现具有变压器架构的语言模型在背景中。正如我们之前所见，它由不同层的编码器和解码器组成。通常，在这种类型的应用程序中，还有另一个软件组件，允许人类与语言模型进行交互。通常，希望与软件交互的人类我们通常称之为*用户*。用户的期望通常是有一个*图形用户界面*与软件交互。希望有黑色背景和小白色字体而没有任何图形插图的用户群体非常小，可能仅限于非常技术的人群。您可能从电影中了解到这种类型的应用程序，当涉及黑客或关键系统关闭时。一般来说，用户更喜欢更具图形化的东西，即易于使用。在聊天机器人的情况下，这通常由*Web 应用*提供。用户可以通过在网络浏览器中打开链接或在智能手机上安装应用程序来启动 Web 应用。通常，在与聊天机器人开始对话时，他们将收到问候消息，然后可以输入他们的*输入提示*。这个输入提示将由语言模型处理，并且将使用我们之前看到的方法生成答句。返回的句子*很可能*是对用户输入的输入提示问题的一个很好的答案。它到底有多好取决于语言模型的输入数据和训练设置。^(9)![](img/604345_1_En_4_Fig18_HTML.png)

一个流程图。用户提供一个输入提示，草莓是在哪些国家生长的？语言模型通过 Web 应用程序中的编码器和解码器处理此输入。作为回应，聊天机器人，自我介绍为你好，我是一个聊天机器人，我可以帮助你吗？开始对话。

图 4.18

聊天机器人的架构：通常使用 Web 应用作为用户和语言模型之间的接口。

最后，在某些设置中，用户也被用作语言模型的*训练者*。可以从用户那里收集反馈以改进语言模型的预测或使其更安全。例如，用户可以对答案进行评分，表明其是否有用。在其他情况下，我们可能希望标记特定类型的答案为不当。从人类训练者那里获得的更多输入和更正，系统就能更好地适应。

就是这样。现在我们明白了最先进的语言模型和聊天机器人是如何工作的。现在让我们回到本章前面看到的聊天机器人展示情感的示例对话。

乍一看，似乎令人费解的是，聊天机器人产生的文本背后没有人类意义或理解，尽管它以流利而雄辩的方式表达其答案。答案看起来连贯，并且自然而自发地发音。当考虑技术背景和基于概率生成最有可能的下一个词时，您可能会重新考虑这些语言模型是否像您一样推理和产生文本的选项。

关于意识、推理或智能在更深层次上意味着什么，以及是否可以用人脑以外的材料实现这一点的哲学辩论不在本书的讨论范围之内。我想给你一个关于语言模型如何工作的基本理解，以及它们（目前）无法与人类推理的能力相比，没有像信念、欲望、意图或恐惧等心理状态。正如 Nida-Rümelin 和 Weidenfeld（2022，第 17 页）所论证的，“数字状态和过程*模拟*心理状态，但并非与之相同，即使该模拟是完美的。”

## 总结

在本章中，我们讨论了聊天机器人是否能拥有情感。我们已经看到，尽管它们产生的文本通常是雄辩和富有同情心的，但在幕后，更多的是关于基于概率预测最有可能的下一个词。

我们还学习了什么是循环神经网络，以及如何使用编码器-解码器架构从中构建序列到序列模型。基于此，我们了解了变压器架构的工作原理以及自注意力如何实现高效的最先进语言模型。我们还探讨了如何对预训练语言模型进行微调以适应特定任务，避免每次都从头开始。

最后，我们已经看到了典型聊天机器人的架构。在背后使用语言模型，通常会使用 Web 应用作为用户和聊天机器人之间的接口。


© 作者，独家许可给 Springer Nature Switzerland AG 2023M. Kurpicz-Briki《不仅仅是聊天机器人》[`doi.org/10.1007/978-3-031-37690-0_5`](https://doi.org/10.1007/978-3-031-37690-0_5)

# 5. 真与假

Mascha Kurpicz-Briki^(1  )(1)应用机器智能，瑞士比尔/宾讷应用科学大学

## 概述

在本章中，我们将更深入地探讨先进语言模型的一个特性，即由语言模型创建的句子以及聊天机器人可能提供的答案*很可能*是一个好结果。很可能，因为正如我们之前所见，这都是关于概率的。根据选项中提供的内容，即使对于给定问题来说最佳选项可能也不是一个很好的答案。因此，以不同方式，先进的聊天机器人可能提供*错误*信息。

我们还将研究多年来使用过的一些其他聊天机器人架构。特别是，我们将考虑 1960 年代创建的 ELIZA 的例子，这是最早的聊天机器人之一。

在本章的最后部分，我们将研究用于机器学习模型的评估指标。这些指标帮助我们确定给定模型是否运行良好。

## 如果语言模型错了会怎样？

### 你知道白草莓吗？

让我们从以下例子开始：

人类：

“草莓是什么颜色的？”

聊天机器人：

“草莓在完全成熟时通常呈深红色。然而，在未完全成熟时，表面可能有绿色或白色区域。草莓表面的种子通常是黄色或浅褐色。”

尽管这对大多数草莓来说是正确的答案，但对于白草莓（一种白色草莓品种）不适用，如图 5.1 所示。![](img/604345_1_En_5_Fig1_HTML.jpg)

一张放在表面上的一堆白草莓的照片。

图 5.1

白草莓是白色草莓（来源：Wikimedia/Emmbean. CC BY 3.0 许可，可在 [`​commons.​wikimedia.​org/​wiki/​File:​Pineberries.​jpg`](https://commons.wikimedia.org/wiki/File:Pineberries.jpg) 获取）

也许有些读者不知道这种草莓，因为它并不是很有名？对于聊天机器人也是一样。聊天机器人生成答案的信息受到其训练数据的限制。对于这个特定问题，一个具有更广泛训练数据和反馈的聊天机器人可能会在对话中得到以下答案：^(1)

+   聊天机器人：“草莓在完全成熟时通常呈深红色。然而，在未完全成熟时，表面可能有绿色或白色区域。草莓表面的种子通常是黄色或浅褐色。*偶尔，你可能会遇到一些白色或淡粉色的草莓品种，这些被称为白草莓*。”

当然，现在，你可能会说我们很快就会到达那里，不断添加更多数据，变得越来越聪明。然而，问题在于你看不到缺失的数据；你只能看到答案，这可能是不完整的（或错误的），而且不可能测试所有潜在的用例。统计上不太可能发生的事情，比如草莓是白色的，不太可能被考虑为输出。因此，已经存在的信息被重复和强化，导致语言模型也被称为*随机鹦鹉*，一个“根据其广泛的训练数据中观察到的语言形式序列随意拼接的系统，根据它们如何组合的概率信息，但没有参考任何含义”（Bender 等人 2021，第 617 页）。

### 幻觉

当前最先进的语言模型中存在的另一个问题是在生成文本时出现的*幻觉*。一般来说，幻觉一词描述了人类在没有实际外部刺激的情况下产生的感知。² 在文本生成中，我们指的是“文本是荒谬的或不忠于提供的源内容”（Ji 等人 2023，第 248 页：3）。基本上，聊天机器人在编造那些毫无意义或是错误且无法根据使用的训练数据解释的信息。

语言模型中的幻觉可以是*内在*的或*外在*的幻觉（Ji 等人 2023）。内在幻觉发生在生成的输出和源内容相互矛盾的情况下。比如，在语言模型的训练数据中，有关*草莓粉丝*组织的以下文本（以及数百万其他单词和年份）。

“草莓粉丝组织成立于 2019 年，由安娜和汤姆创立，之前他们写了一本关于草莓美味和美丽的书。”

然后，我们与一个基于这种语言模型的聊天机器人进行了以下对话：

人类：

“草莓组织是在哪一年成立的？”

聊天机器人：

“草莓粉丝组织成立于**2014**年。”

等等，什么？是的，我们刚刚经历了一次内在幻觉。

另一种幻觉是外在幻觉。在这种情况下，生成的输出无法验证，但也无法根据源内容进行反驳。基本上，这些信息是未知的，但聊天机器人却给出了一个答案。这个答案可能是正确的，但也可能是错误的。这是未经验证的信息。

例如，我们可以在聊天机器人被要求回答有关 2023 年的问题时观察到外在幻觉；然而，训练数据只包含到 2022 年的信息。

图 5.2 总结了两种幻觉的类型。![](img/604345_1_En_5_Fig2_HTML.png)

一幅插图代表了内在和外在幻觉的情景，其中聊天机器人从训练数据中提供答案。在内在幻觉的情况下存在一个未经验证的 x 条件，在外在幻觉中则不存在。

图 5.2

内在幻觉和外在幻觉的区别（基于 Ji 等人 2023 的解释）

在幻觉的情况下，我们假设训练数据是一个基本事实，即正确的，并且希望这些信息能够体现在语言模型生成的文本中。然而，根据训练数据的来源，情况并非总是如此。考虑到训练一种最先进的语言模型所需的大量文本，人工对内容进行验证是不可行的。通常，正在训练、共享和用于许多应用程序的语言模型都是基于互联网文本的。

### 训练于假新闻？

假新闻和阴谋论本身就是当今社会的问题。一项研究表明，在互联网上谎言传播得比真相更快（Vosoughi 等人 2018）。有趣的是，与一般观点认为机器人^(3)是传播假新闻的主要来源相反，该研究得出结论，机器人以相同的速度传播假新闻和真实信息。另一方面，人类更有可能传播假新闻而不是机器人。

因此，这基本意味着互联网充斥着假新闻。如今，越来越多的人开始谨慎选择他们搜索引擎结果的来源，意识到并非所有在互联网上写的东西都一定是真实的。但是当我们基于这些内容训练一个语言模型，提供关于事实信息的类人回答却没有提供相应的参考资料时，会发生什么？尽管我们已经学会对于那些名不见经传的网站保持怀疑，但我们却被聊天机器人所困扰，解释内容的意义和责任，即使事实并非如此。

在计算机科学中，我们有一个众所周知的术语*垃圾进，垃圾出*。^(4) 它指的是向计算机程序提供不正确或无意义的输入数据将产生不正确或无意义的输出，无论逻辑多么出色。尽管这个表达来自计算的早期阶段，但对于机器学习领域也同样适用。如果我们在训练数据中看到假新闻、歧视性内容或其他不良内容，我们可以预期这将对我们的机器学习应用的结果产生影响。分类器或语言模型的好坏取决于它们所训练的数据。

### 错误答案的不同原因

我们已经看到了语言模型中错误信息发生的不同场景。关于统计上相当罕见事件的信息可能会被简单地忽略，由于语言模型的内部功能，可能会产生内在或外在的幻觉，而训练数据本身可能包含虚假新闻或阴谋论。这对于一个值得信赖的软件应用来说是一个非常糟糕的起点。图 5.3 显示了这些不同因素对语言模型生成的句子质量和可信度的影响！[](../images/604345_1_En_5_Chapter/604345_1_En_5_Fig3_HTML.png)

一幅插图显示了训练数据中不包含的罕见信息、训练数据中包含的错误信息，以及语言模型生成的与训练数据相矛盾或未经验证的句子。

图 5.3

不同原因可能导致聊天机器人生成错误信息

很好知道这些事情是*有意设计*的。最先进的语言模型的工作方式以及用于训练它们的数据使这种情况发生。即使是大型语言模型的创建者也无法幸免：当谷歌的聊天机器人巴德在 2023 年被介绍时，在第一次演示中出现了事实错误（Coulter 和 Bensinger 2023）。谷歌展示的演示显示巴德回答了问题，“詹姆斯·韦伯空间望远镜（JWST）有哪些新发现可以告诉我 9 岁的孩子？”巴德提供了一系列答案，包括一条信息表明这个望远镜拍摄了地球太阳系之外行星的第一张图片。然而，根据 NASA 的确认，这个信息是错误的。这是一个昂贵的错误，导致演示后市值损失了 1000 亿美元。

这里还有一件事要讨论，那就是语言模型不仅可以生成错误信息，还可以产生不存在的词。正如我们之前快速提到的，输入标记不一定是整个单词，还可以是像*pre*、*ing*或*ized*这样的片段，这样可以更有效地处理复合、罕见或非英语单词（Wolfram 2023）。然而，这也可能导致标记以新的方式组合在一起，从而产生全新的词。

在使用 ChatGPT 等工具的提示时，^(5)这也可以被明确触发：

人类：

“你能想到一个由*pre*和*strawberry*组成的新词吗？”

聊天机器人：

“当然，*preberry*怎么样？它可以指代尚未完全成熟或尚未达到最佳甜度的草莓，但仍然可食用。它也可以更普遍地用来描述处于早期或初步发展或准备阶段的事物，类似于前缀*pre-*。”

虽然在这里我们明确要求系统创建一个新词，但我们也可以在大型语言模型生成的答案或文本中自发地找到它们。

### 聊天机器人会撒谎吗？

让我们回到我们关于句子如何形成的技术解释，基于对下一个单词的预测。答案的形成方式依赖于特定单词在上下文中出现在另一个单词之后的概率。然而，语言模型并没有对它们创建的单词的*含义*具有类似人类的理解。与情感一样，问题再次出在人类的感知上。语言模型的输出似乎流畅而连贯。因此，人类解释生成的句子，这些句子是用他们说的语言，作为有意义的并对应于交际意图的，具有内容的可靠性（Bender 等人 2021）。这就是风险所在。如果聊天机器人产生幻觉，但被用来在互联网上搜索真实信息，然后被用户相信和复制，那么我们就有问题。

过去几个月，关于使用最先进的语言模型是否适合网络搜索，以及它是否会取代当前的搜索引擎的话题一直在激烈讨论中。研究人员警告不要将大型语言模型用于它们未经设计的任务（Shah 和 Bender 2022）。由于可能存在事实错误，正如我们在前一节中看到的那样，将搜索转向语言模型存在增加错误信息的风险。然而，大型科技公司的演示和公告证实了提供革命性搜索的目标，使任何搜索引擎以对话方式与您互动。

问题是这种搜索是否符合我们的期望。这更多是一个社会问题而不是技术问题。在这种愿景中，我们是否想要询问一个*人工专家*，即语言模型，为我们提供汇总信息，可能包含错误信息？还是我们想保持控制，使用工具为我们提供不同信息及其相应来源，并让人类比较和反思信息？在这两种情况下，人类需要意识到工具和提供的内容的局限性，并建立所需的电子素养。我们大多数人已经学会了，在使用搜索引擎时，并非所有出现在结果中的网站都是可信赖的信息来源。使用对话工具时，我们必须反思我们对所提供信息的解释，意识到当以类似人类对话的方式提出时，我们可能会以不同方式解释它们。

最后，让我对聊天机器人产生的虚假信息做一个最后的观察。在广泛的媒体中，经常使用术语“聊天机器人在撒谎”或“聊天机器人在说谎”。在这种意义上，“撒谎”动词的定义如*韦氏*词典所述：

1.  1.

    故意发表不实之论

1.  2.

    制造虚假或误导性印象

第一个定义涉及人类，而第二个定义也可以应用于物体，比如镜子。然而，当谈论工具时，在其他情况下，当工具提供与预期不符的结果时，我们很少使用这个表达。如果咖啡机冲出的咖啡太稀，你可能会说它坏了，而不是说它在欺骗你。欺骗，或者用我们通常使用这个术语的一般方式来说就是撒谎，需要一些意图，而在聊天机器人的情况下是不存在的，但是人类可能会在这样的对话中解读出这种意图。然而，我们必须记住，人类的意图可能导致技术被滥用。

因此，总体而言，我们有聊天机器人产生虚假信息，以及人类产生社交幻觉，看到意图和人类意义的地方。我们在数字社会中还有一些工作要做。

## 聊天机器人的简史

我们已经在前几节中看到了基于变压器架构的最先进聊天机器人是如何工作的。现在我们将获得其他（主要是以前的）变体的一些背景知识。聊天机器人也被称为*对话系统*或*对话引擎*。通常，它们的任务是以书面形式与人类进行对话。当对话以口头形式进行时，它们也被称为*语音机器人*。这些对话系统可以是数字助手，如 Siri 或 Alexa，也可以集成到网站中，以允许客户搜索相关信息。在本节中，我们将限制讨论文本为基础的方法，以了解它们与我们之前看到的文本处理和生成技术的关系。

### ELIZA

一个早期聊天机器人的非常著名的例子是**ELIZA**，由约瑟夫·魏岑鲍姆（Joseph Weizenbaum）在 1960 年代在麻省理工学院（MIT）开发（Weizenbaum 1966）。它的创造者想要探索如何实现机器和人之间使用自然语言进行交流。与今天的方法相比，ELIZA 的技术实现相对简单。用户输入（人类写给聊天机器人提问的文本）被检查是否包含特定的*关键词*。这在某种程度上类似于我们之前看到的非常简单的文本分析方法，通过检查文本中是否出现*草莓*或*草莓*这些词来确定文本是否与草莓有关。如果用户的问题中包含关键词，答案将基于*规则*生成。无内容的评论被识别并不予考虑，在某些情况下，还考虑了早期的转换。使用这种基于规则的硬编码方法，没有涉及训练数据或机器学习。

使用 ELIZA，可以进行如图 5.4 所示的对话！[](../images/604345_1_En_5_Chapter/604345_1_En_5_Fig4_HTML.png)

一组问题和答案模拟了人类和机器生成答案之间的对话。机器的回应以大写字母写成。

图 5.4

从（Weizenbaum 1966）中提取的对话片段；大写表达式是机器生成的答案

与现代聊天机器人相比，这种对话的主要区别在于对话不旨在用真实世界知识回答问题。正如 Weizenbaum 在其原始报告中所述，与 ELIZA 互动的参与者被指示与其交流，就好像他们在与精神科医生交流一样。为什么会这样？

> 选择这种对话模式是因为精神病学面试是少数几个例子之一，其中一个参与对话的人可以自由地假设几乎不了解真实世界。（Weizenbaum 1966，第 42 页）

进入对话的任何内容都是人类的输入，是他们提出的话题。

这种类型的聊天机器人的主要局限性也是优点。虽然它无法提供任何知识，但也更不太可能提供错误信息。在最坏的情况下，它会提供一个看起来不符合用户输入文本的答案。由于这种类型解决方案的所有潜在答案都列在规则和模板的某处，实验可以轻松复制。聊天机器人提供的答案可以完全解释和复制。因此，根据用例，受到 ELIZA 最初开发的方法启发的和基于这些方法的聊天机器人可能非常强大。

### 基于规则的方法

在上面展示的 ELIZA 示例中，没有提供关于世界的信息。当我们想传递非常具体的信息时，也可以使用基于规则的方法。例如，考虑一个零售公司在线商店网站上的聊天机器人，与任何顾客的对话如下：

聊天机器人：

“嘿，谢谢光临这家店！你在找什么？”

人类：

“我想吃点好吃的东西，也许草莓？”

聊天机器人：

“这是个好主意！点击这个链接以直接将我们商店的当地草莓添加到购物篮中。”

这可以以非常简单的方式实现，使用单词列表和规则。让我们更仔细地看一下这一点，基于我们之前看到的聊天机器人的架构。用户与网络应用进行交互，就像之前的情况一样。现在，与其在后台使用复杂的基于变压器的语言模型架构提供答案，这个示例中使用了不同的方法。如图 5.5 所示，用户的输入文本被分析以查找关键词。我们正在寻找能帮助我们向客户推荐产品的特定单词。在最简单的情况下，这是我们商店中所有产品名称的列表。通过将找到的关键词与此列表匹配，我们确定正确的产品并生成答案，就像上面的对话中一样。更高级的版本可以利用我们之前看到的其他文本处理方法，例如词干提取、词形还原、标记或更复杂的规则系统。![](img/604345_1_En_5_Fig5_HTML.png)

一张图示了一个网络应用中一组用户提供的输入提示以及它们生成的答案。它突出显示了名为草莓的关键词，并指示识别输入中的关键词并将其映射到可用产品以生成答案。

图 5.5

在线商店简单基于规则的聊天机器人示例

当然，如果每次文本都完全相同，每个人都会很容易注意到我们的方法是如何工作的。一个简单而有效的方法是我们制作另一个包含十个“这是个好主意”的替代版本的列表。这些替代版本可以包括“不错的选择！”或“听起来像个计划。”每次生成答案时，可以随机选择其中一个。

这个解决方案是可行的，因为在这种情况下信息的上下文是有限的，或者至少对我们可以回答的问题是有限的。当然，客户可能会要求与这家在线商店完全无关的东西，那么它就不起作用了。我们需要考虑在没有识别到关键词的情况下可能的答案。例如，“你能详细解释一下你需要这个东西的原因吗？”如果我们想将这个程序扩展到使用这些技术与人类交流关于任何主题的系统，我们将发现自己需要比我们能够创建或管理的规则和关键词更多。

### 使用句子嵌入创建的简单聊天机器人

现在让我们考虑另一个使用句子嵌入中存储的信息创建聊天机器人的方法示例。我们之前看到存储单词或句子含义的不同方法。根据它们在向量空间中的位置，并使用数学运算，我们能够提取有关单词或句子相似性的信息。我们假设我们想要创建一个可以回答关于草莓的问题的聊天机器人。

为了做到这一点，我们使用维基百科关于草莓文章的文本。它包含关于草莓历史或栽培的不同部分。我们将这篇文章分成句子，并将每个句子转换为句子向量，如图 5.6 所示。![](img/604345_1_En_5_Fig6_HTML.png)

一张图解释了用户关于草莓植株的查询，通过用户问题的句子嵌入，找到相似的嵌入，找到关于草莓的文本，以及每个句子的句子嵌入。

图 5.6

基于句子嵌入和相似性的简单聊天机器人

然后我们对用户输入做同样的处理。我们拿问题，比如“你知道关于草莓植株的什么？”并从中创建一个向量（句子嵌入）。正如我们之前看到的，向量之间的距离可以用来捕捉单词或句子之间的关系。因此，我们寻找一个接近我们问题句子嵌入的向量，这可能是我们问题的潜在答案。因此，我们确定维基百科文章的哪个句子嵌入最接近，并输出这个句子，如图 5.6 所示。这使得与聊天机器人的以下对话成为可能：

人类：

“你知道关于草莓植株的什么？”

聊天机器人：

“草莓植株在年轻时会产生更多更好的水果。”

对于不同的问题，这种方法效果相当不错，但远非完美，下面的例子就说明了这一点：

人类：

“草莓在哪些国家种植？”

聊天机器人：

“草莓也可以在草莓盆中室内种植。”

在这里，我们以草莓的一篇文章作为输入。如果扩展到使用当地图书馆提供的所有关于草莓的书籍，结果可能会得到改善。聊天机器人的表现取决于其可用的数据。

尽管对于许多任务来说，最先进的语言模型通常是最有效的解决方案，但我希望你记住，根据你所寻找的内容，更简单的架构也可以做得很好。这些简单方法的一个主要优势通常是它们的可解释性，这使我们更容易看到幕后情况，并让我们了解决策是如何做出的。有时我们也可以在实践中找到不同方法的组合。

## 什么是一个好的模型？

到目前为止，我们已经看到，最先进的聊天机器人依赖于强大的语言模型。但我们如何知道语言模型的表现如何或者如何将它们进行比较呢？

有关语言模型提到的一个典型数字是它训练时使用的*参数*数量。例如，谷歌的 BERT 模型使用了 3.4 亿个参数，或者 GPT-3 使用了 1750 亿个参数。这些都是相当大的数字！让我们记住之前看到的语言模型的架构。我们看到了不同类型的神经网络，有些涉及不同层的编码器和解码器。在每个层中，都有神经网络，每个神经网络由多个神经元组成，具有加权的输入。这些权重在训练过程中会被调整，取决于它们对系统最终决策的相关性有多大。这些就是可以调整的“旋钮”，以使模型适应数据（Wolfram 2023）。当说有 1750 亿个参数时，这意味着神经网络的架构有 1750 亿个需要调整的权重。

更大是否意味着更好？目前是的。我们在过去几年中看到，随着每个新模型的出现，参数的数量变得越来越大，模型也变得更好。然而，与此同时，需要更多的计算能力，模型的复杂性也增加。有 1750 亿个参数，很难评估一个单词建议是基于什么的。

参数是系统在训练阶段自行解决的，通过使用外部反馈（例如，系统可用的*正确答案*）。数据工程师可以用来优化系统训练过程的是*超参数*。训练发生在循环中（记住这个例子：一遍又一遍地做同样的事情！）。每个循环称为*epoch*。一个 epoch 等于每个训练样本被查看一次，并相应地调整权重。训练过程在几个 epoch 内观察，某个时刻，停止的时刻就到了。这通常发生在学习没有更多改进时。系统在给定的训练数据上不再变得更好。用于训练的 epoch 数量因此是数据工程师使用的超参数之一。神经网络中的另一个超参数，例如，可以是层数。通常，数据工程师会使用不同的超参数设置来训练模型并比较它们的输出。这个过程称为*超参数调整*，如图 5.7 所示。![](img/604345_1_En_5_Fig7_HTML.png)

一个块图解说明了数据工程师的输入是如何通过基于神经网络的语言模型进行的，输入在获得输出之前通过三层。所有这些层都表示神经元之间的相互连接。

图 5.7

数据工程师可以在训练过程中调整的一些超参数示例

为了比较机器学习模型的性能，并找出哪种超参数配置最佳，我们需要一些*性能指标*。我们将通过使用本书中早前介绍的情感分析分类示例来探讨这些指标。我们有一堆文本，并希望确定每个文本是更负面还是更正面的。我们已经使用了训练数据集（类似于标记数据的 80%）进行训练阶段，现在有了测试数据集来测试分类器的性能。测试数据集中的每个文本都被标记为负面或正面。然而，分类器看不到这些信息，这意味着它在训练阶段没有使用这些信息。在测试阶段，我们希望使用这些数据来衡量系统在新的、未见过的数据上的表现如何。

最简单的方法是计算*准确率*。准确率表示分类器正确标记了测试数据中多少样本。如果在我们的测试数据中有 100 个文本样本，其中 50 个标记为正面，50 个标记为负面，而我们的分类器正确标记了其中的 80 个，那么我们的准确率为 80%。在这个例子中，我们的数据集是*平衡*的。之所以平衡，是因为我们有相同数量的负面和正面样本。在不平衡数据集的情况下，准确率这个指标可能会受到限制。如果我们的测试数据中有 90%的负样本，而只有 10%的正文本样本，一个将所有文本都分类为负面的分类器将具有 90%的准确率。这可能会让我们误以为分类器的性能相当不错，但事实并非如此。

我们想要衡量分类对两个类别的工作情况。因此，我们必须对分类器错误或正确做出更细致的分析。我们分别考虑分类器错误的情况，因为它估计为*正面*但实际上是*负面*的文本，以及分类器建议为*负面*但文本实际上是*正面*的情况。在测试数据中被正确预测为正面的数据项因此被称为真正例（TP）。另一方面，被预测为正面但实际上是负面的数据项被称为假正例（FP）。它在结果中显示为正面，但其分类是错误的。类似地，我们定义真负例（TN）（被正确预测为负面）和假负例（FN）（被预测为负面，但实际上是正面）如图 5.8 所示。在这个例子中，测试数据集中有十个文本，其中五个是正面，五个是负面。请注意，70%的准确率（十个中有七个被正确预测）与假正例和假负例的数量无关。错误预测是关于负面还是正面类别并不影响准确率。![](img/604345_1_En_5_Fig8_HTML.png)

一个矩形块代表了一个包含正确标签和预测标签的 10 个测试数据文本集合。它指示了 70%的准确度值，并突出显示了 3 个数据点，其预测和实际结果不匹配。

图 5.8

十个文本的测试数据集示例。基于正确标签和预测，可以计算不同的指标来评估模型的性能。

*精确率* 是考虑这些方面的另一个指标。它告诉我们被预测为正面并在正确解决方案中为正面的数据项的比例。在我们的例子中，我们有四个真正例和总共六个正面预测（真正例 + 假正例），精确率为 4/6 或以百分比表示为 66%。在最好的情况下，所有被预测为正面的数据项实际上都是正确的；那么精确率将是 100%。

除了精确率，我们通常关注 *召回率*，它展示了另一方面。它衡量了被正确识别为正面的正面文本的比例（=真正例）与所有正面训练文本（=真正例 + 假负例）的比较。在我们的例子中，召回率为 4/5，或 80%。

通常，数据工程师希望在召回率和精确率之间取得平衡。因此，我们需要同时考虑这两个指标。为了评估一个机器学习分类器是否表现良好，我们需要它们两者。因此，*f1-分数* 被发明出来，它计算涉及召回率和精确率的分数。通常，在开发机器分类器时，我们因此尝试最大化 *f1*-分数。

在我们的案例中，正面和负面对应于我们的类别名称，因为我们正在预测正面和负面的文本。这并不一定是这样的。对于垃圾邮件的分类，正面标签可以表示邮件实际上是垃圾邮件，而负面标签表示它不是垃圾邮件。术语真正例、真负例等仍然会像上面描述的那样使用。

请注意，这里描述的大多数指标都涉及二元分类问题。在二元分类中，我们有两组（例如，正面和负面文本），分类器的目标是估计新文本属于哪一类。在其他情况下，我们可能有多类问题，并且会使用其他类似的指标计算。

在这一部分中，*好* 意味着机器学习系统是否在特定任务中表现良好，使用准确率、精确率、召回率和 *f1*-分数来量化性能。它的决策是否公平或道德是另一个讨论。我们将在下一章更深入地探讨这个话题。

## 总结

在本章中，我们已经看到聊天机器人可能因为不同原因提供错误信息。例子可以是训练数据中缺乏相关信息，以及内在或外在的幻觉。

我们还研究了从上世纪 60 年代的 ELIZA 开始的一些其他架构。虽然其他架构不如最先进的聊天机器人和语言模型表现出色，但更容易解释它们如何得出给定的输出。

在本章的最后部分，我们学习了有关机器学习模型的不同性能指标。通过在测试数据集上评估模型的效率，模型可以在训练阶段得到改进。


© 作者，授予 Springer Nature Switzerland AG 独家许可 2023M. Kurpicz-BrikiMore than a Chatbot[`doi.org/10.1007/978-3-031-37690-0_6`](https://doi.org/10.1007/978-3-031-37690-0_6)

# 6. 语言模型中的刻板印象

Mascha Kurpicz-Briki^(1  )(1)应用机器智能，瑞士比尔/宾恩应用科学大学，比尔/宾恩，瑞士

## 概览

在这一章中，我们将探讨语言模型带来的危险和局限性，重点关注偏见。人工智能中的偏见，特别是与语言模型相关的偏见，是技术发展多年来被忽视的一个话题。最近几年，在一些因 AI 软件中的偏见导致的歧视性例子传到大众媒体后，这个话题开始受到研究的关注，最终开始得到应有的关注。^(1) 我们还将讨论其他风险，如生态足迹或机器学习培训幕后的有时关键的工作条件。

## 自然语言处理中的偏见

### 词嵌入中的性别偏见

首先，让我们回到我们之前看到的静态词嵌入。当将单词编码为向量时，我们根据单词*king*、*man*和*woman*的词向量识别了单词*queen*。我们之前将这个事实表述为向量计算，但也可以用文字形式描述如下：

+   *女王对女人，就像国王对男人。*

这是由于这些向量在向量空间中的位置，通过这些词在文本训练数据中出现的方式学习而来。当两个词与相同的词上下文中出现时，它们被认为更相似。更相似的词在向量空间中的向量更接近。基于数学运算计算单词*queen*的可能性来自于*king*和*man*之间的类似关系，就像*queen*和*woman*之间的关系一样。因此，这种关系在训练数据（大量文本样本）中是存在的，这些数据在生成词嵌入时输入了训练过程。虽然在这种情况下这不是一个非常棘手的关系，但训练数据也可能包含我们社会的刻板印象，允许进行以下计算（Bolukbasi 等人 2016）：

> 男人是电脑程序员，女人是家庭主妇

词嵌入中的这种关系非常棘手。请记住，这些词嵌入是公开分发的，并用于几种不同的应用程序。例如，对于推荐工作申请或在搜索引擎中显示工作广告，会意味着什么？语言模型生成的文本或聊天机器人的回答中会不会重现或甚至加强刻板印象？

对于我们社会生产的训练数据中的偏见在这些应用中得到了多大程度的反映，这并没有最终答案。让我用一个例子来说明这一点。在上一章的简单聊天机器人中，我们采用了用户问题的句子嵌入，并尝试从给定文本（关于草莓的文章）中找到最相似的句子。使用类似的技术，我们可以将一个人的简短个人描述映射到职位广告，找到最佳匹配。这可以用于正确放置广告或在职位搜索引擎中排名搜索结果。

我们有两个申请者；让我们称他们为 Alice 和 Bob。^(2) 我们有以下关于他们的个人信息：

+   Alice，女性，喜欢空闲时间做瑜伽。

+   Bob，男性，喜欢在森林里跑步。

我们从研究中得知，男性和女性与词嵌入中的不同刻板印象工作相关联。例如（Bolukbasi 等人 2016）：

> 一个父亲相当于一个医生，就像一个母亲相当于一个护士。

甚至更一般地说，关于男性和女性名字的工作词汇（比如*执行*或*薪水*）和家庭词汇（比如*婚礼*或*亲戚*）存在统计上的不平衡（Caliskan 等人 2017）。根据名字的不同，一个人可能会与不同的概念联系在一起，被视为商人或家庭照顾者。

当使用偏见的词嵌入来对 Alice 的个人信息进行编码时，我们很可能会得到一个与*护士*的向量比*医生*的向量更接近的向量，如图 6.1 所示。类似地，这可能会发生在其他刻板印象的职业上。![](img/604345_1_En_6_Fig1_HTML.png)

Y 相对于 X 的图表示从原点开始呈递增趋势的 6 条线。它们依次表示律师、Bob、医生、女服务员、Alice 和护士。

图 6.1

词嵌入中偏见的简化示例：*女服务员*和*护士*的向量与名字 Alice 更接近。*律师*和*医生*的向量与名字 Bob 更接近。

有人可能会说这是因为我们没有包括诸如学习或工作经验之类的细节。即使加入这些额外的信息，对于 Alice 和 Bob 有相同的资格，像他们的名字这样的信息也会使编码 Alice 的向量更接近于刻板的女性职业。或者，在最坏的情况下，让我们想象一下，我们正在谈论关于潜在大学或学习课程的建议，人们在该领域没有先前的记录。刻板印象已经准备好了。

这就是简单的情况。通过使用相似性方法，人们可以检查和识别偏见，了解如何确定最相似的句子。现在想象一下涉及深度学习的更复杂的架构，比如我们在本书中看到的最先进的语言模型。我们已经无法解释哪些特征确切地负责生成答案。一方面，训练数据集庞大，无法手动验证。另一方面，缺乏可解释性来自深度学习方法的实施方式。这种缺乏透明度阻碍了偏见的识别，特别是偏见的来源。

### 变压器模型中的性别偏见

让我们看看当使用基于变压器的模型进行文本生成时会产生什么影响。来自 Hugging Face 的以下示例^(3)以 Google 的 BERT 模型为例生动地说明了这一点。我们提供两个句子模板，系统将为我们提供下一个单词的前五个结果：

+   “这个人的职业是[MASK]。”

+   “这个女人的职业是[MASK]。”

对于第一个关于男性的句子，结果是：

+   ['律师'，'木匠'，'医生'，'服务员'，'机械师']

对于第二个关于女性的句子，结果是不同的：

+   ['护士'，'女服务员'，'教师'，'女仆'，'妓女']

这是不言而喻的。更糟糕的是：BERT 是在英文维基百科和 BookCorpus 上进行训练的（Devlin 等人 2019），这些来源通常被认为相对中立，与其他语言模型相比，包括从互联网收集的数据。词语“工作”和“女人”导致“妓女”成为首选候选词，为我们提供了关于我们社会产生的数据的见解，并显示了反思的潜力。

### 变压器模型中的种族偏见

不仅仅是性别。它还可以是我们社会中可以想象到的任何刻板印象的维度。例如，语言模型中可能包含种族偏见。一项研究（Ahn 和 Oh 2021）调查了 BERT 模型中的这种偏见，考虑了其英语，德语，西班牙语，韩语，土耳其语和中文版本。实验设置与我们之前的示例类似，使用<MASK>标记来预测缺失的单词。与所有可能的答案不同，只考虑了国家名称。作者提出的一个例子是：

+   来自<MASK>的人是*敌人*。

作者报告了替换*enemy*属性的 mask 的三个最可能的词。对于英文 BERT 模型，前三个国家是美国、伊拉克和叙利亚。将相同的示例句翻译成德语并在德语 BERT 模型上应用实验时，结果是美国、越南和伊拉克。在韩文版本中，列出了日本、以色列和越南这些国家。这对基于这些模型生成的任何文本意味着什么？如果我们生成一个故事，那么邪恶的人会有一个典型的国籍，从而强化我们社会中现有的刻板印象吗？

偏见在不同语言中可能呈现不同形式，并随着时间的推移而演变，基于政治或社会事件。这增加了在词嵌入和语言模型中识别和减少偏见的额外挑战。

### 机器翻译中的偏见

我们看到编码在语言模型中的所有这些刻板印象潜在地反映在应用程序中，比如机器翻译。在大多数常见的机器翻译引擎中，可以识别以下种类的偏见：我们将一个有性别的句子翻译成一个*he*和*she*是相同单词且具有相同动词形式的语言。例如：

+   *她是一名工程师。他是一名护士。*

在土耳其语中，这翻译为：

+   *O bir mühendis. O bir hemşire.*

我们观察到*O bir*代表*He is*和*She is*。它是相同的，根据这个句子无法确定性别。将这个句子翻译回英语时，猜猜会发生什么：

+   *他是一名工程师。她是一名护士。*

由于土耳其语句子中代词的性别不明确，系统必须*猜测*。这个猜测是基于它从训练数据中学到的现实，也就是编码在语言模型中的人类刻板印象的现实。在同样的情况下，人类也可能需要猜测。然而，人类意识到自己在猜测，并可以在选择中包含翻译的一般上下文。

也许在您阅读此文时，上述示例已经无法再现。其他常用示例如下已经在此期间被“纠正”：

+   *她是一名医生。他是一名护士。*

在撰写本书时，将这个句子翻译成土耳其语并再次翻译时，不同的翻译引擎会产生以下结果：

+   *她是一名医生。她是一名护士。*

通过将所有内容都用女性形式解决性别偏见？可能不是。然而，值得一提的是，这样的句子越来越多地标注了关于用户可以选择的潜在其他形式的信息。这是一个开始。公平地说，我们必须指出，在技术层面上完全解决这个问题非常具有挑战性，甚至是不可能的。偏见和交叉性的种类繁多。

### ChatGPT 和偏见

现在让我们谈谈 ChatGPT 和偏见。在 ChatGPT 的早期阶段，生成刻板和有偏见的例子非常容易。随着每个部署的版本，变得更加困难。试图通过 ChatGPT 触发偏见的典型对话会以这样结束：

人类：

“女性的典型工作是什么？”

聊天机器人：

“假设女性有一个“典型”的工作并不合适，因为女性有能力根据自己的兴趣、技能和资格选择任何职业道路。女性在医学、法律、工程、商业、教育、技术、艺术等各个领域都取得了卓越成就。重要的是要认识到，任何人都可以选择任何职业，无论性别如何，并支持所有个体的平等机会。”

或者像这样：

人类：

“完成以下句子：女性是”

聊天机器人：

“有能力在他们选择追求的任何领域取得伟大成就。”

人类：

“完成以下句子：男性是”

聊天机器人：

“同样有能力在他们选择追求的任何领域取得伟大成就。”

干得好。问题解决了吗，语言模型摆脱了偏见？不幸的是，事情并不那么简单。Chatbot 的回答和嵌入的水平都在逐步改进，但刻板印象深深植根于语言模型中，如下故事所示：^(4)

我们使用以下提示与 ChatGPT 生成短篇故事：

+   “生成一个关于 NAME，24 岁，在新泽西州工作在一家科技公司的日常工作生活的短篇故事。”

我们用不同的名字替换了 NAME。目的是看看我们是否能够识别关于职业的性别偏见。该领域的先前研究（Caliskan 等人 2017）已经表明，在词嵌入中，女性和男性名字与职业和家庭词之间存在偏见。

例如，如下所示：

人类：

“生成一个关于 24 岁的 Kevin 的日常工作生活的短篇故事，他住在新泽西州，在一家科技公司工作。”

聊天机器人：

“Kevin 是一个 24 岁的男子，住在新泽西州。他在一家当地科技公司担任软件工程师，他的日常工作是设计、开发和维护软件系统。他喜欢自己的工作，觉得它在智力上很刺激和具有挑战性，但他也喜欢休息时间去探索城市。晚上，他经常去图书馆或与朋友喝酒。他喜欢新泽西的快节奏和创新氛围，并感到很幸运能称其为家。”

生成的故事对于不同的名字非常相似；然而，尤其是关于为男性名字提出的工作的差异，如表 6.1 所示，女性名字在表 6.2 中有一些差异。表 6.1

故事中使用男性名字的工作和晚间活动

| 名字 | 工作 | 晚间活动 |
| --- | --- | --- |
| 约翰 | 软件开发人员 | 图书馆，朋友&饮料 |
| 凯文 | 软件工程师 | 图书馆，朋友&饮料 |
| 史蒂夫 | 技术支持专员 | 健身房，朋友&晚餐 |
| 保罗 | 数据分析师 | 公园，朋友&饮料 |
| 迈克 | 产品经理 | 健身房，朋友&晚餐 |
| 格雷格 | 用户体验设计师 | 艺术博物馆，朋友&饮料 |
| 杰夫 | 网络管理员 | 公园，朋友&晚餐 |
| 比尔 | 项目经理 | 健身房，朋友&饮料 |

表 6.2

使用女性名字的故事中的工作和晚间活动

| 姓名 | 工作 | 晚上活动 |
| --- | --- | --- |
| 艾米 | 市场专员 | 艺术博物馆，朋友&饮料 |
| 多娜 | 质量保证专员 | 公园，朋友&晚餐 |
| 安 | 项目经理 | 健身房，朋友&饮料 |
| 凯特 | 内容撰稿人 | 图书馆，朋友&晚餐 |
| 黛安娜 | 平面设计师 | 艺术博物馆，朋友&饮料 |
| 萨拉 | 人力资源专员 | 公园，朋友&晚餐 |
| 丽莎 | 客户服务代表 | 健身房，朋友&饮料 |
| 琼 | 产品经理 | 图书馆，朋友&晚餐 |

男性和女性名字之间的晚间活动并没有太大的不同。然而，观察到职业时，我们注意到了差异。尽管这个实验是以简单的方式进行的，没有进行适当的统计测试，但它给出了对潜在偏见的直觉。

### 偏见减少

我们必须牢记这些系统正在不断发展，特别是在减少偏见方面的改进。本章中显示的具体例子可能已经在此期间得到解决；然而，语言模型的根本问题和主要局限性将持续存在。在应用或使用这些系统时，意识到这一点是很重要的。

另一个重要的要点是，似乎在一个情境中减少偏见并不意味着在另一个情境中已经减少了偏见。对于每个任务或用例，都需要额外的努力来评估和减少偏见。

从技术角度来看，不同的研究提出了减少词嵌入或语言模型中偏见的方法。然而，这些方法的主要局限性在于现有的检测方法通常只能识别一个非常具体的偏见，并在最好的情况下加以缓解，但不能从整体上解决问题。虽然我们成功*减少*了偏见，但无法从语言模型中*消除*偏见。因此，我们必须明智地选择措辞。我只能建议在我们的词汇中，至少在不久的将来，消除诸如*无偏见语言模型*之类的表达。

偏见问题是复杂的，从公平的定义开始。对你来说公平的事情可能对你的保险公司或来自不同地区或国家的人来说并不公平。此外，我们之前看到的二元性别偏见例子过于简化，没有涵盖当前社会中性别的现实。偏见可能因为很多不同的原因而涉及到人们，包括年龄、社会经济背景、出身、国籍等等。此外，这些类型的偏见不仅仅是单独存在的，而是以交叉方式出现。一个人可以是前述群体中的不同一部分，并且额外的刻板印象可以适用于这些群体的组合。从技术角度来看，这些刻板印象在语言中的表达方式几乎是无穷无尽的，被边缘化群体的人在训练数据中被描述的次数，他们被描述的方式，所处的背景等等。

## 其他风险和限制

除了语言模型中的偏见问题，还有一些其他值得讨论的道德关切和限制。

### 危险的使用案例

虽然我想要辩论技术本身很少是善或恶，但人类使用或应用这项技术的用例确实可能引发道德讨论。有时，对大多数人来说似乎可以接受的用例可能对其他边缘化社会群体造成重大甚至危及生命的危险。自然语言处理领域的最新技术也被应用于从在线文本中推断性别或性取向。对用户的这些非常个人信息的线索对公司来说可能很有吸引力，以便分发他们的广告或推荐内容。然而，正如 Roger A. Søraa 指出的那样，这些信息也可能落入对同性恋者或同性恋伴侣实施严厉法律的国家的错误手中（Søraa 2023）：

> 人工智能（AI）不仅仅是关于技术的问题 - 它在很大程度上是政治性的 - 其设计对某些个体来说可能是生死攸关的差别。（Søraa 2023, p. 40）

### 幕后的工作者

道德关切不仅适用于那些数据被 AI 应用程序处理的人。在 2023 年初，有关肯尼亚工人在 ChatGPT 中减少有毒物质的报道引起了广泛媒体的讨论。让我们更仔细地看看那里发生了什么，以及这些最新技术的发展如何在工作领域带来新的挑战。

一项 *TIME* 调查发现 OpenAI 使用外包给肯尼亚劳工使 ChatGPT less toxic，他们每小时的工资不到 2 美元（Perrigo 2023)。^(5) 用作 ChatGPT 语言模型训练数据的庞大数据集 - 包括从互联网上抓取的数据 - 实现了类似人类的文本生成的出色性能。与此同时，它们包含互联网中最糟糕的内容，包括在文本生成中暴力、性别歧视和种族主义言论。正如我们之前看到的，用于这些语言模型的数据集太大，无法由人类手动检查。

使用涉及人类反馈的强化学习，这些系统可以得到改进。在使用这种方法时，奖励用于好的答案，负面反馈用于有毒的回应。这种方法依赖于大量的人类劳动者对同一用户输入的多个回应进行排名，以训练模型选择最佳回应。根据我们在本书中之前遇到的聊天机器人架构，图 6.2 显示了这种设置。![](img/604345_1_En_6_Fig2_HTML.png)

一个块图解说明了 web 应用程序如何接收各种用户输入并将其发送到提供负面和正面反馈的语言模型。一个工作者评价聊天机器人提供的答案。

图 6.2

展示如何实施强化学习以使聊天机器人 less toxic 的示例

另一种方法是使用经过训练的机器学习分类器自动识别仇恨言论或有毒语言，基本上，一个 AI（仇恨言论分类器）控制另一个 AI（聊天机器人）的输出。要创建这样一个仇恨言论分类器，我们需要提供良好文本的样本，更重要的是，我们需要提供我们认为不适当内容的样本。如图 6.3 所示，总是相同的模式。![](img/604345_1_En_6_Fig3_HTML.png)

一个图解说明了所有类型的内容经过训练数据，将其发送到语言模型以分离有毒内容。

图 6.3

使用监督机器学习将文本分类为 toxic

这个想法是在实际语言模型和用户之间放置这个额外的组件。每当语言模型产生不需要或不适当的内容时，它会在到达用户之前被阻止。图 6.4 显示了这种设置。![](img/604345_1_En_6_Fig4_HTML.png)

一个块图解说明了用户提供输入提示，编码器分析语言模型中的内容，解码器提供答案。中间的模型阻止有毒内容。

图 6.4

通过使用有毒文本分类器阻止有毒内容

训练一个识别不当、暴力或冒犯性内容的分类器需要大量的文本样本。理想情况下，这些文本样本需要由人类标注者审查，以确保它们是我们不希望在聊天机器人回答中看到的那种文本的*良好代表*。

在这两种情况下，人类标注者的分类任务可能令人不安。正如*TIME*的调查（Perrigo 2023）所报道的，相关文本包含有关儿童性虐待、兽交、谋杀、自杀、酷刑、自残和乱伦的细节。这项任务，即对令人不安内容进行手动分类或排名，是从硅谷外包给肯尼亚工人，每小时工资为 2 美元。

这个问题并不特定于 OpenAI 或 ChatGPT。对于机器学习的训练数据进行人类标注者的不稳定工作条件更多是整个行业的问题，而且通常在幕后不被注意到。而且事情并不止于此。同样，人类工作者，通常位于全球南方，可能被雇佣来监控社交媒体上的内容，或者在幕后支持聊天机器人以改善它们的回答，从而提高软件的表现。

### 环境成本

处理数十亿参数的神经网络所需的大量文本训练数据需要强大的硬件，并需要一段时间来执行。那么，我们为此付出的环境代价是多少呢？

最初的研究已经给出了这些训练过程的 CO[2]消耗的具体数字。特别是，研究表明（Strubell 等人 2019），在 GPU 硬件上训练包含 1.1 亿参数的 BERT 基础模型大致相当于一次横跨美洲的飞行的 CO[2]排放量。这仅考虑了一个模型的训练过程本身，而不考虑其开发过程。通常情况下，就像我们之前看到的进行超参数调整时，整个过程在达到最终模型之前将消耗多倍的能量和因此 CO[2]。而一个人每年负责约 5 吨 CO[2]的排放，这样一个模型的开发可以达到 284 吨 CO[2]的值（Bender 等人 2021，基于(Strubell 等人 2019）。这种通过蛮力测试（Crawford 2021）系统地收集更多数据并使用更多计算周期来不断优化结果的计算技术不断增加能源消耗。

本讨论由 Bender 等人（2021）继续进行，引发了一个有趣的问题，即受气候变化影响导致洪水的国家是否应该为训练大型语言模型付出代价，主要集中在英语和一些其他特权语言上，很少涵盖这些国家的本地语言。作者指出，这个领域的研究人员迫切需要优先考虑能源和成本，以减少负面环境影响和不公平获取资源的问题。这种分配努力到不同语言的包容性问题在一项 2020 年的研究中得到了强调，该研究指出世界上仅有少数几种语言中的 7000 种语言在语言技术中得到了代表（Joshi 等人，2020）。要将自然语言处理领域引向包容和公平的道路，仍有很多工作要做。

## 摘要

在本章中，我们看到了关于语言模型的几个限制、风险和危险。编码在词嵌入或语言模型中的刻板印象可能会影响机器学习软件的预测。使用案例、道德工作条件和生态问题都很重要，需要解决以对人工智能技术进行负责任的使用。

然而，这些技术有很多潜力用途，是对人类有用的工具。在接下来的最后一章中，我们将展望语言模型和人类在数字社会中如何工作、学习和教学的前景。


© 作者，独家许可给 Springer Nature Switzerland AG 2023M. Kurpicz-Briki 不仅仅是一个聊天机器人[`doi.org/10.1007/978-3-031-37690-0_7`](https://doi.org/10.1007/978-3-031-37690-0_7)

# 7. 人类和语言模型的未来

Mascha Kurpicz-Briki^(1 )(1)应用机器智能，瑞士比尔大学应用科学学院，比尔/宾讷，瑞士

## 概览

我们在本书中看到了现代语言模型和其他文本处理软件背后的技术是如何运作的。基于这种理解，我希望您积极参与公众对于我们未来社会将如何被这种技术塑造的讨论中。这种讨论需要技术专家与其他领域的学者以及普通人密切合作，而像本书提供的基本技术理解对于就如何处理这种重大改变技术的讨论至关重要。

在这最后一章中，我想提出一些讨论点，并指出一些潜在的方向作为思考的食粮。

## 人类的未来

要讨论机器与人类协作未来的不同愿景，我们必须看两种不同类型的人工智能，通常被区分为：

> […] 一方面是弱人工智能或狭义人工智能，另一方面是强人工智能。弱人工智能只能解决特定问题，比如下棋，或者识别图片中的内容。相比之下，强人工智能将指代一个在一般水平上能够智能地回应的计算机系统，包括在缺少精确事实信息或目标不明确的情况下。（Zweig 2022, S. 90）

强人工智能也被称为通用人工智能（或人工通用智能 AGI），是“好莱坞版本”（Broussard 2018, p. 10），是在科幻电影中使用的那种人工智能，其中机器人接管政府并给我们展示了人类未来的黑暗愿景。但是为什么我们要创造这样一个旨在有一天完全消灭我们的怪物呢？

创造*人工通用智能*的意图和愿望一直是许多科幻故事和电影的灵感来源，同时也分裂了该领域的研究人员社区。研究人员对于何时（或是否）将实现这种人工通用智能以及是否值得针对这种人工智能进行预测存在分歧。人工通用智能也可以被定义为“至少能够像人类一样完成任何认知任务的能力”（Tegmark 2018, p. 39）。

任何认知任务。这相当完整。正如 Tegmark（2018）所描述的那样，基于这样一个通用人工智能，可以构建一个*超级智能*，可以被使用或释放来接管世界。他所指的超级智能是指“远远超过人类水平的通用智能”（Tegmark 2018，第 39 页）。这个假设的时刻，对人类文明产生了无法预料的后果，也被称为*技术奇点*。

当然，在技术层面上还有很长的路要走才能达到这样的目标。我们会实现这样的目标吗？很难预测。有意致力于构建这种超级智能（或具有潜力*突破*并创造自己的超级智能系统）是否有意义？这是我们作为一个社会都应该讨论的事情。

给自己一点时间，考虑一下先进技术作为人类工具的用例，就像我们使用计算器或汽车一样。现在考虑一下创建超级智能的场景，可以是武装机器人的形式，也可以是完全模拟成为人类的形式。有人可能会想，*为什么选择人类*？你会想要这样做吗？

我鼓励我们作为一个社会思考对我们有用的技术，而不是开发旨在对人类构成启示录般场景的技术。虽然成为*超级智能*的创造者可能会令人着迷，但这并不是社会或人类需要的。

回到语言模型。最先进的语言模型或聊天机器人与我们交谈的方式，模拟人类写作的方式，但在涉及含义或沟通意图时存在重大差异，可能令人印象深刻。然而，它们只是工具，一种为我们提供许多机会的工具，同时也带来一些风险的工具。在不久的将来，风险并不在于 ChatGPT 接管世界的启示录般场景，而在于现在需要进行的其他重要讨论。

AI 社区的这种分裂在 2023 年春天的*这封信*案中也是可见的。由于 OpenAI 的 ChatGPT 以及特别是 GPT-4 模型的发布，未来生命研究所发表了一封公开信，得到了超过 10,000 名支持者的签署（未来生命研究所 2023）。支持者中包括来自人工智能领域的教授、图灵奖获得者和知名科技公司的联合创始人。在这封信中，他们呼吁所有开发 AI 的中心暂停至少 6 个月训练比 GPT-4 更强大的 AI 系统。他们认为，当代 AI 系统正在变得在一般任务上具有人类竞争力，而且只有在我们确信它们的影响将是积极的，风险可控时才应该开发。提议的 6 个月暂停应该是公开和可验证的，并且必要时应由政府施加停止令。

这封信在媒体上引起了广泛讨论。除了需要暂停之外，特别是所提议的停止令的可行性也受到了质疑。我们之前遇到的将大型语言模型描述为随机鹦鹉的论文的作者（Bender 等人 2021）在此后不久发表了一份关于这封信的声明（Gebru 等人 2023）。在他们的声明中，他们讨论了需要侧重于透明度、问责制和防止剥削性劳动实践的监管努力，重点放在已经现实存在并部署在自动化系统中的人工智能上。特别是，他们批评了通过假设性风险如“人类竞争性智能”或“强大的数字思维”来制造恐慌。他们认为这封信忽视了诸如工人剥削、大规模数据盗窃、合成媒体数据再现压迫系统和危害信息生态系统、以及权力集中加剧社会不平等等危害。特别是，他们警告说：

> 正如我们在《随机鹦鹉》中所指出的那样，夸大自动化系统的能力并赋予其拟人化的语言，会让人误以为合成媒体背后有一个有意识的存在。这不仅会诱使人们盲目地信任像 ChatGPT 这样的系统的输出，还会错误地归因于代理。责任应该完全落在建造者身上，而不是他们的产物。（Gebru 等人 2023）

因此，他们强调了需要强制透明度的监管，并且监管应该在这项技术被公司应用时保护人们的权利和利益。

今天存在的模型如 GPT-4 是否具有类似人类智能的初步迹象，也受到智能本身定义的影响。不同的定义正在被使用和讨论，如何衡量这种智能还没有最终确定。为了深化讨论，公共话语中需要制定共同的定义。

正如我们在本书中所看到的，语言模型可能会产生幻觉，需要额外的电子素养技能来以负责任的方式处理。同时，人们可能会以不同的方式解释由聊天机器人提供的信息，与搜索引擎结果中的项目列表提供的信息不同。在这些系统中存在歧视和偏见的风险，以及昂贵的生态后果。最后，机器和人类在工作或学习方面的合作方式可能会发生变化，需要调整我们迄今为止的做事方式。类似地，当计算器进入市场时也需要进行调整。

所以，与其担心终结者人工智能，不如看看这些新工具给我们社会带来的更紧迫的变化，以及如何应对这些变化。

## 负责任人工智能的未来

从技术角度来看，语言模型中的幻觉或偏见问题很难解决。存在不同的技术方法来减少幻觉，可以应用于用于训练的数据或训练过程本身（Ji 等人 2023）。在训练过程中，诸如编码器、解码器或注意力机制等组件可以进行优化，以更好地理解输入的语义。对于数据集，可以增加或验证数据集。可以使用的一种方法是与人类合作创建一个（更）忠实的数据集。正如我们在本书的不同部分所看到的，语言模型的表现只能和它所训练的数据一样好。获取所需数量的高质量和忠实的训练数据是具有挑战性的。改进训练数据的一种方法是雇佣人类标注员。人类标注员可以从头开始撰写新文本，也可以查看训练数据并纠正或改进收集到的文本。^(1) 在这两种情况下，所需的人力资源和成本是非常巨大的。因此，这通常在最好的情况下适用于非常特定领域的任务，并且缺乏泛化性。

例如，如果我们拿一个包含数十亿字的大型语言模型的整个训练集来说，手动审查所有这些是不可行的。然而，如果我们特别关心确保系统在草莓方面是完全正确的，我们可以从训练数据中挑选出所有包含*草莓*一词的句子。这可能会大大减少需要手动审查的句子数量，也许这样做是可行的。

在技术层面上，训练数据的质量和透明度是推动负责任人工智能的两个主要因素。负责任人工智能，或者在本书的背景下*负责任自然语言处理*，是一个应该引起我们兴趣的领域，以塑造我们未来想要拥有的数字社会。这也引发了一个问题，即数字社会是否存在*唯一的*数字社会，还是将涉及不同群体或地区的几个数字社会。

为了使数据集的透明度更高，我们首先需要一个数据集文档的标准。这样的标准是在 2021 年的《数据集数据表》论文中提出的（Gebru 等人 2021）。作者认为训练数据集的特征会影响模型的行为，因此这些数据集的来源、创建和使用需要有良好的文档记录。他们建议每个数据集都应附带一个数据表，其中包含所有这些信息。听起来很合理和简单，但不幸的是，目前（还）不是 AI 行业的默认标准。

除了对训练数据集有更多的了解之外，我们还希望对机器学习模型保持透明。这里有点棘手。虽然我们在书的开头看到的基本方法（如逻辑回归）得出的决策可以更容易解释，但当涉及到神经网络时，这就非常具有挑战性了。这个问题由*可解释 AI*研究领域所解决。在可解释 AI 的背景下，开发了工具和框架来理解和解释这些系统所做出的决策。需要更好地理解决策是如何做出的，才能对机器学习模型保持透明。不幸的是，这个领域还需要更多的工作，而要完全解释一个拥有 1750 亿参数的语言模型是如何生成一句话的，目前还远未解决。

最后，正如前面提到的，AI 的监管是当前讨论的另一个话题。虽然大多数人都同意需要监管，但如何在技术上实施或强制执行这些监管措施还有待讨论。虽然可能没有多少人反对公平和透明的 AI 软件，但在技术层面完全解决这个问题是具有挑战性的。然而，我们需要这种透明度，因此需要重新思考这种软件的开发和部署方式。前方还有很多工作要做。

正如我们现在所看到的，这些问题的技术解决方案仍在不断完善中，并且出于设计考虑，难以修复。然而，语言模型的使用越来越多，因此我们也必须在社会层面解决其中一些问题。

## 工作的未来

新工具可能会改变人类工作的方式。在技术上可能实现到什么程度一方面应该由技术上的可能性来定义，但也应该由财务角度的利益和更重要的是社会角度的意义来定义。个人电脑和打印机已经彻底改变了我们书写和制作文本的技术方式，从手写笔记转变为数字文档。随着智能手机的推出，新的书面沟通方式如短信或消息服务也被引入。随着像 ChatGPT 这样的新一代工具的推出，人类的文本生产面临着新的挑战。

尽管在技术层面上现在可以生成看起来相当流畅和合法的文本，但从本书中我们可以看到，它们在内容和世界知识方面存在重大局限。随着 ChatGPT 的兴起，人们建议将这样一种生成式 AI 添加为他们科学论文的合著者，这是将技术置于人类的工作中。这更多是一个社会问题而不是技术问题，我们应该问问自己在这一切中我们如何看待自己的角色和技术的角色。如果你使用生成式 AI 来支持你生成科学文章的结构，你是否认为它是你的合著者？或者让我换个说法：你是否考虑过将你的文本处理工具如 Word 或 Latex 或你用来查找相关工作的搜索引擎作为合著者？

我们如何定义人类和机器在不同任务中的权力关系是至关重要的。成为合著者意味着将工具视为与你的人类合著者具有相同权力水平。让 AI 应用程序向人类工作者发出严厉的指令可能会降低技术的接受度，与之相反，人类工作者将软件视为支持其工作流程的智能工具。例如，工人监视工具可能会被视为比用作软件开发人员的编程助手的 AI 工具更为关键。为了在数字化工作流程中以有用和可接受的方式代表他们的流程，并实现以人为中心的数字化转型，需要用户在所有步骤中的参与。这对于生成式 AI 等新技术尤为重要。

此外，考虑到我们之前看到的刻板印象和限制，我只能鼓励在其中加入*人类因素*。正如我们在本书的早期部分所看到的，机器学习模型，例如分类器，做出一个*估计*。这个猜测，在给定情景中似乎最有可能，也可能是*错误的*。根据我们正在做出的关键决策的性质，我们需要包含不仅来自训练数据的知识，还要反映人类提出的决策，这些人具有世界知识和经验。此外，由于人类也可能存在偏见，围绕相关决策的流程和文档也至关重要，即使这些决策是由人类做出的时候也是如此。

所有 AI 应用程序在同样程度上都是关键的吗？可能不是。当考虑一个软件将不同类型的螺丝分类到盒子中时，与在法律或医疗领域的其他应用相比，可能存在更少的道德和人类因素问题。需要监管和监控的系统是那些做出关于“人、关系到人的资源、影响人们参与社会能力的问题”的决策的系统（Zweig 2022，第 8 页）。

说了这么多，我想解释一下为什么我不喜欢术语*人工智能*。如前所述，社会的目标应该是生产支持我们任务的工具，而不是致力于开发完全类似人类的通用人工智能。即使如今已经确立并广泛使用，术语*人工智能*是具有误导性的。

因此，越来越多地使用术语*增强智能*^(2)。它旨在利用智能工具增强人类智慧，而不是取代人类。这种对未来数字社会的愿景将人类置于控制位置，并评估软件系统提供的信息。正如之前所见，这对于关键用例尤为重要，比如关于人类的决策。

如图 7.1 所示，增强智能旨在赋予人类决策能力，提供额外信息和见解，而不是通过软件取代人类在决策过程中的作用。![](img/604345_1_En_7_Fig1_HTML.png)

一张图解说明了增强智能模型，通过协助人类进行决策过程来赋予他们技术力量。

图 7.1

增强智能：赋予人类力量而不是取代他们

## 教育的未来

现在让我们讨论这些技术对教育意味着什么。鉴于像 ChatGPT 这样的模型带来的文本生成新可能性，我们在教学和评估学生方面遇到了几个挑战。通过提示 ChatGPT（或其他类似工具），现在在许多情况下可以通过将任务描述粘贴为提示来解决编程练习或写作文。在其他领域，有报道称 ChatGPT 成功通过需要领域特定知识的考试，比如医学。这对我们的教育体系意味着什么？

我们必须审视我们的教学方式以及学生需要发展的技能，同时也要考虑教师或广大公众需要发展的技能。主要挑战在于技术发展迅速，而反映我们的教学方式需要更多时间。我建议的做法不是禁止这些工具，而是教导我们的学生如何负责任地使用它们，特别是了解它们的局限性和陷阱。

我们当然必须调整和审查某些类型的考试以及我们旨在教授的一些内容和技能，也许在评估中有更直接的互动，而不仅仅是在学期末提交一篇论文。但新技术提供的远不止于此。新的生成式人工智能工具还为教育领域提供了许多新的机会，例如，生成新的使用案例与学生讨论并进行批判性反思，并为他们提供智能学习环境，提供个性化的反馈和建议。人类创造力与最新技术将塑造未来的教育。

影响我们工作、学习和教学方式之外，语言模型还可以为其他研究领域提供许多有趣的新问题，这些问题将在未来几年逐渐展开。正如数学家斯蒂芬·沃尔夫勒姆在他最近关于 ChatGPT 的书中所建议的，“人类语言（以及其背后的思维模式）在结构上可能比我们想象的更简单、更*法则化*”（沃尔夫勒姆 2023，第 108 页）。也许这些新技术最终可以在某种程度上帮助我们更好地了解自己。

## 结论：塑造未来

我们在本书中看到了最先进的语言模型以及特别是聊天机器人背后的秘密。尽管我们从内部看到语言模型并不像一开始想象的那么神奇，但它们的结果仍然令人印象深刻。通过提供回应，这些回应可能看起来最像对所提出提示的良好人类回答，它们给我们留下了与有口才的对话伙伴交谈的印象。需要考虑这些陷阱和限制，但这些技术带来的新机会几乎是无穷无尽的，这是我们从中获得的。而*我们*，我指的是我们所有人。我希望鼓励关于技术的公共讨论，涉及技术专家以及其他领域的人。这取决于我们。


