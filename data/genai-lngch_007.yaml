- en: 6 Developing Software
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 开发软件
- en: Join our book community on Discord
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们在Discord上的书籍社区
- en: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
- en: '![Qr code Description automatically generated](../media/file42.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的二维码描述](../media/file42.png)'
- en: 'While this book is about integrating generative AI and, in particular, large
    language models (LLMs) into software applications, in this chapter, we’ll talk
    about how we can leverage LLMs to help in software development. This is a big
    topic and software development was highlighted in reports by several consultancies
    such as KPMG and McKinsey as one of the domains impacted most by generative AI.We’ll
    first discuss how LLMs could help in coding tasks, and we’ll go through a lot
    of literature as an overview to see how far we have come in automating software
    engineers. We’ll also discuss a lot of the recent progress and new models. Then,
    we’ll play around with a few models evaluating the generated code qualitatively.
    Next, we’ll implement a fully-automated agent for software development tasks.
    We go through the design choices and show a bit of the results that we got in
    an agent implementation of only a few lines of Python with LangChain. There are
    a lot of possible extensions to this approach, which we’ll also go through.Throughout
    the chapter, we’ll work on different approaches to software development, which
    you can find in the `software_development` directory in the Github repository
    for the book at [https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain)The
    main sections are:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这本书是关于将生成式人工智能，特别是大型语言模型（LLMs）集成到软件应用程序中，但在本章中，我们将讨论如何利用LLMs来帮助软件开发。这是一个重要的主题，软件开发被几家咨询公司如KPMG和麦肯锡的报告所强调，是受生成式人工智能影响最大的领域之一。我们首先讨论LLMs如何帮助编码任务，并概述我们在自动化软件工程师方面取得了多大进展。我们还将讨论许多最新进展和新模型。然后，我们将尝试几种模型，定性评估生成的代码。接下来，我们将实现一个完全自动化的软件开发任务代理。我们将讨论设计选择，并展示我们在LangChain中仅用几行Python代码实现的代理实现的一些结果。这种方法有许多可能的扩展，我们也将讨论。在整个章节中，我们将探讨软件开发的不同方法，您可以在书籍的Github存储库中的`software_development`目录中找到。主要部分包括：
- en: Software development and AI
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 软件开发和人工智能
- en: Writing code with LLMs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LLMs编写代码
- en: Automated software development
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化软件开发
- en: We'll begin the chapter by giving a broad overview over the state-of-the-art
    of using AI for software development.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从使用人工智能进行软件开发的最新技术概述开始本章。
- en: Software development and AI
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软件开发和人工智能
- en: 'The emergence of powerful AI systems like ChatGPT has sparked great interest
    in using AI as a tool to assist software developers. A June 2023 report by KPMG
    estimated that about 25% of software development tasks could be automated away.
    A McKinsey report from the same month highlighted software development as a function,
    where generative AI can have a significant impact in terms of cost reduction and
    efficiency gain. The idea of utilizing artificial intelligence to aid programming
    is not new, but has rapidly evolved alongside advances in computing and AI. The
    two areas are intertwined as we’ll see.Early efforts in language and compiler
    design the 1950s and 60s sought to make it easier to write software. Data processing
    languages like **FLOW-MATIC** (also known as: **Business Language version 0**),
    designed under Grace Hopper at Remington Rand in 1955, generated code from English-like
    statements. Similarly, programming languages such as **BASIC** (**Beginners’ All-purpose
    Symbolic Instruction Code**), created at Dartmouth College in 1963, aimed to make
    it easier to write software in an interpreted environment.Other efforts further
    simplified and standardized the programming syntax and interfaces. The **flow-based
    programming** (**FBP**) paradigm, invented by J. Paul Morrison in the early 1970s,
    allows to define applications as connected black box processes, which exchange
    data by message passing. Visual low-code or no-code platforms followed in the
    same mold with popular proponents such as LabVIEW, extensively used for system
    design in Electronical Engineering, and the KNIME extract, transform, load tool
    for data science.Some of the earliest efforts to automate coding itself through
    AI were **expert systems**, which emerged in the 1980s. As a form of narrow AI,
    they focused on encoding domain knowledge and rules to provide guidance. These
    would be formulated in a very specific syntax and executed in rule engines. These
    encoded best practices for programming tasks like debugging, though their usefulness
    was constrained by the need for meticulous rule-based programming.For software
    development, from command line editors such as ed (1969), to vim and emacs (1970s),
    to today’s integrated development environment (IDEs) such as Visual Studio (first
    released in 1997) and PyCharm (since 2010), these tools have helped developers
    write code, navigate in complex projects, refactor, get highlighting and setup
    and run tests. IDE’s also integrated and provide feedback from code validation
    tools, some of which have been around since the 1970s. Prominently, Lint, written
    by Stephen Curtis Johnson in 1978 at Bell Labs can flag bugs, stylistic errors
    and suspicious constructs. Many tools apply formal methods; however, machine learning
    has been applied including genetic programming and neural network based approaches
    for at least 20 years. In this chapter, we’ll how far we’ve come with analyzing
    code using deep neural networks, especially transformers.This brings us to the
    present day, where models have been trained to produce full or partial programs
    based on natural language descriptions (in coding assistants or chatbots) or some
    code inputs (completion).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 强大的AI系统如ChatGPT的出现引发了人们对将AI作为辅助软件开发人员的工具的极大兴趣。根据KPMG在2023年6月发布的一份报告，大约25%的软件开发任务可以自动化处理。同月的麦肯锡报告强调了软件开发作为一个领域，在这个领域，生成式AI可以在成本降低和效率提升方面产生重大影响。利用人工智能来辅助编程的想法并不新鲜，但随着计算和人工智能的进步，这一想法迅速发展。正如我们将看到的那样，这两个领域是相互交织在一起的。20世纪50年代和60年代早期的语言和编译器设计工作旨在使软件编写更加容易。1955年，由Grace
    Hopper在雷明顿兰德公司设计的数据处理语言**FLOW-MATIC**（又称：**商业语言版本0**）从类似英语的语句中生成代码。同样，20世纪60年代在达特茅斯学院创建的**BASIC**（**初学者通用符号指令代码**）等编程语言旨在使在解释环境中编写软件更加容易。其他努力进一步简化和标准化了编程语法和接口。20世纪70年代初由J.
    Paul Morrison发明的**基于流程的编程**（**FBP**）范式允许将应用程序定义为连接的黑盒进程，通过消息传递交换数据。视觉低代码或无代码平台也遵循了相同的模式，其中LabVIEW等流行的支持者被广泛用于电子工程中的系统设计，而KNIME提取、转换、加载工具则用于数据科学。20世纪80年代出现的**专家系统**是最早尝试通过AI自动编码的努力之一。作为一种狭义AI形式，它们专注于编码领域知识和规则以提供指导。这些规则将以非常特定的语法制定，并在规则引擎中执行。这些编码了编程任务的最佳实践，如调试，尽管它们的实用性受到了对细致的基于规则的编程的需求的限制。对于软件开发，从命令行编辑器如ed（1969年），到vim和emacs（20世纪70年代），再到今天的集成开发环境（IDEs）如Visual
    Studio（1997年首次发布）和PyCharm（自2010年以来），这些工具帮助开发人员编写代码，导航复杂项目，重构，进行高亮显示，设置和运行测试。IDE还集成并提供来自代码验证工具的反馈，其中一些工具自20世纪70年代以来就存在。著名的Lint由贝尔实验室的Stephen
    Curtis Johnson于1978年编写，可以标记错误，风格错误和可疑结构。许多工具应用形式方法；然而，机器学习已经应用了包括遗传编程和基于神经网络的方法在内的方法至少20年。在本章中，我们将看到使用深度神经网络，特别是transformers来分析代码的进展。这将我们带到了当今，模型已经被训练出根据自然语言描述（在编码助手或聊天机器人中）或一些代码输入（完成）生成完整或部分程序的能力。
- en: Present day
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 当今
- en: 'Researchers at DeepMind published two papers in the journals Nature and Science,
    respectively, that represent important milestones in using AI to transform foundational
    computing, in particular using reinforcement learning to discover optimized algorithms.
    In October 2022, they released algorithms discovered by their model **AlphaTensor**
    for matrix multiplication problems, which can speed up this essential computation
    required by deep learning models, but also in many other applications. **AlphaDev**
    uncovered novel sorting algorithms that were integrated into widely used C++ libraries,
    improving performance for millions of developers. It also generalized its capabilities,
    discovering a 30% faster hashing algorithm now used billions of times daily. These
    discoveries demonstrate AlphaDev''s ability to surpass human-refined algorithms
    and unlock optimizations difficult at higher programming levels.Their model **AlphaCode**,
    published as a paper in February 2022, showcases an AI-powered coding engine that
    creates computer programs at a rate comparable to that of an average programmer.
    They report results on different datasets including HumanEval and others, which
    we’ll come to in the next section. The DeepMind researchers highlight the large-scale
    sampling of candidate pool of algorithms and a filtering step to select from it.
    The model was celebrated as a breakthrough achievement; however, the practicality
    and scalability of their approach is unclear.Today, new code LLMs such as ChatGPT
    and Microsoft''s Copilot are highly popular generative AI models, with millions
    of users and significant productivity-boosting capabilities. There are different
    tasks related to programming that LLMs can tackle such as these:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind的研究人员分别在《自然》和《科学》杂志上发表了两篇论文，这些论文代表了使用人工智能来改变基础计算的重要里程碑，特别是使用强化学习来发现优化算法。2022年10月，他们发布了由他们的模型**AlphaTensor**发现的用于矩阵乘法问题的算法，这可以加速深度学习模型所需的这种基本计算，也适用于许多其他应用。**AlphaDev**发现了集成到广泛使用的C++库中的新型排序算法，提高了数百万开发人员的性能。它还泛化了其能力，发现了一种比现在每天使用数十亿次的哈希算法快30%的算法。这些发现展示了AlphaDev超越人类优化算法并解锁在更高编程级别难以实现的优化的能力。他们的模型**AlphaCode**于2022年2月发表的一篇论文中展示了一个由人工智能驱动的编码引擎，以与普通程序员相当的速度创建计算机程序。他们报告了在不同数据集上的结果，包括HumanEval等，我们将在下一节中介绍。DeepMind的研究人员强调了大规模抽样候选算法池和选择的过滤步骤。该模型被誉为突破性成就；然而，他们方法的实用性和可扩展性尚不清楚。如今，像ChatGPT和微软的Copilot这样的新代码LLM是非常受欢迎的生成式人工智能模型，拥有数百万用户和显著的提高生产力的能力。LLM可以处理与编程相关的不同任务，例如：
- en: 'Code completion: This task involves predicting the next code element based
    on the surrounding code. It is commonly used in integrated development environments
    (IDEs) to assist developers in writing code.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码补全：此任务涉及根据周围代码预测下一个代码元素。它通常用于集成开发环境（IDE）中，以帮助开发人员编写代码。
- en: 'Code summarization/documentation: This task aims to generate a natural language
    summary or documentation for a given block of source code. This summary helps
    developers understand the purpose and function of the code without having to read
    the actual code.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码摘要/文档：此任务旨在为给定的源代码块生成自然语言摘要或文档。这个摘要帮助开发人员理解代码的目的和功能，而无需阅读实际代码。
- en: 'Code search: The objective of code search is to find the most relevant code
    snippets based on a given natural language query. This task involves learning
    the joint embeddings of the query and code snippets to return the expected ranking
    order of code snippets. Neural code search is specifically focused on in the experiment
    mentioned in the text.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码搜索：代码搜索的目标是根据给定的自然语言查询找到最相关的代码片段。这项任务涉及学习查询和代码片段的联合嵌入，以返回代码片段的预期排名顺序。在文本中提到的实验中，神经代码搜索专注于这一点。
- en: 'Bug finding/fixing: AI systems can reduce manual debugging efforts and enhance
    software reliability and security. Many bugs and vulnerabilities are hard to find
    for programmers, although there are typical patterns for which code validation
    tools exist. As an alternative, LLMs can spot problems with a code and (when prompted)
    correct them. Thus, these systems can reduce manual debugging efforts and help
    improve software reliability and security.'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bug 查找/修复：AI 系统可以减少手动调试工作量，增强软件的可靠性和安全性。许多程序员很难找到代码中的错误和漏洞，尽管存在着代码验证工具的典型模式。作为替代方案，LLMs
    可以发现代码中的问题，并在提示时进行修正。因此，这些系统可以减少手动调试工作量，帮助提高软件的可靠性和安全性。
- en: 'Test generation: Similar to code completion, LLMs can generate unit tests (compare
    Bei Chen and others, 2022) and other types of tests enhancing the maintainability
    of a code base.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试生成：类似于代码补全，LLMs 可以生成单元测试（参见 Bei Chen 等人，2022 年）和其他类型的测试，增强代码库的可维护性。
- en: AI programming assistants combine the interactivity of earlier systems with
    cutting-edge natural language processing. Developers can query bugs in plain English
    or describe desired functions, receiving generated code or debugging tips. However,
    risks remain around code quality, security, and excessive dependence. Striking
    the right balance of computer augmentation while maintaining human oversight is
    an ongoing challenge.Let’s look at the current performance of AI systems for coding,
    particularly code LLMs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: AI 编程助手将早期系统的互动性与尖端自然语言处理相结合。开发人员可以用简单的英语查询错误或描述所需的功能，接收生成的代码或调试提示。然而，围绕代码质量、安全性和过度依赖仍存在风险。在保持人类监督的同时找到计算机增强的正确平衡是一个持续的挑战。让我们看看目前用于编码的
    AI 系统的性能，特别是代码 LLMs。
- en: Code LLMs
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代码 LLMs
- en: 'Quite a few AI models have emerged, each with their own strengths and weaknesses,
    which are continuously competing with each other to improve and deliver better
    results. This comparison should give an overview over some of the largest and
    most popular models:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 出现了许多 AI 模型，每个模型都有其优势和劣势，它们不断竞争以改进并提供更好的结果。这种比较应该概述一些最大和最受欢迎的模型：
- en: '| **Model** | **Reads files** | **Runs code** | **Tokens** |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **读取文件** | **运行代码** | **标记** |'
- en: '| ChatGPT; GPT 3.5/4 | No | No | up to 32k |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT; GPT 3.5/4 | 否 | 否 | 最多 32k |'
- en: '| ChatGPT: Code interpreter | Yes | Yes | up to 32k |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT: 代码解释器 | 是 | 是 | 最多 32k |'
- en: '| Claude 2 | Yes | No | 100k |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 克劳德 2 | 是 | 否 | 100k |'
- en: '| Bard | No | Yes | 1k |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 巴德 | 否 | 是 | 1k |'
- en: '| Bing | Yes | No | 32k |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 必应 | 是 | 否 | 32k |'
- en: 'Figure 6.1: Public chat interfaces for software development.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1：软件开发的公共聊天界面。
- en: While this competition benefits users by providing a wider range of options,
    it also means that relying solely on ChatGPT may no longer be the optimal choice.
    Users now face the decision of selecting the most suitable model for each specific
    task.The latest wave leverages machine learning and neural networks for more flexible
    intelligence. Powerful pre-trained models like GPT-3 enable context-aware, conversational
    support. Deep learning approaches also empower bug detection, repair recommendations,
    automated testing tools, and code search.Microsoft's GitHub Copilot, which is
    based on OpenAI’s Codex, draws on open source code to suggest full code blocks
    in real-time. According to a Github report in June 2023, developers accepted the
    AI assistant’s suggestions about 30 percent of the time, which suggests that the
    tool can provide useful suggestions, with less experienced developers profiting
    the most.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种竞争通过提供更广泛的选择范围使用户受益，但这也意味着仅依赖 ChatGPT 可能不再是最佳选择。用户现在面临选择为每个特定任务选择最合适的模型的决定。最新的浪潮利用机器学习和神经网络实现更灵活的智能。强大的预训练模型如
    GPT-3 可以实现上下文感知、对话支持。深度学习方法还赋予了 bug 检测、修复建议、自动化测试工具和代码搜索更多的能力。微软的 GitHub Copilot
    基于 OpenAI 的 Codex，利用开源代码实时建议完整的代码块。根据 2023 年 6 月的 Github 报告，开发人员接受 AI 助手的建议约占
    30%，这表明该工具可以提供有用的建议，经验不足的开发人员受益最多。
- en: '**Codex** is a model, developed by OpenAI. It is capable of parsing natural
    language and generating code and powers GitHub Copilot. A descendant of the GPT-3
    model, it has been fine-tuned on publicly available code from GitHub, 159 gigabytes
    of Python code from 54 million GitHub repositories, for programming applications.'
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**Codex** 是由 OpenAI 开发的模型。它能够解析自然语言并生成代码，为 GitHub Copilot 提供动力。作为 GPT-3 模型的后代，它已经在
    GitHub 上公开可用的代码上进行了微调，来自 5400 万个 GitHub 仓库的 159GB Python 代码，用于编程应用。'
- en: 'To illustrate the progress made in creating software, let’s look at quantitative
    results in a benchmark: the **HumanEval dataset**, introduced in the Codex paper
    (“*Evaluating Large Language Models Trained on Code*”, 2021) is designed to test
    the ability of large language models to complete functions based on their signature
    and docstring. It evaluates the functional correctness of synthesizing programs
    from docstrings. The dataset includes 164 programming problems that cover various
    aspects such as language comprehension, algorithms, and simple mathematics. Some
    of the problems are comparable to simple software interview questions. A common
    metric on HumanEval is pass@k (pass@1) – this refers to the fraction of correct
    samples when generating k code samples per problem.This table summarizes the progress
    of AI models on the HumanEval task (source: Suriya Gunasekar and others, “*Textbooks
    Are All You Need*”, 2023; [https://arxiv.org/pdf/2306.11644.pdf](https://arxiv.org/pdf/2306.11644.pdf)):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明在创建软件方面取得的进展，让我们看一下基准测试中的定量结果：Codex论文中介绍的**HumanEval数据集**（“*评估基于代码训练的大型语言模型*”，2021年）旨在测试大型语言模型根据其签名和文档字符串完成函数的能力。它评估了从文档字符串合成程序的功能正确性。该数据集包括164个编程问题，涵盖语言理解、算法和简单数学等各个方面。其中一些问题与简单的软件面试问题相当。HumanEval上的一个常见指标是pass@k（pass@1）-
    这指的是在为每个问题生成k个代码样本时的正确样本比例。以下表总结了AI模型在HumanEval任务上的进展（来源：苏里亚·古纳塞卡等人，“*只需教科书*”，2023年；[https://arxiv.org/pdf/2306.11644.pdf](https://arxiv.org/pdf/2306.11644.pdf)）：
- en: '![Figure 6.2: Model comparison on coding task benchmarks (HumanEval and MBPP).
    The performance metrics are self-reported. This table only includes models as
    opposed to other approaches, for example reasoning strategies. Llama2’s self-reported
    performance on HumanEval is 29.9%.](../media/file43.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2：编码任务基准上的模型比较（HumanEval和MBPP）。性能指标是自报告的。此表仅包括模型，而不包括其他方法，例如推理策略。Llama2在HumanEval上的自报告性能为29.9%。](../media/file43.png)'
- en: 'Figure 6.2: Model comparison on coding task benchmarks (HumanEval and MBPP).
    The performance metrics are self-reported. This table only includes models as
    opposed to other approaches, for example reasoning strategies. Llama2’s self-reported
    performance on HumanEval is 29.9%.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：编码任务基准上的模型比较（HumanEval和MBPP）。性能指标是自报告的。此表仅包括模型，而不包括其他方法，例如推理策略。Llama2在HumanEval上的自报告性能为29.9%。
- en: 'Please note that the data used in training most LLM models includes some amount
    of source code. For example, The Pile dataset, which was curated by EleutherAI''s
    GPT-Neo for training open-source alternatives of the GPT models, GPT-Neo, includes
    at least about 11% of code from Github (102.18GB). The Pile was used in training
    of Meta’s Llama, Yandex''s YaLM 100B, and many others.Although, HumanEval has
    been broadly used as a benchmark for code LLMs, there are a multitude of benchmarks
    for programming. Here’s an example question and the response from an advanced
    computer science test given to Codex:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，大多数LLM模型训练中使用的数据包括一定量的源代码。例如，由EleutherAI的GPT-Neo策划的Pile数据集，用于训练GPT模型的开源替代品，GPT-Neo，包括来自Github约11%的代码（102.18GB）。Pile被用于Meta的Llama、Yandex的YaLM
    100B等许多模型的训练。尽管HumanEval已广泛用作代码LLM的基准测试，但还有许多编程基准测试。以下是一个给Codex的高级计算机科学测试的示例问题和回应：
- en: '![Figure 6.3: A question given in a CS2 exam (left) and the Codex response
    (source “My AI Wants to Know if This Will Be on the Exam: Testing OpenAI’s Codex
    on CS2 Programming Exercises” James Finnie-Ansley and others, 2023).](../media/file44.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3：CS2考试中的一个问题（左）和Codex的回应（来源“我的AI想知道这会不会出现在考试中：测试OpenAI的Codex对CS2编程练习的影响”詹姆斯·芬尼-安斯利等人，2023年）。](../media/file44.png)'
- en: 'Figure 6.3: A question given in a CS2 exam (left) and the Codex response (source
    “My AI Wants to Know if This Will Be on the Exam: Testing OpenAI’s Codex on CS2
    Programming Exercises” James Finnie-Ansley and others, 2023).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：CS2考试中的一个问题（左）和Codex的回应（来源“我的AI想知道这会不会出现在考试中：测试OpenAI的Codex对CS2编程练习的影响”詹姆斯·芬尼-安斯利等人，2023年）。
- en: 'There are many interesting studies that shed a light on AI’s capability to
    help software developers or that expand on that capability as summarized in this
    table:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多有趣的研究揭示了人工智能帮助软件开发人员的能力，或者扩展了这种能力，如下表所总结的：
- en: '| **Authors** | **Publication Date** | **Conclusions** | **Task** | **Model/Strategy
    Analyzed** |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **作者** | **出版日期** | **结论** | **任务** | **分析的模型/策略** |'
- en: '| Abdullah Al Ishtiaq and others | April 2021 | Pre-trained language models
    like BERT can enhance code search through improved semantic understanding. | Code
    search | BERT |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| Abdullah Al Ishtiaq 和其他人 | 2021年4月 | 像 BERT 这样的预训练语言模型可以通过改进语义理解来增强代码搜索。
    | 代码搜索 | BERT |'
- en: '| Mark Chen et al. (OpenAI) | July 2021 | Evaluates Codex on code generation,
    shows potential to advance program synthesis | Code generation | Codex |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| Mark Chen 等人（OpenAI） | 2021年7月 | 对 Codex 进行代码生成评估，显示了推进程序合成的潜力。 | 代码生成 |
    Codex |'
- en: '| Ankita Sontakke and others | March 2022 | Even state-of-the-art models produce
    poor quality code summaries, indicating they may not understand code. | Code summarization
    | Transformer models |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Ankita Sontakke 和其他人 | 2022年3月 | 即使是最先进的模型也会产生质量低劣的代码摘要，表明它们可能不理解代码。 | 代码摘要
    | Transformer 模型 |'
- en: '| Bei Chen et al. (Microsoft) | July 2022 | CODE-T leverages LLMs to auto-generate
    test cases, reducing human effort and improving code evaluation. It achieves 65.8%
    HumanEval pass@1. | Code generation, testing | CODET |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| Bei Chen 等人（微软） | 2022年7月 | CODE-T 利用 LLM 自动生成测试用例，减少人力投入，提高代码评估。它在 HumanEval
    pass@1 中达到了65.8%。 | 代码生成，测试 | CODET |'
- en: '| Eric Zelikman et al. (Stanford) | December 2022 | Parsel framework enables
    LLMs to decompose problems and leverage strengths, improving performance on hierarchical
    reasoning | Program synthesis, planning | Codex |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| Eric Zelikman 等人（斯坦福大学） | 2022年12月 | Parsel 框架使 LLM 能够分解问题并利用优势，提高了在分层推理上的表现。
    | 程序合成，规划 | Codex |'
- en: '| James Finnie-Ansley and others | January 2023 | Codex outperforms most students
    on advanced CS2 programming exams. | CS2 programming | Codex |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| James Finnie-Ansley 和其他人 | 2023年1月 | Codex 在高级 CS2 编程考试中表现优异，超过大多数学生。 | CS2
    编程 | Codex |'
- en: '| Yue Liu and others | February 2023 | Existing automated code generation has
    limitations in robustness and reliability. | Code generation | 5 NMT models |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| Yue Liu 和其他人 | 2023年2月 | 现有的自动代码生成在鲁棒性和可靠性方面存在局限性。 | 代码生成 | 5 个 NMT 模型 |'
- en: '| Mingyang Geng and others | February 2023 | A two-stage approach significantly
    increased effectiveness of code summarization. | Code summarization | LLM + reinforcement
    learning |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Mingyang Geng 和其他人 | 2023年2月 | 两阶段方法显著提高了代码摘要的有效性。 | 代码摘要 | LLM + 强化学习 |'
- en: '| Noah Shinn et al. | March 2023 | Reflexion enables trial-and-error learning
    via verbal reflection, achieving 91% HumanEval pass@1 | Coding, reasoning | Reflexion
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| Noah Shinn 等人 | 2023年3月 | Reflexion 通过口头反思实现试错学习，实现了91% 的 HumanEval pass@1。
    | 编码，推理 | Reflexion |'
- en: '| Haoye Tian and others | April 2023 | ChatGPT shows promise for programming
    assistance but has limitations in robustness, generalization, and attention span.
    | Code generation, program repair, code summarization | ChatGPT |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| Haoye Tian 和其他人 | 2023年4月 | ChatGPT 在编程辅助方面表现出潜力，但在鲁棒性、泛化性和注意力持久性方面存在局限。
    | 代码生成，程序修复，代码摘要 | ChatGPT |'
- en: '| Chuqin Geng and others | April 2023 | ChatGPT demonstrates impressive capabilities
    for intro programming education but would only get a B- grade as a student. |
    Intro functional programming course | ChatGPT |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| Chuqin Geng 和其他人 | 2023年4月 | ChatGPT 在入门编程教育中展示出令人印象深刻的能力，但作为学生只能获得 B- 的成绩。
    | 入门函数式编程课程 | ChatGPT |'
- en: '| Xinyun Chen and others | April 2023 | Self-debugging technique enables language
    models to identify and correct mistakes in generated code. | Code generation |
    Self-Debugging |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| Xinyun Chen 和其他人 | 2023年4月 | 自我调试技术使语言模型能够识别和纠正生成代码中的错误。 | 代码生成 | Self-Debugging
    |'
- en: '| Masum Hasan and others | April 2023 | Transforming text to an intermediate
    formal language enabled more efficient app code generation from descriptions.
    | App code generation | Seq2seq networks |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| Masum Hasan 和其他人 | 2023年4月 | 将文本转换为中间形式语言使得从描述中更有效地生成应用程序代码成为可能。 | 应用程序代码生成
    | Seq2seq 网络 |'
- en: '| Anis Koubaa and others | May 2023 | ChatGPT struggles with complex programming
    problems and is not yet suitable for fully automated programming. It performs
    much worse than human programmers. | Programming problem solving | ChatGPT |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| Anis Koubaa 和其他人 | 2023年5月 | ChatGPT 在复杂编程问题上表现不佳，尚不适合完全自动化编程。它的表现远远不如人类程序员。
    | 编程问题解决 | ChatGPT |'
- en: '| Wei Ma and others | May 2023 | ChatGPT understands code syntax but is limited
    in analyzing dynamic code behavior. | Complex code analysis | ChatGPT |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| Wei Ma 和其他人 | 2023年5月 | ChatGPT 理解代码语法，但在分析动态代码行为方面受限。 | 复杂代码分析 | ChatGPT
    |'
- en: '| Raymond Li et al. (BigCode) | May 2023 | Introduces 15.5B parameter StarCoder
    trained on 1 trillion GitHub tokens, achieves 40% HumanEval pass@1 | Code generation,
    multiple languages | StarCoder |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| Raymond Li 等人（BigCode） | 2023年5月 | 推出了由1万亿GitHub令牌训练的15.5B参数StarCoder，实现了40%的HumanEval
    pass@1 | 代码生成，多种语言 | StarCoder |'
- en: '| Amos Azaria and others | June 2023 | ChatGPT has errors and limitations,
    so outputs should be independently verified. It is best used by experts well-versed
    in the domain. | General capabilities and limitations | ChatGPT |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| Amos Azaria 和其他人 | 2023年6月 | ChatGPT存在错误和局限性，因此输出应该经过独立验证。最好由精通领域的专家使用。 |
    通用功能和限制 | ChatGPT |'
- en: '| Adam Hörnemalm | June 2023 | ChatGPT increased efficiency for coding and
    planning but struggled with communication. Developers wanted more integrated tooling.
    | Software development | ChatGPT |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| Adam Hörnemalm | 2023年6月 | ChatGPT提高了编码和规划的效率，但在沟通方面存在困难。开发人员希望有更多集成的工具。
    | 软件开发 | ChatGPT |'
- en: '| Suriya Gunasekar et al. (Microsoft) | June 2023 | High-quality data enables
    smaller models to match larger models, altering scaling laws | Code generation
    | Phi-1 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| Suriya Gunasekar 等人（微软） | 2023年6月 | 高质量的数据使较小的模型能够匹配较大的模型，改变了缩放定律 | 代码生成
    | Phi-1 |'
- en: 'Figure 6.2: Literature review of AI for programming tasks. The publication
    dates refer mostly to the preprint releases.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：用于编程任务的人工智能文献综述。出版日期主要指的是预印本发布日期。
- en: 'This is just a small subset of studies, but hopefully this helps to shed a
    light on some of the developments in the field. Recent research explored how ChatGPT
    can support programmers’ daily work activities like coding, communication, and
    planning. Other research describes new models (such as Codex, StarCoder, or Phi-1)
    or approaches for planning or reasoning to execute these models.Most recently,
    the paper “*Textbooks Are All You Need*” by Suriya Gunasekar and others at Microsoft
    Research (2023) introduced phi-1, a 1.3B parameter Transformer-based language
    model for code. The paper demonstrates how high-quality data can enable smaller
    models to match larger models for code tasks. The authors start with a 3 TB corpus
    of code from The Stack and StackOverflow. A large language model (LLM) filters
    this to select 6B high-quality tokens. Separately, GPT-3.5 generates 1B tokens
    mimicking textbook style. A small 1.3B parameter model phi-1 is trained on this
    filtered data. Phi-1 is then fine-tuned on exercises synthesized by GPT-3.5\.
    Results show phi-1 matches or exceeds the performance of models over 10x its size
    on benchmarks like HumanEval and MBPP.The core conclusion is that high-quality
    data significantly impacts model performance, potentially altering scaling laws.
    Instead of brute force scaling, data quality should take precedence. The authors
    reduce costs by using a smaller LLM to select data, rather than expensive full
    evaluation. Recursively filtering and retraining on selected data could enable
    further improvements. It’s important to appreciate that there’s a massive step
    change in difficulty between short code snippets, where task specifications are
    translated directly into code and the right API calls have to be issued in a sequence
    specific to the task, and generating complete programs, which relies on a much
    deeper understanding and reasoning about the task, the concepts behind, and planning
    how to accomplish it. However, reasoning strategies can make a big difference
    for short snippets as well as the paper “*Reflexion: Language Agents with Verbal
    Reinforcement Learning*” by Noah Shinn and others (2023) shows. The authors propose
    a framework called Reflexion that enables LLM agents (implemented in LangChain)
    to learn quickly and efficiently from trial-and-error using verbal reinforcement.
    The agents verbally reflect on task feedback signals and store their reflective
    text in an episodic memory buffer, which helps the agents make better decisions
    in subsequent trials. The authors demonstrate the effectiveness of Reflexion in
    improving decision-making in diverse tasks such as sequential decision-making,
    coding, and language reasoning. Reflexion has the potential to outperform previous
    state-of-the-art models, such as GPT-4, in specific tasks, as shown by its 91%
    pass@1 accuracy on the HumanEval coding benchmark, which beats any approach previously
    published including GPT-4’s 67% (as reported by OpenAI).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '这只是研究的一个小子集，但希望这有助于揭示该领域的一些发展。最近的研究探讨了ChatGPT如何支持程序员的日常工作活动，如编码、沟通和规划。其他研究描述了新模型（如Codex、StarCoder或Phi-1）或规划或推理执行这些模型的方法。最近，微软研究院的Suriya
    Gunasekar等人在2023年发表的论文“*只需教科书*”介绍了phi-1，这是一个基于Transformer的1.3B参数语言模型用于代码。该论文展示了高质量数据如何使较小的模型能够匹配较大的模型进行代码任务。作者从The
    Stack和StackOverflow的3 TB代码语料库开始。一个大型语言模型（LLM）对其进行过滤，选择了6B高质量标记。另外，GPT-3.5生成了1B标记，模仿教科书风格。一个小的1.3B参数模型phi-1在这些过滤数据上进行了训练。然后，phi-1在GPT-3.5合成的练习上进行微调。结果显示，phi-1在HumanEval和MBPP等基准测试中与其大小超过10倍的模型的性能相匹配或超过。核心结论是高质量数据显著影响模型性能，可能改变缩放规律。数据质量应优先于蛮力缩放。作者通过使用较小的LLM来选择数据，而不是昂贵的完整评估，降低了成本。递归地过滤和在选定数据上重新训练可能会带来进一步的改进。重要的是要意识到，在短代码片段中，任务规范直接转换为代码，必须按照特定任务的顺序发出正确的API调用，而生成完整程序则依赖于对任务、背后的概念以及如何完成任务的深入理解和推理。然而，推理策略对于短代码片段也能产生很大的影响，正如Noah
    Shinn等人在2023年发表的论文“*反思：语言代理与口头强化学习*”所示。作者提出了一个名为Reflexion的框架，使LLM代理（在LangChain中实现）能够通过口头强化的试错学习快速有效地学习。代理人口头反思任务反馈信号，并将其反思文本存储在一个情节性记忆缓冲区中，这有助于代理人在随后的试验中做出更好的决策。作者展示了Reflexion在改善顺序决策、编码和语言推理等各种任务中决策制定的有效性。Reflexion有潜力在特定任务中胜过以往的最先进模型，如其在HumanEval编码基准测试中的91%
    pass@1准确率所示，这超过了以往任何已发布的方法，包括GPT-4的67%（由OpenAI报告）。  '
- en: Outlook
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 展望
- en: Looking forward, advances in multimodal AI may further evolve programming tools.
    Systems capable of processing code, documentation, images, and more could enable
    a more natural workflow. The future of AI as a programming partner is bright,
    but requires thoughtful coordination of human creativity and computer-enabled
    productivity.While promising, effectively leveraging AI programming assistants
    requires establishing standards through workshops to create useful prompts and
    pre-prompts for tasks. Focused training ensures proper validation of generated
    code. Integrating AI into existing environments rather than stand-alone browsers
    improves developer experience. As research continues, AI programming assistants
    present opportunities to increase productivity if thoughtfully implemented with
    an understanding of limitations. With careful oversight, AI stands to automate
    tedious tasks, freeing developers to focus on complex programming problems.Legal
    and ethical concerns arise during the pre-training phase, specifically regarding
    the rights of content creators whose data is used to train the models. Copyright
    laws and fair use exemptions are debated in relation to the use of copyrighted
    data by machine learning models.For example, the Free Software Foundation has
    raised concerns about potential copyright violations associated with code snippets
    generated by Copilot and Codex. They question whether training on public repositories
    falls within fair use, how developers can identify infringing code, the nature
    of machine learning models as modifiable source code or compilations of training
    data, and the copyrightability of machine learning models. Further, an internal
    GitHub study found that a small percentage of generated code contained direct
    copies from the training data, including incorrect copyright notices. OpenAI recognizes
    the legal uncertainty surrounding these copyright implications and calls for authoritative
    resolution. The situation has been compared to the Authors Guild, Inc. v. Google,
    Inc. court case regarding fair use of text snippets in Google Books. Ideally,
    we want to be able to do this without relying on a cloud-based service that charges
    us for a request and that may force us to give up the ownership of our data. However,
    it’s very convenient to outsource the AI so that all we have to implement are
    the prompts and the strategies of how to issue calls with our client. Many of
    the open-source models have made impressive progress on coding tasks, and they
    have the advantage of full transparency and openness about their development process.
    Most of them have been trained on code that’s been released under permissive licenses,
    therefore they are not coming with the same legal concerns as other commercial
    products.There is a broader impact of these systems beyond coding itself on education
    and the ecosystem around software development. For example, the emergence of ChatGPT
    resulted in a massive traffic decline for the popular Stack Overflow question-and-answer
    forum for programmers. After initially blocking any contributions generated using
    large language models (LLMs), Stack Overflow launched Overflow AI to bring enhanced
    search, knowledge ingestion, and other AI features to Stack products. New semantic
    search is to provide intelligent, conversational results using Stack’s knowledge
    base.Large language models like Codex and ChatGPT excel in code generation for
    common problems, but struggle with new ones and long prompts. Most importantly,
    ChatGPT understands syntax well but has limitations in analyzing dynamic code
    behavior. In programming education, AI models surpass many students but have a
    lot of room for improvement, however, they haven’t yet reached the level of being
    able to replace programmers and human intelligence. Scrutiny is necessary as mistakes
    can occur, making expert supervision crucial. The potential of AI tools in coding
    is encouraging but challenges remain in robustness, generalization, attention
    span, and true semantic understanding. Further development is needed to ensure
    reliable and transparent AI programming tools that can augment developers, allowing
    them to write code faster with fewer bugs.In the next section, we’ll see how we
    can generate software code with LLMs and how we can execute this from within LangChain.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 展望未来，多模态人工智能的进步可能会进一步发展编程工具。能够处理代码、文档、图像等的系统可以实现更自然的工作流程。人工智能作为编程伙伴的未来光明，但需要人类创造力和计算机增强生产力的深思熟虑协调。虽然有前景，但有效利用人工智能编程助手需要通过研讨会建立标准，为任务创建有用的提示和预提示。专注的培训确保生成的代码得到正确验证。将人工智能整合到现有环境中而不是独立的浏览器可以提高开发者体验。随着研究的继续进行，人工智能编程助手提供了增加生产力的机会，如果能够深思熟虑地实施并了解其局限性。在预训练阶段出现了法律和伦理问题，特别是涉及使用内容创建者数据训练模型的权利。版权法和公平使用豁免在与机器学习模型使用受版权保护数据相关的讨论中备受争议。例如，自由软件基金会对Copilot和Codex生成的代码片段可能侵犯版权提出了担忧。他们质疑在公共存储库上进行训练是否属于公平使用，开发者如何识别侵权代码，机器学习模型的性质是否为可修改源代码或训练数据的编译，以及机器学习模型的可版权性。此外，GitHub的一项内部研究发现，少部分生成的代码直接复制了训练数据，包括错误的版权声明。OpenAI认识到围绕这些版权问题的法律不确定性，并呼吁权威解决。这种情况被比作了关于Google
    Books中文本片段公平使用的Authors Guild, Inc. v. Google, Inc.法庭案例。理想情况下，我们希望能够在不依赖收费云服务的情况下完成这一切，并且不会被迫放弃我们的数据所有权。然而，外包人工智能非常方便，因此我们只需实现提示和如何与客户发出调用的策略。许多开源模型在编码任务上取得了令人瞩目的进展，并且在其开发过程中具有完全透明和开放的优势。它们大多是在发布了宽松许可证下的代码上进行训练的，因此不会带来与其他商业产品相同的法律问题。这些系统对编码本身以及软件开发周围生态系统产生了更广泛的影响。例如，ChatGPT的出现导致了程序员流行的Stack
    Overflow问答论坛的大量流量下降。在最初阻止使用大型语言模型（LLMs）生成的任何贡献后，Stack Overflow推出了Overflow AI，为Stack产品带来了增强的搜索、知识吸收和其他人工智能功能。新的语义搜索将使用Stack的知识库提供智能、对话式的结果。像Codex和ChatGPT这样的大型语言模型在解决常见问题的代码生成方面表现出色，但在新问题和长提示方面表现不佳。最重要的是，ChatGPT在理解语法方面表现良好，但在分析动态代码行为方面存在局限性。在编程教育中，人工智能模型超越了许多学生，但仍有很大的改进空间，然而，它们尚未达到能够取代程序员和人类智慧的水平。仔细审查是必要的，因为错误可能会发生，专家监督至关重要。编码中人工智能工具的潜力令人鼓舞，但在稳健性、泛化性、注意力跨度和真正的语义理解方面仍存在挑战。需要进一步发展以确保可靠和透明的人工智能编程工具，可以增强开发者，使他们能够更快地编写代码并减少错误。在接下来的部分中，我们将看到如何使用LLMs生成软件代码以及如何从LangChain内部执行这些代码。
- en: Writing code with LLMs
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 LLMs 编写代码
- en: 'Let’s start off by applying a model to write code for us. We can use one of
    the publicly available models for generating code. I’ve listed a few examples
    before such as ChatGPT or Bard. From LangChain, we can call OpenAI’s LLMs, PaLM’s
    code-bison, or a variety of open-source models for example through Replicate,
    HuggingFace Hub, or – for local models – Llama.cpp, GPT4All, or HuggingFace Pipeline
    integrations.Let’s have a look at StarCoder This screenshot shows the model in
    a playground on HuggingFace Spaces:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从应用模型来为我们编写代码开始。我们可以使用一些公开可用的模型来生成代码。我之前列举了一些示例，比如 ChatGPT 或 Bard。从 LangChain，我们可以调用
    OpenAI 的 LLMs，PaLM 的 code-bison，或者通过 Replicate、HuggingFace Hub 等多种开源模型，或者通过本地模型，比如
    Llama.cpp、GPT4All 或 HuggingFace Pipeline 集成。让我们来看看 StarCoder。这个截图展示了模型在 HuggingFace
    Spaces 上的游乐场：
- en: '![Figure 6.3: StarCoder Models Playground. We can choose between different
    models: StarCoder, StarCoderPlus, StarCoderBase.This is available at https://huggingface.co/spaces/bigcode/bigcode-playground](../media/file45.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.3：StarCoder 模型游乐场。我们可以在 StarCoder、StarCoderPlus、StarCoderBase 之间进行选择。此处可访问
    https://huggingface.co/spaces/bigcode/bigcode-playground](../media/file45.png)'
- en: 'Figure 6.3: StarCoder Models Playground. We can choose between different models:
    StarCoder, StarCoderPlus, StarCoderBase.This is available at https://huggingface.co/spaces/bigcode/bigcode-playground'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3：StarCoder 模型游乐场。我们可以在 StarCoder、StarCoderPlus、StarCoderBase 之间进行选择。此处可访问
    https://huggingface.co/spaces/bigcode/bigcode-playground
- en: 'Please note that, as the description says, the StarCoder model is not instruction-tuned,
    which means that we can’t give it tasks to do. We can’t say “write a class that…”
    but we can ask it to complete a text as shown in the screenshot, where we prompt
    the model with “`# dataclass of customer including an alphanumeric id, a name,
    and a birthday`” – let’s try this!We can toggle settings for temperature, max
    new tokens, top-n, and a repetition penalty. For anything non-trivial, we need
    to get the max new tokens setting up.I am getting this code, which gives us a
    useful data model for our hypothetical customer:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，正如描述所说，StarCoder 模型没有经过指令调整，这意味着我们不能给它任务去执行。我们不能说“编写一个包含…”，但我们可以要求它完成一个文本，就像截图中展示的那样，我们用“`#
    dataclass of customer including an alphanumeric id, a name, and a birthday`”提示模型
    – 让我们试试这个！我们可以切换温度、最大新标记、top-n 和重复惩罚的设置。对于任何非平凡的事情，我们需要调整最大新标记的设置。我得到了这段代码，为我们的假想客户提供了一个有用的数据模型：
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This is missing the imports, which would usually come before the comment prompt,
    so I can’t fault the model for it, we need to add these two lines to the top:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这里缺少了导入部分，通常应该在注释提示之前，所以我不能因此责怪模型，我们需要在顶部添加这两行：
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This doesn’t throw an error when we run it as is, which is impressive, however,
    there are a few things not ideal or even wrong with it when we look a bit more
    in detail.On the first glance, I personally don’t like the commenting style and
    the lack of docstrings. There are some `print()` statements left in the code;
    often this is done for debugging purposes. This shouldn’t be in production code.
    It’s probably not ideal to have empty strings as defaults for `firstname` and
    `lastname`. I would expect the customer id to be assigned based on a hashing function
    – I can’t see why the id should have exactly 8 characters as enforced in the validation
    function for the property. There are more problems with this code that affect
    its correctness and readability, but there are a few more crucial problems in
    the code including attempting to write read-only attributes. `firstname` and `lastname`
    in the dataclass – `frozen=True` in a dataclass means that attributes can’t be
    changed after creation. The logic about creating a variable name from `firstname`
    and `lastname` using regular expressions in a non-standard syntax is strange to
    say the least – and incorrect. The last name gets dropped in the process. The
    filters on the reversed name is also highly suspect. I leave it here. This is
    giving us some good ideas and a structure to start with, but it’s not production
    code. It doesn’t even work. You can see this code as `customer.py` in the book’s
    Github repo.Let’s try this again. Perhaps we started off on a bad foot. We started
    a code snippet in bad syntax expected for beginners and expected code that works.
    That’s not realistic. Let’s try again, and start with a prompt that is more up
    to standard:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们按原样运行时，这不会引发错误，这令人印象深刻，但是，当我们更详细地查看时，有一些事情并不理想，甚至是错误的。乍一看，我个人不喜欢注释风格和缺乏文档字符串。代码中留下了一些`print()`语句；通常这是为了调试目的而做的。这不应该出现在生产代码中。将空字符串作为`firstname`和`lastname`的默认值可能并不理想。我希望客户ID是基于哈希函数分配的
    - 我不明白为什么ID应该在验证属性的验证函数中强制为8个字符。这段代码还有更多问题影响其正确性和可读性，但代码中还有一些更为关键的问题，包括尝试编写只读属性。在数据类中的`firstname`和`lastname`
    - 在数据类中，`frozen=True`意味着属性在创建后无法更改。使用非标准语法中的正则表达式从`firstname`和`lastname`创建变量名的逻辑至少是奇怪的
    - 且不正确。姓氏在这个过程中被丢弃。对反转名称的过滤也是非常可疑的。我就说到这里。这给了我们一些好的想法和一个开始的结构，但这不是生产代码。它甚至不起作用。您可以在书的Github存储库中看到这段代码作为`customer.py`。让我们再试一次。也许我们一开始就走错了。我们以初学者预期的错误语法开始了一个代码片段，并期望能够运行的代码。这是不现实的。让我们再试一次，并从一个更符合标准的提示开始：
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We get the following result:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下结果：
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'It’s good to see that the customer id is created using a hash as expected.
    There’s some boilerplate code for comparing two different customer objects. However,
    again there are problems, similar ones to before. First, it’s missing the imports,
    which I don’t understand after our prompt which would be a module docstring to
    be found at the start of a file, and the imports would come right after. Second,
    it’s again attempting to set an attribute after initialization of the class that’s
    supposed to be frozen. After fixing these two problems, we get our first `Customer()`,
    then there’s a problem, where the customer id is referenced with the wrong name.
    After fixing this, we can initialize our customer, look at the attributes, and
    compare one customer to another. I can see how this approach is starting to become
    useful for writing boilerplate code. You can see this code as `customer2.py` in
    the book’s Github repo. Let’s try an instruction-tuned model so we can give it
    tasks! StarChat, which is based on StarCoder, is available on HuggingFace under
    [https://huggingface.co/spaces/HuggingFaceH4/starchat-playground](https://huggingface.co/spaces/HuggingFaceH4/starchat-playground)
    This screenshot shows an example with StarChat:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 很高兴看到客户ID像预期的那样使用哈希创建。有一些用于比较两个不同客户对象的样板代码。然而，再次出现了问题，与之前类似的问题。首先，缺少导入，我不明白我们的提示之后会在文件开头找到一个模块文档字符串，然后导入会紧随其后。其次，又一次尝试在类初始化后设置一个应该是冻结的属性。修复这两个问题后，我们得到了我们的第一个`Customer()`，然后出现了一个问题，客户ID被引用为错误的名称。修复这个问题后，我们可以初始化我们的客户，查看属性，并比较一个客户和另一个客户。我可以看到这种方法开始对编写样板代码变得有用。您可以在书的Github存储库中看到这段代码作为`customer2.py`。让我们尝试一个指令调整模型，这样我们就可以给它任务！StarChat，基于StarCoder，可以在HuggingFace的[https://huggingface.co/spaces/HuggingFaceH4/starchat-playground](https://huggingface.co/spaces/HuggingFaceH4/starchat-playground)下找到。此屏幕截图显示了StarChat的示例：
- en: '![Figure 6.4: StarChat implementing a function in Python for calculating prime
    numbers. Please note that not all the code is visible in the screenshot.](../media/file46.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图6.4：StarChat在Python中实现计算素数的函数。请注意，截图中并非所有代码都可见。](../media/file46.png)'
- en: 'Figure 6.4: StarChat implementing a function in Python for calculating prime
    numbers. Please note that not all the code is visible in the screenshot.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4：StarChat在Python中实现计算素数的函数。请注意，截图中并非所有代码都可见。
- en: 'You can find the complete code listing on Github.For this very example that
    should be well-known in first year Computer Science courses, no imports are needed.
    The algorithm’s implementation is straightforward. It executes right away and
    gives the expected result. Within LangChain, we can use the `HuggingFaceHub` integration
    like this:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Github上找到完整的代码清单。对于这个在第一年计算机科学课程中应该很有名的例子，不需要导入任何内容。算法的实现很简单。它立即执行并给出预期的结果。在LangChain中，我们可以像这样使用`HuggingFaceHub`集成：
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As of August 2023, this LangChain integration has some issues with timeouts
    – hopefully, this is going to get fixed soon. We are not going to use it here.Llama2
    is not one of the best models for coding with a pass@1 of about 29 as mentioned
    earlier, however, we can try it out on HuggingFace chat:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 截至2023年8月，这个LangChain集成存在一些超时问题 - 希望这很快就会得到解决。我们不打算在这里使用它。正如之前提到的，Llama2并不是编码的最佳模型之一，通过率约为29，但是，我们可以在HuggingFace聊天中尝试一下：
- en: '![Figure 6.5: HuggingFace chat with Llama2 at https://huggingface.co/chat/](../media/file47.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5：HuggingFace聊天与Llama2在https://huggingface.co/chat/](../media/file47.png)'
- en: 'Figure 6.5: HuggingFace chat with Llama2 at https://huggingface.co/chat/'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5：HuggingFace聊天与Llama2在https://huggingface.co/chat/
- en: 'Please note that this is only the beginning of the output. Llama2 finds a good
    implementation and the explanations are spot on. Well done, StarCoder and Llama2!
    – Or perhaps, this was just too easy? There so many ways to get code completion
    or generation. We can run even try a small local model:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这只是输出的开始。Llama2找到了一个很好的实现，解释也很到位。干得好，StarCoder和Llama2！ - 或者，这太容易了？有很多方法可以获得代码完成或生成。我们甚至可以尝试一个小型本地模型：
- en: '[PRE5]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'CodeGen is a model by Salesforce AI Research. CodeGen 350 Mono performed 12.76%
    pass@1 in HumanEval. As of July 2023, new versions of CodeGen was released with
    only 6B parameters that are very competitive, which clocks in at a performance
    of 26.13%. This last model was trained on the BigQuery dataset containing C, C++,
    Go, Java, Javascript, and Python, as well as the BigPython dataset, which consists
    of 5.5TB of Python code. Another interesting, small model is Microsoft’s CodeBERT
    (2020), a model for program synthesis that has been trained and tested on Ruby,
    Javascript, Go, Python, Java, and PHP.. Since this model was released before the
    HumanEval benchmark, the performance statistics for the benchmark were not part
    of the initial publication.We can now get the output from the pipeline directly
    like this:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: CodeGen是Salesforce AI Research的一个模型。 CodeGen 350 Mono在HumanEval中的通过率为12.76%。截至2023年7月，发布了新版本的CodeGen，只有6B参数，非常具有竞争力，性能达到26.13%。这个最后的模型是在包含C、C++、Go、Java、Javascript和Python的BigQuery数据集上训练的，以及包含5.5TB
    Python代码的BigPython数据集。另一个有趣的小型模型是微软的CodeBERT（2020），这是一个用于程序合成的模型，已经在Ruby、Javascript、Go、Python、Java和PHP上进行了训练和测试。由于这个模型是在HumanEval基准发布之前发布的，因此基准的性能统计数据不是初始出版物的一部分。我们现在可以直接从流水线中获取输出，就像这样：
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Alternatively, we can wrap this pipeline via the LangChain integration:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以通过LangChain集成来包装这个流水线：
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is a bit verbose. There’s also the more convenient constructor method `HuggingFacePipeline.from_model_id()`.I
    am getting something similar to the StarCoder output. I had to add an `import
    math`, but the function works. This pipeline we could use in a LangChain agent,
    however, please note that this model is not instruction-tuned, so you cannot give
    it tasks, only completion tasks. You can also use all of these models for code
    embeddings. Other models that have been instruction-tuned and are available for
    chat, can act as your techie assistant to help with advice, document and explain
    existing code, or translate code into other programming languages – for the last
    task they need to have been trained on enough samples in these languages.Please
    note that the approach taken here is a bit naïve. For example, we could be taking
    more samples and choose between them such as in the a few of the papers we’ve
    discussed.Let’s now try to implement a feedback cycle for code development, where
    we validate and run the code and change it based on feedback.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点啰嗦。还有更方便的构造方法`HuggingFacePipeline.from_model_id()`。我得到了类似于StarCoder输出的东西。我不得不添加一个`import
    math`，但这个函数有效。我们可以在LangChain代理中使用这个管道，但请注意，这个模型没有经过指导，所以你不能给它任务，只能完成任务。你也可以使用所有这些模型进行代码嵌入。其他经过指导并可用于聊天的模型，可以充当您的技术助手，帮助提供建议，文档和解释现有代码，或将代码翻译成其他编程语言
    - 对于最后一个任务，它们需要在这些语言中经过足够的样本训练。请注意，这里采取的方法有点天真。例如，我们可以采集更多样本并在它们之间进行选择，就像我们讨论过的一些论文中那样。现在让我们尝试为代码开发实现一个反馈循环，其中我们验证并运行代码，并根据反馈进行更改。
- en: Automated software development
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动化软件开发
- en: We’ll now going to write a fully-automated agent that will write code for us
    and fix any problems responding to feedback.In LangChain, we have several integrations
    for code execution like these the `LLMMathChain`, which executes Python code to
    solve math questions, and the `BashChain` that executes Bash terminal commands,
    which can help with system administration tasks. However, these are for problem
    solving with code rather than creating a software.This can, however, work quite
    well.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将编写一个完全自动化的代理，它将为我们编写代码并根据反馈修复任何问题。在LangChain中，我们有几个用于执行代码的集成，比如`LLMMathChain`，它执行Python代码来解决数学问题，以及`BashChain`，它执行Bash终端命令，可以帮助处理系统管理任务。然而，这些是用于通过代码解决问题而不是创建软件。不过，这种方法可能效果很好。
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can see how the prime number calculations get processed quite well under
    the hood between OpenAI’s LLM and the Python interpreter:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到在OpenAI的LLM和Python解释器之间，质数计算是如何在内部很好地处理的：
- en: '[PRE9]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We get to the right answer about the prime numbers, however, it’s not entirely
    clear how this approach would scale for building software products, where it is
    about modules, abstractions, separation of concerns, and maintainable code. There
    are a few interesting implementations for this around. The MetaGPT library approaches
    this with an agent simulation, where different agents represent job roles in a
    company or IT department:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经得出了关于质数的正确答案，然而，目前还不太清楚这种方法在构建软件产品方面的可扩展性，其中涉及模块、抽象、关注点分离和可维护代码。目前有一些有趣的实现方法。MetaGPT库通过代理模拟来处理这个问题，其中不同的代理代表公司或IT部门中的工作角色：
- en: '[PRE10]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This is a really inspiring use case of an agent simulation. The llm-strategy
    library by Andreas Kirsch generates code for dataclasses using decorator patterns.
    Other examples for automatic software development include AutoGPT and BabyGPT
    although these often tend to get stuck in loops or stop because of failures. A
    simple planning and feedback loop like this can be implemented in LangChain with
    a ZeroShot Agent and a planner. The Code-It project by Paolo Rechia and Gpt-Engineer
    by AntonOsika both follows such as pattern as illustrated in this graph for Code-It:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常鼓舞人心的代理模拟用例。Andreas Kirsch的llm-strategy库使用装饰器模式为数据类生成代码。自动软件开发的其他示例包括AutoGPT和BabyGPT，尽管这些通常会陷入循环或因失败而停止。像这样的简单规划和反馈循环可以在LangChain中使用ZeroShot代理和规划器实现。Paolo
    Rechia的Code-It项目和AntonOsika的Gpt-Engineer都遵循这样的模式，如Code-It的图表所示：
- en: '![Figure 6.6: Code-It control flow (source: https://github.com/ChuloAI/code-it).](../media/file48.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.6：Code-It 控制流（来源：https://github.com/ChuloAI/code-it）](../media/file48.jpg)'
- en: 'Figure 6.6: Code-It control flow (source: https://github.com/ChuloAI/code-it).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6：Code-It 控制流（来源：https://github.com/ChuloAI/code-it）。
- en: 'Many of these steps consist of specific prompts that are sent to LLMs with
    instructions to break down the project or to set up the environment. It’s quite
    impressive to implement the full feedback loop with all the tools. We can implement
    a relatively simple feedback loop in different ways in LangChain, for example
    using `PlanAndExecute` chain, a `ZeroShotAgent`, or `BabyAGI`. Let’s go with `PlanAndExecute`!The
    main idea is to set up a chain and execute it with the objective of writing a
    software, like this:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤中的许多都包括发送给 LLMs 的具体提示，指示其拆分项目或设置环境。通过使用所有工具实现完整的反馈循环是非常令人印象深刻的。在 LangChain
    中，我们可以以不同的方式实现相对简单的反馈循环，例如使用`PlanAndExecute`链，`ZeroShotAgent`或`BabyAGI`。让我们选择`PlanAndExecute`！主要思想是设置一个链并执行它，目的是编写软件，就像这样：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'I am omitting the imports here, but you can find the full implementation in
    the Github repo of the book. The other options can be found there as well. There
    are a few more pieces to this, but this could already write some code, depending
    on the instructions that we give. One thing we need is clear instructions for
    a language model to write Python code in a certain form:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里省略了导入部分，但你可以在书籍的 Github 仓库中找到完整的实现。其他选项也可以在那里找到。这还有一些其他要点，但根据我们给出的指示，这已经可以编写一些代码了。我们需要的一件事是为语言模型提供清晰的指令，以便以某种形式编写
    Python 代码：
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We need to make sure, we take a model that is able to come up with code. We’ve
    discussed already the models we can choose between for this. I’ve chosen a longer
    context so we don’t get cut off in the middle of a function, and a low temperature,
    so it doesn’t get too wild.However, on its own this model wouldn’t be able to
    store it to file, do anything meaningful with it, and act on the feedback from
    the execution. We need to come up with code and then test it, and see if it works.
    Let’s see how we can implement this – that’s in the `tools` argument to the agent
    executor, let’s see how this is defined!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要确保选择一个能够生成代码的模型。我们已经讨论了我们可以选择的模型。我选择了一个较长的上下文，这样我们就不会在函数中间被切断，以及一个较低的温度，这样它就不会变得太疯狂。然而，单独使用这个模型无法将其存储到文件中，也无法对其进行有意义的操作，并根据执行的反馈进行操作。我们需要想出代码然后测试它，看看它是否有效。让我们看看我们如何实现这一点
    - 这是传递给代理执行器的`tools`参数，让我们看看这是如何定义的！
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `PythonDeveloper` class has all the logic about taking tasks given in any
    form and translating them into code. I won’t go into all the detail here, however,
    the main idea is here:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`PythonDeveloper` 类包含了关于接受任何形式任务并将其转换为代码的所有逻辑。我不会在这里详细介绍，但主要思想在这里：'
- en: '[PRE14]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'I am again leaving out a few pieces. The error handling if very simplistic
    here. In the implementation on Github, we can distinguish different kinds of errors
    we are getting, such as these:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我再次略过了一些部分。这里的错误处理非常简单。在 Github 上的实现中，我们可以区分我们遇到的不同类型的错误，比如这些：
- en: '`ModuleNotFoundError`: this means that the code tries to work with packages
    that we don’t have installed. I’ve implemented logic to install these packages.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ModuleNotFoundError`: 这意味着代码尝试使用我们未安装的包。我已经实现了安装这些包的逻辑。'
- en: '`NameError`: using variable names that don’t exist.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NameError`: 使用不存在的变量名。'
- en: '`SyntaxError`: the code often doesn’t close parentheses or is not even code'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SyntaxError`: 代码经常不关闭括号或根本不是代码。'
- en: '`FileNotFoundError`: the code relies on files that don’t exist. I’ve found
    a few times that the code tried showing images that were made up.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FileNotFoundError`: 代码依赖不存在的文件。我发现有几次代码试图显示虚构的图像。'
- en: '`SystemExit`: if something more dramatic happens and Python crashes.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SystemExit`: 如果发生更严重的情况导致 Python 崩溃。'
- en: 'I’ve implemented logic to install packages for `ModuleNotFoundError`, and clearer
    messages for some of these problems. In the case of missing images, we could add
    a generative image model to create these. Returning all this as enriched feedback
    to the code generation, results in increasingly specific output such as this:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经实现了为`ModuleNotFoundError`安装包的逻辑，并为其中一些问题提供了更清晰的消息。在缺少图像的情况下，我们可以添加一个生成图像模型来创建这些图像。将所有这些作为丰富的反馈返回给代码生成，会产生越来越具体的输出，例如：
- en: '[PRE15]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The Python code itself gets compiled and executed in a subdirectory and we
    take redirect the output of the Python execution in order to capture it – both
    of this is implemented as Python contexts. Please be cautious with executing code
    on your system, because some of these approaches are quite sensitive to security,
    because they lack a sandboxed environment, although tools and frameworks exists
    such as codebox-api, RestrictedPython, pychroot, or setuptools’ DirectorySandbox
    to just name a few of these for Python.So let’s set tools:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Python代码本身被编译并在子目录中执行，我们重定向Python执行的输出以捕获它 - 这两者都被实现为Python上下文。请谨慎在您的系统上执行代码，因为其中一些方法对安全性非常敏感，因为它们缺乏沙盒环境，尽管存在诸如codebox-api、RestrictedPython、pychroot或setuptools的DirectorySandbox等工具和框架，仅举几例供Python使用。所以让我们设置工具：
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'An internet search is definitely worth adding to make sure we are implementing
    something that has to do with our objective. I’ve seen a few implementations of
    Rock, Paper, Scissors instead of Tetris.We can define additional tools such as
    a planner that breaks down the tasks into functions. You can see this in the repo.Running
    our agent executor with the objective to implement tetris, every time the results
    are a bit different. I see several searches for requirements and game mechanics,
    and several times a code is produced and run. The pygame library is installed.
    The final code snippet is not the final product, but it brings up a window:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 通过进行互联网搜索，确保我们正在实现与我们目标相关的内容是绝对值得的。我看过一些实现了石头、剪刀、布而不是俄罗斯方块的例子。我们可以定义额外的工具，比如将任务分解为函数的计划工具。你可以在仓库中看到这一点。每次以实现俄罗斯方块为目标来运行我们的代理执行器时，结果都会有些不同。我看到了几次搜索需求和游戏机制，以及几次生成和运行代码。pygame库已安装。最终的代码片段并不是最终产品，但它会弹出一个窗口：
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The code is not too bad in terms of syntax – I guess the prompt must have helped.
    However, in terms of functionality it’s very far from Tetris. This implementation
    of a fully-automated agent for software development is still quite experimental.
    It’s also very simple and basic, consisting only of about 340 lines of Python
    including the imports, which you can find on Github.I think a better approach
    could be to break down all the functionality into functions and maintain a list
    of functions to call, which can be used in all subsequent generations of code.
    We could also try a test-driven development approach or have a human give feedback
    rather than a fully automated process.An advantage to our approach is however
    that it’s easy to debug, since all steps including searches and generated code
    are written to a log file in the implementation.Let’s summarize!
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 就语法而言，代码并不算太糟糕 - 我想提示可能有所帮助。然而，就功能而言，它与俄罗斯方块相去甚远。这种用于软件开发的全自动代理的实现仍然相当实验性。它也非常简单和基础，仅包括约340行Python代码，包括导入部分，你可以在Github上找到。我认为一个更好的方法可能是将所有功能分解为函数，并维护一个要调用的函数列表，这可以在所有后续代码生成中使用。我们也可以尝试测试驱动的开发方法，或者让人类给出反馈，而不是完全自动化的过程。然而，我们方法的一个优点是很容易调试，因为所有步骤，包括搜索和生成的代码，都写入了实现的日志文件中。让我们总结一下！
- en: Summary
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we’ve discussed LLMs for source code, and how they can help
    in developing software. There are quite a few areas, where LLMs can benefit software
    development, mostly as coding assistants.We’ve applied a few models for code generation
    using naïve approaches and we’ve evaluated them qualitatively. We’ve seen that
    the suggested solutions seem superficially correct but don’t actually perform
    the task or are full of bugs. This could particularly affect beginners, and could
    have significant implications regarding safety and reliability.In previous chapters,
    we’ve seen LLMs used as goal-driven agents to interact with external environments.
    In coding, compiler errors, results of code execution can be used to provide feedback
    as we’ve seen. Alternatively, we could have used human feedback or implemented
    tests. Let’s see if you remember some of the key takeaways from this chapter!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了用于源代码的LLMs，以及它们如何帮助开发软件。有很多领域可以从LLMs中受益，主要是作为编码助手。我们应用了一些模型来使用天真的方法生成代码，并对其进行了定性评估。我们看到建议的解决方案表面上看起来是正确的，但实际上并没有执行任务，或者充满了错误。这可能会特别影响初学者，并且可能对安全性和可靠性产生重大影响。在之前的章节中，我们看到LLMs被用作目标驱动的代理与外部环境进行交互。在编码中，编译器错误、代码执行的结果可以用来提供反馈，正如我们所见。或者，我们可以使用人类反馈或实施测试。让我们看看你是否记得本章的一些关键要点！
- en: Questions
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: 'Please have a look to see if you can come up with the answers to these questions
    from memory. I’d recommend you go back to the corresponding sections of this chapter,
    if you are unsure about any of them:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 请看看你是否能够从记忆中找到这些问题的答案。如果你对任何问题不确定，我建议你回到本章的相应部分查看：
- en: What can LLMs do to help in software development?
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLMs可以如何帮助软件开发？
- en: How do you measure a code LLM’s performance on coding tasks?
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何衡量代码LLM在编码任务上的表现？
- en: Which code LLM models are available, both open- and closed-source?
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有哪些代码LLM模型可用，包括开源和闭源？
- en: How does the Reflexion strategy work?
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Reflexion 策略是如何工作的？
- en: What options do we have available to establish a feedback loop for writing code?
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们有哪些选项可用于建立写代码的反馈循环？
- en: What do you think is the impact of generative AI on software development?
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你认为生成式人工智能对软件开发有什么影响？
