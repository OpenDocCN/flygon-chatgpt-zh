© 作者，独家许可给 Springer Nature Switzerland AG 2023M. Kurpicz-Briki《不仅仅是聊天机器人》[https://doi.org/10.1007/978-3-031-37690-0_5](https://doi.org/10.1007/978-3-031-37690-0_5)

# 5. 真与假

Mascha Kurpicz-Briki^([1](#Aff2)  )(1)应用机器智能，瑞士比尔/宾讷应用科学大学

## 概述

在本章中，我们将更深入地探讨先进语言模型的一个特性，即由语言模型创建的句子以及聊天机器人可能提供的答案*很可能*是一个好结果。很可能，因为正如我们之前所见，这都是关于概率的。根据选项中提供的内容，即使对于给定问题来说最佳选项可能也不是一个很好的答案。因此，以不同方式，先进的聊天机器人可能提供*错误*信息。

我们还将研究多年来使用过的一些其他聊天机器人架构。特别是，我们将考虑 1960 年代创建的 ELIZA 的例子，这是最早的聊天机器人之一。

在本章的最后部分，我们将研究用于机器学习模型的评估指标。这些指标帮助我们确定给定模型是否运行良好。

## 如果语言模型错了会怎样？

### 你知道白草莓吗？

让我们从以下例子开始：

人类：

“草莓是什么颜色的？”

聊天机器人：

“草莓在完全成熟时通常呈深红色。然而，在未完全成熟时，表面可能有绿色或白色区域。草莓表面的种子通常是黄色或浅褐色。”

尽管这对大多数草莓来说是正确的答案，但对于白草莓（一种白色草莓品种）不适用，如图 [5.1](#Fig1) 所示。![](../images/604345_1_En_5_Chapter/604345_1_En_5_Fig1_HTML.jpg)

一张放在表面上的一堆白草莓的照片。

图 5.1

白草莓是白色草莓（来源：Wikimedia/Emmbean. CC BY 3.0 许可，可在 [https://​commons.​wikimedia.​org/​wiki/​File:​Pineberries.​jpg](https://commons.wikimedia.org/wiki/File:Pineberries.jpg) 获取）

也许有些读者不知道这种草莓，因为它并不是很有名？对于聊天机器人也是一样。聊天机器人生成答案的信息受到其训练数据的限制。对于这个特定问题，一个具有更广泛训练数据和反馈的聊天机器人可能会在对话中得到以下答案：^([1](#Fn1))

+   聊天机器人：“草莓在完全成熟时通常呈深红色。然而，在未完全成熟时，表面可能有绿色或白色区域。草莓表面的种子通常是黄色或浅褐色。*偶尔，你可能会遇到一些白色或淡粉色的草莓品种，这些被称为白草莓*。”

当然，现在，你可能会说我们很快就会到达那里，不断添加更多数据，变得越来越聪明。然而，问题在于你看不到缺失的数据；你只能看到答案，这可能是不完整的（或错误的），而且不可能测试所有潜在的用例。统计上不太可能发生的事情，比如草莓是白色的，不太可能被考虑为输出。因此，已经存在的信息被重复和强化，导致语言模型也被称为*随机鹦鹉*，一个“根据其广泛的训练数据中观察到的语言形式序列随意拼接的系统，根据它们如何组合的概率信息，但没有参考任何含义”（Bender等人[2021](#CR3)，第617页）。

### 幻觉

当前最先进的语言模型中存在的另一个问题是在生成文本时出现的*幻觉*。一般来说，幻觉一词描述了人类在没有实际外部刺激的情况下产生的感知。[^2](#Fn2) 在文本生成中，我们指的是“文本是荒谬的或不忠于提供的源内容”（Ji等人[2023](#CR22)，第248页：3）。基本上，聊天机器人在编造那些毫无意义或是错误且无法根据使用的训练数据解释的信息。

语言模型中的幻觉可以是*内在*的或*外在*的幻觉（Ji等人[2023](#CR22)）。内在幻觉发生在生成的输出和源内容相互矛盾的情况下。比如，在语言模型的训练数据中，有关*草莓粉丝*组织的以下文本（以及数百万其他单词和年份）。

“草莓粉丝组织成立于2019年，由安娜和汤姆创立，之前他们写了一本关于草莓美味和美丽的书。”

然后，我们与一个基于这种语言模型的聊天机器人进行了以下对话：

人类：

“草莓组织是在哪一年成立的？”

聊天机器人：

“草莓粉丝组织成立于**2014**年。”

等等，什么？是的，我们刚刚经历了一次内在幻觉。

另一种幻觉是外在幻觉。在这种情况下，生成的输出无法验证，但也无法根据源内容进行反驳。基本上，这些信息是未知的，但聊天机器人却给出了一个答案。这个答案可能是正确的，但也可能是错误的。这是未经验证的信息。

例如，我们可以在聊天机器人被要求回答有关2023年的问题时观察到外在幻觉；然而，训练数据只包含到2022年的信息。

图[5.2](#Fig2)总结了两种幻觉的类型。![](../images/604345_1_En_5_Chapter/604345_1_En_5_Fig2_HTML.png)

一幅插图代表了内在和外在幻觉的情景，其中聊天机器人从训练数据中提供答案。在内在幻觉的情况下存在一个未经验证的x条件，在外在幻觉中则不存在。

图 5.2

内在幻觉和外在幻觉的区别（基于Ji等人[2023](#CR22)的解释）

在幻觉的情况下，我们假设训练数据是一个基本事实，即正确的，并且希望这些信息能够体现在语言模型生成的文本中。然而，根据训练数据的来源，情况并非总是如此。考虑到训练一种最先进的语言模型所需的大量文本，人工对内容进行验证是不可行的。通常，正在训练、共享和用于许多应用程序的语言模型都是基于互联网文本的。

### 训练于假新闻？

假新闻和阴谋论本身就是当今社会的问题。一项研究表明，在互联网上谎言传播得比真相更快（Vosoughi等人[2018](#CR39)）。有趣的是，与一般观点认为机器人^([3](#Fn3))是传播假新闻的主要来源相反，该研究得出结论，机器人以相同的速度传播假新闻和真实信息。另一方面，人类更有可能传播假新闻而不是机器人。

因此，这基本意味着互联网充斥着假新闻。如今，越来越多的人开始谨慎选择他们搜索引擎结果的来源，意识到并非所有在互联网上写的东西都一定是真实的。但是当我们基于这些内容训练一个语言模型，提供关于事实信息的类人回答却没有提供相应的参考资料时，会发生什么？尽管我们已经学会对于那些名不见经传的网站保持怀疑，但我们却被聊天机器人所困扰，解释内容的意义和责任，即使事实并非如此。

在计算机科学中，我们有一个众所周知的术语*垃圾进，垃圾出*。^([4](#Fn4)) 它指的是向计算机程序提供不正确或无意义的输入数据将产生不正确或无意义的输出，无论逻辑多么出色。尽管这个表达来自计算的早期阶段，但对于机器学习领域也同样适用。如果我们在训练数据中看到假新闻、歧视性内容或其他不良内容，我们可以预期这将对我们的机器学习应用的结果产生影响。分类器或语言模型的好坏取决于它们所训练的数据。

### 错误答案的不同原因

我们已经看到了语言模型中错误信息发生的不同场景。关于统计上相当罕见事件的信息可能会被简单地忽略，由于语言模型的内部功能，可能会产生内在或外在的幻觉，而训练数据本身可能包含虚假新闻或阴谋论。这对于一个值得信赖的软件应用来说是一个非常糟糕的起点。图[5.3](#Fig3)显示了这些不同因素对语言模型生成的句子质量和可信度的影响！[](../images/604345_1_En_5_Chapter/604345_1_En_5_Fig3_HTML.png)

一幅插图显示了训练数据中不包含的罕见信息、训练数据中包含的错误信息，以及语言模型生成的与训练数据相矛盾或未经验证的句子。

图5.3

不同原因可能导致聊天机器人生成错误信息

很好知道这些事情是*有意设计*的。最先进的语言模型的工作方式以及用于训练它们的数据使这种情况发生。即使是大型语言模型的创建者也无法幸免：当谷歌的聊天机器人巴德在2023年被介绍时，在第一次演示中出现了事实错误（Coulter和Bensinger [2023](#CR8)）。谷歌展示的演示显示巴德回答了问题，“詹姆斯·韦伯空间望远镜（JWST）有哪些新发现可以告诉我9岁的孩子？”巴德提供了一系列答案，包括一条信息表明这个望远镜拍摄了地球太阳系之外行星的第一张图片。然而，根据NASA的确认，这个信息是错误的。这是一个昂贵的错误，导致演示后市值损失了1000亿美元。

这里还有一件事要讨论，那就是语言模型不仅可以生成错误信息，还可以产生不存在的词。正如我们之前快速提到的，输入标记不一定是整个单词，还可以是像*pre*、*ing*或*ized*这样的片段，这样可以更有效地处理复合、罕见或非英语单词（Wolfram [2023](#CR42)）。然而，这也可能导致标记以新的方式组合在一起，从而产生全新的词。

在使用ChatGPT等工具的提示时，^([5](#Fn5))这也可以被明确触发：

人类：

“你能想到一个由*pre*和*strawberry*组成的新词吗？”

聊天机器人：

“当然，*preberry*怎么样？它可以指代尚未完全成熟或尚未达到最佳甜度的草莓，但仍然可食用。它也可以更普遍地用来描述处于早期或初步发展或准备阶段的事物，类似于前缀*pre-*。”

虽然在这里我们明确要求系统创建一个新词，但我们也可以在大型语言模型生成的答案或文本中自发地找到它们。

### 聊天机器人会撒谎吗？

让我们回到我们关于句子如何形成的技术解释，基于对下一个单词的预测。答案的形成方式依赖于特定单词在上下文中出现在另一个单词之后的概率。然而，语言模型并没有对它们创建的单词的*含义*具有类似人类的理解。与情感一样，问题再次出在人类的感知上。语言模型的输出似乎流畅而连贯。因此，人类解释生成的句子，这些句子是用他们说的语言，作为有意义的并对应于交际意图的，具有内容的可靠性（Bender等人[2021](#CR3)）。这就是风险所在。如果聊天机器人产生幻觉，但被用来在互联网上搜索真实信息，然后被用户相信和复制，那么我们就有问题。

过去几个月，关于使用最先进的语言模型是否适合网络搜索，以及它是否会取代当前的搜索引擎的话题一直在激烈讨论中。研究人员警告不要将大型语言模型用于它们未经设计的任务（Shah和Bender [2022](#CR33)）。由于可能存在事实错误，正如我们在前一节中看到的那样，将搜索转向语言模型存在增加错误信息的风险。然而，大型科技公司的演示和公告证实了提供革命性搜索的目标，使任何搜索引擎以对话方式与您互动。

问题是这种搜索是否符合我们的期望。这更多是一个社会问题而不是技术问题。在这种愿景中，我们是否想要询问一个*人工专家*，即语言模型，为我们提供汇总信息，可能包含错误信息？还是我们想保持控制，使用工具为我们提供不同信息及其相应来源，并让人类比较和反思信息？在这两种情况下，人类需要意识到工具和提供的内容的局限性，并建立所需的电子素养。我们大多数人已经学会了，在使用搜索引擎时，并非所有出现在结果中的网站都是可信赖的信息来源。使用对话工具时，我们必须反思我们对所提供信息的解释，意识到当以类似人类对话的方式提出时，我们可能会以不同方式解释它们。

最后，让我对聊天机器人产生的虚假信息做一个最后的观察。在广泛的媒体中，经常使用术语“聊天机器人在撒谎”或“聊天机器人在说谎”。在这种意义上，“撒谎”动词的定义如*韦氏*词典所述：

1.  1.

    故意发表不实之论

1.  2.

    制造虚假或误导性印象

第一个定义涉及人类，而第二个定义也可以应用于物体，比如镜子。然而，当谈论工具时，在其他情况下，当工具提供与预期不符的结果时，我们很少使用这个表达。如果咖啡机冲出的咖啡太稀，你可能会说它坏了，而不是说它在欺骗你。欺骗，或者用我们通常使用这个术语的一般方式来说就是撒谎，需要一些意图，而在聊天机器人的情况下是不存在的，但是人类可能会在这样的对话中解读出这种意图。然而，我们必须记住，人类的意图可能导致技术被滥用。

因此，总体而言，我们有聊天机器人产生虚假信息，以及人类产生社交幻觉，看到意图和人类意义的地方。我们在数字社会中还有一些工作要做。

## 聊天机器人的简史

我们已经在前几节中看到了基于变压器架构的最先进聊天机器人是如何工作的。现在我们将获得其他（主要是以前的）变体的一些背景知识。聊天机器人也被称为*对话系统*或*对话引擎*。通常，它们的任务是以书面形式与人类进行对话。当对话以口头形式进行时，它们也被称为*语音机器人*。这些对话系统可以是数字助手，如Siri或Alexa，也可以集成到网站中，以允许客户搜索相关信息。在本节中，我们将限制讨论文本为基础的方法，以了解它们与我们之前看到的文本处理和生成技术的关系。

### ELIZA

一个早期聊天机器人的非常著名的例子是**ELIZA**，由约瑟夫·魏岑鲍姆（Joseph Weizenbaum）在1960年代在麻省理工学院（MIT）开发（Weizenbaum [1966](#CR40)）。它的创造者想要探索如何实现机器和人之间使用自然语言进行交流。与今天的方法相比，ELIZA的技术实现相对简单。用户输入（人类写给聊天机器人提问的文本）被检查是否包含特定的*关键词*。这在某种程度上类似于我们之前看到的非常简单的文本分析方法，通过检查文本中是否出现*草莓*或*草莓*这些词来确定文本是否与草莓有关。如果用户的问题中包含关键词，答案将基于*规则*生成。无内容的评论被识别并不予考虑，在某些情况下，还考虑了早期的转换。使用这种基于规则的硬编码方法，没有涉及训练数据或机器学习。

使用ELIZA，可以进行如图[5.4](#Fig4)所示的对话！[](../images/604345_1_En_5_Chapter/604345_1_En_5_Fig4_HTML.png)

一组问题和答案模拟了人类和机器生成答案之间的对话。机器的回应以大写字母写成。

图5.4

从（Weizenbaum [1966](#CR40)）中提取的对话片段；大写表达式是机器生成的答案

与现代聊天机器人相比，这种对话的主要区别在于对话不旨在用真实世界知识回答问题。正如Weizenbaum在其原始报告中所述，与ELIZA互动的参与者被指示与其交流，就好像他们在与精神科医生交流一样。为什么会这样？

> 选择这种对话模式是因为精神病学面试是少数几个例子之一，其中一个参与对话的人可以自由地假设几乎不了解真实世界。（Weizenbaum [1966](#CR40)，第42页）

进入对话的任何内容都是人类的输入，是他们提出的话题。

这种类型的聊天机器人的主要局限性也是优点。虽然它无法提供任何知识，但也更不太可能提供错误信息。在最坏的情况下，它会提供一个看起来不符合用户输入文本的答案。由于这种类型解决方案的所有潜在答案都列在规则和模板的某处，实验可以轻松复制。聊天机器人提供的答案可以完全解释和复制。因此，根据用例，受到ELIZA最初开发的方法启发的和基于这些方法的聊天机器人可能非常强大。

### 基于规则的方法

在上面展示的ELIZA示例中，没有提供关于世界的信息。当我们想传递非常具体的信息时，也可以使用基于规则的方法。例如，考虑一个零售公司在线商店网站上的聊天机器人，与任何顾客的对话如下：

聊天机器人：

“嘿，谢谢光临这家店！你在找什么？”

人类：

“我想吃点好吃的东西，也许草莓？”

聊天机器人：

“这是个好主意！点击这个链接以直接将我们商店的当地草莓添加到购物篮中。”

这可以以非常简单的方式实现，使用单词列表和规则。让我们更仔细地看一下这一点，基于我们之前看到的聊天机器人的架构。用户与网络应用进行交互，就像之前的情况一样。现在，与其在后台使用复杂的基于变压器的语言模型架构提供答案，这个示例中使用了不同的方法。如图[5.5](#Fig5)所示，用户的输入文本被分析以查找关键词。我们正在寻找能帮助我们向客户推荐产品的特定单词。在最简单的情况下，这是我们商店中所有产品名称的列表。通过将找到的关键词与此列表匹配，我们确定正确的产品并生成答案，就像上面的对话中一样。更高级的版本可以利用我们之前看到的其他文本处理方法，例如词干提取、词形还原、标记或更复杂的规则系统。![](../images/604345_1_En_5_Chapter/604345_1_En_5_Fig5_HTML.png)

一张图示了一个网络应用中一组用户提供的输入提示以及它们生成的答案。它突出显示了名为草莓的关键词，并指示识别输入中的关键词并将其映射到可用产品以生成答案。

图 5.5

在线商店简单基于规则的聊天机器人示例

当然，如果每次文本都完全相同，每个人都会很容易注意到我们的方法是如何工作的。一个简单而有效的方法是我们制作另一个包含十个“这是个好主意”的替代版本的列表。这些替代版本可以包括“不错的选择！”或“听起来像个计划。”每次生成答案时，可以随机选择其中一个。

这个解决方案是可行的，因为在这种情况下信息的上下文是有限的，或者至少对我们可以回答的问题是有限的。当然，客户可能会要求与这家在线商店完全无关的东西，那么它就不起作用了。我们需要考虑在没有识别到关键词的情况下可能的答案。例如，“你能详细解释一下你需要这个东西的原因吗？”如果我们想将这个程序扩展到使用这些技术与人类交流关于任何主题的系统，我们将发现自己需要比我们能够创建或管理的规则和关键词更多。

### 使用句子嵌入创建的简单聊天机器人

现在让我们考虑另一个使用句子嵌入中存储的信息创建聊天机器人的方法示例。我们之前看到存储单词或句子含义的不同方法。根据它们在向量空间中的位置，并使用数学运算，我们能够提取有关单词或句子相似性的信息。我们假设我们想要创建一个可以回答关于草莓的问题的聊天机器人。

为了做到这一点，我们使用维基百科关于草莓文章的文本。它包含关于草莓历史或栽培的不同部分。我们将这篇文章分成句子，并将每个句子转换为句子向量，如图[5.6](#Fig6)所示。![](../images/604345_1_En_5_Chapter/604345_1_En_5_Fig6_HTML.png)

一张图解释了用户关于草莓植株的查询，通过用户问题的句子嵌入，找到相似的嵌入，找到关于草莓的文本，以及每个句子的句子嵌入。

图5.6

基于句子嵌入和相似性的简单聊天机器人

然后我们对用户输入做同样的处理。我们拿问题，比如“你知道关于草莓植株的什么？”并从中创建一个向量（句子嵌入）。正如我们之前看到的，向量之间的距离可以用来捕捉单词或句子之间的关系。因此，我们寻找一个接近我们问题句子嵌入的向量，这可能是我们问题的潜在答案。因此，我们确定维基百科文章的哪个句子嵌入最接近，并输出这个句子，如图[5.6](#Fig6)所示。这使得与聊天机器人的以下对话成为可能：

人类：

“你知道关于草莓植株的什么？”

聊天机器人：

“草莓植株在年轻时会产生更多更好的水果。”

对于不同的问题，这种方法效果相当不错，但远非完美，下面的例子就说明了这一点：

人类：

“草莓在哪些国家种植？”

聊天机器人：

“草莓也可以在草莓盆中室内种植。”

在这里，我们以草莓的一篇文章作为输入。如果扩展到使用当地图书馆提供的所有关于草莓的书籍，结果可能会得到改善。聊天机器人的表现取决于其可用的数据。

尽管对于许多任务来说，最先进的语言模型通常是最有效的解决方案，但我希望你记住，根据你所寻找的内容，更简单的架构也可以做得很好。这些简单方法的一个主要优势通常是它们的可解释性，这使我们更容易看到幕后情况，并让我们了解决策是如何做出的。有时我们也可以在实践中找到不同方法的组合。

## 什么是一个好的模型？

到目前为止，我们已经看到，最先进的聊天机器人依赖于强大的语言模型。但我们如何知道语言模型的表现如何或者如何将它们进行比较呢？

有关语言模型提到的一个典型数字是它训练时使用的*参数*数量。例如，谷歌的BERT模型使用了3.4亿个参数，或者GPT-3使用了1750亿个参数。这些都是相当大的数字！让我们记住之前看到的语言模型的架构。我们看到了不同类型的神经网络，有些涉及不同层的编码器和解码器。在每个层中，都有神经网络，每个神经网络由多个神经元组成，具有加权的输入。这些权重在训练过程中会被调整，取决于它们对系统最终决策的相关性有多大。这些就是可以调整的“旋钮”，以使模型适应数据（Wolfram [2023](#CR42)）。当说有1750亿个参数时，这意味着神经网络的架构有1750亿个需要调整的权重。

更大是否意味着更好？目前是的。我们在过去几年中看到，随着每个新模型的出现，参数的数量变得越来越大，模型也变得更好。然而，与此同时，需要更多的计算能力，模型的复杂性也增加。有1750亿个参数，很难评估一个单词建议是基于什么的。

参数是系统在训练阶段自行解决的，通过使用外部反馈（例如，系统可用的*正确答案*）。数据工程师可以用来优化系统训练过程的是*超参数*。训练发生在循环中（记住这个例子：一遍又一遍地做同样的事情！）。每个循环称为*epoch*。一个epoch等于每个训练样本被查看一次，并相应地调整权重。训练过程在几个epoch内观察，某个时刻，停止的时刻就到了。这通常发生在学习没有更多改进时。系统在给定的训练数据上不再变得更好。用于训练的epoch数量因此是数据工程师使用的超参数之一。神经网络中的另一个超参数，例如，可以是层数。通常，数据工程师会使用不同的超参数设置来训练模型并比较它们的输出。这个过程称为*超参数调整*，如图[5.7](#Fig7)所示。![](../images/604345_1_En_5_Chapter/604345_1_En_5_Fig7_HTML.png)

一个块图解说明了数据工程师的输入是如何通过基于神经网络的语言模型进行的，输入在获得输出之前通过三层。所有这些层都表示神经元之间的相互连接。

图5.7

数据工程师可以在训练过程中调整的一些超参数示例

为了比较机器学习模型的性能，并找出哪种超参数配置最佳，我们需要一些*性能指标*。我们将通过使用本书中早前介绍的情感分析分类示例来探讨这些指标。我们有一堆文本，并希望确定每个文本是更负面还是更正面的。我们已经使用了训练数据集（类似于标记数据的80%）进行训练阶段，现在有了测试数据集来测试分类器的性能。测试数据集中的每个文本都被标记为负面或正面。然而，分类器看不到这些信息，这意味着它在训练阶段没有使用这些信息。在测试阶段，我们希望使用这些数据来衡量系统在新的、未见过的数据上的表现如何。

最简单的方法是计算*准确率*。准确率表示分类器正确标记了测试数据中多少样本。如果在我们的测试数据中有100个文本样本，其中50个标记为正面，50个标记为负面，而我们的分类器正确标记了其中的80个，那么我们的准确率为80%。在这个例子中，我们的数据集是*平衡*的。之所以平衡，是因为我们有相同数量的负面和正面样本。在不平衡数据集的情况下，准确率这个指标可能会受到限制。如果我们的测试数据中有90%的负样本，而只有10%的正文本样本，一个将所有文本都分类为负面的分类器将具有90%的准确率。这可能会让我们误以为分类器的性能相当不错，但事实并非如此。

我们想要衡量分类对两个类别的工作情况。因此，我们必须对分类器错误或正确做出更细致的分析。我们分别考虑分类器错误的情况，因为它估计为*正面*但实际上是*负面*的文本，以及分类器建议为*负面*但文本实际上是*正面*的情况。在测试数据中被正确预测为正面的数据项因此被称为真正例（TP）。另一方面，被预测为正面但实际上是负面的数据项被称为假正例（FP）。它在结果中显示为正面，但其分类是错误的。类似地，我们定义真负例（TN）（被正确预测为负面）和假负例（FN）（被预测为负面，但实际上是正面）如图[5.8](#Fig8)所示。在这个例子中，测试数据集中有十个文本，其中五个是正面，五个是负面。请注意，70%的准确率（十个中有七个被正确预测）与假正例和假负例的数量无关。错误预测是关于负面还是正面类别并不影响准确率。![](../images/604345_1_En_5_Chapter/604345_1_En_5_Fig8_HTML.png)

一个矩形块代表了一个包含正确标签和预测标签的10个测试数据文本集合。它指示了70%的准确度值，并突出显示了3个数据点，其预测和实际结果不匹配。

图5.8

十个文本的测试数据集示例。基于正确标签和预测，可以计算不同的指标来评估模型的性能。

*精确率* 是考虑这些方面的另一个指标。它告诉我们被预测为正面并在正确解决方案中为正面的数据项的比例。在我们的例子中，我们有四个真正例和总共六个正面预测（真正例 + 假正例），精确率为4/6或以百分比表示为66%。在最好的情况下，所有被预测为正面的数据项实际上都是正确的；那么精确率将是100%。

除了精确率，我们通常关注 *召回率*，它展示了另一方面。它衡量了被正确识别为正面的正面文本的比例（=真正例）与所有正面训练文本（=真正例 + 假负例）的比较。在我们的例子中，召回率为4/5，或80%。

通常，数据工程师希望在召回率和精确率之间取得平衡。因此，我们需要同时考虑这两个指标。为了评估一个机器学习分类器是否表现良好，我们需要它们两者。因此，*f1-分数* 被发明出来，它计算涉及召回率和精确率的分数。通常，在开发机器分类器时，我们因此尝试最大化 *f1*-分数。

在我们的案例中，正面和负面对应于我们的类别名称，因为我们正在预测正面和负面的文本。这并不一定是这样的。对于垃圾邮件的分类，正面标签可以表示邮件实际上是垃圾邮件，而负面标签表示它不是垃圾邮件。术语真正例、真负例等仍然会像上面描述的那样使用。

请注意，这里描述的大多数指标都涉及二元分类问题。在二元分类中，我们有两组（例如，正面和负面文本），分类器的目标是估计新文本属于哪一类。在其他情况下，我们可能有多类问题，并且会使用其他类似的指标计算。

在这一部分中，*好* 意味着机器学习系统是否在特定任务中表现良好，使用准确率、精确率、召回率和 *f1*-分数来量化性能。它的决策是否公平或道德是另一个讨论。我们将在下一章更深入地探讨这个话题。

## 总结

在本章中，我们已经看到聊天机器人可能因为不同原因提供错误信息。例子可以是训练数据中缺乏相关信息，以及内在或外在的幻觉。

我们还研究了从上世纪60年代的ELIZA开始的一些其他架构。虽然其他架构不如最先进的聊天机器人和语言模型表现出色，但更容易解释它们如何得出给定的输出。

在本章的最后部分，我们学习了有关机器学习模型的不同性能指标。通过在测试数据集上评估模型的效率，模型可以在训练阶段得到改进。
