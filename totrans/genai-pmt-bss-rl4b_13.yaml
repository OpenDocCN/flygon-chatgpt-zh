- en: HISTORY AND EVOLUTION OF NLPThe NLP was born in the middle of the twentieth
    century. However, the term "Natural Language Processing" was first coined in the
    50s by the American linguist and computer scientist Allen Newell. Newell was one
    of the first to understand the importance of natural language processing for the
    development of artificial intelligence and proposed using machine learning and
    natural language recognition techniques to create machines that can understand
    human language.The first natural language processing project was the "Teodele,”
    a project developed in Italy in the 50s and 60s that aimed to create a system
    capable of automatically translating natural language into machine code.The project
    was funded by the Italian government and was developed at the University of Pisa
    in collaboration with other Italian universities and research centers. The scientific
    direction of the project was entrusted to Professor Antonio Zampolli, who later
    became one of the most critical experts in computational linguistics in Italy
    and, at that time, was a professor of General Linguistics and Computational Linguistics
    at the Faculty of Letters of the University of Pisa. Other team members included
    computer scientists Antonio Marti and Piero Molinelli, linguists Gianfranco Berardi
    and Giuseppe Grossi, and many others.The Teodele project's technology relied on
    syntactic text analysis to detect speech and sentence structure components. Unfortunately,
    the system was handicapped by natural language’s richness, making it impossible
    to discern ambiguities and nuances of meaning. Nevertheless, despite the project's
    failure, Teodele's research team made significant contributions to the growth
    of NLP, particularly in syntactic analysis, dictionaries, and machine translation
    rules.NLP made significant gains in the 1960s and 1970s due to the introduction
    of the first computers and innovative natural language processing techniques.Around
    this time, several universities and research institutions worldwide began developing
    natural language analysis and machine translation programs. They concentrated
    on constructing dictionaries and grammatical rules for language processing, which
    was the foundation for developing language processing algorithms.The SHRDLU project,
    founded by American researcher Terry Winograd at the Massachusetts Institute of
    Technology, was one of the first essential undertakings of this time (MIT). SHRDLU
    was an artificial intelligence software that interacted with users and manipulated
    virtual objects in a virtual world by using simplified natural language. The study
    revealed the capability of developing natural-user interfaces for machine control,
    but it also illustrated the limitations of rules-based natural language processing.The
    earliest natural language analysis systems based on machine learning techniques
    were created in the 1970s. Gerald Gaspar, an English scholar, pioneered the notion
    of "context grammar," a syntactic approach that employed machine learning techniques
    to understand natural language grammatical principles. Several of the shortcomings
    of rules-based language processing systems were overcome by this method.NLP began
    to find practical applications in numerous domains, such as machine translation,
    sentiment analysis, online query processing, generating chatbots and virtual assistants,
    and much more in the 1980s and 1990s.Throughout the 2000s, NLP research concentrated
    on developing statistical models for natural language processing that employed
    enormous data to train models. These statistical models are useful for machine
    translation, text production, and sentiment analysis applications.Deep learning
    techniques, particularly neural networks, have enabled considerable advancement
    in NLP in recent years. The use of neural network-based language models, such
    as recurrent neural networks and transformative neural networks, in particular,
    has enabled the development of models capable of generating coherent and natural
    text, answering user questions, and comprehending the context in which natural
    language is used.Furthermore, the employment of reinforcement learning models
    has enabled the development of conversational bots capable of interacting with
    users increasingly naturally and fluidly.
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理的历史和演变自然语言处理诞生于二十世纪中叶。然而，术语“自然语言处理”最早是由美国语言学家和计算机科学家艾伦·纽厄尔在50年代首次提出的。纽厄尔是最早意识到自然语言处理对人工智能发展的重要性之一，并提议利用机器学习和自然语言识别技术来创建能够理解人类语言的机器。第一个自然语言处理项目是“Teodele”，这是一个在50年代和60年代在意大利开发的项目，旨在创建一个能够自动将自然语言翻译成机器代码的系统。该项目由意大利政府资助，在比萨大学与其他意大利大学和研究中心合作开发。该项目的科学方向由安东尼奥·扎波利教授负责，他后来成为意大利计算语言学中最重要的专家之一，当时是比萨大学文学院的普通语言学和计算语言学教授。其他团队成员包括计算机科学家安东尼奥·马蒂和皮耶罗·莫林利，语言学家吉安弗兰科·贝拉尔迪和朱塞佩·格罗西等。Teodele项目的技术依赖于句法文本分析来检测语音和句子结构组件。不幸的是，该系统受制于自然语言的丰富性，无法辨别意义的歧义和细微差别。尽管项目失败，Teodele的研究团队在NLP的发展中做出了重要贡献，特别是在句法分析、词典和机器翻译规则方面。由于引入了第一台计算机和创新的自然语言处理技术，NLP在60年代和70年代取得了重大进展。在这个时候，全球各地的几所大学和研究机构开始开发自然语言分析和机器翻译程序。他们专注于构建语言处理的词典和语法规则，这是发展语言处理算法的基础。由美国研究员特里·维诺格拉德在麻省理工学院创立的SHRDLU项目是当时的重要项目之一。SHRDLU是一种人工智能软件，通过简化的自然语言与用户交互并在虚拟世界中操作虚拟对象。这项研究展示了开发自然用户界面用于机器控制的能力，但也展示了基于规则的自然语言处理的局限性。基于机器学习技术的最早自然语言分析系统是在70年代创建的。英国学者杰拉尔德·加斯帕率先提出了“上下文语法”的概念，这是一种利用机器学习技术理解自然语言语法原则的句法方法。这种方法克服了基于规则的语言处理系统的一些缺点。在80年代和90年代，NLP开始在许多领域找到实际应用，如机器翻译、情感分析、在线查询处理、生成聊天机器人和虚拟助手等。在2000年代，NLP研究集中于开发利用大量数据训练模型���统计模型，这些统计模型对机器翻译、文本生成和情感分析应用非常有用。近年来，深度学习技术，特别是神经网络，使NLP取得了重大进展。特别是利用基于神经网络的语言模型，如循环神经网络和转换神经网络，使得能够生成连贯和自然的文本，回答用户问题，并理解自然语言使用的上下文成为可能。此外，采用强化学习模型使得开发能够与用户越来越自然和流畅地交互的对话机器人成为可能。
