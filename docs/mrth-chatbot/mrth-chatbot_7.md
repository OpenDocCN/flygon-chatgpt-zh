© 作者，独家许可给 Springer Nature Switzerland AG 2023M. Kurpicz-Briki 不仅仅是一个聊天机器人[`doi.org/10.1007/978-3-031-37690-0_7`](https://doi.org/10.1007/978-3-031-37690-0_7)

# 7. 人类和语言模型的未来

Mascha Kurpicz-Briki^(1 )(1)应用机器智能，瑞士比尔大学应用科学学院，比尔/宾讷，瑞士

## 概览

我们在本书中看到了现代语言模型和其他文本处理软件背后的技术是如何运作的。基于这种理解，我希望您积极参与公众对于我们未来社会将如何被这种技术塑造的讨论中。这种讨论需要技术专家与其他领域的学者以及普通人密切合作，而像本书提供的基本技术理解对于就如何处理这种重大改变技术的讨论至关重要。

在这最后一章中，我想提出一些讨论点，并指出一些潜在的方向作为思考的食粮。

## 人类的未来

要讨论机器与人类协作未来的不同愿景，我们必须看两种不同类型的人工智能，通常被区分为：

> […] 一方面是弱人工智能或狭义人工智能，另一方面是强人工智能。弱人工智能只能解决特定问题，比如下棋，或者识别图片中的内容。相比之下，强人工智能将指代一个在一般水平上能够智能地回应的计算机系统，包括在缺少精确事实信息或目标不明确的情况下。（Zweig 2022, S. 90）

强人工智能也被称为通用人工智能（或人工通用智能 AGI），是“好莱坞版本”（Broussard 2018, p. 10），是在科幻电影中使用的那种人工智能，其中机器人接管政府并给我们展示了人类未来的黑暗愿景。但是为什么我们要创造这样一个旨在有一天完全消灭我们的怪物呢？

创造*人工通用智能*的意图和愿望一直是许多科幻故事和电影的灵感来源，同时也分裂了该领域的研究人员社区。研究人员对于何时（或是否）将实现这种人工通用智能以及是否值得针对这种人工智能进行预测存在分歧。人工通用智能也可以被定义为“至少能够像人类一样完成任何认知任务的能力”（Tegmark 2018, p. 39）。

任何认知任务。这相当完整。正如 Tegmark（2018）所描述的那样，基于这样一个通用人工智能，可以构建一个*超级智能*，可以被使用或释放来接管世界。他所指的超级智能是指“远远超过人类水平的通用智能”（Tegmark 2018，第 39 页）。这个假设的时刻，对人类文明产生了无法预料的后果，也被称为*技术奇点*。

当然，在技术层面上还有很长的路要走才能达到这样的目标。我们会实现这样的目标吗？很难预测。有意致力于构建这种超级智能（或具有潜力*突破*并创造自己的超级智能系统）是否有意义？这是我们作为一个社会都应该讨论的事情。

给自己一点时间，考虑一下先进技术作为人类工具的用例，就像我们使用计算器或汽车一样。现在考虑一下创建超级智能的场景，可以是武装机器人的形式，也可以是完全模拟成为人类的形式。有人可能会想，*为什么选择人类*？你会想要这样做吗？

我鼓励我们作为一个社会思考对我们有用的技术，而不是开发旨在对人类构成启示录般场景的技术。虽然成为*超级智能*的创造者可能会令人着迷，但这并不是社会或人类需要的。

回到语言模型。最先进的语言模型或聊天机器人与我们交谈的方式，模拟人类写作的方式，但在涉及含义或沟通意图时存在重大差异，可能令人印象深刻。然而，它们只是工具，一种为我们提供许多机会的工具，同时也带来一些风险的工具。在不久的将来，风险并不在于 ChatGPT 接管世界的启示录般场景，而在于现在需要进行的其他重要讨论。

AI 社区的这种分裂在 2023 年春天的*这封信*案中也是可见的。由于 OpenAI 的 ChatGPT 以及特别是 GPT-4 模型的发布，未来生命研究所发表了一封公开信，得到了超过 10,000 名支持者的签署（未来生命研究所 2023）。支持者中包括来自人工智能领域的教授、图灵奖获得者和知名科技公司的联合创始人。在这封信中，他们呼吁所有开发 AI 的中心暂停至少 6 个月训练比 GPT-4 更强大的 AI 系统。他们认为，当代 AI 系统正在变得在一般任务上具有人类竞争力，而且只有在我们确信它们的影响将是积极的，风险可控时才应该开发。提议的 6 个月暂停应该是公开和可验证的，并且必要时应由政府施加停止令。

这封信在媒体上引起了广泛讨论。除了需要暂停之外，特别是所提议的停止令的可行性也受到了质疑。我们之前遇到的将大型语言模型描述为随机鹦鹉的论文的作者（Bender 等人 2021）在此后不久发表了一份关于这封信的声明（Gebru 等人 2023）。在他们的声明中，他们讨论了需要侧重于透明度、问责制和防止剥削性劳动实践的监管努力，重点放在已经现实存在并部署在自动化系统中的人工智能上。特别是，他们批评了通过假设性风险如“人类竞争性智能”或“强大的数字思维”来制造恐慌。他们认为这封信忽视了诸如工人剥削、大规模数据盗窃、合成媒体数据再现压迫系统和危害信息生态系统、以及权力集中加剧社会不平等等危害。特别是，他们警告说：

> 正如我们在《随机鹦鹉》中所指出的那样，夸大自动化系统的能力并赋予其拟人化的语言，会让人误以为合成媒体背后有一个有意识的存在。这不仅会诱使人们盲目地信任像 ChatGPT 这样的系统的输出，还会错误地归因于代理。责任应该完全落在建造者身上，而不是他们的产物。（Gebru 等人 2023）

因此，他们强调了需要强制透明度的监管，并且监管应该在这项技术被公司应用时保护人们的权利和利益。

今天存在的模型如 GPT-4 是否具有类似人类智能的初步迹象，也受到智能本身定义的影响。不同的定义正在被使用和讨论，如何衡量这种智能还没有最终确定。为了深化讨论，公共话语中需要制定共同的定义。

正如我们在本书中所看到的，语言模型可能会产生幻觉，需要额外的电子素养技能来以负责任的方式处理。同时，人们可能会以不同的方式解释由聊天机器人提供的信息，与搜索引擎结果中的项目列表提供的信息不同。在这些系统中存在歧视和偏见的风险，以及昂贵的生态后果。最后，机器和人类在工作或学习方面的合作方式可能会发生变化，需要调整我们迄今为止的做事方式。类似地，当计算器进入市场时也需要进行调整。

所以，与其担心终结者人工智能，不如看看这些新工具给我们社会带来的更紧迫的变化，以及如何应对这些变化。

## 负责任人工智能的未来

从技术角度来看，语言模型中的幻觉或偏见问题很难解决。存在不同的技术方法来减少幻觉，可以应用于用于训练的数据或训练过程本身（Ji 等人 2023）。在训练过程中，诸如编码器、解码器或注意力机制等组件可以进行优化，以更好地理解输入的语义。对于数据集，可以增加或验证数据集。可以使用的一种方法是与人类合作创建一个（更）忠实的数据集。正如我们在本书的不同部分所看到的，语言模型的表现只能和它所训练的数据一样好。获取所需数量的高质量和忠实的训练数据是具有挑战性的。改进训练数据的一种方法是雇佣人类标注员。人类标注员可以从头开始撰写新文本，也可以查看训练数据并纠正或改进收集到的文本。^(1) 在这两种情况下，所需的人力资源和成本是非常巨大的。因此，这通常在最好的情况下适用于非常特定领域的任务，并且缺乏泛化性。

例如，如果我们拿一个包含数十亿字的大型语言模型的整个训练集来说，手动审查所有这些是不可行的。然而，如果我们特别关心确保系统在草莓方面是完全正确的，我们可以从训练数据中挑选出所有包含*草莓*一词的句子。这可能会大大减少需要手动审查的句子数量，也许这样做是可行的。

在技术层面上，训练数据的质量和透明度是推动负责任人工智能的两个主要因素。负责任人工智能，或者在本书的背景下*负责任自然语言处理*，是一个应该引起我们兴趣的领域，以塑造我们未来想要拥有的数字社会。这也引发了一个问题，即数字社会是否存在*唯一的*数字社会，还是将涉及不同群体或地区的几个数字社会。

为了使数据集的透明度更高，我们首先需要一个数据集文档的标准。这样的标准是在 2021 年的《数据集数据表》论文中提出的（Gebru 等人 2021）。作者认为训练数据集的特征会影响模型的行为，因此这些数据集的来源、创建和使用需要有良好的文档记录。他们建议每个数据集都应附带一个数据表，其中包含所有这些信息。听起来很合理和简单，但不幸的是，目前（还）不是 AI 行业的默认标准。

除了对训练数据集有更多的了解之外，我们还希望对机器学习模型保持透明。这里有点棘手。虽然我们在书的开头看到的基本方法（如逻辑回归）得出的决策可以更容易解释，但当涉及到神经网络时，这就非常具有挑战性了。这个问题由*可解释 AI*研究领域所解决。在可解释 AI 的背景下，开发了工具和框架来理解和解释这些系统所做出的决策。需要更好地理解决策是如何做出的，才能对机器学习模型保持透明。不幸的是，这个领域还需要更多的工作，而要完全解释一个拥有 1750 亿参数的语言模型是如何生成一句话的，目前还远未解决。

最后，正如前面提到的，AI 的监管是当前讨论的另一个话题。虽然大多数人都同意需要监管，但如何在技术上实施或强制执行这些监管措施还有待讨论。虽然可能没有多少人反对公平和透明的 AI 软件，但在技术层面完全解决这个问题是具有挑战性的。然而，我们需要这种透明度，因此需要重新思考这种软件的开发和部署方式。前方还有很多工作要做。

正如我们现在所看到的，这些问题的技术解决方案仍在不断完善中，并且出于设计考虑，难以修复。然而，语言模型的使用越来越多，因此我们也必须在社会层面解决其中一些问题。

## 工作的未来

新工具可能会改变人类工作的方式。在技术上可能实现到什么程度一方面应该由技术上的可能性来定义，但也应该由财务角度的利益和更重要的是社会角度的意义来定义。个人电脑和打印机已经彻底改变了我们书写和制作文本的技术方式，从手写笔记转变为数字文档。随着智能手机的推出，新的书面沟通方式如短信或消息服务也被引入。随着像 ChatGPT 这样的新一代工具的推出，人类的文本生产面临着新的挑战。

尽管在技术层面上现在可以生成看起来相当流畅和合法的文本，但从本书中我们可以看到，它们在内容和世界知识方面存在重大局限。随着 ChatGPT 的兴起，人们建议将这样一种生成式 AI 添加为他们科学论文的合著者，这是将技术置于人类的工作中。这更多是一个社会问题而不是技术问题，我们应该问问自己在这一切中我们如何看待自己的角色和技术的角色。如果你使用生成式 AI 来支持你生成科学文章的结构，你是否认为它是你的合著者？或者让我换个说法：你是否考虑过将你的文本处理工具如 Word 或 Latex 或你用来查找相关工作的搜索引擎作为合著者？

我们如何定义人类和机器在不同任务中的权力关系是至关重要的。成为合著者意味着将工具视为与你的人类合著者具有相同权力水平。让 AI 应用程序向人类工作者发出严厉的指令可能会降低技术的接受度，与之相反，人类工作者将软件视为支持其工作流程的智能工具。例如，工人监视工具可能会被视为比用作软件开发人员的编程助手的 AI 工具更为关键。为了在数字化工作流程中以有用和可接受的方式代表他们的流程，并实现以人为中心的数字化转型，需要用户在所有步骤中的参与。这对于生成式 AI 等新技术尤为重要。

此外，考虑到我们之前看到的刻板印象和限制，我只能鼓励在其中加入*人类因素*。正如我们在本书的早期部分所看到的，机器学习模型，例如分类器，做出一个*估计*。这个猜测，在给定情景中似乎最有可能，也可能是*错误的*。根据我们正在做出的关键决策的性质，我们需要包含不仅来自训练数据的知识，还要反映人类提出的决策，这些人具有世界知识和经验。此外，由于人类也可能存在偏见，围绕相关决策的流程和文档也至关重要，即使这些决策是由人类做出的时候也是如此。

所有 AI 应用程序在同样程度上都是关键的吗？可能不是。当考虑一个软件将不同类型的螺丝分类到盒子中时，与在法律或医疗领域的其他应用相比，可能存在更少的道德和人类因素问题。需要监管和监控的系统是那些做出关于“人、关系到人的资源、影响人们参与社会能力的问题”的决策的系统（Zweig 2022，第 8 页）。

说了这么多，我想解释一下为什么我不喜欢术语*人工智能*。如前所述，社会的目标应该是生产支持我们任务的工具，而不是致力于开发完全类似人类的通用人工智能。即使如今已经确立并广泛使用，术语*人工智能*是具有误导性的。

因此，越来越多地使用术语*增强智能*^(2)。它旨在利用智能工具增强人类智慧，而不是取代人类。这种对未来数字社会的愿景将人类置于控制位置，并评估软件系统提供的信息。正如之前所见，这对于关键用例尤为重要，比如关于人类的决策。

如图 7.1 所示，增强智能旨在赋予人类决策能力，提供额外信息和见解，而不是通过软件取代人类在决策过程中的作用。![](img/604345_1_En_7_Fig1_HTML.png)

一张图解说明了增强智能模型，通过协助人类进行决策过程来赋予他们技术力量。

图 7.1

增强智能：赋予人类力量而不是取代他们

## 教育的未来

现在让我们讨论这些技术对教育意味着什么。鉴于像 ChatGPT 这样的模型带来的文本生成新可能性，我们在教学和评估学生方面遇到了几个挑战。通过提示 ChatGPT（或其他类似工具），现在在许多情况下可以通过将任务描述粘贴为提示来解决编程练习或写作文。在其他领域，有报道称 ChatGPT 成功通过需要领域特定知识的考试，比如医学。这对我们的教育体系意味着什么？

我们必须审视我们的教学方式以及学生需要发展的技能，同时也要考虑教师或广大公众需要发展的技能。主要挑战在于技术发展迅速，而反映我们的教学方式需要更多时间。我建议的做法不是禁止这些工具，而是教导我们的学生如何负责任地使用它们，特别是了解它们的局限性和陷阱。

我们当然必须调整和审查某些类型的考试以及我们旨在教授的一些内容和技能，也许在评估中有更直接的互动，而不仅仅是在学期末提交一篇论文。但新技术提供的远不止于此。新的生成式人工智能工具还为教育领域提供了许多新的机会，例如，生成新的使用案例与学生讨论并进行批判性反思，并为他们提供智能学习环境，提供个性化的反馈和建议。人类创造力与最新技术将塑造未来的教育。

影响我们工作、学习和教学方式之外，语言模型还可以为其他研究领域提供许多有趣的新问题，这些问题将在未来几年逐渐展开。正如数学家斯蒂芬·沃尔夫勒姆在他最近关于 ChatGPT 的书中所建议的，“人类语言（以及其背后的思维模式）在结构上可能比我们想象的更简单、更*法则化*”（沃尔夫勒姆 2023，第 108 页）。也许这些新技术最终可以在某种程度上帮助我们更好地了解自己。

## 结论：塑造未来

我们在本书中看到了最先进的语言模型以及特别是聊天机器人背后的秘密。尽管我们从内部看到语言模型并不像一开始想象的那么神奇，但它们的结果仍然令人印象深刻。通过提供回应，这些回应可能看起来最像对所提出提示的良好人类回答，它们给我们留下了与有口才的对话伙伴交谈的印象。需要考虑这些陷阱和限制，但这些技术带来的新机会几乎是无穷无尽的，这是我们从中获得的。而*我们*，我指的是我们所有人。我希望鼓励关于技术的公共讨论，涉及技术专家以及其他领域的人。这取决于我们。
