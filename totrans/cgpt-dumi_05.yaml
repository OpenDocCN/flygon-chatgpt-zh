- en: Chapter 4
  prefs: []
  type: TYPE_NORMAL
- en: Understanding GPT Models in ChatGPT
  prefs: []
  type: TYPE_NORMAL
- en: IN THIS CHAPTER
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](images/00003.jpg) Comparing three models in ChatGPT'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](images/00003.jpg) Understanding the difference the upgrades make'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](images/00003.jpg) Creating plans to leverage the models'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](images/00003.jpg) Grasping the importance of extended prompts'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ChatGPT’s models have evolved quickly. The research model using GPT-3 was released
    in November 2022 for public training and testing. By January 2023, Open AI was
    quietly rolling out an upgrade, GPT-3.5, a more stable version and the precursor
    for GPT-4, which was released in March 2023\. In this chapter, you learn about
    these models and how each affects ChatGPT’s performance.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Summarizing Model Progress
  prefs: []
  type: TYPE_NORMAL
- en: As of this writing, ChatGPT defaults to GPT-3.5, but ChatGPT Plus users can
    choose from any of the models listed in the pull-down menu at the top center of
    the user interface, as shown in [Figure 4-1](#filepos175241).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Essentially, GPT-3.5 is an early and partial manifestation of GPT-4 before it
    was fully trained. OpenAI used Chat 3.5 to further develop several specialized
    systems, including ChatGPT.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](images/00056.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[FIGURE 4-1:](#filepos174880) Model options and other selections for ChatGPT
    Plus users.'
  prefs: []
  type: TYPE_NORMAL
- en: The incremental rollout of GPT-3.5 was immediately useful to users and developers
    through its increased stability, better performance, and significant cost cut
    for developers.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: GPT-3.5 is better than GPT-3 in a number of ways, but the two most prevalent
    are increased alignment with user intentions and more refined controls on toxic
    or biased content. GPT-3.5 is less likely to offend or hallucinate and more stable
    overall.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: GPT-4 is the long-awaited and much ballyhooed full version release of this latest
    in the GPT series of models. While the jump between GPT-2 and GPT-3 was bigger
    and more impressive, the jump between GPT-3 and GPT-4 is more significant, useful,
    and notable, mainly because GPT-4 is a high-powered, more stable, and safer model.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Public interest in GPT-4 was high before its release, in March 2023\. ChatGPT
    has cycled through three model versions in a mere four months since the freebie
    research model became publicly available. That in itself is a remarkable achievement.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Comparing GPT-4 to Earlier ChatGPT Models
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4, the latest version to power ChatGPT, is a multimodal model, which means
    this large language model (LLM) can work with images and text in the prompt, but
    its responses are rendered solely as text. ChatGPT-3.5 can use only text in prompts
    and in its responses. ChatGPT-4 also uses more computations on much larger databases
    than its predecessors.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Image interpretation is a unique AI skill often referred to as computer vision
    or machine vision, a nod of recognition of AI’s progression towards more humanlike
    qualities, in this case, the inclusion of sight as an input source.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: With this skill, AI is not just analyzing or matching images but also extracting
    data from them as a human would. For example, a person can look at a receipt and
    immediately understand the exact cost of that transaction or calculate an appropriate
    tip or both.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Similarly, AI can use image inputs to extract the data needed to perform face
    recognition, read content in the image, find evidence in the image of a crime
    scene, or spot a health condition in an X-ray film.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Thus, an AI model that can use images as an input is a very big deal. It's not
    a skill that earlier forms of AI could typically master. Even so, you might wonder
    why it’s notable now, given that multimodal models already exist. After all, OpenAI’s
    own DALL-E 2 is multimodal, with prompts that can be comprised of alphanumeric
    text, images, or both. Plus, DALL-E 2 outputs images. Does this mean it's using
    a better multimodal model than ChatGPT?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The answer is no. DALL-E 2 uses the same GPT models as ChatGPT. But the GPT-4
    model enhances DALL-E 2 with greater creativity, more realism in image creation
    and editing, and far better resolution. DALL-E is an image generator and DALL-E
    2 remains an image generator but with a far more powerful engine.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: By comparison, ChatGPT was previously a single modality system, designed for
    only alphanumeric prompts. Now that ChatGPT uses the GPT-4 model, it has been
    adapted to be multimodal, meaning it can now accept images in its prompts. However,
    ChatGPT is a text generator and remains a text generator, with a few impressive
    upgrades courtesy of the new GPT-4 model.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Those upgrades in capabilities surpass the capabilities of earlier AI systems.
    For example, GPT-4 can interpret images, explain visual humor, and base its reasoning
    on visual input.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Expanding the types of input enables the model to do more complex tasks and
    deeper, more refined analyses. In short, ChatGPT-4 has enhanced problem-solving,
    creative superpowers (for an AI model), and a mind-blowingly massive general knowledge
    base. It’s the largest of the large language models to date.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: However, as mentioned, ChatGPT-4 can't output images. It generates text only,
    just like earlier ChatGPT models, but with more depth in its considerations of
    your input and expectations.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Choosing ChatGPT Models
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT gives you the option of selecting the model you want to use, as shown
    in [Figure 4-2](#filepos181106). Given that ChatGPT-4 can't generate images, if
    you don’t have any images to add to the prompt, do you still need ChatGPT-4? Or
    could you just stick with an earlier version and proceed as usual?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](images/00076.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[FIGURE 4-2:](#filepos180796) You can choose the GPT model to use in your chat.'
  prefs: []
  type: TYPE_NORMAL
- en: The answer depends on what you’re doing with ChatGPT and what you expect or
    need from it. If higher performance is important, reliability measures place the
    GPT-4 model well ahead of GPT-3 and GPT-3.5\. For example, GPT-4 scores alongside
    the top 10 percent of humans taking a simulated bar exam. By comparison, GPT-3.5
    scored with the bottom 10 percent. And that’s not the only human-to-AI comparative
    test in which GPT-4 has rated high. [Figure 4-3](#filepos182274) shows a list
    of academic and professional exams with GPT-4 scores compiled from OpenAI’s GPT-4
    Technical Report, which you can find at [`https://cdn.openai.com/papers/gpt-4.pdf`](https://cdn.openai.com/papers/gpt-4.pdf).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](images/00019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[FIGURE 4-3:](#filepos181891) GPT-4 rankings on academic and professional exams.'
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 performs better because it is better trained. It is built on GPT-3's training,
    including lessons learned from the ChatGPT research model, and is further aligned
    with user intention by OpenAI’s adversarial testing program. The result is a powerful,
    more stable, and higher-performance large language model with multimodal input
    capabilities.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Multimodality is a rising trend in AI research. Two examples of competing multimodal
    models are Microsoft’s recently released Kosmos-1 and Google’s recently enhanced
    PaLM language model.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: But ChatGPT-4 and other multimodal models also carry greater and new risks given
    their increased capability and the tremendous scale of their training. OpenAI
    has taken significant steps to make ChatGPT-4 safer, but safer is a relative term
    and not a guarantee of anything one might consider to be unquestionably safe.
    I say this not to diminish OpenAI’s work in installing significant safety measures,
    but to make sure your expectations are reasonable and you know to proceed with
    caution.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Determine what level of risk acceptance is comfortable for you and then take
    any additional steps as necessary. Precautionary steps should include routinely
    editing and fact-checking ChatGPT responses. But you might want to take additional
    steps, such as consulting an attorney before accepting or implementing ChatGPT
    responses to legal questions or legal documents it has created for you. Consult
    a physician before accepting medical advice from a ChatGPT response regarding
    a correct and safe treatment. And so forth.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Compared to earlier models, ChatGPT-4 has a far greater capacity for reasoning
    and better regulated guardrails that tend to keep the model from shying away from
    questions unnecessarily or boldly offending. Those are reasons enough to use the
    newer model, even if you have neither the need nor the desire to add images to
    the prompt.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: But this is not to say that ChatGPT-3.5 or ChatGPT-3 are obsolete. These models
    are magnificent technological accomplishments and among the largest of large language
    models. Both continue to work well in many use cases. Either can be a good choice
    for several reasons, including if you're on the ChatGPT-4 waitlist or facing a
    traffic-jammed queue for ChatGPT-4 access.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: None of these models is an AI lightweight. Understand the model differences
    and choose according to your needs and preferences. Your ChatGPT chat history
    follows you from model to model, unless you clear it, so you don't need to worry
    about losing any of your earlier work if you trade up (subject to storage and
    token limits).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: However, most users will elect to use GPT-4 or whatever subsequent new model
    arises, either to intentionally benefit from the upgrade or as a consequence of
    the product’s default setting.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learning about GPT-4 Advancements
  prefs: []
  type: TYPE_NORMAL
- en: Of all the advancements in the GPT-4 model, the one that is the most important
    and relevant in terms of overall performance is predictability. The model produces
    outputs that AI human trainers can predict. Predictability is not a strong suit
    in previous ChatGPT versions.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Being able to predict outputs is essential in determining an AI model’s reliability
    and accuracy. For example, if the machine — be it a simple calculator or a generative
    AI model— is presented with the problem of 2+5 and solves it for the answer 7
    every time, the machine is 100 percent accurate and reliable for that calculation.
    However, if it answers 7 only half the time and answers with random numbers the
    other half of the time, it is not considered sufficiently reliable even though
    it gets the answer correct 50 percent of the time.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: OpenAI researchers were able to accurately predict at least some of GPT-4’s
    performance, which is a major achievement in AI development. To get to that enviable
    point, OpenAI researchers spent the past two years rebuilding their entire deep
    learning stack and codesigning a supercomputer with Microsoft, as described in
    [Chapter 2](index_split_004.html#filepos95605). They used this method also to
    produce the GPT-3.5 upgrade, which OpenAI explains was a “first test run” of the
    GPT-4 model to work out the bugs and improve its foundations.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The company released GPT-4's text input capabilities via ChatGPT and the API.
    OpenAI collaborated with a partner, Be My Eyes, to aid with the image input capability
    via the Virtual Volunteer tool, which Be My Eyes used GPT-4 to build. It’s a real-world
    realization of the classic idiom “one hand washes the other and both wash the
    face.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As a recap of ChatGPT model performance measures, consider the comparative ratings
    with state-of-the-art (SOTA) models in [Figure 4-4](#filepos188299).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](images/00044.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[FIGURE 4-4:](#filepos188185) Comparison of ChatGPT models using traditional
    benchmarks provided by OpenAI.'
  prefs: []
  type: TYPE_NORMAL
- en: Adjusting to GPT-4's Limitations
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 also has limitations. It can still hallucinate, that is, deliver information
    as facts that are not facts and make reasoning errors accordingly. Even so, the
    frequency of hallucinatory events is greatly decreased. GPT-4 consistently scores
    40 percent higher than GPT-3.5 on OpenAI’s internal adversarial factuality evaluations
    (battling AI models that test each other's results; think of it as very fast adversarial
    fact-checking).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: GPT-4 is blind to current events and information, given its data age is cut
    off at September 2021\. In other words, its database is largely comprised of data
    scraped from the internet up until that date and has not been updated as of this
    writing. For ChatGPT-4, which is built on GPT-4, to consider recent data or data
    not available on the internet, you must enter that data in the prompt, use a specialized
    plug-in such as Wolfram or Zapier, or use the Browsing plug-in to connect ChatGPT-4
    to the live internet.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: GPT-4 is highly confident in its answers. However, it doesn't always double-check
    its work for errors, so it can hallucinate (be highly confident of an answer that
    is provably wrong).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Risks common to all AI models are increased by virtue of GPT-4's increase in
    scalability — a reference to the sheer size of the database, model parameters,
    and number of users. However, these risks are known and OpenAI lessens their effect
    in GPT-4 through the addition of several safety properties and model-level interventions.
    It can still be manipulated to behave badly, but OpenAI is steadily working to
    make this much harder to do with each new iteration.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: OpenAI uses OpenAI Evals, which is the framework for creating and running benchmarks
    for evaluating AI models such as GPT-4\. OpenAI recently open-sourced this framework
    to enable crowdsourcing and sharing of benchmarks to produce more reliable AI
    models through better testing and training.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Users should take care to fact-check outputs by any ChatGPT model. But this
    extra step isn’t so different from checking your own work or someone else's prior
    to publishing or moving to production, now is it?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As of this writing, OpenAI has not made entering images in the prompt an available
    option for the public. This feature is currently being tested by select users
    and developers. There is a developer waitlist for the API in anticipation of a
    future rollout date.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
