# 第7章

# ChatGPT技术概述：介绍

# 介绍

人工智能或机器学习提供了跨多种形式的自动化监督和无监督学习，无论是文本、图像还是语音，可能是跨不同类型，如数值数据、上下文数据、基于特征的数据、基于模式的数据。自然语言处理（NLP）一直是人工智能领域的一个子领域，它仅占据了几乎五分之一的市场份额和解决方案数量，专注于计算机与人类语言之间的互动。

# 自然语言处理简介

NLP使用计算技术使计算机能够理解、解释和生成人类语言。它是人工智能的重要组成部分之一，涉及语言任务并自动化分析和从任何短语中获取有意义的上下文的过程。这些任务包括情感分析、上下文映射、聊天机器人、内容预测、字幕、答案生成、机器翻译、内容分类等，并且在银行业、金融业、客户服务、健康和医疗、教育以及几乎所有其他实体中使用。近年来，NLP取得了重大进展，这要归功于大型数据集、强大的计算资源和先进的机器学习算法。凭借其处理和理解人类语言的能力，NLP正在帮助弥合人类和机器之间的差距，并使我们与技术的互动更直观和自然。

# NLP的发展

根据斯坦福大学的说法，对NLP的第一个需求始于二战期间的紧急翻译。回到20世纪50年代，研究人员开始探索使用计算机理解和生成人类语言的可能性。1950年，艾伦·图灵提出了“图灵测试”，这是机器智能的一个基准，涉及计算机进行与人类无法区分的对话的能力。这导致了早期NLP系统的开发，例如20世纪60年代开发的“ELIZA”程序，模拟了计算机和人类治疗师之间的对话。

在20世纪70年代，研究人员开始开发更先进的NLP算法，如“SHRDLU”程序，它可以理解自然语言命令并在模拟环境中操作虚拟对象。在20世纪80年代和90年代，研究人员专注于开发用于语言处理的统计模型，这使计算机能够从大量的人类语言数据集中学习。

在20世纪2000年代和2010年代，NLP在深度学习算法的发展和维基百科、社交媒体数据等大型数据集的可用性方面取得了重大进展。这些进展导致了更复杂的NLP应用程序的开发，如语音助手、聊天机器人和机器翻译。

在上个十年的后半段，自然语言处理（NLP）继续取得进展，研究人员在深度学习、迁移学习和预训练等领域取得了重大进展。

NLP中最重要的发展之一是大型预训练语言模型的出现，如BERT（双向编码器表示来自变压器）、GPT-2（生成式预训练变压器2）和GPT-3。这些模型经过大量文本数据的训练，可以执行各种NLP任务，包括文本分类、问答和语言生成。它们使研究人员能够在各种NLP基准测试中取得最先进的结果。

NLP方面的另一个重要发展是迁移学习的使用，其中模型首先在大型数据集上进行预训练，然后针对特定任务进行微调。这种方法已被用于在各种NLP任务上实现高性能，包括情感分析、命名实体识别和文本分类。

除了这些进展，研究人员还专注于改进NLP模型的鲁棒性和公平性。这包括开发方法来检测和减轻语言数据和模型中的偏见，并确保NLP应用对来自不同语言和文化背景的人们是可访问的。

总的来说，NLP方面的这些进展为开发更复杂和准确的基于语言的应用程序打开了新的可能性，从聊天机器人到虚拟助手，可能会对许多行业产生深远的影响。从那时起，LUNAR-科学定性数据，ELIZA-第一个聊天机器人，从今天的复杂模型和用例，如智能Alexa，对话机器人是Siri，具有高级复杂的神经网络后端。在ChatGPT的背景下，这是一个现代先进的NLP架构，能够以更接近人类感知和解释的定量和定性准确性和精度执行非常高级的任务。在此期间，从Word2Vec模型到今天的ChatGPT，通过神经网络、LSTM模型、编码器-解码器、注意力模型、Transformer模型、Google的BERT、imageBERT等，这个过程的改进逐渐而持续地发展。

# [GPT和ChatGPT](toc.xhtml#s66a)

谈到**生成式预训练变压器**（GPT），它是一个复杂的神经网络架构，支撑着ChatGPT的第3.5版GPT系列（称为InstructGPT），是他们最新的开发。由Google于2017年创建的Transformers模型是这个GPT模型的基础和初步元素。它基于首次在论文“**注意力就是你所需要的**”中提出的基于注意力的模型的直觉。

# [GPT系列由OpenAI提供](toc.xhtml#s67a)

在2019年至2022年期间，整个GPT系列通过OpenAI进行了许多技术模型和超参数的调整，并且他们一直在改进许多微观层面的变化。整个GPT-3模型包括大约1750亿个参数，这是谷歌在2018年推出的语言模型BERT的50倍，尽管在NLP研究中有一些装载较重的语言模型，如NVIDIA的Megatron-NLG，具有5300亿个参数，由560个DGX A100服务器组成，每个服务器包含八个A100 80GB GPU，能够自动完成短语和陈述。谷歌的PaLM扩展到了5400亿个参数，是另一个例子，这是一个高度多任务的NLP模型，训练在世界上最大的TPU上，拥有6144个芯片。谷歌还推出了LaMDA；与传统模型经常提供的基于任务的回复相反，该模型可以以自由形式产生对话聊天，也有大约1370亿个参数。 *Dr Alan D. Thompson*博客系列的以下气泡图解释了语言模型中大参数最近发展的估计：

![](images/Figure-7.1.jpg)

**图7.1：** *具有大参数的领先NLP模型[来源：Lifearchitect.ai]*

# [需要记住的要点](toc.xhtml#s68a)

+   自然语言处理（NLP）一直是人工智能领域的一个子领域，它只占据了几乎五分之一的市场份额和解决方案数量，专注于计算机与人类语言之间的交互。

+   NLP使用计算技术使计算机能够理解、解释和生成人类语言。

+   近年来，由于大型数据集、强大的计算资源和先进的机器学习算法的可用性，自然语言处理取得了重大进展。

+   凭借其处理和理解人类语言的能力，自然语言处理正在帮助弥合人类和机器之间的差距，并使我们与技术的互动更直观和自然。

+   在2000年代和2010年代，随着深度学习算法的发展和维基百科、社交媒体数据等大型数据集的可用性，自然语言处理取得了重大进展。

+   在上个十年的后半段，自然语言处理（NLP）继续取得进展，研究人员在深度学习、迁移学习和预训练等领域取得了重大进展。

+   这些模型是在大量文本数据上训练的，可以执行各种自然语言处理任务，包括文本分类、问答和语言生成。

+   除了这些进展之外，研究人员还专注于改进自然语言处理模型的健壮性和公平性。

+   这包括开发方法来检测和减轻语言数据和模型中的偏见，并确保自然语言处理应用对来自不同语言和文化背景的人们是可访问的。

+   总的来说，自然语言处理的这些进展为开发更复杂和准确的基于语言的应用打开了新的可能性，从聊天机器人到虚拟助手，可能会对许多行业产生深远的影响。

# 加入我们书籍的Discord空间

加入书籍的Discord Workspace，获取最新更新、优惠、全球科技动态、新发布和作者交流的信息：

[**https://discord.bpbonline.com**](https://discord.bpbonline.com)

![](images/dis.jpg)
