- en: © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023M.
    Kurpicz-BrikiMore than a Chatbot[https://doi.org/10.1007/978-3-031-37690-0_5](https://doi.org/10.1007/978-3-031-37690-0_5)
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: © 作者，独家许可给 Springer Nature Switzerland AG 2023M. Kurpicz-Briki《不仅仅是聊天机器人》[https://doi.org/10.1007/978-3-031-37690-0_5](https://doi.org/10.1007/978-3-031-37690-0_5)
- en: 5. The True and the False
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5. 真与假
- en: Mascha Kurpicz-Briki^([1](#Aff2)  )(1)Applied Machine Intelligence, Bern University
    of Applied Sciences, Biel/Bienne, Switzerland
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Mascha Kurpicz-Briki^([1](#Aff2)  )(1)应用机器智能，瑞士比尔/宾讷应用科学大学
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'In this chapter, we will dive a bit deeper into a property of state-of-the-art
    language models that were shortly discussed earlier: the sentence created by a
    language model and thus also the answer that a chatbot might provide are *most
    likely* to be a good result. Most likely, because as we have seen previously,
    it is all about probabilities. Depending on what is available in the options,
    even the best option for a given problem might not be a very good answer. Therefore,
    in different ways, state-of-the-art chatbots can provide *wrong* information.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将更深入地探讨先进语言模型的一个特性，即由语言模型创建的句子以及聊天机器人可能提供的答案*很可能*是一个好结果。很可能，因为正如我们之前所见，这都是关于概率的。根据选项中提供的内容，即使对于给定问题来说最佳选项可能也不是一个很好的答案。因此，以不同方式，先进的聊天机器人可能提供*错误*信息。
- en: We will also look at some other chatbot architectures that have been used over
    the years. In particular, we will consider the example of ELIZA, one of the first
    chatbots created in the 1960s.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将研究多年来使用过的一些其他聊天机器人架构。特别是，我们将考虑 1960 年代创建的 ELIZA 的例子，这是最早的聊天机器人之一。
- en: In the last part of the chapter, we will investigate evaluation metrics for
    machine learning models. These metrics help us to identify whether a given model
    is working well.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后部分，我们将研究用于机器学习模型的评估指标。这些指标帮助我们确定给定模型是否运行良好。
- en: What if the Language Model Is Wrong?
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如果语言模型错了会怎样？
- en: Do You Know Pineberries?
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你知道白草莓吗？
- en: 'Let’s start with the following example:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从以下例子开始：
- en: 'Human:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 人类：
- en: “What color does a strawberry have?”
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: “草莓是什么颜色的？”
- en: 'Chatbot:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人：
- en: “Strawberries are typically a deep red color when they are fully ripe. However,
    when they are not fully ripe, they may have green or white areas on the surface.
    The seeds on the surface of the strawberry are typically a yellow or light brown
    color.”
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: “草莓在完全成熟时通常呈深红色。然而，在未完全成熟时，表面可能有绿色或白色区域。草莓表面的种子通常是黄色或浅褐色。”
- en: Whereas this is a correct answer for most strawberries, this is not the case
    for the pineberry, a white strawberry cultivar, shown in Fig. [5.1](#Fig1).![](../images/604345_1_En_5_Chapter/604345_1_En_5_Fig1_HTML.jpg)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这对大多数草莓来说是正确的答案，但对于白草莓（一种白色草莓品种）不适用，如图 [5.1](#Fig1) 所示。![](../images/604345_1_En_5_Chapter/604345_1_En_5_Fig1_HTML.jpg)
- en: A photograph of a bunch of Pineberries lying on the surface.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一张放在表面上的一堆白草莓的照片。
- en: Fig. 5.1
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1
- en: 'Pineberries are white strawberries (Credit: Wikimedia/Emmbean. CC BY 3.0 License,
    available at [https://​commons.​wikimedia.​org/​wiki/​File:​Pineberries.​jpg](https://commons.wikimedia.org/wiki/File:Pineberries.jpg))'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 白草莓是白色草莓（来源：Wikimedia/Emmbean. CC BY 3.0 许可，可在 [https://​commons.​wikimedia.​org/​wiki/​File:​Pineberries.​jpg](https://commons.wikimedia.org/wiki/File:Pineberries.jpg)
    获取）
- en: Maybe some readers did not know about this type of strawberry because it is
    not very well-known? The same for the chatbot. The information a chatbot has to
    generate the answers is limited by the training data it was fed to. For this specific
    question, a chatbot with a wider range of training data and feedback might get
    to the following answer in the conversation:^([1](#Fn1))
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 也许有些读者不知道这种草莓，因为它并不是很有名？对于聊天机器人也是一样。聊天机器人生成答案的信息受到其训练数据的限制。对于这个特定问题，一个具有更广泛训练数据和反馈的聊天机器人可能会在对话中得到以下答案：^([1](#Fn1))
- en: 'Chatbot: “Strawberries are typically a deep red color when they are fully ripe.
    However, when they are not fully ripe, they may have green or white areas on the
    surface. The seeds on the surface of the strawberry are typically a yellow or
    light brown color. *Occasionally, you may come across a variety of strawberries
    that are white or pale pink in color, and these are known as white strawberries*.”'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聊天机器人：“草莓在完全成熟时通常呈深红色。然而，在未完全成熟时，表面可能有绿色或白色区域。草莓表面的种子通常是黄色或浅褐色。*偶尔，你可能会遇到一些白色或淡粉色的草莓品种，这些被称为白草莓*。”
- en: Of course, now, you could say that we are soon there, adding more and more data,
    getting smarter and smarter. However, the problem is that you would not see the
    missing data; you can only see the answer, which might be incomplete (or wrong),
    and it is impossible to test all the potential use cases. Statistically less likely
    things, such as strawberries being white, are less likely to be considered for
    the output. Thus, the information already there are being repeated and reinforced,
    leading to language models being also referred to as *stochastic parrots*, a “system
    for haphazardly stitching together sequences of linguistic forms it has observed
    in its vast training data, according to probabilistic information about how they
    combine, but without any reference to meaning” (Bender et al. [2021](#CR3), p.
    617).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，现在，你可能会说我们很快就会到达那里，不断添加更多数据，变得越来越聪明。然而，问题在于你看不到缺失的数据；你只能看到答案，这可能是不完整的（或错误的），而且不可能测试所有潜在的用例。统计上不太可能发生的事情，比如草莓是白色的，不太可能被考虑为输出。因此，已经存在的信息被重复和强化，导致语言模型也被称为*随机鹦鹉*，一个“根据其广泛的训练数据中观察到的语言形式序列随意拼接的系统，根据它们如何组合的概率信息，但没有参考任何含义”（Bender等人[2021](#CR3)，第617页）。
- en: Hallucinations
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 幻觉
- en: Another problem in state-of-the-art language models are *hallucinations* in
    the generation of texts. The word hallucinations in general describes a sensory
    perception of humans that occurs in absence of an actual external stimulus.^([2](#Fn2))
    In text generation, we mean “text that is nonsensical or unfaithful to the provided
    source content” (Ji et al. [2023](#CR22), p. 248:3). Basically, the chatbot is
    making up information that are not making any sense or are making sense but are
    wrong and cannot be explained based on the training data that was used.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当前最先进的语言模型中存在的另一个问题是在生成文本时出现的*幻觉*。一般来说，幻觉一词描述了人类在没有实际外部刺激的情况下产生的感知。[^2](#Fn2)
    在文本生成中，我们指的是“文本是荒谬的或不忠于提供的源内容”（Ji等人[2023](#CR22)，第248页：3）。基本上，聊天机器人在编造那些毫无意义或是错误且无法根据使用的训练数据解释的信息。
- en: Hallucinations in language models can be *intrinsic* or *extrinsic* hallucinations
    (Ji et al. [2023](#CR22)). Intrinsic hallucinations happen when the generated
    output and the source content are contradicting each other. Let’s say in the training
    data of the language model, there is the following text about the *strawberry
    fan* organization (along with millions of other words and years).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型中的幻觉可以是*内在*的或*外在*的幻觉（Ji等人[2023](#CR22)）。内在幻觉发生在生成的输出和源内容相互矛盾的情况下。比如，在语言模型的训练数据中，有关*草莓粉丝*组织的以下文本（以及数百万其他单词和年份）。
- en: “The strawberry fan organization was founded in 2019, by Anna and Tom, after
    having written a book about the beauty and the wonderful taste of strawberries.”
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: “草莓粉丝组织成立于2019年，由安娜和汤姆创立，之前他们写了一本关于草莓美味和美丽的书。”
- en: 'We then have the following conversation with a chatbot, being based upon this
    language model:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们与一个基于这种语言模型的聊天机器人进行了以下对话：
- en: 'Human:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 人类：
- en: “In which year was the organization dealing with strawberries founded?”
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: “草莓组织是在哪一年成立的？”
- en: 'Chatbot:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人：
- en: “The strawberry fan organization was founded in **2014**.”
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: “草莓粉丝组织成立于**2014**年。”
- en: Wait, what? Yes, we just experienced an intrinsic hallucination.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 等等，什么？是的，我们刚刚经历了一次内在幻觉。
- en: The other type of hallucinations are extrinsic hallucinations. In this case,
    the generated output cannot be verified but can also not be contradicted based
    on the source content. Basically, the information is unknown, but the chatbot
    states an answer to it. This answer might be true but might also be wrong. It
    is unverified information.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种幻觉是外在幻觉。在这种情况下，生成的输出无法验证，但也无法根据源内容进行反驳。基本上，这些信息是未知的，但聊天机器人却给出了一个答案。这个答案可能是正确的，但也可能是错误的。这是未经验证的信息。
- en: For example, we could observe an extrinsic hallucination in the case where the
    chatbot is asked to answer a question about the year 2023; however, the training
    data did only include information up to the year 2022.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以在聊天机器人被要求回答有关2023年的问题时观察到外在幻觉；然而，训练数据只包含到2022年的信息。
- en: Figure [5.2](#Fig2) summarizes the two types of hallucinations.![](../images/604345_1_En_5_Chapter/604345_1_En_5_Fig2_HTML.png)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图[5.2](#Fig2)总结了两种幻觉的类型。![](../images/604345_1_En_5_Chapter/604345_1_En_5_Fig2_HTML.png)
- en: An illustration represents the scenarios in intrinsic and extrinsic hallucinations,
    where the answers are provided by the chatbot from the training data. There is
    an x-condition in the case of intrinsic hallucinations that is unverified in extrinsic.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一幅插图代表了内在和外在幻觉的情景，其中聊天机器人从训练数据中提供答案。在内在幻觉的情况下存在一个未经验证的x条件，在外在幻觉中则不存在。
- en: Fig. 5.2
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2
- en: The difference between intrinsic and extrinsic hallucinations (Based on the
    explanations in Ji et al. [2023](#CR22))
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 内在幻觉和外在幻觉的区别（基于Ji等人[2023](#CR22)的解释）
- en: In the case of hallucinations, we assume that the training data is a ground
    truth, that is, correct, and that it is desirable that this information is reflected
    in the texts that the language model produces. However, depending on where the
    training data is coming from, this is not necessarily the case. Given the large
    quantities of text that are necessary to train a state-of-the-art language model,
    no manual verification of the contents by humans are feasible. Typically, the
    language models that are being trained, shared, and used in many applications
    are based on texts from the Internet.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在幻觉的情况下，我们假设训练数据是一个基本事实，即正确的，并且希望这些信息能够体现在语言模型生成的文本中。然而，根据训练数据的来源，情况并非总是如此。考虑到训练一种最先进的语言模型所需的大量文本，人工对内容进行验证是不可行的。通常，正在训练、共享和用于许多应用程序的语言模型都是基于互联网文本的。
- en: Trained on Fake News?
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练于假新闻？
- en: Fake news and conspiracy theories are itself a problem of today’s society. A
    study has shown that lies spread faster on the Internet than the truth (Vosoughi
    et al. [2018](#CR39)). Interestingly, and as opposed to the general opinion that
    bots^([3](#Fn3)) are a major source of distributing fake news, the study concludes
    that bots distribute fake news and real information at the same rate. On the other
    side, humans are more likely to spread fake news than the bots.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 假新闻和阴谋论本身就是当今社会的问题。一项研究表明，在互联网上谎言传播得比真相更快（Vosoughi等人[2018](#CR39)）。有趣的是，与一般观点认为机器人^([3](#Fn3))是传播假新闻的主要来源相反，该研究得出结论，机器人以相同的速度传播假新闻和真实信息。另一方面，人类更有可能传播假新闻而不是机器人。
- en: So, this means basically that the Internet is full of fake news. Nowadays, more
    and more people start to select the sources of their search engine’s results carefully,
    being aware that not everything that is written on the Internet is necessarily
    true. But what happens when we train a language model based on these contents,
    which provides humanlike answers about factual information without providing the
    corresponding references? Whereas we have learned to be suspicious about websites
    with unknown names, we get trapped by chatbots and interpret meaning and accountability
    for content, even if there is not.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这基本意味着互联网充斥着假新闻。如今，越来越多的人开始谨慎选择他们搜索引擎结果的来源，意识到并非所有在互联网上写的东西都一定是真实的。但是当我们基于这些内容训练一个语言模型，提供关于事实信息的类人回答却没有提供相应的参考资料时，会发生什么？尽管我们已经学会对于那些名不见经传的网站保持怀疑，但我们却被聊天机器人所困扰，解释内容的意义和责任，即使事实并非如此。
- en: In computer science, we have the well-known term *garbage in, garbage out*.^([4](#Fn4))
    It refers to the fact that incorrect or non-sense input data to a computer program
    will produce incorrect or nonsense output, no matter how good the logic is. Whereas
    the expression comes from the very early days of computing, it is also true for
    the field of machine learning. If we see fake news, discriminatory content, or
    other unwanted things in the training data, we can expect it to have an impact
    of the outcome of our machine learning applications. The classifiers or language
    models are only as good as the data they have been trained on.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机科学中，我们有一个众所周知的术语*垃圾进，垃圾出*。^([4](#Fn4)) 它指的是向计算机程序提供不正确或无意义的输入数据将产生不正确或无意义的输出，无论逻辑多么出色。尽管这个表达来自计算的早期阶段，但对于机器学习领域也同样适用。如果我们在训练数据中看到假新闻、歧视性内容或其他不良内容，我们可以预期这将对我们的机器学习应用的结果产生影响。分类器或语言模型的好坏取决于它们所训练的数据。
- en: Different Reasons for Wrong Answers
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 错误答案的不同原因
- en: We have seen different scenarios how wrong information in language models can
    happen. Information about statistically rather rare events can be simply left
    out, intrinsic or extrinsic hallucinations can be produced due to the internal
    functioning of the language model, and the training data itself can contain fake
    news or conspiracy theories. That’s a pretty bad starting point for a trustworthy
    software application. Figure [5.3](#Fig3) shows these different influence factors
    on the quality and trustworthiness of the sentences produced by the language model.![](../images/604345_1_En_5_Chapter/604345_1_En_5_Fig3_HTML.png)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了语言模型中错误信息发生的不同场景。关于统计上相当罕见事件的信息可能会被简单地忽略，由于语言模型的内部功能，可能会产生内在或外在的幻觉，而训练数据本身可能包含虚假新闻或阴谋论。这对于一个值得信赖的软件应用来说是一个非常糟糕的起点。图[5.3](#Fig3)显示了这些不同因素对语言模型生成的句子质量和可信度的影响！[](../images/604345_1_En_5_Chapter/604345_1_En_5_Fig3_HTML.png)
- en: An illustration indicates the rare information not contained in the training
    data, the wrong information contained in the training data, and the sentences
    produced by the language model that contradict or are unverified with regard to
    the training data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一幅插图显示了训练数据中不包含的罕见信息、训练数据中包含的错误信息，以及语言模型生成的与训练数据相矛盾或未经验证的句子。
- en: Fig. 5.3
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3
- en: Different reasons can cause a chatbot to produce a wrong information
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 不同原因可能导致聊天机器人生成错误信息
- en: 'It is good to know that these kinds of things happen *by design*. The way how
    state-of-the-art language models work and the data that are used to train them
    make this happen. Even the creators of large language models are not safe of it:
    when Google’s chatbot Bard was presented in 2023, it made a factual error in the
    first demo (Coulter and Bensinger [2023](#CR8)). The demo produced by Google showed
    Bard answering the question, “What new discoveries from the James Webb Space Telescope
    (JWST) can I tell my 9-year old about?” Bard provided a list of answers, including
    one information indicating that the very first pictures of a planet outside the
    Earth’s solar system was taken by this telescope. However, as confirmed by NASA,
    this information was wrong. This was an expensive error, contributing to a loss
    of 100 billion dollars in market value following the demo.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 很好知道这些事情是*有意设计*的。最先进的语言模型的工作方式以及用于训练它们的数据使这种情况发生。即使是大型语言模型的创建者也无法幸免：当谷歌的聊天机器人巴德在2023年被介绍时，在第一次演示中出现了事实错误（Coulter和Bensinger
    [2023](#CR8)）。谷歌展示的演示显示巴德回答了问题，“詹姆斯·韦伯空间望远镜（JWST）有哪些新发现可以告诉我9岁的孩子？”巴德提供了一系列答案，包括一条信息表明这个望远镜拍摄了地球太阳系之外行星的第一张图片。然而，根据NASA的确认，这个信息是错误的。这是一个昂贵的错误，导致演示后市值损失了1000亿美元。
- en: One more thing to discuss here is the fact that language models can not only
    generate wrong information but also produce non-existing words. As we have quickly
    remarked previously, the input tokens are not necessarily entire words but can
    also be pieces like *pre*, *ing*, or *ized*, which enables a more efficient processing
    of compound, rare, or non-English words (Wolfram [2023](#CR42)). However, this
    can also lead to tokens being put together in a new way, resulting in completely
    new words.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有一件事要讨论，那就是语言模型不仅可以生成错误信息，还可以产生不存在的词。正如我们之前快速提到的，输入标记不一定是整个单词，还可以是像*pre*、*ing*或*ized*这样的片段，这样可以更有效地处理复合、罕见或非英语单词（Wolfram
    [2023](#CR42)）。然而，这也可能导致标记以新的方式组合在一起，从而产生全新的词。
- en: 'When using prompts of tools like ChatGPT,^([5](#Fn5)) this can also be explicitly
    triggered:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用ChatGPT等工具的提示时，^([5](#Fn5))这也可以被明确触发：
- en: 'Human:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 人类：
- en: “Can you think of a new word composed of the tokens *pre* and *strawberry*?”
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: “你能想到一个由*pre*和*strawberry*组成的新词吗？”
- en: 'Chatbot:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人：
- en: “Sure, how about *preberry*? It could refer to a strawberry that is not yet
    fully ripe or has not yet reached its peak sweetness, but is still edible. It
    could also be used more generally to describe something that is in the early or
    preliminary stages of development or readiness, similar to the prefix *pre-*.”
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: “当然，*preberry*怎么样？它可以指代尚未完全成熟或尚未达到最佳甜度的草莓，但仍然可食用。它也可以更普遍地用来描述处于早期或初步发展或准备阶段的事物，类似于前缀*pre-*。”
- en: Whereas here we explicitly asked the system to create a new word, we can also
    find them spontaneously in answers or texts generated by large language models.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在这里我们明确要求系统创建一个新词，但我们也可以在大型语言模型生成的答案或文本中自发地找到它们。
- en: Do Chatbots Lie?
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聊天机器人会撒谎吗？
- en: Let’s get back to our technical explanation on how the sentences are formed,
    based on the prediction of the next word. The way the answers are formed are relying
    on the probability that a specific word comes after the other, under consideration
    of the context. However, language models do not have a humanlike understanding
    of the *meaning* of the words that they create. As with the emotions, the problem
    is again the human perception. The output of the language models seems fluent
    and coherent. Therefore, humans are interpreting the produced sentences, which
    are in the language they speak, as meaningful and corresponding to a communicative
    intent, with an accountability of the content (Bender et al. [2021](#CR3)). And
    this is where the risk lies. If the chatbot is hallucinating but is used to search
    truthful information on the Internet, which is then believed and replicated by
    the user, then we have a problem.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们关于句子如何形成的技术解释，基于对下一个单词的预测。答案的形成方式依赖于特定单词在上下文中出现在另一个单词之后的概率。然而，语言模型并没有对它们创建的单词的*含义*具有类似人类的理解。与情感一样，问题再次出在人类的感知上。语言模型的输出似乎流畅而连贯。因此，人类解释生成的句子，这些句子是用他们说的语言，作为有意义的并对应于交际意图的，具有内容的可靠性（Bender等人[2021](#CR3)）。这就是风险所在。如果聊天机器人产生幻觉，但被用来在互联网上搜索真实信息，然后被用户相信和复制，那么我们就有问题。
- en: The topic whether using state-of-the-art language models is appropriate for
    web search, and whether it will replace the current search engines, has been intensively
    debated over the past months. Researchers warn to use large language models for
    tasks they have not been designed for (Shah and Bender [2022](#CR33)). Due to
    possible factual errors, as we have seen in the previous section, there is a risk
    of an increase of misinformation with this shift of search toward language models.
    However, demos and announcements of large tech companies confirm the aim of providing
    revolution to search, to make any search engine interact with you in a conversational
    manner.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 过去几个月，关于使用最先进的语言模型是否适合网络搜索，以及它是否会取代当前的搜索引擎的话题一直在激烈讨论中。研究人员警告不要将大型语言模型用于它们未经设计的任务（Shah和Bender
    [2022](#CR33)）。由于可能存在事实错误，正如我们在前一节中看到的那样，将搜索转向语言模型存在增加错误信息的风险。然而，大型科技公司的演示和公告证实了提供革命性搜索的目标，使任何搜索引擎以对话方式与您互动。
- en: The question is whether this kind of search is what we are looking for. This
    is more of a societal rather than a technical question. Do we want to ask an expert,
    in the case of this vision an *artificial expert* in form of a language model,
    that provides us aggregated information, potentially containing wrong information?
    Or do we want to stay in control, using a tool to propose us different information
    with their corresponding sources and letting the human compare and reflect the
    information? In both cases, humans need to be aware of the limitations of the
    tools and the proposed contents and build up the required e-literacy. Most of
    us have learned that when using search engines, not necessarily all the websites
    showing up in the results are a trustworthy source of information. With conversational
    tools, we have to reflect our interpretation of the information provided, being
    aware that we might interpret them differently when proposed in a human-like conversation.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是这种搜索是否符合我们的期望。这更多是一个社会问题而不是技术问题。在这种愿景中，我们是否想要询问一个*人工专家*，即语言模型，为我们提供汇总信息，可能包含错误信息？还是我们想保持控制，使用工具为我们提供不同信息及其相应来源，并让人类比较和反思信息？在这两种情况下，人类需要意识到工具和提供的内容的局限性，并建立所需的电子素养。我们大多数人已经学会了，在使用搜索引擎时，并非所有出现在结果中的网站都是可信赖的信息来源。使用对话工具时，我们必须反思我们对所提供信息的解释，意识到当以类似人类对话的方式提出时，我们可能会以不同方式解释它们。
- en: 'Finally, let me make one last observation about the false information produced
    by chatbots. In broad media, often the terminology “the chatbot is lying” or “the
    chatbot is telling lies” is being used. The definition of the verb *to lie* in
    this sense as of *Merriam-Webster* dictionary is as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我对聊天机器人产生的虚假信息做一个最后的观察。在广泛的媒体中，经常使用术语“聊天机器人在撒谎”或“聊天机器人在说谎”。在这种意义上，“撒谎”动词的定义如*韦氏*词典所述：
- en: '1.'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: To make an untrue statement with intent to deceive
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 故意发表不实之论
- en: '2.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: To create a false or misleading impression
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 制造虚假或误导性印象
- en: Whereas the first definition refers to humans, the second definition can also
    be applied to objects, such as the mirror. However, when talking about tools,
    we rarely use this expression in other cases, when tools provide the incorrect
    result compared to what they were expected to do. If the coffee machine produces
    way-too-watery coffee, you would probably be saying that it is broken, not that
    it is scamming you. Scamming, or lying in the general way we use the term, requires
    some intention, intention that is not present in the case of chatbots but that
    is likely interpreted into such conversations by humans. However, we have to keep
    in mind that human intention can lead to the creation of malicious use cases of
    the technology.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个定义涉及人类，而第二个定义也可以应用于物体，比如镜子。然而，当谈论工具时，在其他情况下，当工具提供与预期不符的结果时，我们很少使用这个表达。如果咖啡机冲出的咖啡太稀，你可能会说它坏了，而不是说它在欺骗你。欺骗，或者用我们通常使用这个术语的一般方式来说就是撒谎，需要一些意图，而在聊天机器人的情况下是不存在的，但是人类可能会在这样的对话中解读出这种意图。然而，我们必须记住，人类的意图可能导致技术被滥用。
- en: So overall, we have chatbots hallucinating false information and humans with
    social hallucinations, seeing intent and humanlike meaning where there is not.
    We have some work ahead as digital society.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，总体而言，我们有聊天机器人产生虚假信息，以及人类产生社交幻觉，看到意图和人类意义的地方。我们在数字社会中还有一些工作要做。
- en: A Short History of Chatbots
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聊天机器人的简史
- en: We have seen in previous sections how state-of-the-art chatbots based on the
    transformer architecture work. We will now gain some background knowledge of other
    (mostly previous) variants. Chatbots are also referred to as *dialog systems*
    or *dialog engines.* Typically, their task is to conduct a conversation in written
    form with a human. When the conversation happens in spoken form, they are also
    referred to as *voicebots*. These dialog systems can be digital assistants like
    Siri or Alexa or integrated into websites to allow customers to search for relevant
    information. We limit our discussion in this section to text-based methods to
    see how they relate to the text processing and generation techniques we have seen
    previously.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在前几节中看到了基于变压器架构的最先进聊天机器人是如何工作的。现在我们将获得其他（主要是以前的）变体的一些背景知识。聊天机器人也被称为*对话系统*或*对话引擎*。通常，它们的任务是以书面形式与人类进行对话。当对话以口头形式进行时，它们也被称为*语音机器人*。这些对话系统可以是数字助手，如Siri或Alexa，也可以集成到网站中，以允许客户搜索相关信息。在本节中，我们将限制讨论文本为基础的方法，以了解它们与我们之前看到的文本处理和生成技术的关系。
- en: ELIZA
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ELIZA
- en: A very famous example of an early chatbot is ELIZA, developed by Joseph Weizenbaum
    at the Massachusetts Institute of Technology (MIT) in the 1960s (Weizenbaum [1966](#CR40)).
    Its creator wanted to explore how the communication between machines and humans
    using natural language can be enabled. The technical implementation of ELIZA was,
    compared to today’s methods, rather simple. The user input (the text the human
    is writing to ask a question to the chatbot) was checked for specific *keywords*.
    This is to some extent similar to the very simple text analytics method we have
    seen earlier, where we identified whether a text is about strawberries, by checking
    if the words *strawberry*, or *strawberries* appear in the text. If a keyword
    was present in the user’s question, the answer was generated based on *rules*.
    Content-free remarks were identified and not considered, and under some conditions,
    earlier transformations were considered too. Using this rule-based, hard-coded
    approach, there was no training data or machine learning involved.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一个早期聊天机器人的非常著名的例子是**ELIZA**，由约瑟夫·魏岑鲍姆（Joseph Weizenbaum）在1960年代在麻省理工学院（MIT）开发（Weizenbaum
    [1966](#CR40)）。它的创造者想要探索如何实现机器和人之间使用自然语言进行交流。与今天的方法相比，ELIZA的技术实现相对简单。用户输入（人类写给聊天机器人提问的文本）被检查是否包含特定的*关键词*。这在某种程度上类似于我们之前看到的非常简单的文本分析方法，通过检查文本中是否出现*草莓*或*草莓*这些词来确定文本是否与草莓有关。如果用户的问题中包含关键词，答案将基于*规则*生成。无内容的评论被识别并不予考虑，在某些情况下，还考虑了早期的转换。使用这种基于规则的硬编码方法，没有涉及训练数据或机器学习。
- en: Using ELIZA, conversations as the one shown in Fig. [5.4](#Fig4) were possible.![](../images/604345_1_En_5_Chapter/604345_1_En_5_Fig4_HTML.png)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ELIZA，可以进行如图[5.4](#Fig4)所示的对话！[](../images/604345_1_En_5_Chapter/604345_1_En_5_Fig4_HTML.png)
- en: A set of questions and answers simulates the conversation between a human and
    a machine-generated answer. The responses from the machines are written in uppercase.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一组问题和答案模拟了人类和机器生成答案之间的对话。机器的回应以大写字母写成。
- en: Fig. 5.4
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4
- en: Extract of the conversation from (Weizenbaum [1966](#CR40)); capitalized expression
    are the machine-generated answers
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从（Weizenbaum [1966](#CR40)）中提取的对话片段；大写表达式是机器生成的答案
- en: The major difference in this conversation compared to state-of-the-art chatbots
    is that the conversation does not aim to answer questions with real-world knowledge.
    As Weizenbaum states in its original report, participants interacting with ELIZA
    were instructed to communicate with it as if they were communicating with a psychiatrist.
    Why is that?
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 与现代聊天机器人相比，这种对话的主要区别在于对话不旨在用真实世界知识回答问题。正如Weizenbaum在其原始报告中所述，与ELIZA互动的参与者被指示与其交流，就好像他们在与精神科医生交流一样。为什么会这样？
- en: This mode of conversation was chosen because the psychiatric interview is one
    of the few examples of categorized dyadic natural language communication in which
    one of the participating pair is free to assume the pose of knowing almost nothing
    of the real world. (Weizenbaum [1966](#CR40), p. 42)
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 选择这种对话模式是因为精神病学面试是少数几个例子之一，其中一个参与对话的人可以自由地假设几乎不了解真实世界。（Weizenbaum [1966](#CR40)，第42页）
- en: Any content that comes into the conversation is the input of the human, who
    brings these topics up.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 进入对话的任何内容都是人类的输入，是他们提出的话题。
- en: The major limitation of this kind of chatbot is also an advantage. Whereas it
    is not able to provide any knowledge, it is also much more unlikely to provide
    false information. In the worst case, it would provide an answer that seems not
    to fit the input text from the user. Since all the potential answers are listed
    somewhere in the rules and templates of this type of solution, experiments can
    be replicated easily. The answers provided by the chatbot can be fully explained
    and reproduced. Therefore, depending on the use case, chatbots inspired by and
    based on the methods initially developed in ELIZA can be powerful.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的聊天机器人的主要局限性也是优点。虽然它无法提供任何知识，但也更不太可能提供错误信息。在最坏的情况下，它会提供一个看起来不符合用户输入文本的答案。由于这种类型解决方案的所有潜在答案都列在规则和模板的某处，实验可以轻松复制。聊天机器人提供的答案可以完全解释和复制。因此，根据用例，受到ELIZA最初开发的方法启发的和基于这些方法的聊天机器人可能非常强大。
- en: Rule-Based Approaches
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于规则的方法
- en: 'In the example shown above with ELIZA, no information about the world is provided.
    Rule-based approaches can also be used when we want to transmit very specific
    information. For example, consider a chatbot on a retail company’s online shop
    website that enters the conversation with any customer as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面展示的ELIZA示例中，没有提供关于世界的信息。当我们想传递非常具体的信息时，也可以使用基于规则的方法。例如，考虑一个零售公司在线商店网站上的聊天机器人，与任何顾客的对话如下：
- en: 'Chatbot:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人：
- en: “Hey, thanks for visiting this shop! What are you looking for?”
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: “嘿，谢谢光临这家店！你在找什么？”
- en: 'Human:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 人类：
- en: “I would like to eat something tasty, maybe strawberries?”
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: “我想吃点好吃的东西，也许草莓？”
- en: 'Chatbot:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人：
- en: “That’s a good idea! Click on this link for strawberries to directly add the
    regional strawberries from our shop to the basket.”
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: “这是个好主意！点击这个链接以直接将我们商店的当地草莓添加到购物篮中。”
- en: This could be implemented in a very simple way, using lists of words and rules.
    Let’s have a closer look at this based on the architecture of a chatbot we have
    seen before. The user interacts with the web application, as in the previous case.
    Now, rather than having the complex transformer-based architecture of the language
    model in the background providing the answer, a different approach is used in
    this example. As illustrated in Fig. [5.5](#Fig5), the user’s input text is analyzed
    for keywords. We are looking for specific words that help us propose a product
    to the customer. In the simplest case, this is a list of all the product’s names
    that we have in our shop. By matching the found keywords to this list, we identify
    the right product and generate the answer as in the conversation above. More advanced
    versions can make use of other text processing methods we have seen earlier, for
    example, stemming, lemmatization, or tagging or more complex rule systems.![](../images/604345_1_En_5_Chapter/604345_1_En_5_Fig5_HTML.png)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以以非常简单的方式实现，使用单词列表和规则。让我们更仔细地看一下这一点，基于我们之前看到的聊天机器人的架构。用户与网络应用进行交互，就像之前的情况一样。现在，与其在后台使用复杂的基于变压器的语言模型架构提供答案，这个示例中使用了不同的方法。如图[5.5](#Fig5)所示，用户的输入文本被分析以查找关键词。我们正在寻找能帮助我们向客户推荐产品的特定单词。在最简单的情况下，这是我们商店中所有产品名称的列表。通过将找到的关键词与此列表匹配，我们确定正确的产品并生成答案，就像上面的对话中一样。更高级的版本可以利用我们之前看到的其他文本处理方法，例如词干提取、词形还原、标记或更复杂的规则系统。![](../images/604345_1_En_5_Chapter/604345_1_En_5_Fig5_HTML.png)
- en: A diagram illustrates a set of user-provided input prompts on a web application
    along with their generated answers. It highlights the keyword named strawberry
    and indicates to identify the keywords in the input and map them to the available
    products for generating the answer.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一张图示了一个网络应用中一组用户提供的输入提示以及它们生成的答案。它突出显示了名为草莓的关键词，并指示识别输入中的关键词并将其映射到可用产品以生成答案。
- en: Fig. 5.5
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5
- en: Example of a simple rule-based chatbot for an online shop
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在线商店简单基于规则的聊天机器人示例
- en: Of course, everybody would easily notice how our method works if the text is
    each time exactly the same. A simple yet efficient way to get around this is that
    we make another list of ten alternative versions of “That’s a good idea.” These
    alternatives could include “Good choice!” or “Sounds like a plan.” Each time an
    answer is generated, one of these ten alternatives can be chosen randomly.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果每次文本都完全相同，每个人都会很容易注意到我们的方法是如何工作的。一个简单而有效的方法是我们制作另一个包含十个“这是个好主意”的替代版本的列表。这些替代版本可以包括“不错的选择！”或“听起来像个计划。”每次生成答案时，可以随机选择其中一个。
- en: This solution is feasible, because the context of the information is limited
    in this case or, at least, for the questions we can answer. The customer might
    of course ask for something not related at all to this online shop, then it would
    not work. We would need to think of possible answers, in case no keywords are
    identified. For example, “Can you explain me more in detail what you are needing
    this for?” If we would like to extend this program to a system using these techniques
    to communicate with a human about any kind of topic, we would find ourselves with
    more rules and keywords than we could ever create or manage.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案是可行的，因为在这种情况下信息的上下文是有限的，或者至少对我们可以回答的问题是有限的。当然，客户可能会要求与这家在线商店完全无关的东西，那么它就不起作用了。我们需要考虑在没有识别到关键词的情况下可能的答案。例如，“你能详细解释一下你需要这个东西的原因吗？”如果我们想将这个程序扩展到使用这些技术与人类交流关于任何主题的系统，我们将发现自己需要比我们能够创建或管理的规则和关键词更多。
- en: A Simple Chatbot with Sentence Embeddings
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用句子嵌入创建的简单聊天机器人
- en: Let’s now consider another example of a method to create a chatbot by using
    the information stored in sentence embeddings. We saw earlier that there are different
    ways to store the meaning of words or sentences in vectors. Based on their position
    in the vector space, and using mathematical operations, we were able to extract
    information about similarity of the words or sentences. We assume that we want
    to create a chatbot that can answer questions about strawberries.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑另一个使用句子嵌入中存储的信息创建聊天机器人的方法示例。我们之前看到存储单词或句子含义的不同方法。根据它们在向量空间中的位置，并使用数学运算，我们能够提取有关单词或句子相似性的信息。我们假设我们想要创建一个可以回答关于草莓的问题的聊天机器人。
- en: To do this, we use the text from Wikipedia on the article about strawberries.
    It contains different sections about the history or the cultivation of strawberries.
    We split this article into sentences and convert each sentence into a sentence
    vector, as shown in Fig. [5.6](#Fig6).![](../images/604345_1_En_5_Chapter/604345_1_En_5_Fig6_HTML.png)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们使用维基百科关于草莓文章的文本。它包含关于草莓历史或栽培的不同部分。我们将这篇文章分成句子，并将每个句子转换为句子向量，如图[5.6](#Fig6)所示。![](../images/604345_1_En_5_Chapter/604345_1_En_5_Fig6_HTML.png)
- en: A diagram illustrates a user query regarding the strawberry plant, going through
    the sentence embedding of the user's question, finding similar embedding, finding
    texts about strawberries, and the sentence embedding of each sentence.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一张图解释了用户关于草莓植株的查询，通过用户问题的句子嵌入，找到相似的嵌入，找到关于草莓的文本，以及每个句子的句子嵌入。
- en: Fig. 5.6
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6
- en: A simple chatbot based on sentence embeddings and similarity
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 基于句子嵌入和相似性的简单聊天机器人
- en: 'We then do the same with the user input. We take the question, for example,
    “What do you know about strawberry plants?” and create also a vector (sentence
    embedding) from it. As we have seen earlier, the distance between vectors can
    be used to capture relation between words or sentences. Therefore, we are looking
    for a vector close to our question’s sentence embedding, which might be a potential
    answer to our question. We thus identify which of the sentence embeddings of the
    Wikipedia article is the closest and output this sentence, as shown in Fig. [5.6](#Fig6).
    This enables the following conversation with the chatbot:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们对用户输入做同样的处理。我们拿问题，比如“你知道关于草莓植株的什么？”并从中创建一个向量（句子嵌入）。正如我们之前看到的，向量之间的距离可以用来捕捉单词或句子之间的关系。因此，我们寻找一个接近我们问题句子嵌入的向量，这可能是我们问题的潜在答案。因此，我们确定维基百科文章的哪个句子嵌入最接近，并输出这个句子，如图[5.6](#Fig6)所示。这使得与聊天机器人的以下对话成为可能：
- en: 'Human:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 人类：
- en: “What do you know about strawberry plants?”
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: “你知道关于草莓植株的什么？”
- en: 'Chatbot:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人：
- en: “strawberry plants produce more and better fruit when they are young.”
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: “草莓植株在年轻时会产生更多更好的水果。”
- en: 'This works quite well for different questions, but is far from perfect, as
    the following example shows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不同的问题，这种方法效果相当不错，但远非完美，下面的例子就说明了这一点：
- en: 'Human:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 人类：
- en: “In which countries are strawberries grown?”
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: “草莓在哪些国家种植？”
- en: 'Chatbot:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人：
- en: “strawberries can also be grown indoors in strawberry pots.”
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: “草莓也可以在草莓盆中室内种植。”
- en: Here we used as input the one article about strawberries. When extending this
    by using all books about strawberries that are available at the local library,
    results could probably be improved. The chatbot is only as good as the data it
    has available.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们以草莓的一篇文章作为输入。如果扩展到使用当地图书馆提供的所有关于草莓的书籍，结果可能会得到改善。聊天机器人的表现取决于其可用的数据。
- en: Whereas for many tasks the state-of-the-art language models are by far the most
    efficient solutions, I want you to keep in mind that depending on what you are
    looking for, simpler architectures can do a good job as well. A major advantage
    of these simpler methods is often their explainability, which allows us to see
    behind the scenes easier and let us understand how decisions were made. Sometimes
    we can also find combinations of different methods combined in practice.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对于许多任务来说，最先进的语言模型通常是最有效的解决方案，但我希望你记住，根据你所寻找的内容，更简单的架构也可以做得很好。这些简单方法的一个主要优势通常是它们的可解释性，这使我们更容易看到幕后情况，并让我们了解决策是如何做出的。有时我们也可以在实践中找到不同方法的组合。
- en: What Is a Good Model?
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是一个好的模型？
- en: We have seen so far that state-of-the-art chatbots rely on powerful language
    models. But how can we know how *well* the language models perform or compare
    them among each other?
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到，最先进的聊天机器人依赖于强大的语言模型。但我们如何知道语言模型的表现如何或者如何将它们进行比较呢？
- en: A typical number that is mentioned with regard to language models is the number
    of *parameters* it was trained with. For example, Google’s BERT model has been
    trained with 340 million parameters or GPT-3 with 175 billion parameters. These
    are quite large numbers! Let’s remember the architecture of language models we
    have seen earlier. We saw different types of neural networks, some involving different
    layers of encoders and decoders. In each of these layers, there were neural networks,
    and each neural network consists of several neurons, having inputs that are weighted.
    These weights are adapted during the training, depending how relevant they are
    for the final decision of the system. These are the “knobs” one can turn to make
    the model fit the data (Wolfram [2023](#CR42)). When saying that there are 175
    billion parameters, it means that the architecture of the neural network has 175
    billion weights to tweak.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 有关语言模型提到的一个典型数字是它训练时使用的*参数*数量。例如，谷歌的BERT模型使用了3.4亿个参数，或者GPT-3使用了1750亿个参数。这些都是相当大的数字！让我们记住之前看到的语言模型的架构。我们看到了不同类型的神经网络，有些涉及不同层的编码器和解码器。在每个层中，都有神经网络，每个神经网络由多个神经元组成，具有加权的输入。这些权重在训练过程中会被调整，取决于它们对系统最终决策的相关性有多大。这些就是可以调整的“旋钮”，以使模型适应数据（Wolfram
    [2023](#CR42)）。当说有1750亿个参数时，这意味着神经网络的架构有1750亿个需要调整的权重。
- en: Does larger mean better? For the moment, yes. We have seen over the last years
    how the number of parameters became bigger and bigger with each new model, and
    the models became better. However, at the same time, more computational power
    is required, and the complexity of the model increases. With 175 billion parameters,
    it is pretty hard to assess what a word suggestion was based on.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 更大是否意味着更好？目前是的。我们在过去几年中看到，随着每个新模型的出现，参数的数量变得越来越大，模型也变得更好。然而，与此同时，需要更多的计算能力，模型的复杂性也增加。有1750亿个参数，很难评估一个单词建议是基于什么的。
- en: 'The parameters are figured out by the system itself, during the training phase,
    by using the external feedback (e.g., the *correct answers* available to the system).
    What the data engineer can use to optimize the training process of the system
    are the *hyperparameters*. The training happens in loops (remember the example:
    doing over and over again the same thing!). Each loop is called *epoch*. One epoch
    equates to each training sample has been viewed once and the weights adjusted
    accordingly. The training process is observed over several epochs, and at some
    point, the moment to stop has come. This typically happens when there is no more
    improvement in the learning. The system does not get any better anymore with the
    given training data. The number of epochs used for training is therefore one of
    the hyperparameters that is used by the data engineer. Another hyperparameter
    in a neural network can, for example, be the number of layers. Typically, the
    data engineer would train models with different hyperparameter settings and compare
    their output. This process is called *hyperparameter tuning*, as shown in Fig.
    [5.7](#Fig7).![](../images/604345_1_En_5_Chapter/604345_1_En_5_Fig7_HTML.png)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 参数是系统在训练阶段自行解决的，通过使用外部反馈（例如，系统可用的*正确答案*）。数据工程师可以用来优化系统训练过程的是*超参数*。训练发生在循环中（记住这个例子：一遍又一遍地做同样的事情！）。每个循环称为*epoch*。一个epoch等于每个训练样本被查看一次，并相应地调整权重。训练过程在几个epoch内观察，某个时刻，停止的时刻就到了。这通常发生在学习没有更多改进时。系统在给定的训练数据上不再变得更好。用于训练的epoch数量因此是数据工程师使用的超参数之一。神经网络中的另一个超参数，例如，可以是层数。通常，数据工程师会使用不同的超参数设置来训练模型并比较它们的输出。这个过程称为*超参数调整*，如图[5.7](#Fig7)所示。![](../images/604345_1_En_5_Chapter/604345_1_En_5_Fig7_HTML.png)
- en: A block diagram illustrates how the input from the data engineer goes through
    the language model based on neural networks, where the input passes through three
    layers before obtaining the output. All the layers denote interconnection among
    the neurons.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 一个块图解说明了数据工程师的输入是如何通过基于神经网络的语言模型进行的，输入在获得输出之前通过三层。所有这些层都表示神经元之间的相互连接。
- en: Fig. 5.7
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7
- en: Example of some of the hyperparameters that the data engineer can tune in the
    training process
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程师可以在训练过程中调整的一些超参数示例
- en: To compare the performance of machine learning models and to find out which
    hyperparameter configuration is the best, we need some *performance metrics*.
    We will explore those metrics by using the sentiment analysis classification example
    we introduced earlier in this book. We have a bunch of texts and want to identify
    whether each of them is rather negative or positive. We have used the training
    dataset (something like 80% of the labeled data) for the training phase and have
    now the test dataset to test the performance of the classifier. Each of these
    texts in the test dataset is labeled as negative or positive, respectively. However,
    this information is unseen by the classifier, meaning that it was not used in
    the training phase. In the test phase, we want to use this data to measure how
    good the system performs in new, unseen data.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较机器学习模型的性能，并找出哪种超参数配置最佳，我们需要一些*性能指标*。我们将通过使用本书中早前介绍的情感分析分类示例来探讨这些指标。我们有一堆文本，并希望确定每个文本是更负面还是更正面的。我们已经使用了训练数据集（类似于标记数据的80%）进行训练阶段，现在有了测试数据集来测试分类器的性能。测试数据集中的每个文本都被标记为负面或正面。然而，分类器看不到这些信息，这意味着它在训练阶段没有使用这些信息。在测试阶段，我们希望使用这些数据来衡量系统在新的、未见过的数据上的表现如何。
- en: The simplest way is to calculate the *accuracy*. The accuracy indicates how
    many samples from the test data were labeled correctly by the classifier. If in
    our test data we have 100 text samples, 50 labeled as positive and 50 labeled
    as negative, and our classifier labels 80 of them correctly, then we have an accuracy
    of 80%. In this example, our dataset is *balanced*. Balanced because we have the
    same number of negative and positive examples. In case of unbalanced datasets,
    the metric accuracy can be too limited. If we have 90% negative samples in our
    test data, and only 10% positive text samples, a classifier that classes all text
    as negative would have an accuracy of 90%. This could lead us to think that the
    performance of the classifier is quite good, which is not the case.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法是计算*准确率*。准确率表示分类器正确标记了测试数据中多少样本。如果在我们的测试数据中有100个文本样本，其中50个标记为正面，50个标记为负面，而我们的分类器正确标记了其中的80个，那么我们的准确率为80%。在这个例子中，我们的数据集是*平衡*的。之所以平衡，是因为我们有相同数量的负面和正面样本。在不平衡数据集的情况下，准确率这个指标可能会受到限制。如果我们的测试数据中有90%的负样本，而只有10%的正文本样本，一个将所有文本都分类为负面的分类器将具有90%的准确率。这可能会让我们误以为分类器的性能相当不错，但事实并非如此。
- en: We want to measure how the classification works for both of the classes. We
    therefore have to do a more fine-grained analysis of the predictions the classifier
    made wrong or right. We are considering separately the cases where the classifier
    was wrong, because it estimated *positive* but the text was indeed *negative*
    and the cases that the classifier suggested *negative* and the text was *positive*.
    A data item in the test data that was predicted as positive correctly is therefore
    referred to as true positive (TP). On the other side, a data item that was predicted
    as positive but is in reality negative is referred to as false positive (FP).
    It showed up as positive in the results, but its classification was wrong. In
    similar way, we define true negative (TN) (predicted correctly as negative) and
    false negative (FN) (predicted as negative, but actually positive) as shown in
    Fig. [5.8](#Fig8). In the example, there are ten texts in the test data set, five
    of them are positive, and five of them are negative. Note that the accuracy of
    70% (seven out of ten are predicted correctly) is independent of the number of
    false positives and false negatives. Whether the wrong predictions are concerning
    the negative or the positive class does not impact the accuracy.![](../images/604345_1_En_5_Chapter/604345_1_En_5_Fig8_HTML.png)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要衡量分类对两个类别的工作情况。因此，我们必须对分类器错误或正确做出更细致的分析。我们分别考虑分类器错误的情况，因为它估计为*正面*但实际上是*负面*的文本，以及分类器建议为*负面*但文本实际上是*正面*的情况。在测试数据中被正确预测为正面的数据项因此被称为真正例（TP）。另一方面，被预测为正面但实际上是负面的数据项被称为假正例（FP）。它在结果中显示为正面，但其分类是错误的。类似地，我们定义真负例（TN）（被正确预测为负面）和假负例（FN）（被预测为负面，但实际上是正面）如图[5.8](#Fig8)所示。在这个例子中，测试数据集中有十个文本，其中五个是正面，五个是负面。请注意，70%的准确率（十个中有七个被正确预测）与假正例和假负例的数量无关。错误预测是关于负面还是正面类别并不影响准确率。![](../images/604345_1_En_5_Chapter/604345_1_En_5_Fig8_HTML.png)
- en: A rectangular block represents a set of 10 texts of test data comprising the
    correct labels and predicted labels. It indicates an accuracy value of 70 percent
    and highlights 3 data points that have a mismatch between the predicted and actual
    outcomes.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 一个矩形块代表了一个包含正确标签和预测标签的10个测试数据文本集合。它指示了70%的准确度值，并突出显示了3个数据点，其预测和实际结果不匹配。
- en: Fig. 5.8
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8
- en: Example of a test dataset with ten texts. Based on the correct labels and the
    predictions, the different metrics can be computed to assess the performance of
    the model
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 十个文本的测试数据集示例。基于正确标签和预测，可以计算不同的指标来评估模型的性能。
- en: The *precision* is another metric that considers these aspects. It tells us
    the fraction of data items that have been predicted as positive and are positive
    in the correct solution. In the example, where we have four true positives and
    a total of six positive predictions (true positives + false positives), the precision
    is 4/6 or, expressed in percentage, 66%. In the best case, all data items predicted
    as positive are actually correct; then the precision would be 100%.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*精确率* 是考虑这些方面的另一个指标。它告诉我们被预测为正面并在正确解决方案中为正面的数据项的比例。在我们的例子中，我们有四个真正例和总共六个正面预测（真正例
    + 假正例），精确率为4/6或以百分比表示为66%。在最好的情况下，所有被预测为正面的数据项实际上都是正确的；那么精确率将是100%。'
- en: Along with the precision, we typically look at the *recall*, which shows us
    the other side. It measures the fraction of positive texts that have been correctly
    identified as positive (=true positives) in comparison to all the positive training
    texts (=true positives + false negatives). In our example, the recall is 4/5,
    or 80%.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 除了精确率，我们通常关注 *召回率*，它展示了另一方面。它衡量了被正确识别为正面的正面文本的比例（=真正例）与所有正面训练文本（=真正例 + 假负例）的比较。在我们的例子中，召回率为4/5，或80%。
- en: Typically, the data engineer wants to achieve a trade-off between recall and
    precision. We therefore need to consider both measures along with each other.
    To assess whether a machine learning classifier is performing well, we need them
    both. Therefore, the *f1-score* was invented, which computes a score involving
    both recall and precision. Often, when developing a machine classifier, we therefore
    try to maximize the *f1*-score.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，数据工程师希望在召回率和精确率之间取得平衡。因此，我们需要同时考虑这两个指标。为了评估一个机器学习分类器是否表现良好，我们需要它们两者。因此，*f1-分数*
    被发明出来，它计算涉及召回率和精确率的分数。通常，在开发机器分类器时，我们因此尝试最大化 *f1*-分数。
- en: In our case, positive and negative corresponded to the names of our classes,
    since we are predicting positive and negative texts. This is not necessarily the
    case. For the classification of spam e-mails, the positive label could be that
    the e-mail is effectively spam, and the negative label is that it is not spam.
    The terms true positive, true negative, etc. would still be used as described
    above.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，正面和负面对应于我们的类别名称，因为我们正在预测正面和负面的文本。这并不一定是这样的。对于垃圾邮件的分类，正面标签可以表示邮件实际上是垃圾邮件，而负面标签表示它不是垃圾邮件。术语真正例、真负例等仍然会像上面描述的那样使用。
- en: Note that most of the metrics described here referred to the problem of binary
    classification. In binary classification, we have two groups (e.g., positive and
    negative texts), and the classifier aims to estimate which class a new text belongs
    to. In other cases, we might have multiclass problems, and other yet similar metrics
    calculations would be used.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里描述的大多数指标都涉及二元分类问题。在二元分类中，我们有两组（例如，正面和负面文本），分类器的目标是估计新文本属于哪一类。在其他情况下，我们可能有多类问题，并且会使用其他类似的指标计算。
- en: In this section, with *good*, we meant whether the machine learning system performs
    well for a specific task, using accuracy, precision, recall, and *f1*-score to
    quantify the performance. Whether its decisions are fair or ethical is yet another
    discussion. We will dive deeper into this topic in the next chapter.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，*好* 意味着机器学习系统是否在特定任务中表现良好，使用准确率、精确率、召回率和 *f1*-分数来量化性能。它的决策是否公平或道德是另一个讨论。我们将在下一章更深入地探讨这个话题。
- en: Summary
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have seen how chatbots can provide wrong information for
    different reasons. Examples can be a lack of relevant information in the training
    data as well as intrinsic or extrinsic hallucinations.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经看到聊天机器人可能因为不同原因提供错误信息。例子可以是训练数据中缺乏相关信息，以及内在或外在的幻觉。
- en: We have also looked at some other architectures starting from ELIZA of the 1960s.
    Whereas other architectures are not as performant as the state-of-the-art chatbots
    and language models, it is easier to explain how they came to a given output.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还研究了从上世纪60年代的ELIZA开始的一些其他架构。虽然其他架构不如最先进的聊天机器人和语言模型表现出色，但更容易解释它们如何得出给定的输出。
- en: In the last part of this chapter, we have learned about different performance
    metrics for machine learning models. Evaluating the efficiency of the model on
    a test dataset, the model can be improved during the training phase.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后部分，我们学习了有关机器学习模型的不同性能指标。通过在测试数据集上评估模型的效率，模型可以在训练阶段得到改进。
