- en: 3 Getting Started with LangChain
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 使用LangChain入门
- en: Join our book community on Discord
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Discord上加入我们的书籍社区
- en: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
- en: '![Qr code Description automatically generated](../media/file19.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的二维码描述](../media/file19.png)'
- en: 'In this chapter, we''ll first set up **LangChain** and the libraries needed
    for this book giving instructions for common dependency management tools such
    as **Docker**, **Conda**, **Pip**, and **Poetry**. Then we''ll go through model
    integrations that we can use such as **OpenAI''s** **Chatgpt**, models on Huggingface
    and Jina AI, and others. We''ll introduce, set up, and work with a few providers
    in turn. We''ll get an API key tokens and then do a short practical example. This
    will give us a bit more context at using **LangChain**, and introduce tips and
    tricks for using it effectively. As the final part, we''ll develop a **LangChain**
    application, a practical example that illustrate a way that **LangChain** can
    be applied in a real-world business use case in customer service.The main sections
    are:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先设置**LangChain**和本书所需的库，提供了关于常见依赖管理工具如**Docker**、**Conda**、**Pip**和**Poetry**的说明。然后我们将介绍可以使用的模型集成，如**OpenAI**的**Chatgpt**，Huggingface和Jina
    AI上的模型等。我们将依次介绍、设置和使用几个提供商。我们将获取API密钥令牌，然后进行一个简短的实际示例。这将为我们提供更多关于如何有效使用**LangChain**的上下文，并介绍使用它的技巧和窍门。作为最后一部分，我们将开发一个**LangChain**应用程序，这是一个实际示例，说明了在客户服务的实际业务用例中如何应用**LangChain**。主要部分包括：
- en: How to Set Up **LangChain**?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何设置**LangChain**？
- en: Model Integrations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型集成
- en: Customer Service Helper
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户服务助手
- en: We'll start off the chapter by setting up **LangChain** on your computer.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从在您的计算机上设置**LangChain**开始本章。
- en: How to Set Up LangChain?
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何设置LangChain？
- en: 'In this book, we are talking about LangChain. We can install LangChain by simply
    typing `pip install langchain` from a terminal however, in this book, we''ll also
    be using a variety of other tools and integrations in a few different use cases.
    In order to make sure, all the examples and code snippets work as intended and
    they don''t just work on my machine, but for anyone installing this, I am providing
    different ways to set up an environment.There are various approaches to setting
    up a Python environment. Here, we describe four popular methods for installing
    related dependencies: Docker, Conda, Pip, and Poetry. In case you encounter issues
    during the installation process, consult the respective documentation or raise
    an issue on the Github repository of this book. The different installations have
    been tested at the time of the release of this book, however, things can change,
    and we will update the Github readme online to include workarounds for possible
    problems that could arise.Please find a `Dockerfile` for docker, a `requirements.txt`
    for pip a `pyproject.toml` for poetry and a `langchain_ai.yml` file for **Conda**
    in the book''s repository at [https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain)Let''s
    set up our environment starting with Python.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们谈论的是LangChain。我们可以通过在终端中简单地输入`pip install langchain`来安装LangChain，然而，在本书中，我们还将在几种不同的用例中使用各种其他工具和集成。为了确保所有示例和代码片段按预期工作，并且不仅仅在我的机器上工作，而是对于任何安装此软件的人都能工作，我提供了设置环境的不同方法。设置Python环境有各种方法。在这里，我们描述了四种安装相关依赖项的流行方法：Docker、Conda、Pip和Poetry。如果在安装过程中遇到问题，请参考各自的文档或在本书的Github存储库上提出问题。这些不同的安装在发布本书时已经经过测试，但是事情可能会发生变化，我们将在线更新Github自述文件，包括可能出现问题的解决方法。请在本书的存储库中找到一个用于docker的`Dockerfile`，一个用于pip的`requirements.txt`，一个用于poetry的`pyproject.toml`和一个用于**Conda**的`langchain_ai.yml`文件：[https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain)让我们从Python开始设置我们的环境。
- en: Python installation
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python安装
- en: 'Before setting up a Python environment and installing related dependencies,
    you should usually have Python itself installed. I assume, most people who have
    bought this book will have Python installed, however, just in cases, let''s go
    through it. You may download the latest version from python.org for your operating
    system or use your platform''s package manager. Let''s see this with Homebrew
    for MacOS and apt-get for Ubuntu.On MacOS, with Homebrew, we can do:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置 Python 环境并安装相关依赖项之前，通常应该安装 Python 本身。我假设，大多数购买本书的人都已经安装了 Python，但是，为了确保，让我们来看看。您可以从
    python.org 下载最新版本适用于您操作系统的版本，或者使用您平台的软件包管理器。让我们看看在 MacOS 使用 Homebrew 和在 Ubuntu
    使用 apt-get。在 MacOS 上，使用 Homebrew，我们可以：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For Ubuntu, with apt-get we can do:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Ubuntu，我们可以使用 apt-get：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Tip**: If you are new to programming or Python, it is advised to follow some
    beginner-level tutorials before proceeding with LangChain and the applications
    in this book.'
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**提示**：如果您是编程或 Python 的新手，建议在继续 LangChain 和本书中的应用之前先参考一些初学者级别的教程。'
- en: An important tool for interactively trying out data processing and models is
    the Jupyter notebook and the lab. Let's have a look at this now.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的工具，用于交互式地尝试数据处理和模型的是 Jupyter 笔记本和 lab。让我们现在来看看这个。
- en: Jupyter Notebook and JupyterLab
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Jupyter Notebook 和 JupyterLab
- en: Jupyter Notebook and JupyterLab are open-source web-based interactive environments
    for creating, sharing, and collaborating on computational documents. They enable
    users to write code, display visualizations, and include explanatory text in a
    single document called a notebook. The primary difference between the two lies
    in their interface and functionality.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter Notebook 和 JupyterLab 是用于创建、共享和协作计算文档的开源基于 Web 的交互式环境。它们使用户能够在单个称为笔记本的文档中编写代码、显示可视化效果并包含解释性文本。两者之间的主要区别在于它们的界面和功能。
- en: '**Jupyter Notebook** aims to support various programming languages like Julia,
    Python, and R - in fact, the project name is a reference to these three languages.
    Jupyter Notebook offers a simple user interface that allows users to create, edit,
    and run notebooks with a linear layout. It also supports extensions for additional
    features and customization.'
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**Jupyter Notebook** 的目标是支持像 Julia、Python 和 R 这样的各种编程语言 - 实际上，项目名称是对这三种语言的引用。Jupyter
    Notebook 提供了一个简单的用户界面，允许用户使用线性布局创建、编辑和运行笔记本。它还支持用于额外功能和自定义的扩展。'
- en: ''
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**JupyterLab**, on the other hand, is an enhanced version of Jupyter Notebook.
    Introduced in 2018, JupyterLab offers a more powerful and flexible environment
    for working with notebooks and other file types. It provides a modular, extensible,
    and customizable interface where users can arrange multiple windows (for example,
    notebooks, text editors, terminals) side-by-side, facilitating more efficient
    workflows.'
  id: totrans-22
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 另一方面，**JupyterLab** 是 Jupyter Notebook 的增强版本。JupyterLab 于 2018 年推出，提供了一个更强大和灵活的环境，用于处理笔记本和其他文件类型。它提供了一个模块化、可扩展和可定制的界面，用户可以在其中并排排列多个窗口（例如笔记本、文本编辑器、终端），从而促进更高效的工作流程。
- en: 'You can start up a notebook server on your computer from the terminal like
    this:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以像这样从终端在计算机上启动笔记本服务器：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You should see your browser opening a new tab with the Jupyter notebook like
    this:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到您的浏览器打开一个新标签页，显示类似这样的 Jupyter 笔记本：
- en: '![Figure 3.1: Jupyter Notebook with a LangChain Agent.](../media/file20.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1：带有 LangChain 代理的 Jupyter Notebook。](../media/file20.png)'
- en: 'Figure 3.1: Jupyter Notebook with a LangChain Agent.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1：带有 LangChain 代理的 Jupyter Notebook。
- en: 'Alternatively, we can also use JupyterLab, the next-generation notebook server
    that brings significant improvements in usability. You can start up a JupyterLab
    notebook server from the terminal like this:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们也可以使用 JupyterLab，这是下一代带来显著改进的笔记本服务器。您可以像这样从终端启动 JupyterLab 笔记本服务器：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We should see something like this:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到类似这样的东西：
- en: '![Figure 3.2: Jupyter Lab with a LangChain Agent.](../media/file21.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2：带有 LangChain 代理的 Jupyter Lab。](../media/file21.png)'
- en: 'Figure 3.2: Jupyter Lab with a LangChain Agent.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2：带有 LangChain 代理的 Jupyter Lab。
- en: Either one of these two, the `Jupyter notebook` or `JupyterLab`, will give you
    an **integrated development environment** (**IDE**) to work on some of the code
    that we'll be introducing in this book. After installing Python and the notebook
    or lab, let's quickly explore the differences between dependency management tools
    (**Docker**, **Conda**, **Pip**, and **Poetry**) and use them to fully set up
    our environment for our projects with LangChain!
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`Jupyter notebook`或`JupyterLab`中的任何一个都将为您提供一个**集成开发环境**（**IDE**），用于处理本书中将介绍的一些代码。安装Python和笔记本或实验室后，让我们快速探讨依赖管理工具（**Docker**、**Conda**、**Pip**和**Poetry**）之间的区别，并使用它们完全设置我们的环境，以便在LangChain项目中进行工作！'
- en: Environment management
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 环境管理
- en: 'Before we explore various methods to set up a Python environment for working
    with generative models in **LangChain**, it''s essential to understand the differences
    between primary dependency management tools: **Docker**, **Conda**, **Pip**, and
    **Poetry**. All four are tools widely used in the realm of software development
    and deployment.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探索为在**LangChain**中使用生成模型设置Python环境的各种方法之前，了解主要依赖管理工具之间的区别是至关重要的：**Docker**、**Conda**、**Pip**和**Poetry**。这四个工具在软件开发和部署领域被广泛使用。
- en: '**Docker** is an open-source platform that provides OS-level virtualization
    through containerization. It automates the deployment of applications inside lightweight,
    portable containers, which run consistently on any system with Docker installed.'
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**Docker** 是一个提供操作系统级虚拟化的开源平台，通过容器化自动化应用程序的部署。它在安装了Docker的任何系统上都可以一致地运行轻量级、可移植的容器内的应用程序。'
- en: ''
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Conda** is a cross-platform package manager and excels at installing and
    managing packages from multiple channels, not limited to Python. Geared predominantly
    toward data science and machine learning projects, it can robustly handle intricate
    dependency trees, catering to complex projects with numerous dependencies.'
  id: totrans-38
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**Conda** 是一个跨平台的包管理器，擅长安装和管理来自多个渠道的软件包，不仅限于Python。主要面向数据科学和机器学习项目，它可以强大地处理复杂的依赖关系树，满足具有大量依赖关系的复杂项目的需求。'
- en: ''
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Pip** is the most commonly used package manager for Python, allowing users
    to install and manage third-party libraries easily. However, Pip has limitations
    when handling complex dependencies, increasing the risk of dependency conflicts
    arising during package installation.'
  id: totrans-40
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**Pip** 是Python最常用的包管理器，允许用户轻松安装和管理第三方库。然而，Pip在处理复杂依赖关系时存在局限性，增加了在安装包时出现依赖冲突的风险。'
- en: ''
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Poetry** is a newer package manager that combines the best features of both
    Pip and Conda. Boasting a modern and intuitive interface, robust dependency resolution
    system, and support for virtual environments creation, Poetry offers additional
    functionalities such as dependency isolation, lock files, and version control.'
  id: totrans-42
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**Poetry** 是一个较新的包管理器，结合了Pip和Conda的最佳特性。拥有现代直观的界面、强大的依赖解析系统以及支持创建虚拟环境，Poetry提供了额外的功能，如依赖隔离、锁定文件和版本控制。'
- en: 'Poetry and Conda both streamline virtual environment management, whereas working
    with Pip typically involves utilizing a separate tool like virtualenv. Conda is
    the installation method recommended here. We''ll provide a requirements file for
    pip as well and instructions for poetry, however, some tweaking might be required
    in a few cases.We''ll go through installation with these different tools in turn.
    For all instructions, please make sure you have the book''s repository downloaded
    (using the Github user interface) or cloned on your computer, and you''ve changed
    into the project''s root directory.Here''s how you can find the download option
    on Github:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 诗歌和Conda都简化了虚拟环境管理，而使用Pip通常涉及使用类似virtualenv的单独工具。这里推荐使用Conda进行安装。我们也会为Pip提供一个requirements文件以及Poetry的说明，但在某些情况下可能需要进行一些调整。我们将依次使用这些不同的工具进行安装。对于所有的说明，请确保您已经下载了本书的存储库（使用Github用户界面）或在您的计算机上克隆，并且已经切换到项目的根目录。以下是您在Github上找到下载选项的方法：
- en: '![Figure 3.3: Download options in the Github User Interface (UI).](../media/file22.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图3.3：Github用户界面（UI）中的下载选项。](../media/file22.png)'
- en: 'Figure 3.3: Download options in the Github User Interface (UI).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3：Github用户界面（UI）中的下载选项。
- en: 'If you are new to git, you can press **Download ZIP**, and then unzip the archive
    using your favorite tool.Alternatively, to clone the repository using git and
    change to the project directory, you can type the following commands:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您是 git 的新手，您可以按下 **Download ZIP**，然后使用您喜欢的工具解压缩存档。或者，使用 git 克隆存储库并切换到项目目录，您可以输入以下命令：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now that we have the repository on our machine, let's start with Docker!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在计算机上有了存储库，让我们从 Docker 开始吧！
- en: Docker
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Docker
- en: 'Docker is a platform that enables developers to automate deployment, packaging,
    and management of applications. Docker uses containerization technology, which
    helps standardize and isolate environments. The advantage of using a container
    is that it protects your local environment from any - potentially unsafe - code
    that you run within the container. The downside is that the image might require
    time to build and might require around 10 Gigabytes in storage capacity.Similar
    to the other tools for environment management, Docker is useful because you can
    create a reproducible environment for your project. You can use Docker to create
    an environment with all the libraries and tools you need for your project, and
    share that environment with others.To start with Docker, follow these steps:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 是一个平台，使开发人员能够自动化部署、打包和管理应用程序。Docker 使用容器化技术，有助于标准化和隔离环境。使用容器的优势在于它保护您的本地环境免受您在容器内运行的任何
    - 可能不安全的 - 代码的影响。缺点是镜像可能需要一些时间来构建，并且可能需要大约 10 千兆字节的存储容量。与其他环境管理工具类似，Docker 很有用，因为您可以为项目创建一个可重现的环境。您可以使用
    Docker 创建一个包含您项目所需的所有库和工具的环境，并与他人共享该环境。要开始使用 Docker，请按照以下步骤进行：
- en: 'Install Docker on your machine. You can go to the Docker website.in your web
    browser and follow the installation instructions here: [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的计算机上安装 Docker。您可以在您的网络浏览器中转到 Docker 网站，并按照此处的安装说明进行操作：[https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)
- en: 'In the terminal, run the following command to build the Docker image (please
    note: you need to be in the project root for this to work).'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在终端中运行以下命令构建 Docker 镜像（请注意：您需要在项目根目录中才能正常工作）。
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This will pull, the continuumio/miniconda3 image from Docker Hub, and build
    the image.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这将从 Docker Hub 拉取 continuumio/miniconda3 镜像，并构建镜像。
- en: 'Start the Docker container interactively using the image created:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用创建的镜像交互式地启动 Docker 容器：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This should start our notebook within the container. We should be able to navigate
    to the `Jupyter Notebook` from your browser. We can find it at this address: `http://localhost:8080/`Let''s
    look at conda next.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在容器内启动我们的笔记本。我们应该能够从浏览器中导航到`Jupyter Notebook`。我们可以在此地址找到它：`http://localhost:8080/`接下来让我们看看
    conda。
- en: Conda
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Conda
- en: '`Conda` allows users to manage multiple environments for different projects.
    It works with Python, R, and other languages, and helps with the installation
    of system libraries as well by maintaining lists of libraries associated with
    Python libraries.The best way to get started with conda is to install anaconda
    or miniconda by following the instructions from this link: [https://docs.continuum.io/anaconda/install/](https://docs.continuum.io/anaconda/install/)While
    the `conda` environment takes up less disk space than Docker, starting from anaconda,
    the full environment should still take up about 2.5 Gigabytes. The miniconda setup
    might save you a bit of disk space.There''s also a graphical interface to `conda`,
    Anaconda Navigator, which can be installed on macOS and Windows, and which can
    install any dependencies as well as the `conda` tool from the terminal.Let''s
    continue with the `conda` tool and install the dependencies of this book.To create
    a new environment, execute the following command:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`Conda` 允许用户为不同项目管理多个环境。它适用于 Python、R 和其他语言，并通过维护与 Python 库相关的库列表来帮助安装系统库。开始使用
    conda 的最佳方法是按照此链接中的说明安装 anaconda 或 miniconda：[https://docs.continuum.io/anaconda/install/](https://docs.continuum.io/anaconda/install/)虽然
    conda 环境占用的磁盘空间比 Docker 少，但从 anaconda 开始，完整环境仍然需要大约 2.5 千兆字节。miniconda 设置可能会节省一些磁盘空间。还有一个图形界面可以使用
    `conda`，Anaconda Navigator，可以安装在 macOS 和 Windows 上，并且可以从终端安装任何依赖项以及 `conda` 工具。让我们继续使用
    `conda` 工具并安装本书的依赖项。要创建一个新环境，请执行以下命令：'
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`Conda` lets us create environments with lots of different libraries, but also
    different versions of Python. We are using Python 3.10 throughout this book. Activate
    the environment by running:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`Conda`允许我们创建具有许多不同库的环境，还可以使用不同版本的Python。我们在本书中一直使用Python 3.10。通过运行以下命令激活环境：'
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This is all, we are done. We can see this should be painless and straightforward.
    You can now spin up a `jupyter notebook` or `jupyter lab` within the environment,
    for example:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是全部，我们完成了。我们可以看到这应该是轻松和直接的。您现在可以在环境中启动`jupyter notebook`或`jupyter lab`，例如：
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Let's have a look at pip, an alternative to `conda`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下pip，这是`conda`的一个替代方案。
- en: Pip
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Pip
- en: '`Pip` is the default package manager for Python. It allows you to easily install
    and manage third-party libraries. We can install individual libraries, but also
    maintain a full list of Python libraries.If it''s not already included in your
    Python distribution, install pip following instructions on [https://pip.pypa.io/](https://pip.pypa.io/)To
    install a library with pip, use the following command. For example, to install
    the NumPy library, you would use the following command:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pip`是Python的默认软件包管理器。它允许您轻松安装和管理第三方库。我们可以安装单个库，还可以维护完整的Python库列表。如果它尚未包含在您的Python发行版中，请按照[https://pip.pypa.io/](https://pip.pypa.io/)上的说明安装pip。要使用pip安装库，请使用以下命令。例如，要安装NumPy库，您将使用以下命令：'
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can also use `pip` to install a specific version of a library. For example,
    to install version 1.0 of the NumPy library, you would use the following command:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以使用`pip`来安装库的特定版本。例如，要安装NumPy库的1.0版本，您将使用以下命令：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In order to set up a full environment, we can start with a list of requirements
    - by convention, this list is in a file called `requirements.txt`. I''ve included
    this file in the project''s root directory, which lists all essential libraries.You
    can install all the libraries using this command:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设置一个完整的环境，我们可以从一个要求列表开始 - 按照惯例，这个列表在一个名为`requirements.txt`的文件中。我已经将这个文件包含在项目的根目录中，列出了所有必要的库。您可以使用以下命令安装所有库：
- en: '[PRE12]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Please note, however, as mentioned, that Pip doesn''t take care of the environments.
    Virtualenv is a tool that can help to maintain environments, for example different
    versions of libraries. Let''s see this quickly:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，正如前面提到的，Pip不负责环境。Virtualenv是一个可以帮助维护环境的工具，例如不同版本的库。让我们快速看一下这个：
- en: '[PRE13]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Let's do Poetry next.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来让我们来看一下Poetry。
- en: Poetry
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Poetry
- en: 'Poetry is a dependency management tool for Python that streamlines library
    installation and version control. Installation and usage is straightforward as
    we''ll see. Here''s a quick run-through of poetry:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Poetry是Python的依赖管理工具，简化了库的安装和版本控制。安装和使用都很简单，我们将看到。以下是poetry的快速概述：
- en: Install poetry following instructions on [https://python-poetry.org/](https://python-poetry.org/)
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照[https://python-poetry.org/](https://python-poetry.org/)上的说明安装poetry
- en: Run `poetry install` in the terminal (from the project root as mentioned before)
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在终端中运行`poetry install`（从之前提到的项目根目录）
- en: The command will automatically create a new environment (if you haven't created
    one already) and install all dependencies. This concludes the setup for Poetry.
    We'll get to model providers now.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令将自动创建一个新环境（如果您尚未创建），并安装所有依赖项。这就完成了Poetry的设置。现在我们将开始使用模型提供程序。
- en: Model Integrations
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型集成
- en: 'Before properly starting with generative AI, we need to set up access to models
    such as **large language models** (**LLMs**) or text to image models so we can
    integrate them into our applications. As discussed in *Chapter 1*, *What are Generative
    Models*, there are various **LLMs** by tech giants, like **GPT-4** by **OpenAI**,
    **BERT** and **PaLM-2** by **Google**, **LLaMA** by **Meta AI**, and many more.With
    the help of **LangChain**, we can interact with all of these, for example through
    **Application Programming Interface** (**APIs**), or we can call open-source models
    that we have downloaded on our computer. Several of these integrations support
    text generation and embeddings. We''ll focus on text generation in this chapter,
    and discuss embeddings, vector databases, and neural search in *Chapter 5*, *Building
    a Chatbot like ChatGPT*.There are many providers for model hosting. For **LLMs**,
    currently, **OpenAI**, **Hugging Face**, **Cohere**, **Anthropic**, **Azure**,
    **Google Cloud Platform Vertex AI** (**PaLM-2**), and **Jina AI** are among the
    many providers supported in **LangChain**, however this list is growing all the
    time. You can all the supported integrations for **LLMs** at [https://integrations.langchain.com/llms](https://integrations.langchain.com/llms)As
    for image models, the big developers include **OpenAI** (**DALL-E**), **Midjourney**,
    Inc. (Midjourney), and Stability AI (**Stable Diffusion**). **LangChain** currently
    doesn''t have out-of-the-box handling of models that are not for text, however,
    its docs describe how to work with Replicate, which also provides an interface
    to Stable Diffusion models.For each of these providers, to make calls against
    their Application Programming Interface (API), you''ll first need to create an
    account and obtain an API key. This is free for all of them. With some of them
    you don''t even have to give them your credit card details.In order to set an
    API key in an environment, in Python we can do:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在正式开始生成式人工智能之前，我们需要设置访问**大型语言模型**（**LLMs**）或文本到图像模型等模型的访问权限，以便将它们集成到我们的应用程序中。正如在*第1章*中讨论的*生成模型是什么*中所述，各大科技巨头都有各种**LLMs**，如**OpenAI**的**GPT-4**，**Google**的**BERT**和**PaLM-2**，**Meta
    AI**的**LLaMA**等等。借助**LangChain**的帮助，我们可以与所有这些模型进行交互，例如通过**应用程序编程接口**（**APIs**），或者我们可以调用我们在计算机上下载的开源模型。其中一些集成支持文本生成和嵌入。在本章中，我们将重点讨论文本生成，并在*第5章*中讨论嵌入、向量数据库和神经搜索，构建类似ChatGPT的聊天机器人。有许多模型托管提供商。目前，**OpenAI**、**Hugging
    Face**、**Cohere**、**Anthropic**、**Azure**、**Google Cloud Platform Vertex AI**（**PaLM-2**）和**Jina
    AI**是**LangChain**支持的众多提供商之一，但这个列表一直在不断增长。您可以在[https://integrations.langchain.com/llms](https://integrations.langchain.com/llms)查看所有支持的**LLMs**的集成。至于图像模型，主要开发者包括**OpenAI**（**DALL-E**）、Midjourney公司（Midjourney）和Stability
    AI（**Stable Diffusion**）。**LangChain**目前没有直接处理非文本模型的功能，但其文档描述了如何与Replicate合作，后者也提供了与Stable
    Diffusion模型交互的接口。对于这些提供商中的每一个，要调用他们的应用程序编程接口（API），您首先需要创建一个帐户并获取一个API密钥。这对所有人都是免费的。有些提供商甚至不需要您提供信用卡信息。为了在环境中设置API密钥，在Python中我们可以这样做：
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here `OPENAI_API_KEY` is the environment key appropriate for OpenAI. Setting
    the keys in your environment has the advantage that we don''t include them in
    our code.You can also expose these variables from your terminal like this:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这里`OPENAI_API_KEY`是适用于OpenAI的环境密钥。将密钥设置在您的环境中的好处是我们不需要将它们包含在我们的代码中。您也可以像这样从终端暴露这些变量：
- en: '[PRE15]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Let's go through a few prominent model providers in turn. We'll give an example
    usage for each of them.Let's' start with a Fake LLM that's used for testing so
    we can show the basic idea!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们依次介绍一些知名的模型提供商。我们将为每个模型提供一个示例用法。让我们从一个用于测试的Fake LLM开始，以便展示基本思想！
- en: Fake LLM
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Fake LLM
- en: The Fake LM is for testing. The LangChain documentation has an example for the
    tool use with LLMs. You can execute this example in either Python directly or
    in a notebook.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Fake LM是用于测试的。LangChain文档中有一个关于该工具与LLMs一起使用的示例。您可以直接在Python中执行此示例，也可以在笔记本中执行。
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We connect a tool, a Python **Read-Eval-Print Loop** (**REPL**) that will be
    called depending on the output of the **LLM**. The Fake List **LLM** will give
    two responses, `responses`, that won''t change based on the input. We set up an
    agent that makes decisions based on the ReAct strategy that we explained in chapter
    2, Introduction to LangChain (`ZERO_SHOT_REACT_DESCRIPTION`). We run the agent
    with a text, the question "what''s 2 + 2".We can observe how the Fake LLM output,
    leads to a call to the Python Interpreter, which returns 4\. Please note that
    the action has to match the `name` attribute of the tool, the `PythonREPLTool`,
    which is starts like this:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们连接了一个工具，一个名为 Python **Read-Eval-Print Loop**（**REPL**）的工具，根据**LLM**的输出来调用。虚拟列表**LLM**将给出两个响应，`responses`，这些响应不会根据输入而改变。我们设置了一个代理，根据我们在第2章中解释的
    ReAct 策略做出决策，这个策略是基于 LangChain 的介绍（`ZERO_SHOT_REACT_DESCRIPTION`）。我们用一个文本运行代理，问题是“2
    + 2等于多少”。我们可以观察到虚拟 LLM 的输出如何导致调用 Python 解释器，后者返回 4。请注意，操作必须与工具的`name`属性匹配，即`PythonREPLTool`，它的启动方式如下：
- en: '[PRE17]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The names and descriptions of the tools are passed to the **LLM**, which then
    decides based on the provided information.The output of the Python interpreter
    is passed to the Fake **LLM**, which ignores the observation and returns 4\. Obviously,
    if we change the second response to "`Final Answer: 5`", the output of the agent
    wouldn''t correspond to the question.In the next sections, we''ll make this more
    meaningful by using an actual **LLM** rather than a fake one. One of the first
    providers that anyone will think of at the moment is OpenAI.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 工具的名称和描述被传递给**LLM**，然后根据提供的信息做出决定。Python 解释器的输出被传递给虚拟**LLM**，后者忽略观察结果并返回 4。显然，如果我们将第二个响应更改为“`最终答案：5`”，代理的输出将不对应问题。在接下来的章节中，我们将通过使用一个真实的**LLM**而不是一个虚假的**LLM**来使这更有意义。目前，任何人首先会想到的提供者之一是
    OpenAI。
- en: OpenAI
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenAI
- en: 'As explained in *Chapter 1*, *What are Generative Models?*, OpenAI is an American
    AI research laboratory that is the current market leader in generative AI models,
    especially LLMs. They offer a spectrum of models with different levels of power
    suitable for different tasks. We''ll see in this chapter how to interact with
    OpenAI models with **LangChain''s** and the OpenAI Python client library. OpenAI
    also offers an Embedding class for text embedding models.We will mostly use OpenAI
    for our applications. There are several models to choose from - each model has
    its own pros, token usage counts, and use cases. The main LLM models are GPT-3.5
    and GPT-4 with different token length. You can see the pricing of different models
    at [https://openai.com/pricing](https://openai.com/pricing)We need to obtain an
    OpenAI API key first. In order to create an API key, follow these steps:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如*第1章*中所解释的，*生成模型是什么？*，OpenAI 是一家美国人工智能研究实验室，目前是生成式人工智能模型的市场领导者，尤其是 LLM。他们提供一系列不同功率级别的模型，适用于不同的任务。在本章中，我们将看到如何使用**LangChain**和
    OpenAI Python 客户端库与 OpenAI 模型进行交互。OpenAI 还为文本嵌入模型提供了一个 Embedding 类。我们将主要用 OpenAI
    进行我们的应用。有几种模型可供选择 - 每个模型都有自己的优点、令牌使用计数和用例。主要的 LLM 模型是 GPT-3.5 和 GPT-4，具有不同的令牌长度。你可以在[https://openai.com/pricing](https://openai.com/pricing)看到不同模型的定价。我们首先需要获取一个
    OpenAI API 密钥。要创建 API 密钥，请按照以下步骤操作：
- en: You need to create a login at [https://platform.openai.com/](https://platform.openai.com/)
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要在[https://platform.openai.com/](https://platform.openai.com/)创建一个登录账号。
- en: Set up your billing information.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置你的账单信息。
- en: You can see the **API keys** under *Personal -> View API Keys*.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以在*个人 -> 查看 API 密钥*下看到**API 密钥**。
- en: Click **Create new secret key** and give it a **Name**.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**创建新的秘密密钥**并给它一个**名称**。
- en: 'Here''s how this should look like on the OpenAI platform:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在 OpenAI 平台上的样子：
- en: '![Figure 3.4: OpenAI API platform - Create new secret key.](../media/file23.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4：OpenAI API 平台 - 创建新的秘密密钥。](../media/file23.png)'
- en: 'Figure 3.4: OpenAI API platform - Create new secret key.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4：OpenAI API 平台 - 创建新的秘密密钥。
- en: 'After pressing "**Create secret key**", you should see the message "API key
    generated." You need to copy the key to your clipboard and keep it. We can set
    the key as an environment variable (`OPENAI_API_KEY`) or pass it as a parameter
    every time you construct a class for OpenAI calls.We can use the `OpenAI` language
    model class to set up an **LLM** to interact with. Let''s create an agent that
    calculates using this model - I am omitting the imports from the previous example:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“**创建秘钥**”后，您应该会看到消息“API秘钥已生成”。您需要将该秘钥复制到剪贴板并保存。我们可以将该秘钥设置为环境变量（`OPENAI_API_KEY`），或者在每次构建OpenAI调用类时作为参数传递。我们可以使用`OpenAI`语言模型类来建立一个**LLM**以进行交互。让我们创建一个使用这个模型进行计算的代理
    - 我省略了前一个示例中的导入部分：
- en: '[PRE18]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We should be seeing this output:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到以下输出：
- en: '[PRE19]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This looks quite promising, I think. Let's move on to the next provider and
    more examples!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来相当有前途，我认为。让我们继续前往下一个提供商和更多示例！
- en: Hugging Face
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Hugging Face
- en: 'Hugging Face is a very prominent player in the NLP space, and has considerable
    traction in open-source and hosting solutions. The company is an American company
    that develops tools for building machine learning applications. Its employees
    develop and maintain the Transformers Python library, which is used for natural
    language processing tasks, includes implementations of state-of-the-art and popular
    models like BERT and GPT-2, and is compatible with **PyTorch**, **TensorFlow**,
    and **JAX**.Hugging Face also provides the Hugging Face Hub, a platform for hosting
    Git-based code repositories, machine learning models, datasets, and web applications,
    which provides over 120k models, 20k datasets, and 50k demo apps (Spaces) for
    machine learning. It is an online platform where people can collaborate and build
    ML together.These tools allow users to load and use models, embeddings, and datasets
    from Hugging Face. The `HuggingFaceHub` integration, for example, provides access
    to different models for tasks like text generation and text classification. The
    `HuggingFaceEmbeddings` integration allows users to work with sentence-transformers
    models.They offer various other libraries within their ecosystem, including Datasets
    for dataset processing, *Evaluate* for model evaluation, *Simulate* for simulation,
    and *Gradio* for machine learning demos.In addition to their products, Hugging
    Face has been involved in initiatives such as the BigScience Research Workshop,
    where they released an open large language model called BLOOM with 176 billion
    parameters. They have received significant funding, including a $40 million Series
    B round and a recent Series C funding round led by Coatue and Sequoia at a $2
    billion valuation. Hugging Face has also formed partnerships with companies like
    Graphcore and Amazon Web Services to optimize their offerings and make them available
    to a broader customer base.In order to use Hugging Face as a provider for your
    models, you can create an account and API keys at [https://huggingface.co/settings/profile](https://huggingface.co/settings/profile)You
    can make the token available in your environment as `HUGGINGFACEHUB_API_TOKEN`.Let''s
    see an example, where we use an open-source model developed by Google, the Flan-T5-XXL
    model:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face是自然语言处理领域中非常重要的参与者，在开源和托管解决方案方面具有相当大的影响力。该公司是一家美国公司，开发用于构建机器学习应用程序的工具。其员工开发和维护Transformers
    Python库，用于自然语言处理任务，包括BERT和GPT-2等最先进和流行的模型的实现，并兼容**PyTorch**、**TensorFlow**和**JAX**。Hugging
    Face还提供Hugging Face Hub，这是一个托管基于Git的代码存储库、机器学习模型、数据集和Web应用程序的平台，提供超过12万个模型、2万个数据集和5万个演示应用程序（Spaces）供机器学习使用。这是一个在线平台，人们可以在此协作并共同构建机器学习。这些工具允许用户加载和使用来自Hugging
    Face的模型、嵌入和数据集。例如，`HuggingFaceHub`集成提供了访问不同模型的功能，如文本生成和文本分类。`HuggingFaceEmbeddings`集成允许用户使用句子转换模型。他们在其生态系统中还提供了其他各种库，包括用于数据集处理的Datasets，用于模型评估的*Evaluate*，用于模拟的*Simulate*，以及用于机器学习演示的*Gradio*。除了他们的产品，Hugging
    Face还参与了一些倡议，如BigScience研究研讨会，他们发布了一个名为BLOOM的开放大型语言模型，拥有1760亿个参数。他们获得了大量资金，包括4亿美元的B轮融资和最近由Coatue和Sequoia领投的C轮融资，估值20亿美元。Hugging
    Face还与Graphcore和亚马逊网络服务等公司合作，优化其产品并使其面向更广泛的客户群体。要将Hugging Face用作您模型的提供商，您可以在[https://huggingface.co/settings/profile](https://huggingface.co/settings/profile)上创建帐户和API秘钥。您可以将令牌作为`HUGGINGFACEHUB_API_TOKEN`在您的环境中使用。让我们看一个示例，我们使用了由Google开发的开源模型Flan-T5-XXL：
- en: '[PRE20]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We get the response "`japan`."The **LLM** takes a text input, a question in
    this case, and returns a completion. The model has a lot of knowledge and can
    come up with answers to knowledge questions. We can also get simple recommendations:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了回应"`japan`"。**LLM**接受文本输入，本例中是一个问题，并返回一个完成。该模型具有大量知识，可以回答知识性问题。我们还可以得到简单的建议：
- en: Azure
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Azure
- en: Azure, the cloud computing platform run by Microsoft, integrates with OpenAI
    to provide powerful language models like GPT-3, Codex, and Embeddings. It offers
    access, management, and development of applications and services through their
    global data centers for use cases such as writing assistance, summarization, code
    generation, and semantic search. It provides capabilities such as **software as
    a service** (**SaaS**), **platform as a service** (**PaaS**), and **infrastructure
    as a service** (**IaaS**).Authenticating either through Github or Microsoft credentials,
    we can create an account on Azure under [https://azure.microsoft.com/](https://azure.microsoft.com/)You
    can then create new API keys under *Cognitive Services -> Azure OpenAI*. There
    are a few more steps involved, and personally, I found this process annoying and
    frustrating, and I gave up. After set up, the models should be accessible through
    the `AzureOpenAI()` llm class in **LangChain**.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 由微软运行的云计算平台Azure与OpenAI集成，提供强大的语言模型，如GPT-3、Codex和Embeddings。它通过全球数据中心提供访问、管理和开发应用程序和服务，用例包括写作辅助、摘要、代码生成和语义搜索。它提供**软件即服务**（**SaaS**）、**平台即服务**（**PaaS**）和**基础设施即服务**（**IaaS**）等功能。通过Github或微软凭据进行身份验证，我们可以在[https://azure.microsoft.com/](https://azure.microsoft.com/)上创建Azure账户。然后可以在*Cognitive
    Services -> Azure OpenAI*下创建新的API密钥。这涉及到一些步骤，个人而言，我觉得这个过程很烦人和令人沮丧，所以我放弃了。设置完成后，模型应该可以通过**LangChain**中的`AzureOpenAI()`
    llm类访问。
- en: Google Cloud
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Google Cloud
- en: 'There are many models and functions available through **Google Cloud Platform**
    (**GCP**) and Vertex its machine learning platform. Google Cloud provide access
    to **LLMs** like **LaMDA**, **T5**, and **PaLM**. Google has also updated the
    Google Cloud **Natural Language** (**NL**) API with a new LLM-based model for
    Content Classification. This updated version offers an expansive pre-trained classification
    taxonomy to help with ad targeting, and content-based filtering. The **NL** API''s
    improved v2 classification model is enhanced with over 1,000 labels and support
    for 11 languages with improved accuracy. For models with GCP, you need to have
    gcloud **command line interface** (**CLI**) installed. You can find the instructions
    here: [https://cloud.google.com/sdk/docs/install](https://cloud.google.com/sdk/docs/install)You
    can then authenticate and print a key token with this command from the terminal:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通过**Google Cloud Platform**（**GCP**）和其机器学习平台Vertex提供许多模型和功能。Google Cloud提供访问**LLM**，如**LaMDA**、**T5**和**PaLM**。Google还更新了Google
    Cloud **自然语言**（**NL**）API，使用基于LLM的新模型进行内容分类。这个更新版本提供了一个广泛的预训练分类分类法，以帮助广告定位和基于内容的过滤。**NL**
    API的改进v2分类模型增加了超过1,000个标签，并支持11种语言，具有更高的准确性。对于GCP中的模型，您需要安装gcloud **命令行界面**（**CLI**）。您可以在这里找到说明：[https://cloud.google.com/sdk/docs/install](https://cloud.google.com/sdk/docs/install)然后可以通过终端使用以下命令进行身份验证并打印一个密钥令牌：
- en: '[PRE21]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: You also need to enable Vertex for your project. If you haven't enabled it,
    you should get a helpful error message pointing you to the right website, where
    you have to click on "Enable".Let's run a model!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要为您的项目启用Vertex。如果您还没有启用它，您应该会收到一个有用的错误消息，指向正确的网站，在那里您必须点击"启用"。让我们运行一个模型！
- en: '[PRE22]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We should see this response:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到这个回应：
- en: '[PRE23]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'I''ve set verbose to True in order to see the reasoning process of the model.
    It''s quite impressive it comes up with the right response even given a misspelling
    of the name. The step by step prompt instruction is key to the correct answer.There
    are various models available through Vertex such as these:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我已将verbose设置为True，以便查看模型的推理过程。令人印象深刻的是，即使给出了名字的拼写错误，它也能给出正确的回应。逐步提示指导是得出正确答案的关键。在Vertex中有各种模型可用，例如：
- en: '| **Model** | **Description** | **Properties** |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **描述** | **属性** |'
- en: '| text-bison | Fine-tuned to follow natural language instructions | Max input
    token: 8,192Max output tokens: 1,024Training data: Up to Feb 2023 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| text-bison | 经过微调以遵循自然语言指令 | 最大输入标记数：8,192最大输出标记数：1,024训练数据：截至2023年2月 |'
- en: '| chat-bison | Fine-tuned for multi-turn conversation | Max input token: 4,096
    Max output tokens: 1,024 Training data: Up to Feb 2023 Max turns : 2,500 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| chat-bison | 专为多轮对话而调整 | 最大输入标记：4,096 最大输出标记：1,024 训练数据：截至2023年2月 最大轮数：2,500
    |'
- en: '| code-bison | Fine-tuned to generate code based on a natural language description
    | Max input token: 4,096 Max output tokens: 2,048 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| code-bison | 专为根据自然语言描述生成代码而调整 | 最大输入标记：4,096 最大输出标记：2,048 |'
- en: '| codechat-bison | Fine-tuned for chatbot conversations that help with code-related
    questions | Max input token: 4,096 Max output tokens: 2,048 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| codechat-bison | 专为帮助解决与代码相关问题的聊天机器人对话而调整 | 最大输入标记：4,096 最大输出标记：2,048 |'
- en: '| code-gecko | Fine-tuned to suggest code completion | Max input tokens: 2,048
    Max output tokens: 64 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| code-gecko | 专为建议代码完成而调整 | 最大输入标记：2,048 最大输出标记：64 |'
- en: 'Table 3.1: Models available in Vertex Generative AI. You can check out the
    documentation at [https://cloud.google.com/vertex-ai/docs/generative-ai](https://cloud.google.com/vertex-ai/docs/generative-ai)We
    can also generate code. Let''s see if **Code-Bison** model can solve FizzBuzz,
    a common interview question for entry and mid-level software developer positions:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.1：Vertex Generative AI中可用的模型。您可以查看文档[https://cloud.google.com/vertex-ai/docs/generative-ai](https://cloud.google.com/vertex-ai/docs/generative-ai)。我们还可以生成代码。让我们看看**Code-Bison**模型是否能解决FizzBuzz，这是入门和中级软件开发人员职位常见的面试问题：
- en: '[PRE24]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We are getting this response:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到这个响应：
- en: '[PRE25]python'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE25]python'
- en: answer = []
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 答案 = []
- en: 'for i in range(1, n + 1):'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: for i in range(1, n + 1)：
- en: 'if i % 3 == 0 and i % 5 == 0:'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果i % 3 == 0并且i % 5 == 0：
- en: answer.append("FizzBuzz")
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案.append("FizzBuzz")
- en: 'elif i % 3 == 0:'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 否则如果i % 3 == 0：
- en: answer.append("Fizz")
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案.append("Fizz")
- en: 'elif i % 5 == 0:'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 否则如果i % 5 == 0：
- en: answer.append("Buzz")
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案.append("Buzz")
- en: 'else:'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 否则：
- en: answer.append(str(i))
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案.append(str(i))
- en: return answer
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 返回答案
- en: '[PRE26]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Would you hire code-bison into your team?
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 你会雇用代码-野牛加入你的团队吗？
- en: Anthropic
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Anthropic
- en: Anthropic is an AI startup and public-benefit corporation based in the United
    States. It was founded in 2021 by former members of OpenAI, including siblings
    Daniela Amodei and Dario Amodei. The company specializes in developing general
    AI systems and language models with a focus on responsible AI usage. As of July
    2023, Anthropic has raised $1.5 billion in funding. They have also worked on projects
    such as Claude, an AI chatbot similar to OpenAI's ChatGPT, and have conducted
    research on the interpretability of machine learning systems, specifically the
    transformer architecture.Unfortunately, Claude is not available to the general
    public (yet). You need to apply for access to use Claude and set the `ANTHROPIC_API_KEY`
    environment variable.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Anthropic是一家总部位于美国的人工智能初创公司和公益公司。它由OpenAI的前成员，包括兄弟达尼埃拉·阿莫代和达里奥·阿莫代于2021年创立。该公司专注于开发通用人工智能系统和语言模型，重点关注负责任的人工智能使用。截至2023年7月，Anthropic已筹集了15亿美元的资金。他们还致力于项目，如Claude，一个类似于OpenAI的ChatGPT的AI聊天机器人，并对机器学习系统的可解释性进行研究，特别是变压器架构。不幸的是，Claude目前尚不向普通公众开放。您需要申请访问权限以使用Claude并设置`ANTHROPIC_API_KEY`环境变量。
- en: Jina AI
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Jina AI
- en: 'Jina AI, founded in February 2020 by Han Xiao and Xuanbin He, is a German AI
    company based in Berlin that specializes in providing cloud-native neural search
    solutions with models for text, image, audio, and video. Their open-source neural
    search ecosystem enables businesses and developers to easily build scalable and
    highly available neural search solutions, allowing for efficient information retrieval.
    Recently, **Jina AI** launched *Finetuner*, a tool that enables fine-tuning of
    any deep neural network to specific use cases and requirements.The company has
    raised a total of $37.5 million in funding through three rounds, with their most
    recent funding coming from a Series A round in November 2021\. Notable investors
    in **Jina AI** include **GGV Capital** and **Canaan Partners**.You can set up
    a login under [https://cloud.jina.ai/](https://cloud.jina.ai/)On the platform,
    we can set up APIs for different use cases such as image caption, text embedding,
    image embedding, visual question answering, visual reasoning, image upscale, or
    Chinese text embedding.Here, we are setting up a visual question answering API
    with the recommended model:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Jina AI成立于2020年2月，由韩潇和何选斌创立，是一家总部位于柏林的德国人工智能公司，专注于提供基于云原生的神经搜索解决方案，涵盖文本、图像、音频和视频模型。他们的开源神经搜索生态系统使企业和开发人员能够轻松构建可扩展且高可用的神经搜索解决方案，实现高效的信息检索。最近，**Jina
    AI**推出了*Finetuner*，这是一款工具，可以对任何深度神经网络进行特定用例和需求的微调。该公司通过三轮融资共筹集了3750万美元，最近一轮融资是在2021年11月的A轮融资中获得的。**Jina
    AI**的知名投资者包括**GGV Capital**和**Canaan Partners**。您可以在[https://cloud.jina.ai/](https://cloud.jina.ai/)上设置登录。在该平台上，我们可以为不同用例设置API，如图像描述、文本嵌入、图像嵌入、视觉问答、视觉推理、图像放大或中文文本嵌入。在这里，我们正在设置一个视觉问答API，并使用推荐的模型：
- en: '![Figure 3.5: Visual Question Answering API in Jina AI.](../media/file24.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图3.5：Jina AI中的视觉问答API。](../media/file24.png)'
- en: 'Figure 3.5: Visual Question Answering API in Jina AI.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5：Jina AI中的视觉问答API。
- en: 'We get examples for client calls in Python and cURL, and a demo, where we can
    ask a question. This is cool, unfortunately, these APIs are not available yet
    through **LangChain**. We can implement such calls ourselves by subclassing the
    `LLM` class in **LangChain** as a custom **LLM** interface.Let''s set up another
    chatbot, this time powered by Jina AI. We can generate the API token, which we
    can set as `JINA_AUTH_TOKEN`, at [https://chat.jina.ai/api](https://chat.jina.ai/api)Let''s
    translate from English to French here:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在Python和cURL中获取客户端调用的示例，以及一个演示，我们可以提出问题。这很酷，不幸的是，这些API目前还不能通过**LangChain**使用。我们可以通过在**LangChain**中将`LLM`类子类化为自定义**LLM**接口来实现这些调用。让我们再设置一个聊天机器人，这次由Jina
    AI提供支持。我们可以生成API令牌，并将其设置为`JINA_AUTH_TOKEN`，在[https://chat.jina.ai/api](https://chat.jina.ai/api)上设置。让我们在这里将英语翻译成法语：
- en: '[PRE27]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can set different temperatures, where a low temperature makes the responses
    more predictable. In this case it makes very little difference. We are starting
    the conversation with a system message clarifying the purpose of the chatbot.Let''s
    ask for some food recommendations:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以设置不同的温度，低温会使响应更可预测。在这种情况下，几乎没有什么区别。我们将以系统消息开始对话，澄清聊天机器人的目的。让我们询问一些食物推荐：
- en: '[PRE30]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'I am seeing this response in Jupyter:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我在Jupyter中看到了这个响应：
- en: '[PRE31]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: It ignored the one-word instruction, but I actually liked reading the ideas.
    I think I could try this for my son. With other chatbots, I am getting the suggestion
    of Ratatouille.It's important to understand the difference in LangChain between
    LLMs and Chat Models. LLMs are text completion models that take a string prompt
    as input and output a string completion. Chat models are similar to LLMs but are
    specifically designed for conversations. They take a list of chat messages as
    input, labeled with the speaker, and return a chat message as output. Both LLMs
    and Chat Models implement the Base Language Model interface, which includes methods
    such as `predict()` and `predict_messages()`. This shared interface allows for
    interchangeability between different types of models in applications as well as
    between Chat and LLM models.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 它忽略了单词指令，但我实际上很喜欢阅读这些想法。我想我可以尝试这个给我儿子。在其他聊天机器人中，我得到了Ratatouille的建议。了解LangChain中LLMs和Chat
    Models之间的区别很重要。LLMs是文本完成模型，接受字符串提示作为输入，并输出字符串完成。Chat Models类似于LLMs，但专门设计用于对话。它们接受带有讲话者标签的聊天消息列表作为输入，并返回聊天消息作为输出。LLMs和Chat
    Models都实现了Base Language Model接口，其中包括`predict()`和`predict_messages()`等方法。这种共享接口允许在应用程序中以及Chat和LLM模型之间实现不同类型模型的互换。
- en: Replicate
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 复制
- en: 'Established in 2019, Replicate Inc. is a San Francisco-based startup that presents
    a streamlined process to AI developers, where they can implement and publish AI
    models with minimal code input through the utilization of cloud technology. The
    platform works with private as well as public models and enables model inference
    and fine-tuning. The firm, deriving its most recent funding from a Series A funding
    round of which the invested total was $12.5 million, was spearheaded by Andreessen
    Horowitz, and involved the participation of Y Combinator, Sequoia, and various
    independent investors. Ben Firshman, who drove open-source product efforts at
    Docker, and Andreas Jansson, a former machine learning engineer at Spotify, co-founded
    Replicate Inc. with the mutual aspiration to eliminate the technical barriers
    that were hindering the mass acceptance of AI. Consequently, they created Cog,
    an open-source tool that packs machine learning models into a standard production-ready
    container that can run on any current operating system and automatically generates
    an API. These containers can also be deployed on clusters of GPUs through the
    replicate platform. As a result, developers can concentrate on other essential
    tasks, thereby enhancing their productivity.You can authenticate with your Github
    credentials on [https://replicate.com/](https://replicate.com/)If you then click
    on your user icon on the top left, you''ll find the API tokens - just copy the
    API key and make it available in your environment as `REPLICATE_API_TOKEN`. In
    order to run bigger jobs, you need to set up your credit card (under billing).You
    can find a lot of models available at [https://replicate.com/explore](https://replicate.com/explore)Here''s
    a simple example for creating an image:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 成立于2019年的Replicate Inc.是一家总部位于旧金山的初创公司，为人工智能开发者提供了一个简化的流程，他们可以通过利用云技术以最少的代码输入实现和发布人工智能模型。该平台可以与私有模型和公共模型一起工作，并实现模型推理和微调。该公司最近一轮融资来自一笔总额为1250万美元的A轮融资，由Andreessen
    Horowitz领投，Y Combinator、Sequoia以及各种独立投资者参与其中。Ben Firshman曾在Docker负责开源产品工作，Andreas
    Jansson曾是Spotify的前机器学习工程师，他们共同创立了Replicate Inc.，共同的愿景是消除阻碍人工智能大规模接受的技术障碍。因此，他们创建了Cog，这是一个开源工具，可以将机器学习模型打包成标准的生产就绪容器，可以在任何当前操作系统上运行，并自动生成API。这些容器也可以通过Replicate平台部署在GPU集群上。因此，开发者可以集中精力进行其他重要任务，从而提高他们的生产力。您可以在[https://replicate.com/](https://replicate.com/)上使用您的Github凭据进行身份验证。然后，如果您点击左上角的用户图标，您会找到API令牌
    - 只需复制API密钥并在您的环境中提供`REPLICATE_API_TOKEN`。为了运行更大的作业，您需要设置您的信用卡（在账单下）。您可以在[https://replicate.com/explore](https://replicate.com/explore)找到许多可用的模型。这里有一个创建图像的简单示例：
- en: '[PRE32]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'I got this image:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我得到了这张图片：
- en: '![Figure 3.7: A Book Cover for a book about generative AI with Python - Stable
    Diffusion.](../media/file25.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图3.7：一本关于使用Python进行生成式人工智能的书籍封面 - Stable Diffusion。](../media/file25.png)'
- en: 'Figure 3.7: A Book Cover for a book about generative AI with Python - Stable
    Diffusion.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7：一本关于使用Python进行生成式人工智能的书籍封面 - Stable Diffusion。
- en: I think it's a nice image - is that an AI chip that creates art?Let's see quickly
    how to run a model locally in Huggingface transformers or Llama.cpp!
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这是一幅不错的图像 - 那是一个创作艺术的AI芯片吗？让我们快速看看如何在Huggingface transformers或Llama.cpp中本地运行模型！
- en: Local Models
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本地模型
- en: 'We can also run local models from LangChain. Let''s preface this with a note
    of caution: an LLM is big, which means that it''ll take up a lot of space. If
    you have an old computer, you can try hosted services such as google colabs. These
    will let you run on machines with a lot of memory and different hardware including
    Tensor Processing Units (TPUs) or GPUs.Since both these use cases can take very
    long to run or crash the Jupyter notebook, I haven''t included this code in the
    notebook or the dependencies in the setup instructions. I think it''s still worth
    discussing it here. The advantages of running models locally are complete control
    over the model and not sharing any data over the internet.Let''s see this first
    with the transformers library by Hugging Face.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以从LangChain运行本地模型。在此之前，我想先提醒一下：LLM很大，这意味着它会占用很多空间。如果您有一台旧电脑，您可以尝试托管服务，如google
    colabs。这些服务可以让您在具有大量内存和不同硬件（包括Tensor处理单元（TPUs）或GPU）的机器上运行。由于这两种用例可能需要很长时间才能运行或导致Jupyter笔记本崩溃，我没有在笔记本中包含此代码或在设置说明中包含依赖项。但我认为在这里讨论它仍然是值得的。本地运行模型的优势是完全控制模型，不会在互联网上传输任何数据。让我们首先看看Hugging
    Face的transformers库。
- en: Hugging Face transformers
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Hugging Face transformers
- en: 'I''ll quickly show the general recipe of setting up and running a pipeline:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我将快速展示设置和运行流水线的一般步骤：
- en: '[PRE33]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This model is quite small (355 million parameters), but relative performant,
    and instruction tuned for conversations.Please note that we don''t need an API
    token for local models!This will download everything that''s needed for the model
    such as the tokenizer and model weights. We can then run a text completion to
    give us some content for this chapter.In order to plug in this pipeline into a
    LangChain agent or chain, we can use it the same way that we''ve seen before:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型非常小（3.55亿个参数），但相对性能良好，并且针对对话进行了调整。请注意，对于本地模型，我们不需要API令牌！这将下载模型所需的一切，如分词器和模型权重。然后我们可以运行文本完成以为本章提供一些内容。为了将此流水线插入LangChain代理或链中，我们可以像之前看到的那样使用它：
- en: '[PRE34]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In this example, we also see the use of a `PromptTemplate` that gives specific
    instructions for the task.Let's do Llama.cpp next.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们还看到了使用`PromptTemplate`的示例，为任务提供具体的指令。接下来让我们来看看Llama.cpp。
- en: Llama.cpp
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Llama.cpp
- en: 'Llama.cpp is a C++ program that executes models based on architectures based
    on Llama, one of the first large open-source models released by Meta AI, which
    spawned the development of many other models in turn. Please note that you need
    to have an md5 checksum tool installed. This is included by default in several
    Linux distributions such as Ubuntu. On MacOs, you can install it with brew like
    this:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Llama.cpp是一个C++程序，执行基于Llama架构的模型，Llama是Meta AI发布的第一个大型开源模型之一，随后衍生出许多其他模型的开发。请注意，您需要安装md5校验工具。这在几个Linux发行版中默认包含，如Ubuntu。在MacOs上，您可以像这样使用brew安装：
- en: '[PRE35]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We need to download the llama.cpp repository from Github. You can do this online
    choosing one of the download options on Github, or you can use a git command from
    the terminal like this:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要从Github下载llama.cpp存储库。您可以在线选择Github上的下载选项之一进行此操作，或者您可以从终端使用git命令，如下所示：
- en: '[PRE36]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then we need to install the python requirements, which we can do with the pip
    package installer - let''s also switch to the llama.cpp project root directory
    for convenience:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要安装python要求，可以使用pip软件包安装程序来完成 - 为了方便起见，让我们也切换到llama.cpp项目根目录：
- en: '[PRE37]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'You might want to create a Python environment before you install requirements
    - but this is up to you. Now we need to compile llama.cpp:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能希望在安装要求之前创建一个Python环境 - 但这取决于您。现在我们需要编译llama.cpp：
- en: '[PRE38]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We can parallelize the build with 4 processes. In order to get the Llama model
    weights, you need to sign up with the T&Cs and wait for a registration email from
    Meta. There are tools such as the llama model downloader in the pyllama project,
    but please be advised that they might not conform with the license stipulations
    by Meta. You can download models from Hugging Face - these models should be compatible
    with llama.cpp, such as Vicuna or Alpaca. Let''s assume you have downloaded the
    model weights for the 7B Llama model into the models/7B directory.You can download
    models in much bigger sizes such as 13B, 30B, 65B, however, a note of caution
    is in order here: these models are fairly big both in terms of memory and disk
    space.We have to convert the model to llama.cpp format, which is called **ggml**,
    using the convert script. Then we can optionally quantize the models to save memory
    when doing inference. Quantization refers to reducing the number of bits that
    are used to store weights.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用4个进程并行构建。为了获得Llama模型权重，您需要注册并等待来自Meta的注册电子邮件。有一些工具，比如pyllama项目中的llama模型下载器，但请注意它们可能不符合Meta的许可规定。您可以从Hugging
    Face下载模型 - 这些模型应与llama.cpp兼容，比如Vicuna或Alpaca。假设您已将7B Llama模型的模型权重下载到models/7B目录中。您可以下载更大尺寸的模型，如13B、30B、65B，但是这里需要注意一点：这些模型在内存和磁盘空间方面都相当大。我们需要将模型转换为llama.cpp格式，称为**ggml**，使用转换脚本。然后我们可以选择对模型进行量化，以节省推理时的内存。量化是指减少用于存储权重的位数。
- en: '[PRE39]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'This last file is much smaller than the previous files and will take up much
    less space in memory as well, which means that you can run it on smaller machines.Once
    we have chosen a model that we want to run, we can integrate it into an agent
    or a chain for example as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这个最后的文件比之前的文件要小得多，在内存中占用的空间也要少得多，这意味着您可以在较小的机器上运行它。一旦我们选择了要运行的模型，我们可以将其集成到代理或链中，例如：
- en: '[PRE40]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: )
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: This concludes the introduction to model providers. Let's build an application!
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了模型提供者的介绍。让我们构建一个应用程序！
- en: Customer Service Helper
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 客户服务助手
- en: 'In this section, we''ll build a text classification app in LangChain for customer
    service agents. Given a document such as an email, we want to classify it into
    different categories related to intent, extract the sentiment, and provide a summary.Customer
    service agents are responsible for answering customer inquiries, resolving issues
    and addressing complaints. Their work is crucial for maintaining customer satisfaction
    and loyalty, which directly affects a company''s reputation and financial success.
    Generative AI can assist customer service agents in several ways:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在LangChain中为客服代理构建一个文本分类应用程序。给定一个文档，比如一封电子邮件，我们希望将其分类为与意图相关的不同类别，提取情感，并提供摘要。客服代理负责回答客户查询，解决问题和处理投诉。他们的工作对于维护客户满意度和忠诚度至关重要，这直接影响公司的声誉和财务成功。生成式人工智能可以在几个方面帮助客服代理：
- en: 'Sentiment classification: this helps identify customer emotions and allows
    agents to personalize their response.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分类：这有助于识别客户情绪，并允许代理个性化回应。
- en: 'Summarization: this enables agents to understand the key points of lengthy
    customer messages and save time.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摘要：这使代理能够理解冗长客户消息的要点并节省时间。
- en: 'Intent classification: similar to summarization, this helps predict the customer''s
    purpose and allows for faster problem-solving.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 意图分类：类似于摘要，这有助于预测客户的目的，并允许更快地解决问题。
- en: 'Answer suggestions: this provides agents with suggested responses to common
    inquiries, ensuring that accurate and consistent messaging is provided.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 答案建议：这为代理提供了对常见查询的建议响应，确保提供准确和一致的消息。
- en: 'These approaches combined can help customer service agents respond more accurately
    and in a timely manner, ultimately improving customer satisfaction.Here, we will
    concentrate on the first three points. We''ll document lookups, which we can use
    for answer suggestions in *Chapter 5*, *Building a Chatbot like ChatGPT*.**LangChain**
    is a very flexible library with many integrations that can enable us to tackle
    a wide range of text problems. We have a choice between many different integrations
    to perform these tasks. We could ask any LLM to give us an open-domain (any category)
    classification or choose between multiple categories. In particular, because of
    their large training size, LLMs are very powerful models, especially when given
    few-shot prompts, for sentiment analysis that don''t need any additional training.
    This was analyzed by Zengzhi Wang and others in their April 2023 study "Is ChatGPT
    a Good Sentiment Analyzer? A Preliminary Study". A prompt, for an LLM for sentiment
    analysis could be something like this:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法的结合可以帮助客服代理更准确地并及时地回应，最终提高客户满意度。在这里，我们将集中讨论前三点。我们将记录查找，这可以用于答案建议在*第5章*，*构建类似ChatGPT的聊天机器人*。**LangChain**是一个非常灵活的库，具有许多集成，可以使我们解决各种文本问题。我们可以在执行这些任务时选择许多不同的集成。我们可以要求任何LLM为我们提供一个开放域（任何类别）分类或在多个类别之间进行选择。特别是由于它们的大训练规模，LLMs是非常强大的模型，特别是在给定少量提示的情况下，用于情感分析，不需要任何额外的训练。这是由Zengzhi
    Wang等人在他们2023年4月的研究“ChatGPT是一个好的情感分析器吗？初步研究”中分析的。对于情感分析的LLM的提示可能是这样的：
- en: '[PRE41]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'LLMs can be also very effective at summarization, much better than any previous
    models. The downside can be that these model calls are slower than more traditional
    ML models and more expensive.If we want to try out more traditional or smaller
    models. Cohere and other providers have text classification and sentiment analysis
    as part of their capabilities. For example, NLP Cloud''s model list includes spacy
    and many others: [https://docs.nlpcloud.com/#models-list](https://docs.nlpcloud.com/#models-list)Many
    Hugging Face models are supported for these tasks including:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs在摘要方面也非常有效，比以往任何模型都要好。缺点可能是这些模型调用比传统的ML模型慢，成本更高。如果我们想尝试更传统或更小的模型。Cohere和其他提供商将文本分类和情感分析作为其功能的一部分。例如，NLP
    Cloud的模型列表包括spacy和许多其他模型：[https://docs.nlpcloud.com/#models-list](https://docs.nlpcloud.com/#models-list)许多Hugging
    Face模型支持这些任务，包括：
- en: document-question-answering
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档问答
- en: summarization
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摘要
- en: text-classification
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类
- en: text-question-answering
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本问答
- en: translation
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 翻译
- en: 'We can execute these models either locally, by running a `pipeline` in transformer,
    remotely on the Hugging Face Hub server (`HuggingFaceHub`), or as tool through
    the `load_huggingface_tool()` loader.Hugging Face contains thousands of models,
    many fine-tuned for particular domains. For example, `ProsusAI/finbert` is a BERT
    model that was trained on a dataset called Financial PhraseBank, and can analyze
    sentiment of financial text. We could also use any local model. For text classification,
    the models tend to be much smaller, so this would be less of a drag on resources.
    Finally, text classification could also be a case for embeddings, which we''ll
    discuss in *Chapter 5*, *Building a Chatbot like ChatGPT*.I''ve decided to try
    make do as much as I can with smaller models that I can find on Hugging Face for
    this exercise.We can list the 5 most downloaded models on Hugging Face Hub for
    text classification through the huggingface API:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在本地执行这些模型，通过在 transformer 中运行`pipeline`，在 Hugging Face Hub 服务器上远程执行（`HuggingFaceHub`），或者作为`load_huggingface_tool()`加载器的工具。Hugging
    Face 包含了成千上万的模型，许多经过特定领域微调。例如，`ProsusAI/finbert`是一个在 Financial PhraseBank 数据集上训练的
    BERT 模型，可以分析金融文本的情感。我们也可以使用任何本地模型。对于文本分类，模型往往要小得多，因此这对资源的消耗会较小。最后，文本分类也可能是嵌入的一个案例，我们将在*第5章*，*构建类似
    ChatGPT 的聊天机器人*中讨论。我决定尽可能多地使用我在 Hugging Face 上找到的较小模型来进行这个练习。我们可以通过 huggingface
    API 列出 Hugging Face Hub 上用于文本分类的下载量最高的5个模型：
- en: '[PRE42]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Let''s see the list:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看列表：
- en: '| **Model** | **Downloads** |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **下载量** |'
- en: '| nlptown/bert-base-multilingual-uncased-sentiment | 5805259 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| nlptown/bert-base-multilingual-uncased-sentiment | 5805259 |'
- en: '| SamLowe/roberta-base-go_emotions | 5292490 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| SamLowe/roberta-base-go_emotions | 5292490 |'
- en: '| cardiffnlp/twitter-roberta-base-irony | 4427067 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| cardiffnlp/twitter-roberta-base-irony | 4427067 |'
- en: '| salesken/query_wellformedness_score | 4380505 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| salesken/query_wellformedness_score | 4380505 |'
- en: '| marieke93/MiniLM-evidence-types | 4370524 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| marieke93/MiniLM-evidence-types | 4370524 |'
- en: 'Tablee 3.2: The most popular text classification models on Hugging Face Hub.We
    can see that these models are about small ranges of categories such as sentiment,
    emotions, irony, or well-formedness. Let''s use the sentiment model.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.2：Hugging Face Hub 上最受欢迎的文本分类模型。我们可以看到这些模型涉及到情感、情绪、讽刺或格式良好等小范围的类别。让我们使用情感模型。
- en: '[PRE43]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'I am getting this result:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我得到了这个结果：
- en: '[PRE44]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Not a happy camper. Let''s move on!Let''s see the 5 most popular models for
    summarization as well:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 不是一个开心的露营者。让我们继续！让我们也看看最受欢迎的5个摘要模型：
- en: '| **Model** | **Downloads** |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **下载量** |'
- en: '| t5-base | 2710309 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| t5-base | 2710309 |'
- en: '| t5-small | 1566141 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| t5-small | 1566141 |'
- en: '| facebook/bart-large-cnn | 1150085 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| facebook/bart-large-cnn | 1150085 |'
- en: '| sshleifer/distilbart-cnn-12-6 | 709344 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| sshleifer/distilbart-cnn-12-6 | 709344 |'
- en: '| philschmid/bart-large-cnn-samsum | 477902 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| philschmid/bart-large-cnn-samsum | 477902 |'
- en: 'Table 3.3: The most popular summarization models on Hugging Face Hub.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.3：Hugging Face Hub 上最受欢迎的摘要模型。
- en: 'All these models have a relatively small footprint compared to large models.
    Let''s execute the summarization model remotely on a server:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些模型与大型模型相比具有相对较小的占用空间。让我们在服务器上远程执行摘要模型：
- en: '[PRE45]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Please note that you need to have your `HUGGINGFACEHUB_API_TOKEN` set for this
    to work.I am seeing this summary:A customer''s coffee machine arrived ominously
    broken, evoking a profound sense of disbelief and despair. "This heartbreaking
    display of negligence shattered my dreams of indulging in daily coffee perfection,
    leaving me emotionally distraught and inconsolable," the customer writes. "I hope
    this email finds you amidst an aura of understanding, despite the tangled mess
    of emotions swirling within me as I write to you," he adds.This summary is just
    passable, but not very convincing. There''s still a lot of rambling in the summary.
    We could try other models or just go for an LLM with a prompt asking to summarize.
    Let''s move on.It could be quite useful to know what kind of issue the customer
    is writing about. Let''s ask **VertexAI**:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您需要设置您的`HUGGINGFACEHUB_API_TOKEN`才能使其工作。我看到了这个摘要：一个顾客的咖啡机到达时已经破损，引发了一种深深的不信和绝望感。顾客写道：“这种令人心碎的疏忽行为粉碎了我对每天享受完美咖啡的梦想，让我情感上感到痛苦和无法安慰。”他补充道：“我希望这封邮件能在我写信时内心充满理解的氛围中找到你。”这个摘要还算过得去，但不是很令人信服。摘要中仍然有很多啰嗦的内容。我们可以尝试其他模型，或者直接使用一个带有摘要提示的
    LLM。让我们继续。了解客户写的是什么问题可能会很有用。让我们问问**VertexAI**：
- en: '[PRE46]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: We get "product issues" back, which is correct for the long email example that
    I am using here.I hope it was exciting to see how quickly we can throw a few models
    and tools together in LangChain to get something that looks actually useful. We
    could easily expose this in an interface for customer service agents to see.Let's
    summarize.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了“产品问题”的反馈，这在我这里使用的长电子邮件示例中是正确的。希望看到我们可以多快地将几个模型和工具组合在一起在 LangChain 中得到实际有用的东西是令人兴奋的。我们可以很容易地将其展示在一个界面中，供客服代理查看。让我们总结一下。
- en: Summary
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we've walked through four distinct ways of installing LangChain
    and other libraries needed in this book as an environment. Then, we've introduced
    several providers of models for text and images. For each of them, we explained
    where to get the API token, and demonstrated how to call a model. Finally, we've
    developed an LLM app for text classification in a use case for customer service.
    By chaining together various functionalities in LangChain, we can help reduce
    response times in customer service and make sure answers are accurate and to the
    point.In the chapters 3 and 4, we'll dive more into use cases such as question
    answering through augmented retrieval using tools such as web searches and chatbots
    relying on document search through indexing. Let's see if you remember some of
    the key takeaways from this chapter!
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经介绍了四种不同的安装 LangChain 和本书所需的其他库的方法作为一个环境。然后，我们介绍了几个文本和图像模型的提供者。对于每一个模型，我们解释了如何获取
    API 令牌，并演示了如何调用模型。最后，我们为客服的文本分类开发了一个 LLM 应用程序。通过在 LangChain 中链接各种功能，我们可以帮助减少客服的响应时间，并确保答案准确且切题。在第
    3 章和第 4 章中，我们将更深入地探讨诸如通过增强检索进行问答等用例，使用工具如网络搜索和依赖文档搜索通过索引的聊天机器人。让我们看看您是否记得本章的一些关键要点！
- en: Questions
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: 'Please have a look to see if you can come up with the answers to these questions.
    I''d recommend you go back to the corresponding sections of this chapter, if you
    are unsure about any of them:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 请看看是否能够回答这些问题。如果您对其中任何问题不确定，我建议您回到本章的相应部分查看：
- en: How do you install LangChain?
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何安装 LangChain？
- en: List at least 4 cloud providers of LLMs apart from OpenAI!
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了 OpenAI，至少列出 4 家 LLM 的云服务提供商！
- en: What are Jina AI and Hugging Face?
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是 Jina AI 和 Hugging Face？
- en: How do you generate images with LangChain?
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何使用 LangChain 生成图像？
- en: How do you run a model locally on your own machine rather than through a service?
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何在本地机器上运行模型，而不是通过服务运行？
- en: How do you perform text classification in LangChain?
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何在 LangChain 中执行文本分类？
- en: How can we help customer service agents in their work through generative AI?
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何通过生成式人工智能帮助客服代理工作？
