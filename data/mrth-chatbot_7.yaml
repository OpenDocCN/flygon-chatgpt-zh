- en: © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023M.
    Kurpicz-BrikiMore than a Chatbot[https://doi.org/10.1007/978-3-031-37690-0_7](https://doi.org/10.1007/978-3-031-37690-0_7)
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: © 作者，独家许可给Springer Nature Switzerland AG 2023M. Kurpicz-Briki不仅仅是一个聊天机器人[https://doi.org/10.1007/978-3-031-37690-0_7](https://doi.org/10.1007/978-3-031-37690-0_7)
- en: 7. The Future of Humans and Language Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7. 人类和语言模型的未来
- en: Mascha Kurpicz-Briki^([1](#Aff2)  )(1)Applied Machine Intelligence, Bern University
    of Applied Sciences, Biel/Bienne, Switzerland
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Mascha Kurpicz-Briki^([1](#Aff2) )(1)应用机器智能，瑞士比尔大学应用科学学院，比尔/宾讷，瑞士
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概览
- en: We have seen in this book how the technology behind state-of-the-art language
    models and other text processing software is working. Based on this understanding,
    I want you to participate actively in the public discourse on how our future’s
    society will be shaped by this technology. This discussion requires technical
    experts to collaborate closely with scholars from other domains as well as lay
    people, and a basic technological understanding like this book has provided is
    crucial to have a fruitful discussion on how our societies should deal with such
    majorly altering technologies.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书中看到了现代语言模型和其他文本处理软件背后的技术是如何运作的。基于这种理解，我希望您积极参与公众对于我们未来社会将如何被这种技术塑造的讨论中。这种讨论需要技术专家与其他领域的学者以及普通人密切合作，而像本书提供的基本技术理解对于就如何处理这种重大改变技术的讨论至关重要。
- en: In this final chapter, I thus want to raise some points of discussion and point
    some potential directions as food for thoughts.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在这最后一章中，我想提出一些讨论点，并指出一些潜在的方向作为思考的食粮。
- en: The Future of Humans
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人类的未来
- en: 'To discuss the different visions of the future of machine and human collaboration,
    we have to look at two different kinds of artificial intelligence that are typically
    differentiated:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 要讨论机器与人类协作未来的不同愿景，我们必须看两种不同类型的人工智能，通常被区分为：
- en: '[…] weak or narrow AI on the one hand and strong AI on the other. Weak AI is
    capable only of solving specific problems—playing chess, for example, or recognizing
    what lies in a picture. Strong AI, by contrast, would designate a computer system
    that responds intelligently at a general level, including in situations where
    precise factual information is missing or the objectives are unclear. (Zweig [2022](#CR43),
    S. 90)'
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[…] 一方面是弱人工智能或狭义人工智能，另一方面是强人工智能。弱人工智能只能解决特定问题，比如下棋，或者识别图片中的内容。相比之下，强人工智能将指代一个在一般水平上能够智能地回应的计算机系统，包括在缺少精确事实信息或目标不明确的情况下。（Zweig
    [2022](#CR43), S. 90）'
- en: The strong AI is also referred to as general artificial intelligence (or artificial
    general intelligence AGI) and is the “Hollywood version” (Broussard [2018](#CR5),
    p. 10), the kind of AI that is used in sci-fi movies with robots taking over the
    government and giving us a dark vision of the future of humans. But why should
    we create such a monster that aims to wipe us out completely someday?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 强人工智能也被称为通用人工智能（或人工通用智能AGI），是“好莱坞版本”（Broussard [2018](#CR5), p. 10），是在科幻电影中使用的那种人工智能，其中机器人接管政府并给我们展示了人类未来的黑暗愿景。但是为什么我们要创造这样一个旨在有一天完全消灭我们的怪物呢？
- en: The intention and wish of creating an *artificial general intelligence* have
    been inspirations for many science-fiction tales and movies and at the same time
    divide the community of researchers in the field. Researchers disagree on the
    forecast, when (or if) such an artificial general intelligence will be achieved,
    and whether it is even worth targeting this kind of artificial intelligence. Artificial
    general intelligence can also be defined as the “ability to accomplish any cognitive
    task at least as well as humans” (Tegmark [2018](#CR36), p. 39).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 创造*人工通用智能*的意图和愿望一直是许多科幻故事和电影的灵感来源，同时也分裂了该领域的研究人员社区。研究人员对于何时（或是否）将实现这种人工通用智能以及是否值得针对这种人工智能进行预测存在分歧。人工通用智能也可以被定义为“至少能够像人类一样完成任何认知任务的能力”（Tegmark
    [2018](#CR36), p. 39）。
- en: Any cognitive task. That’s quite complete. As Tegmark ([2018](#CR36)) describes,
    based on such a general artificial intelligence, a *superintelligence* could be
    built, which can be used or unleased to take over the world. With superintelligence,
    he refers to “general intelligence far beyond human level” (Tegmark [2018](#CR36),
    S. 39). This hypothetical moment in time, with unforeseen consequences for human
    civilization, is also referred to as *technology singularity*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 任何认知任务。这相当完整。正如Tegmark（[2018](#CR36)）所描述的那样，基于这样一个通用人工智能，可以构建一个*超级智能*，可以被使用或释放来接管世界。他所指的超级智能是指“远远超过人类水平的通用智能”（Tegmark
    [2018](#CR36)，第39页）。这个假设的时刻，对人类文明产生了无法预料的后果，也被称为*技术奇点*。
- en: Certainly, there would still be a long way to go to reach something like this,
    at a technical level. Will we ever achieve something like this? Hard to predict.
    Does it make sense to aim at building this kind of a superintelligence (or a system
    with capabilities to potentially *break out* and create itself a superintelligence)?
    That’s something we all should discuss, as a society.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在技术层面上还有很长的路要走才能达到这样的目标。我们会实现这样的目标吗？很难预测。有意致力于构建这种超级智能（或具有潜力*突破*并创造自己的超级智能系统）是否有意义？这是我们作为一个社会都应该讨论的事情。
- en: Take a step back and consider the use cases where advanced technology could
    be beneficial as a tool to humans, as we are using other tools like calculators
    or cars. Now consider the scenario of creating a superintelligence, in form of
    armed robots or just by fully simulating being a human. One could be wondering,
    *why humans*? Would you be wanting to do that?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 给自己一点时间，考虑一下先进技术作为人类工具的用例，就像我们使用计算器或汽车一样。现在考虑一下创建超级智能的场景，可以是武装机器人的形式，也可以是完全模拟成为人类的形式。有人可能会想，*为什么选择人类*？你会想要这样做吗？
- en: I encourage us as a society to think about what is useful for us in terms of
    technology, rather than developing technology that aims apocalyptic scenarios
    for mankind. Whereas there might be a fascination to be the creator of a *superintelligence*,
    it is not what the society or humanity needs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我鼓励我们作为一个社会思考对我们有用的技术，而不是开发旨在对人类构成启示录般场景的技术。虽然成为*超级智能*的创造者可能会令人着迷，但这并不是社会或人类需要的。
- en: Coming back to the language models. The way state-of-the-art language models
    or chatbot talk to us, simulating how humans would write, yet with major differences
    when it comes to meaning or communication intention, might be impressive. Still,
    they are a tool, a tool that provides us many opportunities and at the same time
    a tool that brings some risks along. The risks for the near future do not lay
    in the apocalyptic scenario of ChatGPT taking over the world but in other important
    discussions that need to be conducted now.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 回到语言模型。最先进的语言模型或聊天机器人与我们交谈的方式，模拟人类写作的方式，但在涉及含义或沟通意图时存在重大差异，可能令人印象深刻。然而，它们只是工具，一种为我们提供许多机会的工具，同时也带来一些风险的工具。在不久的将来，风险并不在于ChatGPT接管世界的启示录般场景，而在于现在需要进行的其他重要讨论。
- en: This split of the AI community was also visible in the case of *the letter*
    in spring 2023\. As a consequence of the release of OpenAIs ChatGPT and in particular
    the GPT-4 model, the Future of Life Institute published an open letter signed
    by more than 10,000 supporters (Future of Life Institute [2023](#CR14)). Among
    the supporters, you can find professors from the field of AI, a Turing prize winner,
    and co-founders of well-known tech companies. In this letter, they call on all
    centers developing AI to pause for at least 6 months the training of AI systems
    more powerful than GPT-4\. They argued that contemporary AI systems are becoming
    human-competitive at general tasks and that they should only be developed once
    we are confident that their effects will be positive and their risks manageable.
    The proposed pause of 6 months should be public and verifiable and, if necessary,
    enforced by governments applying a moratorium.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: AI社区的这种分裂在2023年春天的*这封信*案中也是可见的。由于OpenAI的ChatGPT以及特别是GPT-4模型的发布，未来生命研究所发表了一封公开信，得到了超过10,000名支持者的签署（未来生命研究所[2023](#CR14)）。支持者中包括来自人工智能领域的教授、图灵奖获得者和知名科技公司的联合创始人。在这封信中，他们呼吁所有开发AI的中心暂停至少6个月训练比GPT-4更强大的AI系统。他们认为，当代AI系统正在变得在一般任务上具有人类竞争力，而且只有在我们确信它们的影响将是积极的，风险可控时才应该开发。提议的6个月暂停应该是公开和可验证的，并且必要时应由政府施加停止令。
- en: 'The letter was heavily discussed in the media. In addition to the need of a
    pause, in particular, the feasibility of the proposed a moratorium was put in
    question. The authors of the paper describing large language models as stochastic
    parrots (Bender et al. [2021](#CR3)), which we have encountered earlier, have
    shortly after published a statement regarding the letter (Gebru et al. [2023](#CR16)).
    In their statement, they discussed the need for regulatory efforts focusing on
    transparency, accountability, and prevention of exploitive labor practice, with
    a focus on AI that is already now real and present, deployed in automated systems.
    In particular, they criticized the fearmongering with hypothetical risks like
    “human-competitive intelligence” or “powerful digital minds.” They argued that
    the letter ignores harms such as worker exploitation, massive data theft, synthetic
    media data reproducing systems of oppression and endangering the information ecosystem,
    and the concentration of power which exacerbates social inequities. Especially,
    they warned that:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这封信在媒体上引起了广泛讨论。除了需要暂停之外，特别是所提议的停止令的可行性也受到了质疑。我们之前遇到的将大型语言模型描述为随机鹦鹉的论文的作者（Bender等人[2021](#CR3)）在此后不久发表了一份关于这封信的声明（Gebru等人[2023](#CR16)）。在他们的声明中，他们讨论了需要侧重于透明度、问责制和防止剥削性劳动实践的监管努力，重点放在已经现实存在并部署在自动化系统中的人工智能上。特别是，他们批评了通过假设性风险如“人类竞争性智能”或“强大的数字思维”来制造恐慌。他们认为这封信忽视了诸如工人剥削、大规模数据盗窃、合成媒体数据再现压迫系统和危害信息生态系统、以及权力集中加剧社会不平等等危害。特别是，他们警告说：
- en: Such language that inflates the capabilities of automated systems and anthropomorphizes
    them, as we note in Stochastic Parrots, deceives people into thinking that there
    is a sentient being behind the synthetic media. This not only lures people into
    uncritically trusting the outputs of systems like ChatGPT, but also misattributes
    agency. Accountability properly lies not with the artifacts but with their builders.
    (Gebru et al. [2023](#CR16))
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 正如我们在《随机鹦鹉》中所指出的那样，夸大自动化系统的能力并赋予其拟人化的语言，会让人误以为合成媒体背后有一个有意识的存在。这不仅会诱使人们盲目地信任像ChatGPT这样的系统的输出，还会错误地归因于代理。责任应该完全落在建造者身上，而不是他们的产物。（Gebru等人[2023](#CR16)）
- en: Therefore, they underlined the need for regulation that enforces transparency
    and that regulations should protect the rights and interests of people when this
    technology is being applied by corporations.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，他们强调了需要强制透明度的监管，并且监管应该在这项技术被公司应用时保护人们的权利和利益。
- en: Whether today’s existing models like GPT-4 do have first signs of human-like
    intelligence or not is also influenced by the definition of intelligence itself.
    Different definitions are being used and are discussed, and how to measure this
    intelligence is not finally decided. To sharpen the discussion, common definitions
    will need to be developed in the public discourse.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 今天存在的模型如GPT-4是否具有类似人类智能的初步迹象，也受到智能本身定义的影响。不同的定义正在被使用和讨论，如何衡量这种智能还没有最终确定。为了深化讨论，公共话语中需要制定共同的定义。
- en: As we have seen throughout this book, language models can hallucinate and require
    additional e-literacy skills to be handled in a responsible manner. At the same
    time, people might interpret information provided by a chatbot differently from
    the information provided in a bullet list in the results of a search engine. There
    are risks when it comes to discrimination and bias in these systems, as well as
    expensive ecological consequences. Finally, the way machines and humans collaborate,
    in terms of work or learning, might change, requiring an adaptation of how we
    have been doing things so far. In a similar way, an adaptation was required when
    calculators were entering the market.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本书中所看到的，语言模型可能会产生幻觉，需要额外的电子素养技能来以负责任的方式处理。同时，人们可能会以不同的方式解释由聊天机器人提供的信息，与搜索引擎结果中的项目列表提供的信息不同。在这些系统中存在歧视和偏见的风险，以及昂贵的生态后果。最后，机器和人类在工作或学习方面的合作方式可能会发生变化，需要调整我们迄今为止的做事方式。类似地，当计算器进入市场时也需要进行调整。
- en: So, rather than worrying about the Terminator AI, let’s look at the more pressing
    changes these new tools bring to our society and how to deal with them.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，与其担心终结者人工智能，不如看看这些新工具给我们社会带来的更紧迫的变化，以及如何应对这些变化。
- en: The Future of Responsible AI
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 负责任人工智能的未来
- en: From a technical perspective, the problems of hallucinations or bias in language
    models are difficult to address. Different technical methods exist to reduce hallucinations,
    either being applied to the data used for training or to the training process
    itself (Ji et al. [2023](#CR22)). In the training process, the components such
    as the encoder, the decoder, or the attention mechanism can be optimized to have
    a better semantic understanding of the input. For the data, the dataset can be
    increased or validated. One method that can be used is to create a (more) faithful
    dataset in collaboration with humans. As we have seen in different parts of this
    book, a language model can only be as good as the data it was trained on. Obtaining
    high-quality and faithful training data in the required quantity is challenging.
    One way to improve the training data is by employing human annotators. Human annotators
    can either write new texts from scratch or go through the training data and correcting
    or improving the collected texts.^([1](#Fn1)) In both cases, the required resources
    in terms of human labor, and thus costs, is very large. Therefore, this is often
    in the best case feasible for very domain-specific tasks and lacks generalization.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术角度来看，语言模型中的幻觉或偏见问题很难解决。存在不同的技术方法来减少幻觉，可以应用于用于训练的数据或训练过程本身（Ji等人[2023](#CR22)）。在训练过程中，诸如编码器、解码器或注意力机制等组件可以进行优化，以更好地理解输入的语义。对于数据集，可以增加或验证数据集。可以使用的一种方法是与人类合作创建一个（更）忠实的数据集。正如我们在本书的不同部分所看到的，语言模型的表现只能和它所训练的数据一样好。获取所需数量的高质量和忠实的训练数据是具有挑战性的。改进训练数据的一种方法是雇佣人类标注员。人类标注员可以从头开始撰写新文本，也可以查看训练数据并纠正或改进收集到的文本。^([1](#Fn1))
    在这两种情况下，所需的人力资源和成本是非常巨大的。因此，这通常在最好的情况下适用于非常特定领域的任务，并且缺乏泛化性。
- en: For example, if we take the entire training set of a large language model, containing
    billions of words, it is not feasible to manually review all of this by humans.
    However, if we say that we are in particular interested in being sure that the
    system is all correct about strawberries, we could pick out from the training
    data all sentences containing the word *strawberry*. This would probably reduce
    the number of sentences to be reviewed by hand by a lot, and maybe it would become
    feasible.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们拿一个包含数十亿字的大型语言模型的整个训练集来说，手动审查所有这些是不可行的。然而，如果我们特别关心确保系统在草莓方面是完全正确的，我们可以从训练数据中挑选出所有包含*草莓*一词的句子。这可能会大大减少需要手动审查的句子数量，也许这样做是可行的。
- en: At a technical level, the quality of the training data and transparency are
    two major enablers for responsible AI. Responsible AI, or in the context of this
    book *responsible natural language processing*, is a field that should interest
    us in order to shape the digital society we would like to have for the future.
    This raises also to the question of whether there is *the* digital society or
    whether there will be several digital societies involving different groups or
    regions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在技术层面上，训练数据的质量和透明度是推动负责任人工智能的两个主要因素。负责任人工智能，或者在本书的背景下*负责任自然语言处理*，是一个应该引起我们兴趣的领域，以塑造我们未来想要拥有的数字社会。这也引发了一个问题，即数字社会是否存在*唯一的*数字社会，还是将涉及不同群体或地区的几个数字社会。
- en: To enable transparency about data sets, we first need a standard for documentation
    of datasets. Such a standard was proposed in the Datasheets for Datasets paper
    in 2021 (Gebru et al. [2021](#CR15)). The authors argue that the characteristics
    of the training data set influence the model’s behavior, and thus the provenance,
    the creation, and the use of such data sets, need to be well documented. They
    suggest that each data set is accompanied by a data sheet, containing all this
    information. Sounds plausible and simple, but, unfortunately, it is currently
    not (yet) the default standard for the AI industry.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使数据集的透明度更高，我们首先需要一个数据集文档的标准。这样的标准是在2021年的《数据集数据表》论文中提出的（Gebru等人[2021](#CR15)）。作者认为训练数据集的特征会影响模型的行为，因此这些数据集的来源、创建和使用需要有良好的文档记录。他们建议每个数据集都应附带一个数据表，其中包含所有这些信息。听起来很合理和简单，但不幸的是，目前（还）不是AI行业的默认标准。
- en: Apart from having more knowledge about the training data sets, we also want
    to be transparent about the machine learning models. And here it is getting a
    bit trickier. Whereas decisions obtained with the basic methods we have seen earlier
    in the beginning of the book, such as logistic regression, can be explained easier,
    when it comes to neural networks, this is very challenging. This problem is addressed
    by the research field of *explainable AI*. In the context of explainable AI, tools
    and frameworks are developed to understand and interpret the decisions such systems
    make. A better understanding of how decisions are made is required to be transparent
    about machine learning models. Unfortunately, more work is required in this field,
    and fully explaining how a 175 billion parameter language model generates a sentence
    is far from being solved.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对训练数据集有更多的了解之外，我们还希望对机器学习模型保持透明。这里有点棘手。虽然我们在书的开头看到的基本方法（如逻辑回归）得出的决策可以更容易解释，但当涉及到神经网络时，这就非常具有挑战性了。这个问题由*可解释AI*研究领域所解决。在可解释AI的背景下，开发了工具和框架来理解和解释这些系统所做出的决策。需要更好地理解决策是如何做出的，才能对机器学习模型保持透明。不幸的是，这个领域还需要更多的工作，而要完全解释一个拥有1750亿参数的语言模型是如何生成一句话的，目前还远未解决。
- en: Finally, as mentioned earlier, regulations of AI are another topic of the current
    discussion. Whereas most people agree that regulations are required, how they
    shall be implemented technically or enforced is subject to discussion. Whereas
    there might be few people being against fair and transparent AI software, it is
    challenging to fully address this at a technical level. However, we need this
    transparency and will thus need to rethink the way such software is developed
    and deployed. There is work ahead.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，正如前面提到的，AI的监管是当前讨论的另一个话题。虽然大多数人都同意需要监管，但如何在技术上实施或强制执行这些监管措施还有待讨论。虽然可能没有多少人反对公平和透明的AI软件，但在技术层面完全解决这个问题是具有挑战性的。然而，我们需要这种透明度，因此需要重新思考这种软件的开发和部署方式。前方还有很多工作要做。
- en: As we have seen now, the technical solution to these problems is still work
    in progress and, by design, difficult to fix. However, language models are used
    more and more, so we have to address some of these problems also at a societal
    level.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们现在所看到的，这些问题的技术解决方案仍在不断完善中，并且出于设计考虑，难以修复。然而，语言模型的使用越来越多，因此我们也必须在社会层面解决其中一些问题。
- en: The Future of Work
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作的未来
- en: New tools may change the way human work happens. To what extent should on one
    side be defined by what is technically possible but also by what is beneficial
    from a financial perspective and, most importantly, by what makes sense from a
    societal perspective. Personal computers and printers have revolutionized the
    technical way in which we write and produce text, shifting from handwritten notes
    to digital documents. With the introduction of smartphones, yet new ways of written
    communication such as SMS or messaging services have been introduced. With a new
    generation of tools like ChatGPT, text production by humans is challenged on a
    new level.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 新工具可能会改变人类工作的方式。在技术上可能实现到什么程度一方面应该由技术上的可能性来定义，但也应该由财务角度的利益和更重要的是社会角度的意义来定义。个人电脑和打印机已经彻底改变了我们书写和制作文本的技术方式，从手写笔记转变为数字文档。随着智能手机的推出，新的书面沟通方式如短信或消息服务也被引入。随着像ChatGPT这样的新一代工具的推出，人类的文本生产面临着新的挑战。
- en: 'Whereas at a technical level it is now possible to produce texts that look
    somewhat eloquent and legit, they have major limitations in terms of content and
    world knowledge, as we have seen throughout this book. When with the rise of ChatGPT
    people suggested adding such a generative AI as coauthor to their scientific paper,
    it is them putting the technology in the job of a human. This is more a societal
    rather than a technical problem, and we should ask ourselves how we see our role
    and technology’s role in all of this. If you use a generative AI to support you
    generating the structure of your scientific article, do you consider it your coauthor?
    Or let me rephrase it: Did you ever consider putting your text-processing tool
    like Word or Latex or the search engine you used to find related work as a coauthor?'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在技术层面上现在可以生成看起来相当流畅和合法的文本，但从本书中我们可以看到，它们在内容和世界知识方面存在重大局限。随着ChatGPT的兴起，人们建议将这样一种生成式AI添加为他们科学论文的合著者，这是将技术置于人类的工作中。这更多是一个社会问题而不是技术问题，我们应该问问自己在这一切中我们如何看待自己的角色和技术的角色。如果你使用生成式AI来支持你生成科学文章的结构，你是否认为它是你的合著者？或者让我换个说法：你是否考虑过将你的文本处理工具如Word或Latex或你用来查找相关工作的搜索引擎作为合著者？
- en: How we define the power relationship between humans and machines for different
    tasks is crucial. Being a coauthor means seeing the tool at the same power level
    as your human coauthor. Having an AI application give severe instructions to human
    workers might reduce the acceptance of the technology as opposed to a human worker
    seeing a software as a smart tool supporting their working processes. For example,
    worker surveillance tools would probably be seen much more critical than an AI
    tool that is used as a programming assistant for software developers. To represent
    their processes in a useful and acceptable way in digital workflows and enable
    a human-centered digital transformation requires user involvement in all steps.
    This is especially true for novel technologies such as generative AI.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何定义人类和机器在不同任务中的权力关系是至关重要的。成为合著者意味着将工具视为与你的人类合著者具有相同权力水平。让AI应用程序向人类工作者发出严厉的指令可能会降低技术的接受度，与之相反，人类工作者将软件视为支持其工作流程的智能工具。例如，工人监视工具可能会被视为比用作软件开发人员的编程助手的AI工具更为关键。为了在数字化工作流程中以有用和可接受的方式代表他们的流程，并实现以人为中心的数字化转型，需要用户在所有步骤中的参与。这对于生成式AI等新技术尤为重要。
- en: Additionally, with the stereotypes and limitations we have seen previously,
    I can only encourage having a *human in the loop*. As we have seen in the very
    early sections of this book, machine learning models, for example, classifiers,
    make an *estimation*. This guess, which seems the most probable for the given
    scenario, can also be *wrong*. Depending on what kind of critical decisions we
    are taking, we need to include more than the knowledge from the training data
    and reflect the proposed decisions by humans having world knowledge and experience.
    Additionally, as humans can be biased too, the processes and documentation around
    relevant decisions are crucial, also when being made by humans.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，考虑到我们之前看到的刻板印象和限制，我只能鼓励在其中加入*人类因素*。正如我们在本书的早期部分所看到的，机器学习模型，例如分类器，做出一个*估计*。这个猜测，在给定情景中似乎最有可能，也可能是*错误的*。根据我们正在做出的关键决策的性质，我们需要包含不仅来自训练数据的知识，还要反映人类提出的决策，这些人具有世界知识和经验。此外，由于人类也可能存在偏见，围绕相关决策的流程和文档也至关重要，即使这些决策是由人类做出的时候也是如此。
- en: Are all AI applications critical in the same way? Probably not. When considering
    a software to sort different types of screws into boxes, there are probably much
    less ethical and human-in-the-loop problems as compared to other applications
    in the legal or medical domain. Systems that call for regulation and monitoring
    are those making decisions about “people, resources that concern people, issues
    that affect people’s ability to participate in society” (Zweig [2022](#CR43),
    p. 8).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 所有AI应用程序在同样程度上都是关键的吗？可能不是。当考虑一个软件将不同类型的螺丝分类到盒子中时，与在法律或医疗领域的其他应用相比，可能存在更少的道德和人类因素问题。需要监管和监控的系统是那些做出关于“人、关系到人的资源、影响人们参与社会能力的问题”的决策的系统（Zweig
    [2022](#CR43)，第8页）。
- en: With all that being said, I want to explain you why I do not like the term *artificial
    intelligence*. As mentioned, the goal of the society should be to produce tools
    supporting us with our tasks, and not aiming to develop a fully humanlike general
    artificial intelligence. The term *artificial intelligence*, even though established
    nowadays and widely used, is therefore misleading.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 说了这么多，我想解释一下为什么我不喜欢术语*人工智能*。如前所述，社会的目标应该是生产支持我们任务的工具，而不是致力于开发完全类似人类的通用人工智能。即使如今已经确立并广泛使用，术语*人工智能*是具有误导性的。
- en: Therefore, the term *augmented intelligence*^([2](#Fn2)) is more and more used
    instead. It aims at the creation of technology that augments the human intelligence
    with smart tools, rather than replacing humans. This vision for the digital society
    of the future leaves the human in the control position and assesses the information
    provided by the software systems. As seen before, this is particularly relevant
    for critical use cases, such as decisions about humans.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，越来越多地使用术语*增强智能*^([2](#Fn2))。它旨在利用智能工具增强人类智慧，而不是取代人类。这种对未来数字社会的愿景将人类置于控制位置，并评估软件系统提供的信息。正如之前所见，这对于关键用例尤为重要，比如关于人类的决策。
- en: As shown in Fig. [7.1](#Fig1), augmented intelligence aims to empower human
    decision-making, providing additional information and insights rather than replacing
    the human in the loop by a software.![](../images/604345_1_En_7_Chapter/604345_1_En_7_Fig1_HTML.png)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[7.1](#Fig1)所示，增强智能旨在赋予人类决策能力，提供额外信息和见解，而不是通过软件取代人类在决策过程中的作用。![](../images/604345_1_En_7_Chapter/604345_1_En_7_Fig1_HTML.png)
- en: A diagram illustrates the augmented intelligence model, which empowers humans
    with technology by assisting them in the decision-making process.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一张图解说明了增强智能模型，通过协助人类进行决策过程来赋予他们技术力量。
- en: Fig. 7.1
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1
- en: 'Augmented intelligence: empowering humans rather than replacing them'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 增强智能：赋予人类力量而不是取代他们
- en: The Future of Education
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 教育的未来
- en: Let’s now discuss what these technologies mean for education. Given the new
    possibilities of text generation with models like ChatGPT, several challenges
    arise for the way we were, until a few months ago quite successfully, teaching
    and assessing students at different levels. By prompting ChatGPT (or other similar
    tools), it is now in many cases possible to solve programming exercises or write
    essays just by pasting the task description as a prompt. In other domains, it
    was reported that ChatGPT successfully passed exams that require domain-specific
    knowledge, as for example in medicine. What does this mean for our education system?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论这些技术对教育意味着什么。鉴于像ChatGPT这样的模型带来的文本生成新可能性，我们在教学和评估学生方面遇到了几个挑战。通过提示ChatGPT（或其他类似工具），现在在许多情况下可以通过将任务描述粘贴为提示来解决编程练习或写作文。在其他领域，有报道称ChatGPT成功通过需要领域特定知识的考试，比如医学。这对我们的教育体系意味着什么？
- en: We have to review the way we teach and what the skills are that need to be developed
    by students but also for teachers or the broad public. The main challenge here
    is that technology is evolving fast, and to reflect the way we teach is taking
    more time. Rather than banning these tools, I suggest teaching our students how
    to use them responsibly and, in particular, what are their limitations and pitfalls.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须审视我们的教学方式以及学生需要发展的技能，同时也要考虑教师或广大公众需要发展的技能。主要挑战在于技术发展迅速，而反映我们的教学方式需要更多时间。我建议的做法不是禁止这些工具，而是教导我们的学生如何负责任地使用它们，特别是了解它们的局限性和陷阱。
- en: We certainly have to adapt and review certain types of exams and some of the
    contents and skills we are aiming to teach and maybe have more direct interactions
    in the assessment rather than just an essay submitted at the end of the semester.
    But the new technologies offer much more than that. New generative AI tools also
    enable new great opportunities in the field of education, for example, to generate
    new use cases to discuss and critically reflect with students and to provide them
    with smart learning environments with individual feedback and suggestions. Human
    creativity along with the latest technology will shape the education of the future.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当然必须调整和审查某些类型的考试以及我们旨在教授的一些内容和技能，也许在评估中有更直接的互动，而不仅仅是在学期末提交一篇论文。但新技术提供的远不止于此。新的生成式人工智能工具还为教育领域提供了许多新的机会，例如，生成新的使用案例与学生讨论并进行批判性反思，并为他们提供智能学习环境，提供个性化的反馈和建议。人类创造力与最新技术将塑造未来的教育。
- en: Apart impacting the way we work, learn, and teach, language models can provide
    also many interesting new questions for other fields of research, to be unfolded
    over the coming years. As the mathematician Stephen Wolfram is suggesting in his
    recent book about ChatGPT, “human language (and the patterns of thinking behind
    it) are somehow simpler and more *law like* in their structure than we thought”
    (Wolfram [2023](#CR42), p. 108). Maybe those new technologies can finally help
    us in some way or the other to better understand ourselves.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 影响我们工作、学习和教学方式之外，语言模型还可以为其他研究领域提供许多有趣的新问题，这些问题将在未来几年逐渐展开。正如数学家斯蒂芬·沃尔夫勒姆在他最近关于ChatGPT的书中所建议的，“人类语言（以及其背后的思维模式）在结构上可能比我们想象的更简单、更*法则化*”（沃尔夫勒姆
    [2023](#CR42)，第108页）。也许这些新技术最终可以在某种程度上帮助我们更好地了解自己。
- en: 'Conclusion: Shaping the Future'
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论：塑造未来
- en: We have seen in this book what is behind the scenes of state-of-the-art language
    models and in particular chatbots. Even though we have seen that from the inside
    language models are less magical than one might have thought in the beginning,
    their results are still impressive. By providing responses that might most likely
    look like good human answers to the proposed prompts, they give us the impression
    of eloquent conversation partners. These pitfalls and limitations need to be considered,
    yet there are nearly endless new opportunities that these technologies bring along
    for our society. It is what we make from it. And with *we*, I mean all of us.
    I want to encourage a public discourse about technology, involving technical experts
    as well as people from other domains. It is in our hands.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书中看到了最先进的语言模型以及特别是聊天机器人背后的秘密。尽管我们从内部看到语言模型并不像一开始想象的那么神奇，但它们的结果仍然令人印象深刻。通过提供回应，这些回应可能看起来最像对所提出提示的良好人类回答，它们给我们留下了与有口才的对话伙伴交谈的印象。需要考虑这些陷阱和限制，但这些技术带来的新机会几乎是无穷无尽的，这是我们从中获得的。而*我们*，我指的是我们所有人。我希望鼓励关于技术的公共讨论，涉及技术专家以及其他领域的人。这取决于我们。
