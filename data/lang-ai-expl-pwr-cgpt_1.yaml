- en: '| ![image](d2d_images/chapter_title_corner_decoration_left.png) |  | ![image](d2d_images/chapter_title_corner_decoration_right.png)
    |'
  id: totrans-0
  prefs: []
  type: TYPE_TB
  zh: '| ![image](d2d_images/chapter_title_corner_decoration_left.png) |  | ![image](d2d_images/chapter_title_corner_decoration_right.png)
    |'
- en: '![image](d2d_images/chapter_title_above.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![image](d2d_images/chapter_title_above.png)'
- en: 'Chapter 2: How ChatGPT Works'
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章：ChatGPT的工作原理
- en: '![image](d2d_images/chapter_title_below.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![image](d2d_images/chapter_title_below.png)'
- en: ChatGPT is an advanced artificial intelligence language model that can generate
    human-like text in response to user input. In this chapter, we will study the
    technical aspects of the ChatGPT architecture, the training process, and the inference
    algorithm.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT是一种先进的人工智能语言模型，可以根据用户输入生成类似人类的文本响应。在本章中，我们将研究ChatGPT架构、训练过程和推理算法的技术方面。
- en: Architecture
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 架构
- en: ChatGPT uses the Transformer architecture, a type of neural network that uses
    an attention mechanism to allow the model to focus on a different part of the
    input sequence while generating the output sequence. This makes it especially
    suitable for language modeling tasks where output (text) sequences can vary in
    length and complexity.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT使用Transformer架构，这是一种使用注意机制的神经网络类型，允许模型在生成输出序列时专注于输入序列的不同部分。这使其特别适用于输出（文本）序列长度和复杂度可能不同的语言建模任务。
- en: With ChatGPT, the input sequence is the user's commands or messages, and the
    output sequence is the response generated by the model. The model is pre-trained
    on a broad corpus of conversational data, enabling it to understand and generate
    natural language responses in conversational contexts.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ChatGPT，输入序列是用户的命令或消息，输出序列是模型生成的响应。模型在广泛的对话数据语料库上进行预训练，使其能够理解和生成对话环境中的自然语言响应。
- en: Training
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 训练
- en: 'ChatGPT is pre-trained using unattended training, which means the model is
    trained on a large corpus of text data without the need for human-annotated labels.
    Pre-training consists of two main phases: pre-training and refinement.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT使用无监督训练进行预训练，这意味着模型在大量文本数据语料库上进行训练，无需人工标注的标签。预训练包括两个主要阶段：预训练和细化。
- en: In the pre-training phase, the model is trained on a large set of text data
    using a language modeling objective. The goal of a language modeling task is to
    predict the next word in the order given by the previous word. This process allows
    the model to learn natural language patterns and structures.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练阶段，模型使用语言建模目标在大量文本数据上进行训练。语言建模任务的目标是根据前一个单词给出的顺序预测下一个单词。这个过程使模型能够学习自然语言的模式和结构。
- en: In the fine-tuning phase, the pre-trained model is further trained on a smaller
    corpus of speech data to improve its ability to generate responses in a conversational
    environment. Model parameters are tailored to specific tasks, such as answering
    questions or engaging in small talk.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调阶段，预训练模型在较小的语音数据语料库上进一步训练，以提高其在对话环境中生成响应的能力。模型参数针对特定任务进行调整，例如回答问题或进行闲聊。
- en: Diploma
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 文凭
- en: When a user enters a text command, ChatGPT uses a pre-trained model to parse
    the command and generate a response. The derivation process involves several steps,
    including tokenization, embedding, and decoding.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户输入文本命令时，ChatGPT使用预训练模型解析命令并生成响应。推导过程涉及几个步骤，包括标记化、嵌入和解码。
- en: In tokenization, the input sequence is broken down into individual words or
    tokens, which are then converted into a numeric representation using an embedding
    layer. The embedding layer maps each word to a high-dimensional vector that captures
    its meaning and context in the input sequence.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在标记化中，输入序列被分解为单个单词或标记，然后使用嵌入层将其转换为数值表示。嵌入层将每个单词映射到一个高维向量，捕捉其在输入序列中的含义和上下文。
- en: Decoding involves generating an output (response) sequence using the Transformer
    architecture and attention mechanism. The model generates a sequence of word-by-word
    outputs, with each word generated based on the context of the previous word and
    attentional mechanisms.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 解码涉及使用Transformer架构和注意力机制生成输出（响应）序列。模型生成逐词输出序列，每个单词基于前一个单词的上下文和注意机制生成。
- en: Diploma
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 文凭
- en: ChatGPT is a highly sophisticated language model that uses deep learning techniques
    to generate human text in response to user input. His unsupervised, learning-based
    pre-learning process allows him to learn natural language patterns and structures,
    while his refinement phase allows him to specialize in human conversational interaction.
    By using the architectural models and attention mechanisms in Transformer, it
    is very effective for language modeling tasks because it can produce coherent
    and contextual responses. In future chapters, we will explore the capabilities
    of ChatGPT in greater detail and discuss the broader implications of using this
    advanced language model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT是一个高度复杂的语言模型，它使用深度学习技术根据用户输入生成人类文本。他的无监督学习预训练过程使他能够学习自然语言的模式和结构，而他的精炼阶段使他能够专注于人类对话互动。通过使用Transformer中的架构模型和注意机制，它对语言建模任务非常有效，因为它可以生成连贯和上下文相关的回应。在未来的章节中，我们将更详细地探讨ChatGPT的能力，并讨论使用这种先进语言模型的更广泛影响。
