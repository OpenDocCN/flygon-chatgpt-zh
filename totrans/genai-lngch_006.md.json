["```\nfrom langchain.embeddings.openai import OpenAIEmbeddings  \nembeddings = OpenAIEmbeddings()  \ntext = \"This is a sample query.\"  \nquery_result = embeddings.embed_query(text)  \nprint(query_result)\nprint(len(query_result))\n```", "```\nfrom langchain.embeddings.openai import OpenAIEmbeddings  \nwords = [\"cat\", \"dog\", \"computer\", \"animal\"]\nembeddings = OpenAIEmbeddings()\ndoc_vectors = embeddings.embed_documents(words)\n```", "```\nfrom scipy.spatial.distance import pdist, squareform\nimport pandas as pd\nX = np.array(doc_vectors)\ndists = squareform(pdist(X))\n```", "```\nimport pandas as pd\ndf = pd.DataFrame(\n    data=dists,\n    index=words,\n    columns=words\n)\ndf.style.background_gradient(cmap='coolwarm')\n```", "```\nfrom langchain.vectorstores import Chroma  \nfrom langchain.embeddings import OpenAIEmbeddings\n```", "```\nvectorstore = Chroma.from_documents(documents=docs, embedding=OpenAIEmbeddings())\n```", "```\nvector_store = Chroma()\n# Add vectors to the vector store:\nvector_store.add_vectors(vectors)\n```", "```\nsimilar_vectors = vector_store.query(query_vector, k)\n```", "```\npinecone.init()\n```", "```\nDocsearch = Pinecone.from_texts([\u201cdog\u201d, \u201ccat\u201d], embeddings)\n```", "```\ndocs = docsearch.similarity_search(\u201cterrier\u201d, include_metadata=True)\n```", "```\nfrom langchain.document_loaders import TextLoader\nloader = TextLoader(file_path=\"path/to/file.txt\")\ndocuments = loader.load()\n```", "```\nfrom langchain.document_loaders import WikipediaLoader\nloader = WikipediaLoader(\"LangChain\")\ndocuments = loader.load()\n```", "```\nfrom langchain.retrievers import KNNRetriever  \nfrom langchain.embeddings import OpenAIEmbeddings  \nwords = [\"cat\", \"dog\", \"computer\", \"animal\"]\nretriever = KNNRetriever.from_texts(words, OpenAIEmbeddings())\n```", "```\nresult = retriever.get_relevant_documents(\"dog\")  \nprint(result)\n```", "```\n[Document(page_content='dog', metadata={}),\n Document(page_content='animal', metadata={}),\n Document(page_content='cat', metadata={}),\n Document(page_content='computer', metadata={})]\n```", "```\nfrom langchain.retrievers import PubMedRetriever  \nretriever = PubMedRetriever()  \ndocuments = retriever.get_relevant_documents(\"COVID\")\nfor document in documents:\n    print(document.metadata[\"title\"])\n```", "```\nThe COVID-19 pandemic highlights the need for a psychological support in systemic sclerosis patients.\nHost genetic polymorphisms involved in long-term symptoms of COVID-19.\nAssociation Between COVID-19 Vaccination and Mortality after Major Operations.\n```", "```\nfrom langchain.retriever import BaseRetriever  \nfrom langchain.schema import Document  \nclass MyRetriever(BaseRetriever):  \n    def get_relevant_documents(self, query: str) -> List[Document]:  \n        # Implement your retrieval logic here  \n        # Retrieve and process documents based on the query  \n        # Return a list of relevant documents  \n        relevant_documents = []  \n        # Your retrieval logic goes here\u2026  \n        return relevant_documents\n```", "```\nfrom typing import Any\nfrom langchain.document_loaders import (\n  PyPDFLoader, TextLoader, \n  UnstructuredWordDocumentLoader, \n  UnstructuredEPubLoader\n)\nclass EpubReader(UnstructuredEPubLoader):\n    def __init__(self, file_path: str | list[str], ** kwargs: Any):\n        super().__init__(file_path, **kwargs, mode=\"elements\", strategy=\"fast\")\nclass DocumentLoaderException(Exception):\n    pass\nclass DocumentLoader(object):\n    \"\"\"Loads in a document with a supported extension.\"\"\"\n    supported_extentions = {\n        \".pdf\": PyPDFLoader,\n        \".txt\": TextLoader,\n        \".epub\": EpubReader,\n        \".docx\": UnstructuredWordDocumentLoader,\n        \".doc\": UnstructuredWordDocumentLoader\n    }\n```", "```\nimport logging\nimport pathlib\nfrom langchain.schema import Document\ndef load_document(temp_filepath: str) -> list[Document]:\n    \"\"\"Load a file and return it as a list of documents.\"\"\"\n    ext = pathlib.Path(temp_filepath).suffix\n    loader = DocumentLoader.supported_extentions.get(ext)\n    if not loader:\n        raise DocumentLoaderException(\n            f\"Invalid extension type {ext}, cannot load this type of file\"\n        )\n    loader = loader(temp_filepath)\n    docs = loader.load()\n    logging.info(docs)\n    return docs\n```", "```\nfrom langchain.embeddings import HuggingFaceEmbeddings \nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import DocArrayInMemorySearch\nfrom langchain.schema import Document, BaseRetriever\ndef configure_retriever(docs: list[Document]) -> BaseRetriever:\n    \"\"\"Retriever to use.\"\"\"\n    # Split each document documents:\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n    splits = text_splitter.split_documents(docs)\n    # Create embeddings and store in vectordb:\n    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n    # Single call to the huggingface model with all texts:\n    vectordb = DocArrayInMemorySearch.from_documents(splits, embeddings)\n    # Define retriever:\n    return vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2, \"fetch_k\": 4})\n```", "```\nif not use_compression:\n    return retriever\nembeddings_filter = EmbeddingsFilter(\n  embeddings=embeddings, similarity_threshold=0.76\n)\nreturn ContextualCompressionRetriever(\n  base_compressor=embeddings_filter,\n  base_retriever=retriever\n)\n```", "```\nfrom langchain.retrievers.document_compressors import EmbeddingsFilter\nfrom langchain.retrievers import ContextualCompressionRetriever\n```", "```\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.chains.base import Chain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.memory import ConversationBufferMemory\ndef configure_chain(retriever: BaseRetriever) -> Chain:\n    \"\"\"Configure chain with a retriever.\"\"\"\n    # Setup memory for contextual conversation\n    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n    # Setup LLM and QA chain; set temperature low to keep hallucinations in check\n    llm = ChatOpenAI(\n        model_name=\"gpt-3.5-turbo\", temperature=0, streaming=True\n    )\n    # Passing in a max_tokens_limit amount automatically\n    # truncates the tokens when prompting your llm!\n    return ConversationalRetrievalChain.from_llm(\n        llm, retriever=retriever, memory=memory, verbose=True, max_tokens_limit=4000\n    )\n```", "```\nimport os\nimport tempfile\ndef configure_qa_chain(uploaded_files):\n    \"\"\"Read documents, configure retriever, and the chain.\"\"\"\n    docs = []\n    temp_dir = tempfile.TemporaryDirectory()\n    for file in uploaded_files:\n        temp_filepath = os.path.join(temp_dir.name, file.name)\n        with open(temp_filepath, \"wb\") as f:\n            f.write(file.getvalue())\n        docs.extend(load_document(temp_filepath))\n    retriever = configure_retriever(docs=docs)\n    return configure_chain(retriever=retriever)\n```", "```\nimport streamlit as st\nfrom langchain.callbacks import StreamlitCallbackHandler\nst.set_page_config(page_title=\"LangChain: Chat with Documents\", page_icon=\"\ud83e\udd9c\")\nst.title(\"\ud83e\udd9c LangChain: Chat with Documents\")\nuploaded_files = st.sidebar.file_uploader(\n    label=\"Upload files\",\n    type=list(DocumentLoader.supported_extentions.keys()),\n    accept_multiple_files=True\n)\nif not uploaded_files:\n    st.info(\"Please upload documents to continue.\")\n    st.stop()\nqa_chain = configure_qa_chain(uploaded_files)\nassistant = st.chat_message(\"assistant\")\nuser_query = st.chat_input(placeholder=\"Ask me anything!\")\nif user_query:\n    stream_handler = StreamlitCallbackHandler(assistant)\n    response = qa_chain.run(user_query, callbacks=[stream_handler])\n    st.markdown(response)\n```", "```\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chains import ConversationChain\n# Creating a conversation chain with memory\nmemory = ConversationBufferMemory()\nchain = ConversationChain(memory=memory)\n# User inputs a message\nuser_input = \"Hi, how are you?\"\n# Processing the user input in the conversation chain\nresponse = chain.predict(input=user_input)\n# Printing the response\nprint(response)\n# User inputs another message\nuser_input = \"What's the weather like today?\"\n# Processing the user input in the conversation chain\nresponse = chain.predict(input=user_input)\n# Printing the response\nprint(response)\n# Printing the conversation history stored in memory\nprint(memory.chat_memory.messages)\n```", "```\nconversation = ConversationChain(\n    llm=llm, \n    verbose=True, \n    memory=ConversationBufferMemory()\n)\n```", "```\nfrom langchain.memory import ConversationBufferWindowMemory\nmemory = ConversationBufferWindowMemory(k=1)\n```", "```\nmemory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\nmemory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"})\n```", "```\nfrom langchain.memory import ConversationKGMemory\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nmemory = ConversationKGMemory(llm=llm)\n```", "```\nfrom langchain.llms import OpenAI\nfrom langchain.chains import ConversationChain\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts.prompt import PromptTemplate\nllm = OpenAI(temperature=0)\n```", "```\ntemplate = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\nCurrent conversation:\n{history}\nHuman: {input}\nAI Assistant:\"\"\"\nPROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\nconversation = ConversationChain(\n    prompt=PROMPT,\n    llm=llm,\n    verbose=True,\n    memory=ConversationBufferMemory(ai_prefix=\"AI Assistant\"),\n)\n```", "```\nfrom langchain.memory import ConversationSummaryMemory\nfrom langchain.llms import OpenAI\n# Initialize the summary memory and the language model\nmemory = ConversationSummaryMemory(llm=OpenAI(temperature=0))\n# Save the context of an interaction\nmemory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n# Load the summarized memory\nmemory.load_memory_variables({})\n```", "```\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import ConversationChain\nfrom langchain.memory import ConversationBufferMemory, CombinedMemory, ConversationSummaryMemory\n# Initialize language model (with desired temperature parameter)\nllm = OpenAI(temperature=0)\n# Define Conversation Buffer Memory (for retaining all past messages)\nconv_memory = ConversationBufferMemory(memory_key=\"chat_history_lines\", input_key=\"input\")\n# Define Conversation Summary Memory (for summarizing conversation)\nsummary_memory = ConversationSummaryMemory(llm=llm, input_key=\"input\")\n# Combine both memory types\nmemory = CombinedMemory(memories=[conv_memory, summary_memory])\n# Define Prompt Template\n_DEFAULT_TEMPLATE = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\nSummary of conversation:\n{history}\nCurrent conversation:\n{chat_history_lines}\nHuman: {input}\nAI:\"\"\"\nPROMPT = PromptTemplate(input_variables=[\"history\", \"input\", \"chat_history_lines\"], template=_DEFAULT_TEMPLATE)\n# Initialize the Conversation Chain\nconversation = ConversationChain(llm=llm, verbose=True, memory=memory, prompt=PROMPT)\n# Start the conversation\nconversation.run(\"Hi!\")\n```", "```\nfrom langchain.memory import ZepMemory  \n# Set this to your Zep server URL  \nZEP_API_URL = \"http://localhost:8000\"  \nZEP_API_KEY = \"<your JWT token>\"  # optional  \nsession_id = str(uuid4())  # This is a unique identifier for the user  \n# Set up ZepMemory instance  \nmemory = ZepMemory(  \n    session_id=session_id,  \n    url=ZEP_API_URL,  \n    api_key=ZEP_API_KEY,  \n    memory_key=\"chat_history\",  \n)\n```", "```\nfrom langchain.chains import OpenAIModerationChain  \nmoderation_chain = OpenAIModerationChain()\n```", "```\nfrom langchain.chains import LLMChain  \nllm_chain = LLMChain(model_name=\"gpt-3.5-turbo\")\n```", "```\nfrom langchain.chains import SequentialChain  \nchain = SequentialChain([llm_chain, moderation_chain])\n```", "```\ninput_text = \"Can you generate a story for me?\"  \noutput = chain.generate(input_text)\n```"]