© 作者，独家许可给 Springer Nature Switzerland AG 2023M. Kurpicz-Briki不仅仅是一个聊天机器人[https://doi.org/10.1007/978-3-031-37690-0_3](https://doi.org/10.1007/978-3-031-37690-0_3)

# 3. 处理书面语言

Mascha Kurpicz-Briki^([1](#Aff2)  )(1)应用机器智能，瑞士比尔/宾讷伯恩应用科学大学

## 概述

从技术角度来看，人类语言长期以来一直吸引着研究人员的兴趣。自然语言处理的第一个应用是在1948年在伦敦的伯贝克学院进行的，这是一个字典查找系统（Hancox [1996](#CR20)）。虽然1948年在其他领域可能相对较近，但在计算机科学领域，这已经非常早了。从那时起，发生了很多事情。如今，自然语言处理技术在我们的日常生活中得到应用，有时是显性的（例如，与聊天机器人交互时），有时是在幕后（例如，在使用在线搜索时）。

在本章中，我们将更多地了解文本如何被自动处理。自然语言处理是指对语音和文本的自动处理（包括生成）。在本书中，我们将文本处理和自然语言处理这两个术语互换使用。我们将看一些常见的自然语言处理应用程序，我相信您在最近与这些系统互动时会认出其中一些。然后，我们将看一些来自自然语言处理领域的常见方法。接着，我们将加深我们的机器学习知识，介绍和理解深度学习和神经网络的优缺点。最后，我们将了解人类语言如何被表示为数学向量以及为什么这对机器学习有益。

## 自然语言处理应用

文本的自动处理在许多场景中都是有用的。受 Lane 等人（[2019](#CR25)）对自然语言处理应用的分类启发，我们一起探讨一些用例，揭示这些应用中潜在的潜力。我们中的许多人每天都在与这些技术无意识地互动。

### 搜索引擎和写作建议

网络和文档搜索应用程序在评估搜索查询（您在搜索框中输入的句子或单词）和可用文档（例如，已收集和索引的互联网上的现有网站）以及识别最佳匹配结果方面严重依赖自然语言处理技术。所有这些通常在一秒钟内完成，您可能并不知道背景中正在进行的极其高效的处理。此外，一些搜索引擎会提出对搜索查询的更正（*您是不是想要*：）或通过评估用户输入的搜索查询提供*自动完成*功能。

自然语言处理技术也可以在我们撰写文本时提供支持。许多文本处理程序会标出拼写错误或提出如何调整语法和风格的建议。这听起来熟悉吗？那么这是另一个用例，您在其中积极与我们在本书中探讨的自然语言处理技术类似的技术进行交互。

### 文本分类

让我们看另一个例子。您是否曾想过您电子邮件收件箱中的*垃圾邮件过滤器*是如何工作的？文本处理技术驱动着对传入电子邮件的分析，以及它们是否对您感兴趣或是垃圾邮件的决定。一些电子邮件提供商使用更先进的过滤，例如提供垃圾邮件、广告或社交媒体通知等类别。但是电子邮件的文本处理并不止于此。进一步的用例可能是按优先级自动排名或提出电子邮件答复，这些功能最近已被不同提供商引入。

但让我们现在继续讨论垃圾邮件检测。根据给定软件产品所使用的底层技术的复杂性，检测垃圾邮件的机制可能会有所不同，也会导致不同质量的结果。在最简单的情况下，可以基于预定义句子或发件人地址进行关键词分析。当然，这可能仍然会让大量垃圾邮件进入您的收件箱。因此，很可能（也希望）没有垃圾邮件检测系统仅依赖于预定义列表。我们还应该记住，垃圾邮件制作者在应对更好的过滤器时变得越来越好。自动文本生成器现在能够编写非常可信的文本，这些文本是专门为不同用途定制的。这是一场关于改进技术的武器竞赛，用于开发看起来非常逼真的垃圾邮件和检测垃圾邮件的技术，两者都使用自然语言处理中最新的机器学习驱动进展。

将电子邮件分类到不同类别是我们在自然语言处理领域称之为*文本分类任务*的一个例子。当使用机器学习方法时，这与我们在上一章中看到的将图像分类为草莓或覆盆子的过程非常相似。假设我们有大量垃圾邮件、广告邮件、社交媒体通知和其他邮件的示例。我们训练数据集中来自这四个类别的所有示例都被标记，这意味着它们被分配了一个*标签*，指示它们属于哪个类别。在训练阶段，机器然后识别区分这些文本组的特点。最后，模型可以将新的文本样本分类到这四个类别中的一个，如图[3.1](#Fig1)所示。![](../images/604345_1_En_3_Chapter/604345_1_En_3_Fig1_HTML.png)

一幅插图描述了使用机器学习模型将电子邮件分类为垃圾邮件或正常邮件的过程。该模型在训练数据上进行训练，并在决定电子邮件是否为垃圾邮件时区分广告和社交媒体类别。

图3.1

使用监督机器学习将电子邮件分类为不同类别

回顾前一章节，这些预测的类别是基于概率得出的最佳猜测，结果的质量取决于训练数据的完整性和质量。即使在最好的情况下，也可能出现错误。牢记这一点，下次收件箱中的电子邮件被错误分类时你也不会感到惊讶。

这种文本分类也可以用于其他用例。例如，可以对来自两个或更多不同作者的大量文本进行机器学习模型训练，从而获得一个能够识别文本作者的分类器。这种技术已被用于确定莎士比亚剧作《亨利八世》中可能由他人撰写的部分，这一问题长期以来一直受到学者们的争论（Plecháč [2021](#CR31)）。作者归属也在*抄袭检测*中发挥着重要作用，这种软件通常应用于科学出版的背景下，以及检查学生论文。该领域的其他工作探讨了如何利用机器学习来识别互联网上的仇恨言论或虚假新闻。

### 情感分析

文本分类也应用于*情感分析*领域。这些方法旨在从文本中提取表明情感或观点的信息，例如，一篇评论文本中作者对产品持有积极还是消极态度。同样，这个任务可以通过不同的方式解决，从非常简单的方法开始。如果我们发现表达如“质量差”，“设计可怕”，或“界面丑陋”，或者相反，“产品精彩”，“设计惊人”，“我喜欢它”等，我们可以很容易地了解评论作者对产品的喜好或厌恶。然而，情感可以以不可数的方式表达，人类的创造力几乎是无限的，因此当我们创建一个包含所有可能的积极和消极表达的词汇表时，我们很快就会达到极限。因此，另一种（除其他方法外）是使用监督机器学习的方法，就像我们讨论过的电子邮件分类的情况一样。基于积极文本和消极文本的示例，训练一个分类器，能够预测新的文本样本是积极还是消极。每当我们有一个问题，想要将文本分类为两个或更多类别，并且对于不同类别有足够多的示例可用时，我们可以利用这种监督学习方法。

一般来说，在文本分类任务的背景下，当我们有两个可能的组别需要区分时，我们称之为*二元分类*，例如，正面和负面文本，否则称为*多类分类*。多类情感分类的一个常见例子是将评论文本映射到它们的1到5星评级，通常在用户提供反馈的网站上看到，例如电影或餐厅。我们会尝试使用机器学习来预测星级评分（即标签/*正确答案*），基于用户撰写的评论文本。这将是一个包含五个类别的多类分类。

### 文本生成

与文本分类任务相反，模型将文本作为输入并预测指定类别之一作为输出，其他任务将文本作为输入并且输出也是文本。我们称这些任务为*文本生成*。这种任务的一个例子是*文本摘要*。当处理数百页的文本时，通常，内容也可以在几页甚至更少的表面层次上进行聚合。语言模型可以自动支持这项任务。*聊天机器人*是文本生成的另一个应用。基于文本作为输入，它们提供类似于人类的文本回答。聊天机器人可以有不同的目标，例如回答客户查询并提供客户正在寻找的信息。在这里，信息的正确性非常重要。正如我们最近看到的ChatGPT的崛起一样，语言模型和特别是聊天机器人产生的内容的正确性并不总是确定的（这自然可能会让某些读者感到惊讶）。我们将在后面的章节中进一步探讨这个问题。目前，我想指出一些聊天机器人的目的也可以是提供尽可能接近人类选择的措辞的最合理文本。文本生成的其他应用包括机器学习模型自动生成电影剧本、歌词或诗歌，文本生成的另一个重要用例是*机器翻译*，其中文本从一种语言翻译为另一种语言，例如从英语到德语。文本生成也可以是非文本输入的结果，例如自动生成图像标题。

### 信息提取

我们可以在文本上进行的另一个任务是*信息提取*。我们希望基于文本获取结构化信息，而这种信息通常被认为是*非结构化数据*（这也适用于图像）。非结构化是因为数据没有预定义的组织原则，与按行和列标识结构化的值表相对。在书面语言的上下文中，术语*非结构化*可能有点令人困惑。在处理给定语言的文本时，当然存在外部约束，比如语言的语法可能会在一定程度上限制句子中单词或对象的顺序。然而，仍然存在灵活性，并非所有句子或文本具有相同的结构。因此，文本有时也被称为*半结构化*数据，可以通过附加信息进行扩展，例如标识句子的主语或为单词类型添加标记（例如，将*to be*标记为动词）。

原始文本数据本身并不等同于以表格形式排列的数据，其中列对应于主语、动词和宾语。我们可以从文本中提取这样的结构化数据，但从结构化表格中我们无法仅仅恢复原始文本。

在信息提取过程中，我们希望从文本中获取非常具体的数据。因此，我们定义了一个数据结构，意味着我们描述了我们希望从计划处理的所有不同文本中获取的结构化数据。这个数据结构可能会因应用程序的用例而有很大不同。例如，在处理法律文本时，提取文本中提到的段落和法律可能是感兴趣的。在医生的笔记中，我们可能希望从文本中提取诊断，并最好将其转换为标准化格式，例如诊断代码。

让我们通过一个例子来看这个问题。图[3.2](#Fig2)的左侧包含非结构化文本。我们希望从文本中提取两件事（数据结构准备在右侧的表格中）：

+   提到了哪些种类的浆果

+   是否提到了人物（例如，通过使用他们的名字）

![](../images/604345_1_En_3_Chapter/604345_1_En_3_Fig2_HTML.png)

一幅插图展示了将非结构化文本转化为结构化数据的过程，其中突出显示了每个文本中提到的浆果和人物，文本1提到草莓、覆盆子和安娜，而文本2提到蓝莓和安娜。

图 3.2

从非结构化文本中提取结构化信息

信息提取的目的是提供关于文本的结构化信息，这些信息可以进一步由人类或另一个软件组件处理。例如，生成的数据表格可以轻松地用于快速获取所有提到安娜或包含有关草莓信息的文本。

### 从应用到方法

在本节中我们介绍的用例示例并不是自然语言处理的所有可能应用的完整列表。所选用例旨在让您感受到这些技术的强大之处，广泛的用途范围，以及它们如何潜在地支持我们的日常工作和生活。我还希望您从本节中了解到，根据问题的复杂性和预期结果，可以应用非常不同的技术。这可以从简单方法到更复杂的方法，如深度学习和大型语言模型。数据工程师的艺术和技巧之一是在当今工具包和研究成果中提供的广泛可能性中为给定应用程序识别正确的方法。在接下来的章节中，我们将更仔细地研究其中一些技术。

## 自然语言处理方法简介

尽管自然语言处理中的最新发展严重依赖于机器学习，但一般情况下并非总是如此。一个简单的文本处理任务可能是根据程序提供的单词列表识别文本中出现的浆果。在这种情况下，不需要训练数据或训练阶段，而是采用类似于之前看到的传统算法。我们将逐步处理给定的句子，将每个单词与列表中的单词进行比较。这些简单方法有一些局限性，例如，我们必须在列表中添加*草莓*和复数形式*草莓*以确保全部获取。图[3.3](#Fig3)给出了这种情况的一个示例！[](../images/604345_1_En_3_Chapter/604345_1_En_3_Fig3_HTML.png)

给定句子中关键词的插图，句子为太阳照耀着，天气很好。一个大田地里种满了草莓，然而在这个地区没有种植覆盆子。关键词列表包括草莓、覆盆子和蓝莓。

图 3.3

保持简单：根据关键词列表在文本中识别特定单词

从计算机的角度来看，文本只是一串字母和符号。我们称将额外结构添加到这些字符串的过程为*parsing*。当人类看文本时，我们当然看到更多的结构 - 比如分开的单词或语法结构。问题在于计算机更喜欢处理数字和进行数学运算，并且通常没有我们在头脑中自动和立即构建这样的文本所需的多年训练。在解析文本时，我们指示计算机，例如进行单词或句子的*tokenization*：我们分隔单词和句子，指定在每个空格后，一个新单词开始。或者在句号后，一个新句子开始。我们通过用编程语言编写指令来指示计算机这样做。

更高级的文本标记可以通过使用*词性标注*、*依存句法分析*或*命名实体识别*来完成。

### 词性标注

词性标注是指用*标签*注释每个单词的类型的过程。例如，strawberry 是一个*名词*。

正如你所想象的那样，这些是需要在不同应用程序中一遍又一遍地完成的常规任务。数据工程师有现有的软件组件（称为库）可供重复使用，而不是每次都重新发明轮子。

在图[3.4](#Fig4)中，我们看到一个例子，说明了这些信息是如何由一个名为 SpaCy 的库自动提供的^([1](#Fn1))，这个库通常用于自然语言处理。![](../images/604345_1_En_3_Chapter/604345_1_En_3_Fig4_HTML.png)

一个文本 read anna is eating the strawberry. 这个句子在词性标注中被分为代词、动词和名词。

图3.4

词性标注的一个例子：提取有关单词类型的信息。例如，“.” 被视为标点符号（PUNCT），“eating” 被分类为动词，而名字“Anna” 是专有名词（PROPN）。

### 依存句法分析

现在让我们看一个依存句法分析的例子。依存句法分析提供了关于句子中单词如何相互关联的信息。我们中的一些人，特别是那些具有更多技术背景的人，在处理这种类型的分析时可能需要查阅我们的语法书。

为了让你有一些直觉，我们将看一段使用编程语言 Python 和库 SpaCy 的编程代码的简短片段，如图[3.5](#Fig5)所示。![](../images/604345_1_En_3_Chapter/604345_1_En_3_Fig5_HTML.png)

该代码利用 s p a C y 处理文本 there was a large strawberry field，并使用 dep 样式可视化其依存解析树。

图3.5

使用 SpaCy 库可视化依存句法分析的代码片段

第一行加载了这个现有库中英语语言的模型。处理另一种语言时会有所不同。例如，为了考虑语法概念来解析我们的文本，我们需要有给定语言的具体信息。在第二行，我们定义了要解析的句子。最后一行启动了依赖解析并启用了可视化。如果你不理解上述代码片段中每个单词的含义，不要担心；主要目标是理解这些工作原理。

基于这三行编程代码，生成了关于我们句子的图示，如图 [3.6](#Fig6) 所示。![](../images/604345_1_En_3_Chapter/604345_1_En_3_Fig6_HTML.png)

一幅插图展示了对句子“there was a large strawberry field”进行句法分析，使用依赖标签，单词按其词性分类，连接由属性如 a t t r、d e t、a m o d、e x p l 和 compound 定义。

图 3.6

使用 SpaCy 库可视化一个示例句子的依赖解析

在这幅插图中我们有两种不同的信息：一方面，我们可以看到底部的单词类型，我们之前称之为词性标签（*was* 是动词，*large* 是形容词）。箭头指示了句子中单词之间的关系，句法依赖。例如，单词 *strawberry* 和 *field* 被识别为复合词。单词 *large* 被标记为形容词修饰语（*amod*）到单词 *field*（我们在这一点上不会深入讨论语法，但你明白了）。

### 命名实体识别

现在我们将考虑一个 *命名实体识别* 任务的示例，这涉及从文本中提取相关信息，例如通过将单词分类为组织、地缘政治实体或数字。

要查看这个示例，我们使用了与之前类似的代码示例，但是我们改变了句子和最后一行的一个单词（将简单的 *dep* 替换为 *ent*，如 *dependency* 替换为 *entity*），如图 [3.7](#Fig7) 所示。![](../images/604345_1_En_3_Chapter/604345_1_En_3_Fig7_HTML.png)

一段代码加载了 SpaCy 的英文模型，处理了给定的文本，然后使用实体样式可视化了命名实体。

图 3.7

用 SpaCy 库可视化命名实体识别的代码片段

在我们的句子中已经识别出了不同的命名实体，如图[3.8](#Fig8)所示：2023年被识别为日期，欧盟被识别为组织（ORG代表公司、机构、机构），美国被识别为地缘政治实体（GPE），*数千*被分类为数字（CARDINAL）。还有一些更多的类别（比如MONEY代表货币值），但这些是最常见的。在我们的文本中自动识别这样的信息可以作为更高级文本处理的入口点。我们注意到，在这种情况下，欧盟可能被视为地缘政治实体而不是组织。这提醒我们这样的模型可能在所有情况下都不完美地工作。![](../images/604345_1_En_3_Chapter/604345_1_En_3_Fig8_HTML.png)

一段文本中提到，在2023年，欧盟和美国等国家和地区吃掉了数千个草莓。

图3.8

使用SpaCy库进行命名实体识别示例的结果

## 机器学习内部

在上一章中，我们看了机器学习的基本过程。我们当时没有完全回答的有趣问题是学习实际上是如何发生的。^([2](#Fn2)) 我们将首先定义经典机器学习，并看看这与深度学习和神经网络有何不同。听起来困难吗？别担心，本部分所需的所有数学背景将逐步介绍。所以，让我们一步一步地进行。

在这一部分，我们将集中讨论监督机器学习。我们以情感分析任务为例。这意味着我们考虑一组积极的文本，即表达作者积极态度的文本，以及另一组消极的文本。正如你可能记得的那样，*监督* 意味着每个文本片段都标有关于它是积极还是消极的信息。这是一个分类任务，因为我们希望训练一个能够将新的、未见过的文本分类为*积极*或*消极*的模型。图[3.9](#Fig9)显示了我们的机器学习分类器的设置，就像我们之前在其他示例中看到的那样。![](../images/604345_1_En_3_Chapter/604345_1_En_3_Fig9_HTML.png)

一幅插图代表了一个简单的二元分类任务的训练数据场景，其中数据样本被标记为积极或消极。

图3.9

情感分析：使用监督机器学习将文本分类为积极或消极

现在，这里有趣的事情是在训练阶段发生了什么。模型是如何创建的，然后能够对未见过的文本进行分类的呢？

### 预处理训练数据

在图[3.9](#Fig9)中显示的训练数据可以以表格形式结构化，第一行包含文本样本，第二行包含标签，如表[3.1](#Tab1)所示。表3.1

训练数据可以以表格形式结构化，其中一列是文本，另一列是标签

| 文本 | 标签 |
| --- | --- |
| 哇，这是一个多么酷的��站！ | 正面 |
| 哦，我真的觉得这很糟糕。 | 负面 |
| … |   |

在我们开始训练之前，需要进行文本*预处理*。这意味着我们必须清理我们考虑作为输入的文本（即训练数据）。在我们进行情感分析的情况下，这些是包含正面或负面文本的文本样本。我们希望排除不相关的信息。在这个阶段需要做什么取决于我们有什么样的文本。例如，一个典型的操作是删除额外信息，比如链接或将所有字母转换为小写并删除标点符号：

+   “哇，这真的是一个很酷的网站！[http://​www.​springernature.​com](http://www.springernature.com)”

在这个阶段之后，可能会被调整为以下形式：

+   “哇，这真是一个很酷的网站”

我们还将句子拆分为可以单独处理的单词（所谓的标记化）：

+   “哇，这真是一个很酷的网站”

变成：

+   [“哇”,”这”,”是”,”真的”,”一个”,”酷”,”网站”]

在某些情况下，我们可能会进一步删除被认为对训练无关紧要的单词，即所谓的停用词。Lane等人（[2019](#CR25)，第51页）将停用词定义为“任何语言中频繁出现但携带较少实质信息的常见词语。”常见的停用词包括*the*、*a*或*on*。

有时，使用*词干提取*或*词形还原*也是有用的。这两种方法的目标相同：通过这些方法修改单词，使它们以一种形式出现，可以确定两个单词是否属于同一个词，即它们是不同形式的同一个词。词干提取是一种通过截取单词的部分来识别词干的技术。例如，*houses*的词干是*house*，*runs*的词干是*run*。然而，这种方法存在一些局限性，通常过于简化，我们可能想要区分的单词被分配到相同的词干。例如，单词*meetings*被改为*meet*，但也许*meeting*会是更好的词干（Hagiwara [2021](#CR19)）。此外，这种方法通常无法处理不规则的单词，比如*caught*，我们可能更希望得到*catch*而不是词干。另一方面，通过词形还原，与使用词干不同，识别单词的原始形式。这可能看起来与词干提取类似，但实际上有所不同：这里不仅仅是截取单词的部分，而是考虑语言的结构，例如对于动词，是在变位前的基本形式。对于我们的例子*meetings*和*caught*，词形还原后的形式分别为*meeting*和*catch*。

### 特征选择

我们将*特征*定义为数据的特征或可测量属性。机器学习中一个重要且常常棘手的部分是*特征选择*，决定哪些是适当的特征以及它们对于训练在其他未见数据上表现最佳的分类器有多重要。在人类决策中，根据上下文，某些方面比其他方面更相关。在对患者进行诊断时，血压或症状描述可能比患者穿的T恤的颜色更相关。因此，血压可能是机器学习训练的一个好特征，而T恤的颜色可能不是最佳选择。

在文本处理的情况下，特征选择可以是文本中的所有单词。在其他情况下，一个好的特征选择可能是考虑最常出现的单词，或者仅考虑形容词，具体取决于用例。假设我们想要根据*词频*进行特征选择。因此，我们需要找出在我们的训练数据中出现最频繁的单词。更具体地说，我们想找出对正面文本和负面文本都*典型*的单词。一些在一般情况下经常出现的单词可能会出现在两组中的前列，例如，像*the*或*a*这样的单词（如果我们之前已经去除了停用词，这可能不是一个大问题）。我们可以通过删除同时出现在正面和负面组中的任何单词来解决这个问题。这样做的直觉是这样的单词对于区分这两组没有任何用处。假设为了这个例子，我们取每组中出现频率最高的三个单词，最终得到以下结果：^([3](#Fn3))

+   积极 = [“好”, “快乐”, “棒极了”]

+   负面 = [“丑陋”, “可怕”, “糟糕”]

### 从文字到数字

正如前面提到的，计算机在处理数字方面比处理人类语言更擅长。那么，对于那些单词该怎么办？

在深入探讨这个问题之前，让我们先看看数学中*向量*的概念。一般来说，我们每天都在使用数字。向量包含的信息比单个数字更多。它们由多个数字的列表组成，其长度被称为向量的维度。因此，例如，向量(0, −2, 5.1)是一个三维向量。对于给定维度*n*，所有可能的n维向量的集合是所谓的向量空间的一个示例。向量通常被描绘为箭头，具有相同长度、相同方向且平行的箭头被视为相同的向量。

在二维向量空间中，向量有两个坐标来指定它们的位置。第一个坐标表示它们相对于x轴的位置，第二个坐标表示它们相对于y轴的位置，如图[3.10](#Fig10)所示。对于三维空间，我们可以想象一个具有三个坐标的立方体。从四维开始（向量空间可以具有非常高的维度），想象起来会变得困难，但工作方式类似：在一个具有20维度的向量空间中，生活在该空间中的向量有20个数字组成它们的坐标。![](../images/604345_1_En_3_Chapter/604345_1_En_3_Fig10_HTML.png)

一个插图描述了在二维中y轴与x轴的向量V向量，其中V向量=矩阵2, 1。

图3.10

一个具有两个维度的向量的示例

我们还需要知道，在这样的向量之间，可以执行数学运算，如加法或减法，类似于数字。

我们需要将我们的单词转换为向量，以便机器学习分类器可以处理它们。每个单词将被转换为一个单一的向量。这样做的最简单方法是使用*one-hot向量*。每个向量的维度等于我们词汇表中可能的单词数。在实际情况下，这些可能是数十万个可能的单词，但让我们保持简单，假设我们的语言具有以下由四个单词组成的词汇表：

+   [“awesome”, “bad”, “good”, “horrible”]

向量空间（维度）将是4。因此，每个向量有四个条目（比图[3.10](#Fig10)中的示例向量多两个）。然后，每个单词将通过具有*0*的向量表示，并且仅在词汇表中的位置上有*1*。我们所说的位置是什么意思？在上面的词汇表中，单词*awesome*位于位置1，*bad*位于位置2，依此类推。^([4](#Fn4)) 因此，*bad*的向量表示将在位置2处有一个1，其他位置为0：^([5](#Fn5))

+   vector[“bad”] = [0,1,0,0]

就是这样。我们已经创建了我们的单词的向量表示。显然，这种简单的方法存在一些限制。在四维空间中，一些零是可以原谅的，但如果我们有数十万维度的向量（以涵盖英语语言的全部词汇），那么就会有许多需要存储的零。我们将在后面的部分看到更高级的方法来使用更少的维度对单词进行向量化。

### 机器学习分类器

在*向量化*这一步之后，我们得到了一组典型于正面文本的向量列表，以及另一组典型于负面文本的向量列表。因此，我们准备训练我们的机器学习分类器。首先要做的是选择我们想要使用的分类器类型。存在不同的数学模型，我们根据数据和需求选择其中之一。通常情况下，最佳表现的分类器很难提前预测。因此，有时尝试不同的分类器以找到适合给定数据集或问题的正确分类器是非常有益的。

一种这样的机器学习分类器被称为*逻辑回归分类器*。这种类型的分类器是*判别式分类器*（Jurafsky和Martin [2023](#CR24)）的一种，旨在学会根据统计差异区分两组。评估不同特征在区分两组方面的重要性。这是通过分配权重来实现的，这意味着为每个特征分配一个值，衡量其对结果的重要性。这意味着它试图弄清楚我们提供的输入中哪些词是最相关的。这是通过逐个查看我们提供的所有特征并根据需要调整*权重*来实现的。

为了更好地理解我们所说的权重，让我们考虑以下例子。我们考虑一个长满美味草莓的草莓植物，大小不一。有一些水果，使得植物倾向于向右侧弯曲。我们想研究是什么导致草莓植物弯曲。可能，巨大的草莓对此的贡献比微小的草莓更大。因此，巨大的草莓比微小的草莓有更大的权重，如图[3.11](#Fig11)所示。![](../images/604345_1_En_3_Chapter/604345_1_En_3_Fig11_HTML.png)

一幅插图描绘了一个带有标有巨大草莓导致植物弯曲和微小草莓影响较小的盆栽。

图3.11

巨大的草莓导致植物弯曲；微小的草莓影响较小

这类似于我们特征的权重。在学习过程的开始，所有单词可能具有相等的权重。随着时间的推移，模型会学习到单词*good*表示积极，因此在积极类别中给予单词*good*更多的权重。因此，在训练之后，当单词*good*出现时，模型的决策将更多地向积极方向*倾斜*，类似于具有较大权重的草莓对草莓植物向右弯曲产生重大影响。

### 损失函数

让我们回到我们在早前章节中看到的将草莓切成均匀片的例子。随着每次迭代，我们都会变得更好一点。但是我们如何衡量我们的改进呢？假设期望的片大小是4毫米。我们希望我们切的每一片都尽可能接近这个参考值，因此我们希望能够熟练地将草莓切成这个特定大小的片。每次我们切一片，我们可以将其大小与期望的片大小进行比较。我们大了2毫米？让我们试着切小一点。这个过程在图[3.12](#Fig12)中展示。我们改进我们的方法，以尽可能接近期望的草莓片大小。在数学术语中，我们希望减少实际片大小和期望片大小之间的差异。在这种情况下，我们谈论的是片大小，但在机器学习的一般术语中，我们谈论的是*损失函数*。学习过程的目标是最小化这个损失函数。在图[3.12](#Fig12)中，第1轮开始时的损失为2毫米，在第N轮为0毫米。损失越接近零，机器学习模型的性能就越好。在学习过程中，机器学习模型的值（例如权重）会被调整以最小化这个损失。![](../images/604345_1_En_3_Chapter/604345_1_En_3_Fig12_HTML.png)

一幅插图代表了在多轮中进行的训练过程，最终通过减小实际片大小和期望片大小之间的差异来最小化损失函数，最终实现实际片大小为4毫米时的0毫米损失。

图 3.12

为了训练模型，我们需要最小化损失。我们越接近期望的片大小，损失就越低。

关于损失函数的一个重要观点是，它明确定义了我们*训练模型*要做什么。整个训练的目的是修改模型以最小化损失。这就是我们所说的*学习*。

现在让我们从切草莓片回到我们先前的情感分析任务。我们想要将文本分类为积极或消极。根据我们选择训练的分类器（例如，之前提到的逻辑回归），损失函数被定义。在最简单的情况下，它是预测值和期望值（训练数据的标签）之间的差异。在其他情况下，可以使用稍微复杂一些的函数，但通常以某种方式量化预测值和期望值之间的差异。

### 训练和测试数据

现在我们有一个模型，可以将文本分类为正面或负面两个类别之一。但是我们如何知道分类器实际表现如何？为了更好地理解这一点，我们引入了*测试数据*的概念。假设我们有100个文本样本标记为正面和100个标记为负面。最好的做法是只使用每个类别的80个样本进行训练过程。将20个正面和20个负面的例子保留下来，以便稍后在这些数据上*验证*我们的机器学习分类器。这意味着我们挑战分类器，让其预测这些片段是正面还是负面，而不包括它们在训练过程中。这就是我们所说的*未见数据*。由于我们知道这些样本的正确分类，我们可以利用这些知识来验证分类器的正确性。如果它能够正确分类40个中的38个（20个正面和20个负面），我们可能对结果感到满意。如果只有40个中的10个被正确分类，我们可能需要重新考虑之前采取的步骤，或者尝试其他特征或调整我们的预处理步骤。^([6](#Fn6))请记住，这只是一个预测，几乎不可能构建一个在所有情况下都正确的系统。

图[3.13](#Fig13)概述了我们一直在进行的文本预处理、特征选择以及最终训练和验证分类器的不同步骤。通常需要这些步骤来准备机器学习训练。![](../images/604345_1_En_3_Chapter/604345_1_En_3_Fig13_HTML.png)

流程图包括预处理、特征选择、从词到数字、机器学习分类器、损失函数以及训练和测试数据。

图3.13

准备机器学习训练所需的不同步骤概述

## 从机器学习到深度学习

在上一节中，我们看到了机器学习模型是如何训练的：特征是预先定义的，系统通过最小化损失函数来学习。让我们称之为*经典机器学习*。

### 线性模型的局限性

此外，逻辑回归和一些其他经典机器学习是所谓*线性模型*的例子。对于分类的情况，这意味着它们只能分离可以通过直线（或高维空间中的超平面）分离的类别。在其他情况下，这是不可能的，数据需要非线性处理才能产生良好的结果。这两个例子在图[3.14](#Fig14)中有所说明。![](../images/604345_1_En_3_Chapter/604345_1_En_3_Fig14_HTML.png)

2个比较图表显示数据可以用线性分类器分离和数据无法用线性分类器分离。

图3.14

在某些情况下，数据可以用线性分类器分离。在其他情况下，这是不可能的。

在我们知道要寻找什么并且能够识别和提取对于区分类别最有用的特征的情况下，经典机器学习分类器如逻辑回归可能会表现良好。一些其他经典机器学习分类器甚至允许处理非线性可分数据。然而，在许多情况下，可能不容易识别相关特征。例如，回想一下我们之前看到的想要区分草莓和覆盆子图片的分类器的情况。对于人眼来说，很容易看出图片上是覆盆子还是草莓，但很难形式化我们所看到的并定义机器学习算法能理解的具体特征。这里的潜在特征可能是，例如，浆果是否在外面有种子。然而，明确指导计算机如何从像素值转换为这些特征将会非常困难。将相同的推理应用于文本，人类可以看出文字之间的含义，并根据电子邮件消息了解某人的情绪。但是，如果要求你根据帮助你注意到这一点的特征给出清晰明确的指示，将很难用言语表达。你可能会说这与*语气*有关，但如何从文本中提取*语气*特征以输入到机器学习算法中呢？在这些任务中，很难从数据中识别/提取相关特征的地方，*深度学习*的力量就派上用场了。深度学习是机器学习的一个子集，涉及一种特定技术：*神经网络*。

### 神经网络

Masato Hagiwara将神经网络描述为：

> 一个将向量转换为另一个向量的通用数学模型。就是这样。与您在流行媒体中读到和听到的不同，它的本质很简单。（Hagiwara [2021](#CR19)，第37页）

我喜欢这个定义，尽管它有点揭示了魔法，因为它把事情说到了点上：最终，这只是数学。

与之前使用逻辑回归的示例一样，我们需要进行预处理，最终将我们的单词转换为向量。暂时，让我们假设我们已经有了文本中每个单词的向量形式的数值表示。

像之前看到的经典机器学习一样，神经网络也依赖于反馈循环来改进预测。非常简化地说，我们可以将神经网络看作是图[3.15](#Fig15)中所示结构的一种。![](../images/604345_1_En_3_Chapter/604345_1_En_3_Fig15_HTML.png)

流程图示输入、处理、估计，基于结果，调整处理以改进估计。

图3.15

与我们之前看到的经典机器学习类似，神经网络也有一个反馈机制来改进预测

一个神经网络可以被看作是一个更复杂的结构，在最后一部分包括一个经典的机器学习模型，类似于之前描述的模型，如图[3.16](#Fig16)所示。同样，在这里，学习是通过最小化损失来进行的。神经网络可以由多个层组成。第一层接收输入数据并输出一组新的特征，作为下一层的输入。在所有层中重复此过程后，最终一层提取出经典模型可以处理的特征。在训练过程中，每一层提取的特征都可以进行修改，直到找到*正确的处理步骤序列*。当神经网络涉及多个层时，我们称之为*深度学习*。根据神经网络的确切设置，更复杂的数学运算是可能的；此外，特征提取可以自动进行，通常比经典机器学习更高效。![](../images/604345_1_En_3_Chapter/604345_1_En_3_Fig16_HTML.png)

流程图表示了一个神经网络架构，最后一层的功能类似于传统的机器学习分类器，旨在最小化损失函数。

图3.16

神经网络一般类似于经典机器学习，但允许更复杂的数学运算

现在我们将深入探讨神经网络的架构，但仍然保持在一个相当高的层次。^([7](#Fn7))

### 神经元：受人脑启发

神经网络在某种程度上受到人脑中的神经元的启发。然而，我们必须明确，这样的系统不能直接与人脑相比较。它们非常简化，人脑的许多方面仍然不为人所理解。考虑以下类比（Rashid [2017](#CR32)）：人类大约有1000亿个神经元，而其他较小的动物或昆虫只有几十万个神经元。尽管我们能够建模比这些动物的大脑更复杂得多的计算机系统，但这些动物可以完成一些对计算机来说难以解决的有用任务。值得一提的是，最新的语言模型的神经元数量与人类大脑相同数量级。因此，仅仅根据神经元数量来比较人类或动物智能和机器智能是困难的。人类或动物智能似乎还有更多的东西。

回到我们的技术系统，神经网络是一组所谓的神经元相互连接在一起。信号从神经网络的一侧输入，经过不同的神经元，最终产生结果。

一个层通常由多个神经元组成。如图 [3.17](#Fig17) 所示，单个神经元具有多个输入值和分配给连接的权重。在神经元内部，输入被处理，权重指示了每个输入对于神经元输出的重要性。当思考权重的含义时，请记住我们之前看到的因为有一个巨大的草莓而向右弯曲的草莓植物。![](../images/604345_1_En_3_Chapter/604345_1_En_3_Fig17_HTML.png)

一个流程插图展示了多个权重输入，接着是神经元和输出。

图 3.17

神经网络中的权重在训练阶段逐渐调整，以减少损失并提高系统性能

好处在于我们不必指定这些权重。这些权重在训练阶段逐渐调整，以减少损失并改善整个系统。

一个神经网络由几个神经元组成，组织在不同的层中。

在图 [3.18](#Fig18) 的示例中，我们有三层，每层有两个神经元。来自第一层神经元的输出是第二层两个神经元的输入。第一层称为*输入层*，第二层是*隐藏层*（可能不止一个），第三层是*输出层*。在每个神经元中，基于输入值和权重进行计算，生成一个输出。这种数学运算依赖于向量和特别是*矩阵*数学，因此重要的是输入和输出是向量而不是人类词语。即使对于高维神经网络，计算机也能够非常有效地执行这些操作。![](../images/604345_1_En_3_Chapter/604345_1_En_3_Fig18_HTML.png)

一个插图展示了一个神经网络，其中包含六个输入层中的神经元实例，接着是三个隐藏层，标记为层 1、层 2 和层 3，最终导致一个输出层。

图 3.18

一个由三层组成的神经网络示例

### 从错误中学习

那么我们如何从错误中学习并提高神经网络的性能呢？就像之前一样，我们希望调整权重以最小化损失。如果我们只调整最后一层的权重，就像我们为逻辑回归的例子所做的那样（只有一层），这只能部分解决问题。最后一层的输出取决于它从前一层接收到的输入。如果我们不调整那里的权重，那么我们又陷入了低复杂度线性模型的情况，因此我们还需要弄清楚如何调整倒数第二层的权重，以此类推。因此，我们必须调整神经网络中每一层的权重。这个过程称为*反向传播*。图 [3.19](#Fig19) 展示了我们之前讨论过的神经网络中反向传播的过程。![](../images/604345_1_En_3_Chapter/604345_1_En_3_Fig19_HTML.png)

一幅插图显示了一个神经网络处理术语神经元在3层，接着是一个输入和输出，后面可能是反向传播，表明一个训练过程或学习机制，

图3.19

神经网络的所有层中都会调整权重。这个过程被称为反向传播。

### 使用神经网络进行情感分析

现在让我们回到情感分类的例子，并在神经网络中描述这个问题。在情感分析中，我们希望将文本分类为消极或积极。对于输入，我们将再次需要我们单词的向量表示。如图[3.20](#Fig20)所示，最后一层具有特殊的格式。由于我们的目标是做出二元决策，最后一个神经元被调整为产生这两个输出选项之一（消极或积极）。

一个流程图表示了一个过程，我们的文本被转换为向量，然后通过多个层处理，最终在第3层产生一个被分类为积极或消极的输出。

图3.20

在文本情感分析的背景下，我们希望提供文本作为输入，并获得关于它们是积极还是消极的预测。

与线性分类器（比如我们之前看到的逻辑回归）相比，神经网络通常提供重要的优势。例如，更复杂的数学运算可能导致文本分类的更好性能。有时，在文本中表示积极或消极情感可能会很棘手。讽刺可能是消极情感的一个强烈指标。然而，就像在逻辑回归中给单词分配权重一样，永远无法捕捉到这个复杂的概念。另一方面，可以想象文本中存在或不存在的单词组合的某种复杂函数可能导致文本中讽刺的良好度量。如果是这样，那么神经网络可能能够做到这一点。

我们现在对神经网络的工作原理有了基本的了解，并准备进一步深入。在下一节中，我们将更仔细地研究词嵌入，这些向量编码了单词的含义。

## 深入研究词嵌入

让我们回到之前遇到的问题：机器学习方法需要数值表示（以向量形式）而无法处理人类可读的文本。为了使我们的文本处理和生成能够利用机器学习方法的优势，提高不同任务的性能，我们需要将单词转换为数学向量。

那么我们如何实现这一点呢？一个解决方案被称为*词嵌入*（有时也被称为*词向量*）。

人类语言中的每个单词，比如英语单词*草莓*或*覆盆子*，都有一个数值表示，一个特定的词嵌入，如图[3.21](#Fig21)所示。暂时假设它们如下：

+   “草莓” = [1,6]

+   “覆盆子” = [1,7]

+   “大象” = [2,1]

![](../images/604345_1_En_3_Chapter/604345_1_En_3_Fig21_HTML.png)

一个插图。文字写着草莓，编号为12、11、3和4。

图3.21

单词“草莓”从人类语言映射到一个向量

举例来说，我们假设我们的向量存在于一个二维空间中（因此，上面括号中有两个数字）。这意味着我们可以轻松地在纸张上绘制它们，通过在二维坐标系上使用点。

### 单词之间的关系

我们从图[3.22](#Fig22)中注意到，单词*草莓*和*覆盆子*的向量彼此之间比与单词*大象*的向量更接近。如果两个单词具有相似的含义，它们的词嵌入将在向量空间中更接近。由于草莓和覆盆子都是浆果，而大象是动物，它们的词嵌入更接近。这种特性使我们能够使用数学运算处理单词的含义。例如，考虑以下谜题（Bolukbasi等人[2016](#CR4)）：

> 男人对国王，女人对X

![](../images/604345_1_En_3_Chapter/604345_1_En_3_Fig22_HTML.png)

一个图示呈现了一系列单词，包括草莓、覆盆子和大象在x和y轴上以不同角度。

图3.22

含义相似的单词具有更接近的词向量

这可以通过使用减法和加法来解决。虽然我们经常使用数字进行加法或减法（我们都熟悉类似1 + 2 - 1 = 2的操作），但也可以用向量进行相同的操作，从而解决谜题^([11](#Fn11))：

+   向量(“女王”) = 向量(“国王”) - 向量(“男人”) + 向量(“女人”)

通过从单词*man*和单词*woman*的向量之和中减去单词*king*的向量，我们可以得到结果单词*queen*。是不是很迷人？

同样，类比问题可以解决，甚至跨越不同领域，例如从科学到音乐（Lane等人[2019](#CR25)）：

> 谁是音乐界的玛丽·居里？

这将被翻译为向量数学如下，类似于上面提到的*king*和*queen*的例子：

+   向量(“解决方案”) = 向量(“玛丽·居里”) - 向量(“科学”) + 向量(“音乐”)

自然地，我们非常兴奋，想要探索这些单词嵌入中更多的关系，并了解它们如何作为机器学习训练的输入。但这些单词嵌入实际上是从哪里来的呢？

### 创建单词嵌入

我们已经看到的词嵌入示例与实际词嵌入之间的主要区别是维度。^([12](#Fn12)) 虽然我们在示例中使用了两个维度以便能够直观地查看词嵌入及其彼此之间的关系，但通常它们有大约300个维度。^([13](#Fn13)) 难以想象吧？我也有同感。但到目前为止我们看到的原理是相同的：基于向量的属性并使用数学运算（幸运的是，它们适用于不同维度的向量），我们仍然能够获得相同的见解。你可能会问，如果我们只能使用2个维度，为什么要使用300个维度？这背后的直觉是，我们有更多的维度，我们可以考虑单词相似的更多方面。更高维度的向量有助于捕获单词的不同属性，从而提高词嵌入之间关系的质量。

因此，让我们看看意义是如何被编码到这些词嵌入中的，以及我们如何获得人类语言中特定单词的词嵌入。我们希望有一个包含所有单词的字典，将每个单词翻译成相应的向量。类似于语言词典将单词从英语翻译成，比如西班牙语，在我们的情况下，字典将单词从英语翻译成词嵌入。要创建这样一个字典，可以使用机器学习，或者更准确地说，可以使用神经网络。

等等，什么？我们使用神经网络生成词嵌入，然后使用它们来编码我们要馈送给神经网络的单词？让我详细解释一下。

我们想要将文本的单词转换为向量，以便在机器学习任务中处理它们，例如，在情感分析的上下文中训练一个二元分类任务的模型。这是一个监督机器学习任务，因为我们训练数据中的文本被标记为正面或负面。词嵌入字典是在实际训练过程之前独立创建的。那些词嵌入可以被训练一次，然后在不同的任务中重复使用。就像你的语言词典在书架上一样，每当你需要它时，例如写信或从外语文本中翻译一个单词，你只需拿起它并查找你需要的单词。设置如图[3.23](#Fig23)所示。![](../images/604345_1_En_3_Chapter/604345_1_En_3_Fig23_HTML.png)

将训练数据中的单词转换为向量表示的过程，可能用于情感分析。讨论了词嵌入的利用、监督二元情感分析以及通过对文本语料库进行无监督训练生成词嵌入的过程。

图3.23

训练情感分析分类器的示例流程：文本训练数据使用已经单独训练过的现有词嵌入进行向量化。

### Word2Vec词嵌入

单词嵌入的词典是使用无监督机器学习方法训练的。在本节中，我们将探讨2013年提出的word2vec方法（Mikolov等人[2013](#CR26)），该方法为一系列自然语言处理任务提供了重大改进。这标志着自然语言处理与单词嵌入的新时代的开始，后来更进一步地改进了基于transformer的模型，我们将在后面的章节中探讨。

单词嵌入背后的理念可以用语言学家J.R. Firth的以下引用来概括：

> 你可以通过一个词的周围词了解它（Firth [1962](#CR13)，第11页）

例如，单词*草莓*和单词*覆盆子*可能与*浆果*、*领域*、*红色*、*美味*等词一起出现。这些共同出现的词使得这两个词在意义上更有可能相似。因此，这两个词应该有相似的向量。

训练单词嵌入的机器学习方法是*无监督*的。这意味着数据不需要被标记。在这种情况下，这是一个重大优势，因为在自然语言的情况下标记会很复杂。有许多关系和背景知识，对于我们人类来说，这些知识是多年学习而来的，很难在标记数据集中表达出来（在之前的示例中使用监督学习时会需要）。例如，草莓是浆果，浆果是植物，植物被人类和动物吃（等等）。

然而，仔细观察后，文本中的单词确实有一种标签。与监督学习场景相比的区别在于，标签是隐含的，不需要在训练之前添加。这些算法不是学习每个单词的*含义*，而是学习与所提到的单词一起出现的常见单词。对于这个任务，文本中每个单词的标签就是紧挨着该单词出现的单词。^([14](#Fn14))

为了让你对这意思有所直觉，让我们看一下图[3.24](#Fig24)中显示的例子。单词*领域*和*与*刚好在*草莓*之前，而*和*和*覆盆子*则紧随其后。在skip-gram方法中（word2vec单词嵌入训练中的一种方法^([15](#Fn15) )），我们会尝试预测每个单词周围的单词。在这种情况下，我们会使用例如围绕*草莓*的单词进行训练。由于我们知道正确答案，我们可以利用这些信息来改进学习并减少错误。![](../images/604345_1_En_3_Chapter/604345_1_En_3_Fig24_HTML.png)

文本中提到有一个有草莓和覆盆子的领域。

图3.24

单词“草莓”以及它之前和之后出现的两个单词

要回到神经网络，单词*草莓*将是我们神经网络的输入。作为输出，我们希望神经网络预测周围的单词。为此，神经网络的结构如图[3.25](#Fig25)所示。我们认识到我们在前一节中看到的架构：有神经元和层。特别是，输入层由单词*草莓*的独热向量组成。使用独热向量，所有字段都是0，只有词汇表中特定单词位置的字段设置为1。这个输入层中的条目数对应于我们在词汇表中有的单词数。在中间，我们有一个隐藏层。隐藏层有特定数量的神经元。神经元的数量对应于生成的单词嵌入应该具有的维度数。例如，如果我们想要生成我们之前看到的两维样本单词嵌入，那么我们在这一层中会有两个神经元。在一个更现实的情况下，我们想要生成300维的单词嵌入，那么我们在这里会有300个神经元。最后，输出层输出一个值，该值对应于词汇表中每个单词的概率。在图[3.25](#Fig25)中，我们看到训练对“草莓+和”这对的示例（许多训练步骤中的一个）。

一个神经网络通过一系列层处理单词草莓，包括输入层、隐藏层和输出层。网络根据训练数据学到的关联为序列中各种单词分配潜在的下一个单词的概率。

图3.25

单词嵌入训练的示例（基于Lane等人([2019](#CR25)，第193页)）

谈到概率时，我们使用0到1之间的值。有些人可能更熟悉使用百分比，但从0到1的概率值转换为百分比值很简单。例如，概率为0.5是50%，0.2是20%。基于此，以98.8%的概率，单词*和*很可能跟在单词*草莓*后面。

在训练过程中，“草莓+和”这对使*和*的分数上升，而训练示例“草莓+带”会使*带*的分数上升。训练是在迭代中进行的，取决于我们考虑的周围单词数。例如，在图[3.24](#Fig24)中所示的情况下，我们有四个周围单词，因此在到达下一个单词之前需要进行四次迭代。因此，我们可能不会看到*和*和*草莓*在一般情况下非常相关；然而，在这个训练步骤中这是正确的答案。

这个过程重复了很多次。首先，我们遍历句子中的所有单词，对每个周围的单词进行一次训练步骤。然后，我们不仅对几个句子这样做，而是对包括数百万页文本的大型文本语料库这样做。

令人惊讶的是，一旦训练完成，输出层可以被忽略。我们实际上要找的是隐藏层。它为每个单词生成一个向量（单词嵌入），编码了它与词汇表中其他单词在语义上的关系，它们经常一起出现。语义相似的单词将具有相似的向量。这意味着我们可以从隐藏层中提取我们需要用来将英语单词映射到单词嵌入的单词向量，一旦训练结束。^([16](#Fn16))

这只是生成单词嵌入的一个过程。除了这里介绍的skip-gram方法，Mikolov等人（[2013](#CR26)）还提出了一种稍有不同的替代方法，该方法反转了任务，根据一组周围的单词预测一个单词。其他方法包括GloVe（Pennington等人[2014](#CR29)）嵌入或fasttext（Mikolov等人[2018](#CR27)）。

### 现成的词嵌入

如我们在图[3.23](#Fig23)中看到的，单词嵌入可以创建一次，然后重复使用。这是幸运的，因为训练单词嵌入需要大量资源。一方面，您需要强大的硬件来执行计算。另一方面，您需要耐心，因为这样的训练可能需要几个小时或几天（甚至在我们将在后面章节中看到的大型语言模型的情况下可能需要更长时间）。此外，训练需要大量文本数据（所谓的语料库）。幸运的是，单词嵌入通常是公开可用的，并且可以被文本处理应用程序下载并直接使用。那么，何时值得生成自己的单词嵌入？由于单词嵌入是与语言相关的，您可能需要为特定语言训练自己的单词嵌入。然而，fasttext（Mikolov等人[2018](#CR27)）提供了157种语言的单词嵌入（Grave等人[2018](#CR18)），所以现在很少出现这种情况。在其他情况下，当您需要特定领域的词汇时，您可能需要训练自己的嵌入。现成的词嵌入（如word2vec或fasttext）依赖于涵盖广泛主题的文本，试图对“通用”语言进行建模。但假设您只处理法律文件。这些文件可能包含许多领域特定的词汇，您可能特别感兴趣的是在单词嵌入中编码了这些词之间的关系。

### 使用库

类似地，为数据工程师提供了实现机器学习或数据准备任务常见方法的*库*。这意味着数据工程师可以在软件中包含预定义组件，而不是每次都重新编写编程语言中的所有指令。通过使用库，数据工程师可以提供输入数据集，进行特定设置，然后，例如，重复使用他人事先实现的逻辑回归算法。这简化了这些技术的应用视角，并将所需的知识转移到了了解可用的库、它们的用途以及如何应用它们。图[3.26](#Fig26)说明了数据工程师用于模型训练的典型组件！[](../images/604345_1_En_3_Chapter/604345_1_En_3_Fig26_HTML.png)

一幅插图展示了词嵌入的概念，其中诸如good和morning这样的词被表示为数值向量，如12、6、8和1、4、190。通过库提供的公开可用的词嵌入，具有在数据工程和建模任务中潜在应用的可能性。

图 3.26

数据工程师可以依赖现有库和公开可用的词嵌入

到目前为止，我们所见过的词嵌入将一个词编码为一个数学向量。这些编码可以在不同应用中共享和重复使用。一旦训练完成，更复杂的语言模型也可以与社区共享。

### 语言模型的高层视图

展望即将遇到的这些语言模型，我想在这一点上给你一个高层次的概述，我们正在处理什么。让我们暂时将*语言模型*定义为将概率与文本片段联系起来的统计模型。通常，它们被，以非常简化的方式陈述，用于预测（部分）句子中的下一个词，旨在产生最佳的类人文本。让我们考虑以下例子：

+   *安娜去田野收集……*

有了语言模型，我们可以预测这个句子中接下来可能是什么词。可能基于语言模型在训练数据中包含的其他文本，像草莓、胡萝卜或番茄这样的词比猫或狗更有可能成为下一个词。训练是通过隐藏原始文本中的一些词然后预测它们来进行的。这在某种程度上类似于我们在本章中看到的词嵌入训练算法，因此我们已经准备好迈向大型语言模型了！

## 摘要

在本章中，我们了解了自然语言处理的应用和方法。我们看到了如何将不同复杂程度的解决方案应用于同一问题。挑战在于确定给定任务最合适的方法。

我们看到了神经网络如何有益于非线性可分数据，通过允许更复杂的数学运算和自动化特征提取。经典机器学习和深度学习都依赖于损失函数，并调整模型权重以最小化损失并改进模型的预测。

在自然语言处理的背景下，向量化对于将人类语言映射到数学向量非常重要，这样计算机可以更轻松地处理。我们已经看到了不同的方法，用于训练词嵌入以及语义上相似的单词对应到更接近的向量。

最后，我们看到了公开可用的词嵌入和库是如何整合到数据工程师的工作流程中的。
