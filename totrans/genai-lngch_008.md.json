["```\nfrom langchain import OpenAI, LLMMathChain\nllm = OpenAI(temperature=0)\nllm_math = LLMMathChain.from_llm(llm, verbose=True)\nllm_math.run(\"What is 2 raised to the 10th power?\")\n```", "```\nfrom langchain.agents.agent_toolkits import create_python_agent\nfrom langchain.tools.python.tool import PythonREPLTool\nfrom langchain.llms.openai import OpenAI\nfrom langchain.agents.agent_types import AgentType\nagent_executor = create_python_agent(\n    llm=OpenAI(temperature=0, max_tokens=1000),\n    tool=PythonREPLTool(),\n    verbose=True,\n    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n)\nagent_executor.run(\n    \"\"\"Understand, write a single neuron neural network in PyTorch.\nTake synthetic data for y=2x. Train for 1000 epochs and print every 100 epochs.\nReturn prediction for x = 5\"\"\"\n)\n```", "```\nEntering new AgentExecutor chain...\nI need to write a neural network in PyTorch and train it on the given data\nAction: Python_REPL\nAction Input: \nimport torch\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(1, 1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n# Define the data\nx_data = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\ny_data = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\nfor epoch in range(1000):  # Train the model\n    y_pred = model(x_data)\n    loss = loss_fn(y_pred, y_data)\n    if (epoch+1) % 100 == 0:\n        print(f'Epoch {epoch+1}: {loss.item():.4f}')\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n# Make a prediction\nx_pred = torch.tensor([[5.0]])\ny_pred = model(x_pred)\nObservation: Epoch 100: 0.0043\nEpoch 200: 0.0023\nEpoch 300: 0.0013\nEpoch 400: 0.0007\nEpoch 500: 0.0004\nEpoch 600: 0.0002\nEpoch 700: 0.0001\nEpoch 800: 0.0001\nEpoch 900: 0.0000\nEpoch 1000: 0.0000\nThought: I now know the final answer\nFinal Answer: The prediction for x = 5 is y = 10.00.\n```", "```\nfrom langchain.agents import load_tools, initialize_agent\nfrom langchain.llms import OpenAI\nfrom langchain.chains.conversation.memory import ConversationBufferMemory\nllm = OpenAI(temperature=0)\ntools = load_tools(['wolfram-alpha'])\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\nagent = initialize_agent(tools, llm, agent=\"conversational-react-description\", memory=memory, verbose=True)\nagent.run(\n    \"\"\"How far are these cities to Tokyo?\n* New York City\n* Madrid, Spain\n* Berlin\n\"\"\")\n```", "```\n> Entering new AgentExecutor chain...\nAI: The distance from New York City to Tokyo is 6760 miles. The distance from Madrid, Spain to Tokyo is 8,845 miles. The distance from Berlin, Germany to Tokyo is 6,845 miles.\n> Finished chain.\n'\nThe distance from New York City to Tokyo is 6760 miles. The distance from Madrid, Spain to Tokyo is 8,845 miles. The distance from Berlin, Germany to Tokyo is 6,845 miles.\n```", "```\nfrom sklearn.datasets import load_iris\ndf = load_iris(as_frame=True)[\"data\"]\n```", "```\nfrom langchain.agents import create_pandas_dataframe_agent\nfrom langchain import PromptTemplate\nfrom langchain.llms.openai import OpenAI\nPROMPT = (\n    \"If you do not know the answer, say you don't know.\\n\"\n    \"Think step by step.\\n\"\n    \"\\n\"\n    \"Below is the query.\\n\"\n    \"Query: {query}\\n\"\n)\nprompt = PromptTemplate(template=PROMPT, input_variables=[\"query\"])\nllm = OpenAI()\nagent = create_pandas_dataframe_agent(llm, df, verbose=True)\n```", "```\nagent.run(prompt.format(query=\"What's this dataset about?\"))\n```", "```\nagent.run(prompt.format(query=\"Plot each column as a barplot!\"))\n```", "```\ndf['difference'] = df['petal length (cm)'] - df['petal width (cm)']\ndf.loc[df['difference'].idxmax()]\nObservation: sepal length (cm)    7.7\nsepal width (cm)     2.8\npetal length (cm)    6.7\npetal width (cm)     2.0\ndifference           4.7\nName: 122, dtype: float64\nThought: I now know the final answer\nFinal Answer: Row 122 has the biggest difference between petal length and petal width.\n```", "```\nagent.run(prompt.format(query=\"Validate the following hypothesis statistically: petal width and petal length come from the same distribution.\"))\n```", "```\nThought: I should use a statistical test to answer this question.\nAction: python_repl_ast \nAction Input: from scipy.stats import ks_2samp\nObservation: \nThought: I now have the necessary tools to answer this question.\nAction: python_repl_ast\nAction Input: ks_2samp(df['petal width (cm)'], df['petal length (cm)'])\nObservation: KstestResult(statistic=0.6666666666666666, pvalue=6.639808432803654e-32, statistic_location=2.5, statistic_sign=1)\nThought: I now know the final answer\nFinal Answer: The p-value of 6.639808432803654e-32 indicates that the two variables come from different distributions.\n```", "```\nimport pandas as pd\nfrom pandasai import PandasAI\ndf = pd.DataFrame({\n    \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"],\n    \"gdp\": [19294482071552, 2891615567872, 2411255037952, 3435817336832, 1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, 14631844184064],\n    \"happiness_index\": [6.94, 7.16, 6.66, 7.07, 6.38, 6.4, 7.23, 7.22, 5.87, 5.12]\n})\nfrom pandasai.llm.openai import OpenAI\nllm = OpenAI(api_token=\"YOUR_API_TOKEN\")\npandas_ai = PandasAI(llm)\npandas_ai(df, prompt='Which are the 5 happiest countries?') \n```", "```\nfrom langchain.llms import OpenAI\nfrom langchain.utilities import SQLDatabase\nfrom langchain_experimental.sql import SQLDatabaseChain\ndb = SQLDatabase.from_uri(\"sqlite:///../../../../notebooks/Chinook.db\")\nllm = OpenAI(temperature=0, verbose=True)\ndb_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)\ndb_chain.run(\"How many employees are there?\")\n```"]