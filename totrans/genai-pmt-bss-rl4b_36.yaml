- en: 'REFERENCES AND CITATIONSGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
    Warde-Farley, D., Ozair, S., ... & Bengio, Y. (2014). Generative adversarial nets.
    In Advances in neural information processing systems (pp. 2672-2680). [Link: [https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)
    ]Radford, A., Metz, L., & Chintala, S. (2016). Unsupervised representation learning
    with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434\.
    [Link: [https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434) ]Brock,
    A., Donahue, J., & Simonyan, K. (2018). Large-scale GAN training for high-fidelity
    raw image synthesis. In International Conference on Learning Representations (ICLR).
    [Link: [https://openreview.net/pdf?id=B1xsqj09Fm](https://openreview.net/pdf?id=B1xsqj09Fm)
    ]Karras, T., Laine, S., & Aila, T. (2018). A style-based generator architecture
    for generative adversarial networks. In Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition (pp. 4401-4410). [Link: [https://openaccess.thecvf.com/content_cvpr_2018/papers/Karras_A_Style-Based_Generator_CVPR_2018_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2018/papers/Karras_A_Style-Based_Generator_CVPR_2018_paper.pdf)
    ]Huang, X., Li, Y., Poursaeed, O., Hopcroft, J., & Belongie, S. (2018). Stacked
    generative adversarial networks. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition (pp. 1866-1875). [Link: [https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Stacked_Generative_Adversarial_CVPR_2017_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Stacked_Generative_Adversarial_CVPR_2017_paper.pdf)
    ]Yang, Z., Yang, D., Dyer, C., He, X., Smola, A., & Hovy, E. (2017). Improved
    variational autoencoders for text modeling using dilated convolutions. In Proceedings
    of the 31st International Conference on Neural Information Processing Systems
    (pp. 2136-2146). [Link: [https://papers.nips.cc/paper/6706-improved-variational-autoencoders-for-text-modeling-using-dilated-convolutions.pdf](https://papers.nips.cc/paper/6706-improved-variational-autoencoders-for-text-modeling-using-dilated-convolutions.pdf)
    ]Salimans, T., Karpathy, A., & Chen, X. (2017). PixelCNN++: Improving the PixelCNN
    with discretized logistic mixture likelihood and other modifications. arXiv preprint
    arXiv:1701.05517\. [Link: [https://arxiv.org/abs/1701.05517](https://arxiv.org/abs/1701.05517)
    ]Brock, A., Lim, T., Ritchie, J. M., & Weston, N. (2019). Neural photo editing
    with introspective adversarial networks. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (pp. 6199-6208). [Link: [https://openaccess.thecvf.com/content_CVPR_2019/papers/Brock_Neural_Photo_Editing_With_Introspective_Adversarial_Networks_CVPR_2019_paper.pdf](https://openaccess.thecvf.com/content_CVPR_2019/papers/Brock_Neural_Photo_Editing_With_Introspective_Adversarial_Networks_CVPR_2019_paper.pdf)
    ]Liu, H., & Ma, L. (2020). A survey on generative adversarial networks: Algorithms,
    theory, and applications. IEEE Access, 8, 29529-295Brown, T. B., Mann, B., Ryder,
    N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Amodei, D. (2020). Language models
    are few-shot learners. arXiv preprint arXiv:2005.14165\. [Link: [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)
    ]Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019).
    Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 9\. [Link:
    [https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
    ]Gao, J., Wenzel, S., Lin, Y., Mansimov, E., Yu, L., & Bengio, Y. (2021). Neural-guided
    constraint logic programming for program synthesis from natural language. In Proceedings
    of the 2021 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies (pp. 1531-1541). [Link: [https://aclanthology.org/2021.naacl-main.123.pdf](https://aclanthology.org/2021.naacl-main.123.pdf)
    ]Dodge, J., & Gane, A. (2021). Fine-tuning pre-trained language models: Weight
    initializations, data orders, and early stopping. In Proceedings of the 2021 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies (pp. 3093-3103). [Link: [https://aclanthology.org/2021.naacl-main.290.pdf](https://aclanthology.org/2021.naacl-main.290.pdf)
    ]Keskar, N. S., McCann, B., Varshney, L. R., Liu, C., Fischer, I., & Bengio, Y.
    (2021). On the importance of initialization and momentum in deep learning. In
    Proceedings of the 38th International Conference on Machine Learning (ICML) (pp.
    5202-5212). [Link: [https://proceedings.icml.cc/static/paper_files/icml/2021/5208-Paper.pdf](https://proceedings.icml.cc/static/paper_files/icml/2021/5208-Paper.pdf)
    ]Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., & Bowman, S. R. (2019).
    GLUE: A multi-task benchmark and analysis platform for natural language understanding.
    In Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP) (pp. 3675-3685). [Link: [https://www.aclweb.org/anthology/D19-1231.pdf](https://www.aclweb.org/anthology/D19-1231.pdf)
    ]Lewis, M., Liu, Y. Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., ...
    & Zettlemoyer, L. (2020). Pretrained language models for conversational AI. arXiv
    preprint arXiv:2004.14294\. [Link: [https://arxiv.org/abs/2004.14294](https://arxiv.org/abs/2004.14294)Dodge,
    J., Gane, A., Zhang, X., Bordes, A., Chopra, S., & Miller, A. (2020). Fine-tuning
    language models from human preferences. arXiv preprint arXiv:2004.14228\. [Link:
    [https://arxiv.org/abs/2004.14228](https://arxiv.org/abs/2004.14228) ]Holtzman,
    A., Buys, J., Forbes, M., & Choi, Y. (2021). The curious case of neural text degeneration.
    arXiv preprint arXiv:2101.05961\. [Link: [https://arxiv.org/abs/2101.05961](https://arxiv.org/abs/2101.05961)
    ]Li, J., Li, W., Li, S., & Liu, X. (2021). A survey of limitations and opportunities
    in generative adversarial networks research. arXiv preprint arXiv:2103.01864\.
    [Link: [https://arxiv.org/abs/2103.01864](https://arxiv.org/abs/2103.01864) ]Keshavarzi,
    M., & Hashemi, S. M. (2021). A survey of the challenges and limitations of natural
    language generation. Natural Language Engineering, 27(2), 211-251\. [Link: [https://www.cambridge.org/core/journals/natural-language-engineering/article/survey-of-the-challenges-and-limitations-of-natural-language-generation/7C26143B9B04DCAE2B67051F7D42435A](https://www.cambridge.org/core/journals/natural-language-engineering/article/survey-of-the-challenges-and-limitations-of-natural-language-generation/7C26143B9B04DCAE2B67051F7D42435A)
    ]Liu, C., McCann, B., Keskar, N. S., Xiong, C., & Socher, R. (2021). On limitations
    of unsupervised bilingual dictionary induction. In Proceedings of the 2021 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies (pp. 1929-1939). [Link: [https://aclanthology.org/2021.naacl-main.151.pdf](https://aclanthology.org/2021.naacl-main.151.pdf)
    ]Yang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhutdinov, R., & Le, Q. V.
    (2019). XLNet: Generalized autoregressive pretraining for language understanding.
    In Advances in neural information processing systems (pp. 5754-5764). [Link: [https://proceedings.neurips.cc/paper/2019/file/2e9cf1a55c6bb74b6f8e3a9abfeaf60c-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/2e9cf1a55c6bb74b6f8e3a9abfeaf60c-Paper.pdf)
    ]'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献和引用Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D.,
    Ozair, S., ... & Bengio, Y. (2014). 生成对抗网络。在神经信息处理系统的进展中（第2672-2680页）。[链接：[https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)
    ]Radford, A., Metz, L., & Chintala, S. (2016). 深度卷积生成对抗网络的无监督表示学习。arXiv预印本arXiv:1511.06434。[链接：[https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434)
    ]Brock, A., Donahue, J., & Simonyan, K. (2018). 大规模GAN训练用于高保真度原始图像合成。在国际学习表示会议（ICLR）中。[链接：[https://openreview.net/pdf?id=B1xsqj09Fm](https://openreview.net/pdf?id=B1xsqj09Fm)
    ]Karras, T., Laine, S., & Aila, T. (2018). 用于生成对抗网络的基于样式的生成器架构。在IEEE计算机视觉与模式识别会议论文集中（第4401-4410页）。[链接：[https://openaccess.thecvf.com/content_cvpr_2018/papers/Karras_A_Style-Based_Generator_CVPR_2018_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2018/papers/Karras_A_Style-Based_Generator_CVPR_2018_paper.pdf)
    ]Huang, X., Li, Y., Poursaeed, O., Hopcroft, J., & Belongie, S. (2018). 堆叠生成对抗网络。在IEEE计算机视觉与模式识别会议论文集中（第1866-1875页）。[链接：[https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Stacked_Generative_Adversarial_CVPR_2017_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Stacked_Generative_Adversarial_CVPR_2017_paper.pdf)
    ]Yang, Z., Yang, D., Dyer, C., He, X., Smola, A., & Hovy, E. (2017). 使用扩张卷积改进文本建模的变分自动编码器。在第31届国际神经信息处理系统大会上（第2136-2146页）。[链接：[https://papers.nips.cc/paper/6706-improved-variational-autoencoders-for-text-modeling-using-dilated-convolutions.pdf](https://papers.nips.cc/paper/6706-improved-variational-autoencoders-for-text-modeling-using-dilated-convolutions.pdf)
    ]Salimans, T., Karpathy, A., & Chen, X. (2017). PixelCNN++：通过离散化逻辑混合似然和其他修改改进PixelCNN。arXiv预印本arXiv:1701.05517。[链接：[https://arxiv.org/abs/1701.05517](https://arxiv.org/abs/1701.05517)
    ]Brock, A., Lim, T., Ritchie, J. M., & Weston, N. (2019). 具有内省对抗网络的神经照片编辑。在IEEE计算机视觉与模式识别会议论文集中（第6199-6208页）。[链接：[https://openaccess.thecvf.com/content_CVPR_2019/papers/Brock_Neural_Photo_Editing_With_Introspective_Adversarial_Networks_CVPR_2019_paper.pdf](https://openaccess.thecvf.com/content_CVPR_2019/papers/Brock_Neural_Photo_Editing_With_Introspective_Adversarial_Networks_CVPR_2019_paper.pdf)
    ]Liu, H., & Ma, L. (2020). 生成对抗网络综述：算法、理论和应用。IEEE Access, 8, 29529-295Brown, T.
    B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Amodei,
    D. (2020). 语言模型是少样本学习者。arXiv预印本arXiv:2005.14165。[链接：[https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)
    ]Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019).
    语言模型是无监督多任务学习者。OpenAI博客, 1(8), 9。[链接：[https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
    ]Gao, J., Wenzel, S., Lin, Y., Mansimov, E., Yu, L., & Bengio, Y. (2021). 用于从自然语言中合成程序的神经引导约束逻辑编程。在北美计算语言学协会2021年会议论文集中（第1531-1541页）。[链接：[https://aclanthology.org/2021.naacl-main.123.pdf](https://aclanthology.org/2021.naacl-main.123.pdf)
    ]Dodge, J., & Gane, A. (2021). 微调预训练语言模型：权重初始化、数据顺序和早停止。在北美计算语言学协会2021年会议论文集中（第3093-3103页）。[链接：[https://aclanthology.org/2021.naacl-main.290.pdf](https://aclanthology.org/2021.naacl-main.290.pdf)
    ]Keskar, N. S., McCann, B., Varshney, L. R., Liu, C., Fischer, I., & Bengio, Y.
    (2021). 深度学习中初始化和动量的重要性。在第38届国际机器学习大会（ICML）中（第5202-5212页）。[链接：[https://proceedings.icml.cc/static/paper_files/icml/2021/5208-Paper.pdf](https://proceedings.icml.cc/static/paper_files/icml/2021/5208-Paper.pdf)
    ]Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., & Bowman, S. R. (2019).
    GLUE：自然语言理解的多任务基准和分析平台。在2019年经验方法自然语言处理会议和第9届国际自然语言处理联合会议（EMNLP-IJCNLP）中（第3675-3685页）。[链接：[https://www.aclweb.org/anthology/D19-1231.pdf](https://www.aclweb.org/anthology/D19-1231.pdf)
    ]Lewis, M., Liu, Y. Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., ...
    & Zettlemoyer, L. (2020). 预训练语言模型用于对话人工智能。arXiv预印本arXiv:2004.14294。[链接：[https://arxiv.org/abs/2004.14294](https://arxiv.org/abs/2004.14294)Dodge,
    J., Gane, A., Zhang, X., Bordes, A., Chopra, S., & Miller, A. (2020). 从人类偏好微调语言模型。arXiv预印本arXiv:2004.14228。[链接：[https://arxiv.org/abs/2004.14228](https://arxiv.org/abs/2004.14228)
    ]Holtzman, A., Buys, J., Forbes, M., & Choi, Y. (2021). 神经文本退化的奇怪案例。arXiv预印本arXiv:2101.05961。[链接：[https://arxiv.org/abs/2101.05961](https://arxiv.org/abs/2101.05961)
    ]Li, J., Li, W., Li, S., & Liu, X. (2021). 生成对抗网络研究的局限性和机会综述。arXiv预印本arXiv:2103.01864。[链接：[https://arxiv.org/abs/2103.01864](https://arxiv.org/abs/2103.01864)
    ]Keshavarzi, M., & Hashemi, S. M. (2021). 自然语言生成的挑战和局限性综述。自然语言工程, 27(2), 211-251。[链接：[https://www.cambridge.org/core/journals/natural-language-engineering/article/survey-of-the-challenges-and-limitations-of-natural-language-generation/7C26143B9B04DCAE2B67051F7D42435A](https://www.cambridge.org/core/journals/natural-language-engineering/article/survey-of-the-challenges-and-limitations-of-natural-language-generation/7C26143B9B04DCAE2B67051F7D42435A)
    ]Liu, C., McCann, B., Keskar, N. S., Xiong, C., & Socher, R. (2021). 无监督双语词典归纳的局限性。在北美计算语言学协会2021年会议论文集中（第1929-1939页）。[链接：[https://aclanthology.org/2021.naacl-main.151.pdf](https://aclanthology.org/2021.naacl-main.151.pdf)
    ]Yang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhutdinov, R., & Le, Q. V.
    (2019). XLNet：用于语言理解的广义自回归预���练。在神经信息处理系统的进展中（第5754-5764页）。[链接：[https://proceedings.neurips.cc/paper/2019/file/2e9cf1a55c6bb74b6f8e3a9abfeaf60c-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/2e9cf1a55c6bb74b6f8e3a9abfeaf60c-Paper.pdf)
    ]
