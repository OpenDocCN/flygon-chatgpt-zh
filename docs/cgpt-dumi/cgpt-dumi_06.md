第五章

警告、伦理和负责任的人工智能

> 在本章中
> 
> ![](img/00003.jpg) 学习有关负责任人工智能运动的知识
> 
> ![](img/00003.jpg) 关注 OpenAI 的警告
> 
> ![](img/00003.jpg) 失去版权和知识产权保护
> 
> ![](img/00003.jpg) 追求可靠性和信任
> 
> ![](img/00003.jpg) 降低风险
> 
> 每个模型改进的目标是增加稳定性和整体性能，包括可靠性、准确性和伦理。在本章中，您将了解这意味着什么，以及为什么这一努力至关重要。

制定负责任的人工智能

> 几乎每个人在涉及人工智能时都有一种谨慎或前兆感。一个名为负责任人工智能的广泛行业努力成立，旨在确保人工智能是通过设计负责任地开发的。这一运动旨在确保人工智能模型从一开始就建立在特定原则之上，而不是在成熟或部署后再添加零碎措施，或者更糟糕的是完全忽视。
> 
> 许多人工智能提供商和社区都支持负责任人工智能运动提倡的基本原则，其中包括以下内容：

+   责任制

+   偏见评估

+   可靠性和安全性

+   公平和可访问性

+   透明度和可解释性

+   隐私和安全

> ChatGPT 的创建者 OpenAI 已多次表示承诺负责任人工智能原则。它还以多种方式为该运动做出了贡献，包括开源 Evals，用于评估 OpenAI 模型的框架，以及开源基准注册表，并贡献政策研究论文。
> 
> OpenAI 的合作伙伴和合作者也致力于负责任人工智能的原则，但当前的经济压力正在考验企业对此的承诺。例如，在与许多最近的科技行业裁员一致的情况下，2023 年 3 月微软裁减了其 AI 伦理和社会团队，该团队负责确保负责任人工智能原则在微软产品发货前得到落实。微软不太可能是最后一个裁减其 AI 产品和服务生产商的大公司。
> 
> 令人担忧的发展
> 
> 直到最近，人工智能是相对少数具有高度专业技能的学者的领域。但是 ChatGPT 在公众场合的爆炸性出现引发了广泛的兴趣。现在几乎每个人都对使用人工智能感兴趣。而且有相当多的人渴望开发自己的人工智能；工具和成本使几乎任何人都可以做到。
> 
> 例如，斯坦福大学的研究人员构建了 Alpaca AI，在几项任务上表现类似于 ChatGPT。Alpaca AI 是建立在一个名为 LLaMA 的开源小型语言模型上的（由 Meta，前身为 Facebook，开发）。斯坦福的研究人员花了不到 600 美元对其进行训练，使其成为 ChatGPT 的廉价仿冒品。
> 
> 然而，便宜的 AI 可能会变成昂贵的 AI。Alpaca AI 非常不安全，经常产生错误和有毒的回应，斯坦福在推出后不久负责地将其下线。然而，用于微调该模型的数据集和代码仍然可以在 GitHub 上供任何人使用。研究人员正在努力发布 Alpaca AI 模型的权重，这也是为了提供一个轻量级模型供 AI 社区研究多个 AI 缺陷，希望能够制定更负责任的 AI。
> 
> 但故事中的噩梦是，现在任何人都可以使用 GitHub 上的数据集和代码构建一个 AI 模型 —— 如果流程经过优化，大约只需花费约 $100，根据斯坦福的说法。考虑到即使云计算成本也可以降低或消除，这并不是一个牵强的估计。我们已经有多份报告显示人们在树莓派计算机和 Pixel 6 智能手机上运行 Alpaca 的代码，从而避免了对云计算的需求。
> 
> 与此同时，AI 模型也在网络上的不良社区中出现。例如，Meta 的 LLaMA 模型在推出一周后据称在 4chan 上泄露了全部内容。就 AI 而言，便宜的往往更糟糕，至少在人类安全方面是如此。例如，如果对其采取行动，AI 模型可能提供错误或有害信息，这可能会对人们造成身体伤害，而错误信息可能助长有害的阴谋论或引发公众动荡。现在想象一下，有人使用几乎没有保障的模型恶意构建 AI。这种想法并不令人安心。
> 
> 别忘了，有时国家也可能有可疑的意图。目前，几乎所有政府都在使用各种类型的 AI，其中大部分是机密信息，因此无法供公众审查。全球各国政府也担心 AI 被用于恐怖袭击、网络攻击或公众起义。
> 
> 对于 AI 可能被用于何种目的的担忧导致 OpenAI 和中国政府禁止中国个人使用 ChatGPT。但一家名为百度的中国公司已经发布了其替代模型，英文名为 Enhanced Representation from kNowledge IntEgration，简称 Ernie Bot。
> 
> Ernie Bot 与 ChatGPT-4 在两个主要方面有所不同。Ernie Bot 生成多模式输出，意味着它生成文本和图像，而 ChatGPT-4 仅生成文本。而 Ernie Bot 无法分析提示中的图像，但 ChatGPT-4 可以。
> 
> Ernie Bot 在安全措施上可能有所不同。目前很难确定百度是否正在为 Ernie Bot 制定足够的安全预防措施。每个人都急于尽快推出他们的 ChatGPT 答案，这对确保适当的安全和道德措施得以实施并发挥作用并不是个好消息。
> 
> 之前，百度发布了被广泛认为与 GPT-3 相当的 Ernie 3.0。2022 年，他们发布了 Ernie-ViLG，可以根据文本提示生成图像。中国实体发布的其他类似 ChatGPT 的机器人包括复旦大学研究人员开发的 MOSS 和一家名为 MiniMax 的初创公司开发的 Inspo。
> 
> 多年来，美国和中国一直对开发和使用人工智能充满兴趣并积极参与其中。其他国家也是如此。在这种背景下，人工智能对冷战和热战、世界商业、个人自由、人权以及其他国家、地缘政治和国际政策问题都产生影响。
> 
> 保护人类免受利用人工智能的人类
> 
> 另一方面，一些国家正在努力遏制人工智能模型扩散的后果。例如，2022 年 12 月，欧盟欧洲理事会提出了一项名为《人工智能法案》的法规，旨在“确保在欧盟市场上投放和在联盟中使用的人工智能系统安全并尊重现有的基本权利和联盟价值观”。截至 2022 年 10 月，美国已有一份新的人工智能权利法案蓝图。英国的“有效人工智能保证生态系统路线图”于 2021 年由数据伦理与创新中心发布。世界经济论坛也发布了一套标准和指南，于 2022 年发布了名为“量子计算治理原则”的文件。
> 
> 几乎在各种现有软件中都可以看到像 ChatGPT 这样强大生成人工智能模型的合法集成。例如，ChatGPT 已经出现在许多微软产品中，从必应到 Office365。GPT-4 API 可以用于将该模型集成到几乎任何软件中。用于合法用途的竞争性人工智能模型也在不断增加。一个例子是由生成对抗网络（GAN）人工智能模型驱动的 Adobe 的 Firefly 工具。
> 
> 了解好的、坏的和丑陋的
> 
> 现在你开始意识到人工智能的可及性和多样性有多么强大了！但人工智能提供商也是如此。当他们的工作可以被窃取和伪造，几个小时内就可以使用，他们会继续有责任地构建和重新训练人工智能模型多久？现在再加上最近经济压力上升导致削减成本的情况。这是否意味着负责任的人工智能团队首先被裁员？
> 
> 这将使人工智能处于何种境地？这将使我们处于何种境地？
> 
> 即使是 OpenAI 的 CEO 山姆·奥尔特曼也公开承认他对人工智能“有点害怕”。他还发出警告，一些开发类似 ChatGPT 工具的人工智能开发者不会应用安全限制。人工智能模型失控只是时间问题。
> 
> 鉴于像 GPT-4 这样的 AI 模型和 ChatGPT 等应用的巨大规模和广泛能力，个人、公民保护机构和团体、政府、AI 提供商等必须加入、坚持或重新致力于持续努力，以将 AI 限制在旨在保护人类的强大和合理的防护措施内。不能削减成本和抄近路，否则将招致严重后果。

注意 OpenAI 的警告

> OpenAI 发布每个 ChatGPT 版本时都会明确发布公开警告。务必注意这些警告。但如果您尚未阅读开头的警告帖子，或者想要检查警告是否已更新，只需询问 ChatGPT 列出最新的警告，就像我在图 5-1 中所做的那样。
> 
> 特别注意有关您隐私的警告。ChatGPT 仍处于训练阶段，从 ChatGPT-3 到 ChatGPT-4 的所有模型都是如此。这意味着您输入的任何提示（图像或文本）可能会被用作训练材料。因此，以下任何条件可能会发生或不会发生：

![](img/00039.jpg)

图 5-1： ChatGPT-4 在提示后列出了有关其使用的警告。

+   安全性可能不会像通常为个人可识别信息（PII）提供的那样高。

+   如果有的话，隐私屏障可能不会延伸或跟随任何数据传输。

+   训练材料可能会在某个时候变为开源或共享。

+   您的提示可能会成为未来 AI 模型的培训数据库的永久部分，因此几乎不可能永远删除。

+   OpenAI 的研究人员和 AI 训练者可能会在审查 ChatGPT 性能时看到您的提示与图像和文本。

> 为了您自己的安心，请按照所有这些情况都可能实现的方式继续进行提示。
> 
> ![](img/00023.jpg) 尽管列出的所有警告都很重要，但我要特别提到不完整或不正确信息的警告。ChatGPT 的一个缺点是它可以被有意用来生成高度令人信服的虚假信息和宣传。我在其他章节中警告过您，但需要重申：ChatGPT 更隐匿的威胁是其能力产生幻觉，或产生听起来非常可信但完全错误的回应。简而言之，不要相信 AI 说的或写的任何话。请仔细检查它输出的每一条信息。

考虑版权和知识产权保护

> OpenAI 从一开始就明确表示，ChatGPT 生成的任何文本都属于您。这一切都很好，除非您试图对其进行版权保护并专门用于赚钱。
> 
> 美国版权局裁定，任何包含人工智能生成内容的作品只能在人类创作程度范围内受版权保护。换句话说，AI 写的部分不受法律版权保护。如果您撰写了作品但使用 AI 生成的图像来说明，您的文字受版权保护，但图像不受保护。如果您改写了 ChatGPT 生成的部分文本，只有您写的文字受版权保护。其余部分基本上留在公共领域供他人使用。
> 
> 不要以为这是美国的怪癖或人工智能偏见的结果，世界知识产权组织（WIPO）报告称，包括西班牙和德国在内的大多数司法管辖区几十年前就对机器生成的版权保护做出了相同的裁决。目前尚不清楚他们是否会改变主意，将 GPT 总体和 ChatGPT 特别视为原创内容创作者。目前，大多数人都坚定地认为“不可能！”
> 
> 全球出版商和代理商表示，他们被希望通过 ChatGPT 生成的作品快速轻松赚钱的人们的书籍、电子书和其他内容淹没。几乎没有一本通过编辑审核，也没有人获得大量轻松的现金。顺便说一句，任何人现在都可以复制这些作品，因为它们被视为公平竞争。
> 
> 此外，ChatGPT 和其他 GPT 模型（如 DALL-E）可能被判有侵犯版权的行为。当大量数据被不加选择地从互联网上抓取而未经付款或许可时，受版权保护的作品和其他受保护的知识产权被添加到模型的训练数据库中。目前，潜在的侵权责任正在进行讨论。此外，由于受版权保护的作品在 ChatGPT 的训练数据库中，它可能偶尔复制确切的用词使用 — 也就是，抄袭 — 这可能为不知情的用户带来责任问题。可能需要一场法庭案件来澄清所有法律细节。
> 
> 请密切关注不断增加的责任问题、新的法庭诉讼和不断发展的法规，因为如果您正在使用 ChatGPT 或类似的人工智能模型，它们可能包含对您努力的新威胁。

寻找可预测性

> 人工智能可以客观地根据可预测性来评判，即在相同或类似问题上提供正确答案的百分比。通常，人工智能模型在各种类型的问题上并不都能得到 100%的可预测性得分；而是在不同类型的问题上有不同的得分。
> 
> 但很少有用户依赖实际的可预测性评分来确定他们对人工智能的信任程度。相反，人们更倾向于依赖他们对人工智能的感知和对其回应的直觉。在本节中，您将看到机器测试和人类感觉如何影响您与 ChatGPT 的工作。
> 
> 追求可靠性
> 
> 在其 GPT-4 技术报告中，OpenAI 断言 GPT-4 在 OpenAI 内部对抗性设计的事实性评估中比最新的 GPT-3.5 迭代高出 19 个百分点。模型比较中的具体得分显示在图 5-2 中，该图描述了各种 ChatGPT 模型在九个类别中的表现。
> 
> 在像 TruthfulQA 这样的公共基准测试中，该测试评估模型在将事实与不正确陈述区分开的能力，GPT-4 的基础模型仅比 GPT-3.5 稍微好一点。在额外的 RLHF（从人类反馈中强化学习）后训练后，GPT-4 模型的表现比 GPT-3.5 更好。令人感到反直觉的是，预训练模型对其答案的置信度通常与正确性的概率相匹配，而后训练模型的置信度则相反。
> 
> 无论是 GPT-4 还是 GPT-3.5 模型都不知道 2021 年截止日期之后发生的事实和事件。这些模型也不会从经验中学习，这可能导致对提示的轻信、推理错误和类似人类错误的错误。推理中也存在偏见。

![](img/00069.jpg)

图 5-2： ChatGPT 在各个类别中的表现得分。

> ![](img/00023.jpg) 简而言之，ChatGPT，无论使用何种模型，都存在可靠性问题。尽管随着模型的演进，这个问题有所改善，但仍然很重要，因此输出应始终经过事实核查，特别是在将 ChatGPT 内容用于关键功能和决策时。
> 
> 读者应始终意识到输出的固有不可靠性，并不要被 ChatGPT 生成的令人信服的语言所影响，以免跳过验证步骤。
> 
> 幻觉与准确性
> 
> 如果你了解生成式 AI 模型，你会提到自信的 AI 或 AI 模型的置信水平。在机器学习的背景下，置信度是 AI 模型根据其拥有的信息（输入或提示）估计其答案（输出）正确性的概率度量。
> 
> 用于确定 AI 对其响应的置信水平的四个类别是可重复性、可信度、充分性和适应性。
> 
> ![](img/00079.jpg) 当说一个 AI 模型有很高的置信度时，并不意味着用户也可以对 AI 的答案有很高的信心。当 ChatGPT 明显且可证明地给出错误答案时，它可能非常确信自己给出了正确答案。这种行为在 AI 行业术语中被称为幻觉。
> 
> AI 的幻觉并非出于恶意；机器并非有意欺骗你。它只是做了数学运算，胡言乱语，自我评价为极其正确，就像患有邓宁-克鲁格效应或妄想症的人可能会做的那样。
> 
> 而且借用《飘》中的一位角色的话，坦率地说，亲爱的，它并不在乎。当 ChatGPT 及其同类产生幻觉时，它们并不在乎。该模型提供一个它高度确信是正确的答案，就此打住。也许有一天，AI 研究人员将能够教导 AI 模型至少谦卑地复核作业，并在失败时感到适当的尴尬。
> 
> 当 ChatGPT 产生幻觉时，它会输出一个令人信服的胡言乱语，如果你采取行动或毫无疑问地接受它为真，可能会对你或他人造成伤害。这种包裹在甜言蜜语中的不准确性通常是研究人员在谈论模型不安全时所指的。该模型不可靠准确，因此你不能毫无保留地相信它输出的任何内容。
> 
> 正如谷歌所说，机器学习，但如果你将知识等同于确定性，那么它们什么也不知道，这在西方思维中经常发生。
> 
> ![](img/00079.jpg) ChatGPT 的工作原理是预测哪些单词会跟随你的提示。它一无所知。它计算概率并输出具有最高正确概率的结果，但这个结果可能完全错误。不要认为这意味着 ChatGPT 是一个玩具或执行简单计算。ChatGPT 是一项惊人的工程壮举。但它也有缺陷。
> 
> 那是否意味着你应该认为有缺陷的 ChatGPT 或其他生成式 AI 模型毫无价值？绝对不是。尽管其输出必须经过持续严格的事实核查，但它仍然可以显著提高生产速度。而且你可以打赌你的竞争对手也在使用它或类似的东西。
> 
> ![](img/00084.jpg) 将 ChatGPT 视为一个你可能需要纠正、指导和辅导的初级助手。尽管它有缺点，但这个助手在为你带来大部分工作所需的东西时非常快速，让你的工作更轻松、更快速。
> 
> 使机器具有人性化
> 
> 在 OpenAI 开发 ChatGPT 和支持它的各种模型的工作中，最显著的成就之一是这个 AI 模型看起来像人类。这一发展是相当现代的奇迹。
> 
> 阿兰·图灵，一个受过良好教育且多才多艺的人，将他著名的 AI 图灵测试称为“模仿游戏”，在这个游戏中，一台机器可以如此逼真地模仿人类智能和对话，以至于人类无法分辨它是机器。他在 1950 年代开发了这个测试，而在那段时间的大部分时间里，机器表现不佳。现在有几个似乎至少暂时通过了测试，但它们通常最终会被揭穿。
> 
> 人们通常知道 ChatGPT 是 AI，但用户在继续与它交谈时很容易忘记这一事实。由于从提示到输出的整个交互都是用自然语言进行，并且以人类对话的速度进行，因此这种体验可能与在网上与另一个人聊天相同。
> 
> 最终，ChatGPT 会犯一个错误，提醒用户这只是人工智能，一切都暴露了。甚至 ChatGPT 在我与它关于这个话题的对话中也承认了这一点，如图 5-3 所示。
> 
> 即便如此，最初的虚假熟悉感也会培养信任。而信任是任何人都不应该放在人工智能身上的最后一件事。
> 
> 几项研究揭示了人类对机器人的人性化和信任的缺陷是持久的。例如，一份来自卡内基梅隆大学和加州大学伯克利分校的研究人员撰写的名为“人类行为中的计算机”报告发现，错误归因导致人们依赖表现不佳的人工智能。换句话说，人们倾向于责怪自己而不是人工智能的错误。此外，人类继续错误地接受责任，这导致他们“进入依赖表现不佳的人工智能的恶性循环”。

![](img/00004.jpg)

图 5-3： ChatGPT Plus（基于 GPT-4 模型构建的高级版本）在被问及是否被误认为是人类时的回应。

> 此外，研究人员发现用户的自信水平，而不是他们对人工智能的信心水平，是决定他们接受还是拒绝人工智能建议的关键因素。他们的研究结果指出了需要“有效校准人类自信心以进行成功的人工智能辅助决策”的需求。简而言之，人类需要接受培训，学会何时信任自己，何时信任人工智能，而不是默认和推诿给人工智能。
> 
> 人类的经验也会影响对假定不可错误的人工智能的接受程度，尤其是像 ChatGPT 这样听起来像人类和仁慈的人工智能。那些对他人意图愤世嫉俗和怀疑的人往往对人工智能持更多怀疑态度。同样，那些对他人更加信任的人往往也会信任人工智能。但这两组人中的成员已知会根据他们与人工智能的个人经验而改变主意。
> 
> 无论如何，要意识到人类和机器的缺点，并相应地进行。抵制自己与 ChatGPT 及其同类友好和信任的冲动。
> 
> 正如谷歌的 People + AI Research (PAIR)计划撰稿人大卫·温伯格所说，不确定性在人类看来是一种弱点，但在人工智能看来是一种优势。想一想这一点。你是基于对 ChatGPT 的信心的错误信任做出决定吗，还是每次都会事实核查呢？

缓解风险和责任

> OpenAI 已经付出了大量努力来提高 ChatGPT 的安全性，不断改进其与人类价值观和目标的一致性。为了实现更大的安全性，他们采取的措施包括来自人类领域专家的反对测试和红队测试的反馈（一组扮演对抗角色寻找漏洞的人类），改进的安全模型（AI 模型的防护栏），以及一个模型辅助的安全管道，通过在安全参数内自动化机器学习过程来提供帮助。
> 
> 使用人类专家进行对抗测试和红队行动，而不仅仅是将两个对立的 AI 模型扔进一个坑中不断地互相较量和完善，这是一个至关重要的举措。诸如网络安全和国际安全专业人员之类的领域专家可以发现并消除或遏制风险，例如恐怖分子使用 ChatGPT 获取脏炸弹的组装说明或人为制造的生物黑客配方。我相信您能理解为什么这样强烈的预防措施是必要的。
> 
> 但其他领域专家在细化有关小众话题的回应、包含冒犯性言论、遏制固有偏见、消除宣传和错误信息以及防止骚乱和社会动荡方面也很重要。
> 
> 诸如 ChatGPT 之类的 AI 模型可以做很多好事，但如果允许其做坏事，对我们所有人来说可能会非常糟糕。使用人类专家处理问题并帮助安装所需的防护措施是绝对必要的。
> 
> OpenAI 还使用强化学习与人类反馈（RLHF）来更好地匹配用户意图的回应。这种方法有助于提高回应的质量。它有助于消除 AI 的不安全和不良行为，即使人类用户有不良意图，但也可能导致机器过于谨慎，即使安全也不回复。
> 
> OpenAI 的基于规则的奖励模型（RBRMs）为 AI 模型提供额外奖励，以避免不当回应和过度谨慎。奖励方法，例如用户点击赞和踩图标时，是进一步强化哪些答案是适当和可取的，哪些不是的指标。ChatGPT 只提供数字奖励，没有甜甜圈或免费度假！
> 
> 作为额外步骤，OpenAI 与外部研究人员合作，以提高模型的性能和安全性，并提高他们对潜在影响的理解。
> 
> 但即使经过所有这些，仍然存在一些风险，用户最好采取严格的预防措施。例如，不要假设与 ChatGPT 或其竞争对手的任何对话是私密的或将保持私密。请参见图 5-4 以了解这类 AI 模型可能存在的许多漏洞的示例。

![](img/00013.jpg)

图 5-4：OpenAI 首席执行官 Sam Altman 关于 ChatGPT 数据泄露的推文。

> 对于发布未经编辑的 ChatGPT 内容的用户也存在潜在责任，因为 ChatGPT 及其类似产品已知会抄袭。版权和知识产权并没有因为机器侵犯而被忽视。在发布内容之前，请务必仔细检查其是否存在抄袭和其他问题，如诽谤。
> 
> 至于对社区、国家和人类的更大风险，迫切需要组织和政府机构的协作努力。否则，这些不断崛起的 AI 模型将迅速失控，无疑会造成巨大伤害。
> 
> 以下是采取的步骤，以帮助减轻与 ChatGPT 使用相关的风险和责任：

+   始终要对其生成的内容进行事实核查。

+   进行人工审查，确保内容准确和及时。

+   披露你正在使用人工智能，这样读者和评论者就不会感到被愚弄。

+   检查人工智能生成的内容是否符合所有法律、法规和指导方针。

+   监控受众反馈，并在人工智能内容引发问题时迅速做出回应。

+   避免依赖自己或与你合作的他人的人工智能。你是负责任的；人工智能只是一个工具。

+   负责任地使用人工智能。永远不要用它做出道德、伦理或法律上错误的事情。

> 这些预防措施应该让你走上减轻风险和避免责任的正确道路。但在高风险应用中，甚至要超越这些措施。
