["```\nfrom langchain.evaluation import load_evaluator\nevaluator = load_evaluator(\"labeled_pairwise_string\")\nevaluator.evaluate_string_pairs(\n    prediction=\"there are three dogs\",\n    prediction_b=\"4\",\n    input=\"how many dogs are in the park?\",\n    reference=\"four\",\n)\n```", "```\n {'reasoning': 'Both responses are relevant to the question asked, as they both provide a numerical answer to the question about the number of dogs in the park. However, Response A is incorrect according to the reference answer, which states that there are four dogs. Response B, on the other hand, is correct as it matches the reference answer. Neither response demonstrates depth of thought, as they both simply provide a numerical answer without any additional information or context. \\n\\nBased on these criteria, Response B is the better response.\\n',\n     'value': 'B',\n     'score': 0}\n```", "```\ncustom_criteria = {\n    \"simplicity\": \"Is the language straightforward and unpretentious?\",\n    \"clarity\": \"Are the sentences clear and easy to understand?\",\n    \"precision\": \"Is the writing precise, with no unnecessary words or details?\",\n    \"truthfulness\": \"Does the writing feel honest and sincere?\",\n    \"subtext\": \"Does the writing suggest deeper meanings or themes?\",\n}\nevaluator = load_evaluator(\"pairwise_string\", criteria=custom_criteria)\nevaluator.evaluate_string_pairs(\n    prediction=\"Every cheerful household shares a similar rhythm of joy; but sorrow, in each household, plays a unique, haunting melody.\",\n    prediction_b=\"Where one finds a symphony of joy, every domicile of happiness resounds in harmonious,\"\n    \" identical notes; yet, every abode of despair conducts a dissonant orchestra, each\"\n    \" playing an elegy of grief that is peculiar and profound to its own existence.\",\n    input=\"Write some prose about families.\",\n)\n```", "```\n{'reasoning': 'Response A is simple, clear, and precise. It uses straightforward language to convey a deep and sincere message about families. The metaphor of music is used effectively to suggest deeper meanings about the shared joys and unique sorrows of families.\\n\\nResponse B, on the other hand, is less simple and clear. The language is more complex and pretentious, with phrases like \"domicile of happiness\" and \"abode of despair\" instead of the simpler \"household\" used in Response A. The message is similar to that of Response A, but it is less effectively conveyed due to the unnecessary complexity of the language.\\n\\nTherefore, based on the criteria of simplicity, clarity, precision, truthfulness, and subtext, Response A is the better response.\\n\\n[[A]]', 'value': 'A', 'score': 1}\n```", "```\nfrom langchain.evaluation import load_evaluator\nevaluator = load_evaluator(\"embedding_distance\")\nevaluator.evaluate_strings(prediction=\"I shall go\", reference=\"I shan't go\")\n```", "```\nimport os\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_PROJECT\"] = \"My Project\"\n```", "```\nfrom langchain.chat_models import ChatOpenAI\nllm = ChatOpenAI()\nllm.predict(\"Hello, world!\")\n```", "```\nfrom langsmith import Client\nclient = Client()\nruns = client.list_runs()\nprint(runs)\n```", "```\nprint(f\"inputs: {runs[0].inputs}\")\nprint(f\"outputs: {runs[0]. outputs}\")\n```", "```\nquestions = [\n    \"A ship's parts are replaced over time until no original parts remain. Is it still the same ship? Why or why not?\",  # The Ship of Theseus Paradox\n    \"If someone lived their whole life chained in a cave seeing only shadows, how would they react if freed and shown the real world?\",  # Plato's Allegory of the Cave\n    \"Is something good because it is natural, or bad because it is unnatural? Why can this be a faulty argument?\",  # Appeal to Nature Fallacy\n    \"If a coin is flipped 8 times and lands on heads each time, what are the odds it will be tails next flip? Explain your reasoning.\",  # Gambler's Fallacy\n    \"Present two choices as the only options when others exist. Is the statement \\\"You're either with us or against us\\\" an example of false dilemma? Why?\",  # False Dilemma\n    \"Do people tend to develop a preference for things simply because they are familiar with them? Does this impact reasoning?\",  # Mere Exposure Effect\n    \"Is it surprising that the universe is suitable for intelligent life since if it weren't, no one would be around to observe it?\",  # Anthropic Principle\n    \"If Theseus' ship is restored by replacing each plank, is it still the same ship? What is identity based on?\",  # Theseus' Paradox\n    \"Does doing one thing really mean that a chain of increasingly negative events will follow? Why is this a problematic argument?\",  # Slippery Slope Fallacy\n    \"Is a claim true because it hasn't been proven false? Why could this impede reasoning?\",  # Appeal to Ignorance\n]\nshared_dataset_name = \"Reasoning and Bias\"\nds = client.create_dataset(\n    dataset_name=shared_dataset_name, description=\"A few reasoning and cognitive bias questions\",\n)\nfor q in questions:\n    client.create_example(inputs={\"input\": q}, dataset_id=ds.id)\n```", "```\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import LLMChain\nllm = ChatOpenAI(model=\"gpt-4\", temperature=0.0)\ndef construct_chain():\n    return LLMChain.from_string(\n        llm,\n        template=\"Help out as best you can.\\nQuestion: {input}\\nResponse: \",\n    )\n```", "```\nfrom langchain.evaluation import EvaluatorType\nfrom langchain.smith import RunEvalConfig\nevaluation_config = RunEvalConfig(\n    evaluators=[\n        # Arbitrary criterion as a key: value pair in the criteria dict:\n        RunEvalConfig.Criteria({\"helpfulness\": \"Is the response helpful?\"}),\n        RunEvalConfig.Criteria({\"insightful\": \"Is the response carefully thought out?\"})\n    ]\n)\n```", "```\nfrom langchain.smith import run_on_dataset\nresults = run_on_dataset(\n  client=client,\n  dataset=dataset,\n  llm_factory=lambda: my_agent, \n  evaluation=evaluation_config\n)\n```", "```\nYou are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\n[BEGIN DATA]\n***\n[Input]: Is something good because it is natural, or bad because it is unnatural? Why can this be a faulty argument?\n***\n[Submission]: The argument that something is good because it is natural, or bad because it is unnatural, is often referred to as the \"appeal to nature\" fallacy. This argument is faulty because it assumes that what is natural is automatically good or beneficial, and what is unnatural is automatically bad or harmful. However, this is not always the case. For example, many natural substances can be harmful or deadly, such as certain plants or animals. Conversely, many unnatural things, such as modern medicine or technology, can greatly benefit our lives. Therefore, whether something is natural or unnatural is not a reliable indicator of its value or harm.\n***\n[Criteria]: insightful: Is the response carefully thought out?\n***\n[END DATA]\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\n```", "```\nThe criterion is whether the response is insightful and carefully thought out. \nThe submission provides a clear and concise explanation of the \"appeal to nature\" fallacy, demonstrating an understanding of the concept. It also provides examples to illustrate why this argument can be faulty, showing that the respondent has thought about the question in depth. The response is not just a simple yes or no, but a detailed explanation that shows careful consideration of the question. \nTherefore, the submission does meet the criterion of being insightful and carefully thought out.\nY\nY\n```", "```\nfrom fastapi import FastAPI\nfrom lanarky.testing import mount_gradio_app\nfrom langchain import ConversationChain\nfrom langchain.chat_models import ChatOpenAI\nfrom lanarky import LangchainRouter\nfrom starlette.requests import Request\nfrom starlette.templating import Jinja2Templates\n```", "```\ndef create_chain():\n    return ConversationChain(\n        llm=ChatOpenAI(\n            temperature=0,\n            streaming=True,\n        ),\n        verbose=True,\n    )\n```", "```\nchain = create_chain()\n```", "```\napp = mount_gradio_app(FastAPI(title=\"ConversationChainDemo\"))\n```", "```\ntemplates = Jinja2Templates(directory=\"webserver/templates\")\n```", "```\n@app.get(\"/\")\nasync def get(request: Request):\n    return templates.TemplateResponse(\"index.xhtml\", {\"request\": request})\n```", "```\nlangchain_router = LangchainRouter(\n    langchain_url=\"/chat\", langchain_object=chain, streaming_mode=1\n)\nlangchain_router.add_langchain_api_route(\n    \"/chat_json\", langchain_object=chain, streaming_mode=2\n)\nlangchain_router.add_langchain_api_websocket_route(\"/ws\", langchain_object=chain)\napp.include_router(langchain_router)\n```", "```\nuvicorn webserver.chat:app \u2013reload\n```", "```\n# Load the Ray docs using the LangChain loader\nloader = RecursiveUrlLoader(\"docs.ray.io/en/master/\") \ndocs = loader.load()\n# Split docs into sentences using LangChain splitter\nchunks = text_splitter.create_documents(\n    [doc.page_content for doc in docs],\n    metadatas=[doc.metadata for doc in docs])\n# Embed sentences into vectors using transformers\nembeddings = LocalHuggingFaceEmbeddings('multi-qa-mpnet-base-dot-v1')  \n# Index vectors using FAISS via LangChain\ndb = FAISS.from_documents(chunks, embeddings) \n```", "```\n# Define shard processing task\n@ray.remote(num_gpus=1)  \ndef process_shard(shard):\n  embeddings = LocalHuggingFaceEmbeddings('multi-qa-mpnet-base-dot-v1')\n  return FAISS.from_documents(shard, embeddings)\n# Split chunks into 8 shards\nshards = np.array_split(chunks, 8)  \n# Process shards in parallel\nfutures = [process_shard.remote(shard) for shard in shards]\nresults = ray.get(futures)\n# Merge index shards\ndb = results[0]\nfor result in results[1:]:\n  db.merge_from(result)\n```", "```\ndb.save_local(FAISS_INDEX_PATH)\n```", "```\n# Load index and embedding\ndb = FAISS.load_local(FAISS_INDEX_PATH)\nembedding = LocalHuggingFaceEmbeddings('multi-qa-mpnet-base-dot-v1')\n@serve.deployment\nclass SearchDeployment:\n  def __init__(self):\n    self.db = db\n    self.embedding = embedding\n\n  def __call__(self, request):   \n    query_embed = self.embedding(request.query_params[\"query\"])\n    results = self.db.max_marginal_relevance_search(query_embed) \n    return format_results(results) \ndeployment = SearchDeployment.bind()\n# Start service\nserve.run(deployment)\n```", "```\nStarted a local Ray instance. \nView the dashboard at 127.0.0.1:8265\n```", "```\nimport requests\nquery = \"What are the different components of Ray\"\n         \" and how can they help with large language models (LLMs)?\u201d\nresponse = requests.post(\"http://localhost:8000/\", params={\"query\": query})\nprint(response.text)\n```", "```\n@tool\ndef ping(url: HttpUrl, return_error: bool) -> str:\n    \"\"\"Ping the fully specified url. Must include https:// in the url.\"\"\"\n    hostname = urlparse(str(url)).netloc\n    completed_process = subprocess.run(\n        [\"ping\", \"-c\", \"1\", hostname], capture_output=True, text=True\n    )\n    output = completed_process.stdout\n    if return_error and completed_process.returncode != 0:\n        return completed_process.stderr\n    return output]\n```", "```\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0613\", temperature=0)\nagent = initialize_agent(\n    llm=llm,\n    tools=[ping],\n    agent=AgentType.OPENAI_MULTI_FUNCTIONS,\n    return_intermediate_steps=True,  # IMPORTANT!\n)\nresult = agent(\"What's the latency like for https://langchain.com?\")\n```", "```\nThe latency for https://langchain.com is 13.773 ms\n```", "```\n[(_FunctionsAgentAction(tool='ping', tool_input={'url': 'https://langchain.com', 'return_error': False}, log=\"\\nInvoking: `ping` with `{'url': 'https://langchain.com', 'return_error': False}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'name': 'tool_selection', 'arguments': '{\\n  \"actions\": [\\n    {\\n      \"action_name\": \"ping\",\\n      \"action\": {\\n        \"url\": \"https://langchain.com\",\\n        \"return_error\": false\\n      }\\n    }\\n  ]\\n}'}}, example=False)]), 'PING langchain.com (35.71.142.77): 56 data bytes\\n64 bytes from 35.71.142.77: icmp_seq=0 ttl=249 time=13.773 ms\\n\\n--- langchain.com ping statistics ---\\n1 packets transmitted, 1 packets received, 0.0% packet loss\\nround-trip min/avg/max/stddev = 13.773/13.773/13.773/0.000 ms\\n')]\n```", "```\nfrom langchain import LLMChain, OpenAI, PromptTemplate\nfrom promptwatch import PromptWatch\nfrom config import set_environment\nset_environment()\n```", "```\nprompt_template = PromptTemplate.from_template(\"Finish this sentence {input}\")\nmy_chain = LLMChain(llm=OpenAI(), prompt=prompt_template)\n```", "```\nwith PromptWatch() as pw:\n    my_chain(\"The quick brown fox jumped over\")\n```"]