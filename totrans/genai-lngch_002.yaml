- en: 1 What Are Generative Models?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Join our book community on Discord
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
  prefs: []
  type: TYPE_NORMAL
- en: '![Qr code Description automatically generated](../media/file0.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Artificial Intelligence** (**AI**) has made significant advancements, impacting
    businesses, societies, and individuals. For about the last decade, deep learning
    has evolved to process and generate unstructured data like text, images, videos,
    and more. These advanced AI models, which are based on deep learning, have gained
    popularity in various industries, and include **large language models** (**LLMs**).There
    is currently a significant level of hype in both the media and the industry surrounding
    AI. This is driven by various factors, including advancements in technology, high-profile
    applications, and the potential for transformative impacts across multiple sectors.In
    this chapter, we''ll discuss generative models, in particular LLMs, and their
    application to domains such as text, image, sound, and video. We''ll go through
    some of the technical background that makes them tick, and how they are trained.We''ll
    start with an introduction clarifying where we are at in terms of the state-of-the-art,
    and what the hype is about.'
  prefs: []
  type: TYPE_NORMAL
- en: Why the hype?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the media, there is substantial coverage of AI-related breakthroughs and
    their potential implications. These range from advancements in natural language
    processing and computer vision to the development of sophisticated language models
    like GPT-3\. Media outlets often highlight AI''s capabilities and its potential
    to revolutionize industries such as healthcare, finance, transportation, and more.Particularly,
    generative models have received a lot of attention due to their ability to generate
    text, images, and other creative content that is often indistinguishable from
    human-generated content. These same models also provide a wide functionality including
    semantic search, content manipulation, and classification. This allows cost-savings
    by automation and can allow humans to leverage their creativity by an unprecedented
    level.This graph, inspired by a blog post about GPT-4 Predictions by Stephen McAleese
    on LessWrong, shows the improvements of LLMs in the **Massive Multitask Language
    Understanding** (**MMLU**) benchmark, which was designed to quantify knowledge
    and problem-solving ability in elementary mathematics, US history, computer science,
    law, and more:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1: Average performance on the Massive Multitask Language Understanding
    (MMLU) benchmark of Large Language Models (LLM). Please note that while most benchmark
    results come from 5-shot, a few the GPT-2, PaLM, and PaLM-2 results refer to fine-tuned
    models.](../media/file1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: Average performance on the Massive Multitask Language Understanding
    (MMLU) benchmark of Large Language Models (LLM). Please note that while most benchmark
    results come from 5-shot, a few the GPT-2, PaLM, and PaLM-2 results refer to fine-tuned
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: You can see the progress in recent years in the benchmark. Particularly to highlight
    is the progress of the models provided through a public user interface by OpenAI,
    especially the improvements between releases, from GTP-2 to GPT-3 and GPT-3.5
    to GPT-4\. These models only recently started to perform better than an average
    human rater, but still haven't reached the performance of a human expert. These
    achievements of human engineering are impressive; however, it should be noted
    that performance of these models depends on the field; most models are still performing
    poorly for on the GSM8K benchmark of grade school math word problems.
  prefs: []
  type: TYPE_NORMAL
- en: '**OpenAI** is an American AI research laboratory that aims to promote and develop
    friendly AI. II was established in 2015 with the support of several influential
    figures and companies, who pledged over $1 billion to the venture. The organization
    initially committed to non-profit, collaborating with other institutions and researchers
    by making its patents and research open to the public. In 2018, Elon Musk resigned
    from the board citing a potential conflict of interest with his role at Tesla.
    In 2019, OpenAI transitioned to a for-profit organization, and subsequently Microsoft
    made significant investments in OpenAI, leading to the integration of OpenAI systems
    with Microsoft''s Azure-based supercomputing platform and into the Bing search
    engine. The most significant achievements of the company include the OpenAI Gym
    for training reinforcement algorithms, and - more recently - the GPT-n models
    and DALL-E, another deep learning model that generates images from text.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Generative Pre-training Transformer** (**GPT**) models, like the recently
    launched OpenAI''s ChatGPT, are prime examples of AI advancements in the sphere
    of LLMs. ChatGPT has greatly improved chatbot capabilities by training at a much
    bigger scale and by being much bigger than previous models. These AI-based chatbots
    can generate human-like responses as real-time feedback to customers and can be
    applied to a wide range of use cases, from software development and testing to
    poetry and business communication. Within the industry, there is a growing sense
    of excitement around AI''s capabilities and its potential impact on business operations.
    We''ll look at this more in *Chapter 10*, *The Future of Generative Models*.As
    AI models like OpenAI''s GPT continue to improve, they could become indispensable
    assets to teams in need of diverse knowledge and skills. For example, GPT-4 could
    be considered a "polymath" that could work tirelessly without demanding compensation
    (beyond subscription or API fees), providing assistance in subjects like Math,
    Verbal, Stats, Macroeconomics, Biology, and even passing the Bar exam. As these
    AI models become more proficient and easily accessible, they are likely to play
    a significant role in shaping the future of work and learning.By making knowledge
    more accessible and adaptable, these models have the potential to level the playing
    field and create new opportunities for people from all walks of life. These models
    have shown potential in areas that require higher levels of reasoning and understanding,
    although progress varies depending on the complexity of the tasks involved.As
    for generative models with images, we could expect models with better capabilities
    to assist in creating visual content, and possibly improvements in computer vision
    tasks such as object detection, segmentation, captioning, and much more.Let''s
    get our terminology a bit clearer, and explain more in detail what is meant by
    generative model, artificial intelligence, deep learning, and machine learning.'
  prefs: []
  type: TYPE_NORMAL
- en: What are Generative Models?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the media, the term artificial intelligence is used a lot when referring
    to these new models. It''s worth distinguishing a bit more clearly how term generative
    model differs from artificial intelligence, deep learning, machine learning, and
    language model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Artificial intelligence** (**AI**) is a broad field of computer science that
    deals with the creation of intelligent agents, which are systems that can reason,
    learn, and act autonomously.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine learning** (**ML**) is a subset of AI that deals with the development
    of algorithms that can learn from data. ML algorithms are trained on a set of
    data, and then they can use that data to make predictions or decisions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep learning** (D**L**) is a subset of ML that uses artificial neural networks
    to learn from data. Neural networks are inspired by the human brain, and they
    are able to learn complex patterns from data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generative models** are a type of ML model that can generate new data. Generative
    models are trained on a set of data, and then they can use that data to create
    new data that is similar to the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language models** are statistical models that predict tokens (typically words)
    in a sequence. Some of these models, which are capable of more complex tasks,
    consist of many parameters (on the order of billions or even trillions), therefore
    they are called **large language model**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The main difference between generative models and other types of ML models
    is that generative models do not just make predictions or decisions. They can
    actually create new data. This makes generative models very powerful, and they
    can be used for a variety of tasks, such as generating images, text, music, and
    video.Here is a table that summarizes the differences between AI, ML, DL, language
    model, and generative models:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Term** | **Definition** |'
  prefs: []
  type: TYPE_TB
- en: '| Artificial intelligence | A broad field of computer science that deals with
    the creation of intelligent agents. |'
  prefs: []
  type: TYPE_TB
- en: '| Machine learning | A subset of AI that deals with the development of algorithms
    that can learn from data. |'
  prefs: []
  type: TYPE_TB
- en: '| Deep learning | A subset of ML that uses artificial neural networks to learn
    from data. |'
  prefs: []
  type: TYPE_TB
- en: '| Generative model | A type of ML model that can generate new data. |'
  prefs: []
  type: TYPE_TB
- en: '| Language model | A type of model, nowadays mostly a deep learning model,
    that predicts tokens in context. |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 1.2: Terminology - Artificial Intelligence, Machine Learning, Deep Learning,
    and Generative Model.Generative models are a powerful type of AI that can generate
    new data samples that resemble the training data. Generative AI models have come
    a long way, enabling the generation of new examples from scratch using patterns
    in data. These models can handle different types of data and are employed across
    various domains, including text generation, image generation, music generation,
    and video generation.For language models, it''s important to note that some of
    them, particularly newer generations, are generative, in the sense that they can
    produce language (text), others are not. These generative models facilitate the
    creation of **synthetic data** to train AI models when real data is scarce or
    restricted. This type of data generation reduces labeling costs and improves training
    efficiency. Microsoft Research took this approach ("Textbooks Are All You Need",
    June 2023) for training their phi-1 model, where they created synthetic text books
    and exercises with GPT-3.5 as their training dataset.In the following sections,
    we''ll look at the different domains of generative models such as text, images,
    sound, video. The applications mostly revolve around content generation, editing,
    and processing (recognition). Let''s start with text!'
  prefs: []
  type: TYPE_NORMAL
- en: Text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Text generation, such as GPT-4 by OpenAI, can generate coherent and grammatically
    correct poems, or code in different languages and extract features like keywords
    and topics. These models have practical applications in fields like content creation
    and **natural language processing** (**NLP**), where the ultimate goal is to create
    algorithms capable of interpreting human language.Language modeling aims to predict
    the next word, character, or even sentence based on the previous ones in a sequence.
    In this sense, language modeling serves as a way of encoding the rules and structures
    of a language in a way that can be understood by a machine. Large language models
    capture the structure of human language in terms of grammar, syntax, and semantics.
    These models are important as they form the backbone of a number of larger NLP
    tasks such as content creation, translation, summarization, machine translation,
    and text editing tasks such as spelling correction.At its core, language modeling,
    and more broadly Natural Language Processing, rely heavily on the quality of the
    representation learning. A well-trained language model encodes information about
    the text it''s trained on and generates new text based on those learnings, thereby
    taking on the task of text generation.Recently, large language models have found
    application for tasks like essay generation, code development, translation, and
    understanding genetic sequences. More broadly applications of language models
    involve multiple areas, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Question answering**: AI chatbots and virtual assistants can provide personalized
    and efficient assistance, reducing response times in customer support and thereby
    enhancing customer experience. These systems can be used for solving specific
    problems like restaurant reservations and ticket booking.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automatic summarization**: Language models can create concise summaries of
    articles, research papers, and other content, enabling users to consume and understand
    information rapidly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentiment analysis**: Analyzing opinions and emotions in texts, language
    models can help businesses understand customer feedback and opinions more efficiently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Topic modeling** and **semantic search**: These models can identify, categorize
    by topics, and compress documents into concise vectors, making content management
    and discovery easier for organizations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine translation**: AI-powered language models can translate texts from
    one language into another, supporting businesses in their global expansion efforts.
    New generative models can perform competitively with commercial products (for
    example Google translate).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite the remarkable achievements, language models still face limitations
    when dealing with complex mathematical or logical reasoning tasks. It remains
    uncertain whether continually increasing the scale of language models will inevitably
    lead to new reasoning capabilities. As mentioned, we also have to consider the
    importance of data quality and scale, as these factors play a significant role
    in improving language model performance in different tasks and areas.Another domain
    for generative models is image generation, let's see what that's about!
  prefs: []
  type: TYPE_NORMAL
- en: Images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Generative AI is extensively used in generating 3D images, avatars, videos,
    graphs, illustrations for virtual or augmented reality, video games graphics design,
    logo creation, image editing or enhancement.This graph illustrates image generation
    from a text prompt with stable diffusion (Source: "Restart Sampling for Improving
    Generative Processes" by Yilun Xu and others at MIT and Google Research, June
    2023; https://arxiv.org/pdf/2306.14878.pdf):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3: Image generation from a text prompt "A transparent sculpture
    of a duck made out of glass".](../media/file2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: Image generation from a text prompt "A transparent sculpture of
    a duck made out of glass".'
  prefs: []
  type: TYPE_NORMAL
- en: 'With stable diffusion models, you can see a wide variety of outcomes using
    only minimal changes to the initial setting of the model or - as in this case
    - numeric solvers and samplers. Although, they sometimes produce striking results,
    this instability and inconsistency is a significant challenge to applying these
    models more broadly.Services like **MidJourney**, **DALL-E 2**, and **Stable Diffusion**
    provide creative and realistic images derived from textual input or other images.
    Services like **DreamFusion**, **Magic3D**, and **Get3D** enable users to convert
    textual descriptions into 3D models and scenes, driving innovation in design,
    gaming, and virtual experiences.There are three main applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image generation**: models can generate images, such as paintings, photographs,
    and sketches. This can be used for a variety of purposes, such as creating art,
    designing products, and generating realistic visual effects.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Image editing**: Models can perform tasks such as removing objects, changing
    colors, and adding effects. This can be used to improve the quality of images,
    and to make them more visually appealing.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Image recognition**: large foundation models can be used to recognize images
    including classifying scenes, but also object detection, for example detecting
    faces.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Models like Generative Adversarial Networks (GANs) and DALL-E. GANs generate
    realistic images that have numerous business applications, while DALL-E creates
    images from textual descriptions, which is helpful in creative industries for
    designing advertisements, products, and fashion.Image editing involves modifying
    an image's semantics by changing content or style attributes using techniques
    like facial attribute editing or image morphing. Optimization- and learning-based
    approaches generate images with styles obtained via latent representations of
    pre-trained GAN models like StyleGAN. Diffusion models have recently been used
    for advanced image editing tasks such as connecting manually designed masked regions
    seamlessly or generating 3D object manipulations through text guidance. These
    techniques enable flexible image generation but face limited diversity issues
    that can be mitigated by incorporating other text inputs into the process.Into
    the category of image editing fall also tasks such as image restoration, which
    means restoring clean images from their degraded versions, which involves tasks
    like image super-resolution, inpainting, denoising, dehazing, and deblurring.
    Deep learning-based methods using CNN and transformer architectures are prevalent
    due to superior visual quality compared to traditional approaches. Generative
    models like GANs and Diffusion Models (DMs) are used for restoration but can suffer
    from complex training processes and mode collapse. Multi-distortion datasets and
    single-network approaches with attention modules or guiding sub-networks improve
    effectiveness for handling multiple degradation types.We'll see next what models
    can do with sound and music.
  prefs: []
  type: TYPE_NORMAL
- en: Sound and Music
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Generative models can develop songs and audio clips based on text inputs, recognize
    objects in videos and create accompanying audio, and create custom music. We can
    classify applications again roughly into generation, editing, and recognition:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Music generation**: Generative models can be used to generate music, such
    as songs, beats, and melodies. This can be used for a variety of purposes, such
    as creating new music, composing soundtracks, and generating personalized playlists.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sound editing**: Generative models can be used to edit sound, such as removing
    noise, changing pitch, and adding effects. This can be used to improve the quality
    of sound, and to make it more sonically appealing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sound recognition**: Generative models can be used to recognize sound, such
    as identifying instruments, classifying genres, and detecting speech. This can
    be used for a variety of purposes, such as music analysis, search, and recommendation
    systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Music generation algorithms started with algorithmic composition in the 1950s
    and have seen recent innovations like Google's WaveNet and OpenAI's Jukebox. These
    models have led to AI composer assistants, which can generate music in various
    styles and enable newer applications like speech synthesis.As a special case,
    speech-to-text generation, also known as **automatic speech recognition** (**ASR**),
    is the process of converting spoken language into text. They are trained on sounds
    and texts. ASR systems are becoming increasingly accurate, and are now used in
    a wide variety of applications. However, there are still some challenges that
    need to be addressed, such as the ability to handle noisy environments and different
    accents. With many potential applications such as voice dialing and computer-assisted
    personal assistance like Alexa and Siri, the technology behind ASR evolved from
    Markov Models to rely on GPTs.We'll see videos next.
  prefs: []
  type: TYPE_NORMAL
- en: Video
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Video generation models like **DeepMind''s** Motion to Video and **NVIDIA''s**
    **Vid2Vid** rely on **GANs** for high-quality video synthesis. They can convert
    videos between different domains, modify existing videos, and animate still images,
    showing great potential for video editing and media production.Tools like Make-a-Video
    and Imagen Video convert natural language prompts into video clips, simplifying
    video production and content creation processes. The broad classes of applications
    are these:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Video generation**: Generative models can be used to generate videos, such
    as short films, animations, and commercials. This can be for creating new content,
    advertising products, and generating realistic visual effects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Video editing**: We can edit videos, such as removing objects, changing colors,
    and adding effects. This can help to improve the quality of videos, and to make
    them more visually appealing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Video recognition**: Models can recognize video, such as identifying objects,
    classifying scenes, and detecting faces. This can help for applications such as
    security, search, and recommendation systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some models can generate content in more than one domain or modality. These
    are called multi-modal models.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Modal
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Multi-modal generative models can generate **text**, **images**, **sound**,
    and **video**. This allows them to create more realistic and immersive experiences.
    Multi-modal models are still in their early stages of development, but they have
    the potential to revolutionize the way we interact with computers and the way
    we experience the world. For example, these advancements have significantly improved
    performance in image captioning tasks, the process of describing an image''s content
    through natural language.Multi-modal models adopt generative architectures that
    fuse images and captions into a single model for shared learning space. The process
    involves a two-step encoder-decoder architecture: visual encoding and language
    decoding.We can distinguish these potential use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Virtual reality**: These models can be used to create virtual reality experiences
    that are more realistic and immersive. This can help in gaming, education, and
    training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Augmented reality**: They can create augmented reality experiences that overlay
    digital content on the real world. This is useful for navigation, shopping, and
    entertainment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we'll discuss the technical background of large language
    models.
  prefs: []
  type: TYPE_NORMAL
- en: What is a GPT?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Large Language Models** (**LLMs**) are deeply trained neural networks adept
    at understanding and generating human language. The current generation of LLMs
    such as ChatGPT are deep neural network architectures that utilize the Transformer
    model and undergo pre-training using unsupervised learning on extensive text data,
    enabling it to learn language patterns and structures.The notable strength of
    the latest generation of LLMs as conversational interface (ChatBot) lies in their
    ability to generate coherent and contextually appropriate responses, even in open-ended
    conversations. This generating the next word based on the preceding words repeatedly,
    the model produces fluent and coherent text often indistinguishable from text
    produced by humans. However, ChatGPT has been observed to "sometimes write plausible-sounding
    but incorrect or nonsensical answers" as expressed in a disclaimer by **OpenAI**.
    This is referred to as hallucination and is just one of the concerns around **LLMs**.A
    **Transformer**, is a deep learning architecture, first introduced in 2017 by
    researchers at Google and the University of Toronto (in an article called "Attention
    is All You Need"), which comprises self-attention and feed-forward neural networks,
    allowing it to effectively capture the word relationships in a sentence. The attention
    mechanism enables the model to focus on different parts of the input sequence.**Generative
    Pre-Trained Transformers** (**GPTs**) were introduced by researchers at OpenAI
    in 2018 together with the first of their eponymous GPT models, GPT-1, and published
    as "Improving Language Understanding by Generative Pre-Training". The pre-training
    process involves predicting the next word in a text sequence, enhancing the model''s
    grasp of language as measured in the quality of the output. Following pre-training,
    the model can be fine-tuned for specific language processing tasks like sentiment
    analysis, language translation, or chat. This combination of unsupervised and
    supervised learning enables GPT models to perform better across a range of NLP
    tasks and reduces the challenges associated with training LLMs.The size of the
    training corpus for LLMs has been increasing drastically. GPT-1, introduced by
    OpenAI in 2018, was trained on BookCorpus with 985 million words. BERT, released
    in the same year, was trained on a combined corpus of **BookCorpus** and **English
    Wikipedia**, totalling **3.3 billion words**. Now, training corpora for LLMs reach
    up to trillions of tokens.This graph illustrates how LLMs have been growing very
    large:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4: Large Language Models from BERT to GPT-4 - size, training budget,
    and organizations.](../media/file3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.4: Large Language Models from BERT to GPT-4 - size, training budget,
    and organizations.'
  prefs: []
  type: TYPE_NORMAL
- en: The size of the data points indicates training cost in terms of petaFLOP days.
    For some models, especially for proprietary and closed-source models this information
    is not known - in these cases, I've placed a cross. For example, for XLNet, the
    paper doesn't give the information about compute in flops, however, the training
    was done on 512 TPU v3 chips over 2.5 days. The colors of the data points show
    the company or organization developing the model - since these might not come
    out in the print or on the Kindle (unless you have a Kindle color), you can find
    a color version of this graph at this URL:The development of GPT models has seen
    significant progress, with OpenAI's GPT-n series leading the way in creating foundational
    AI models. The size of the training corpus for LLMs has been increasing drastically.
    GPT-1, introduced by OpenAI in 2018, was trained on BookCorpus with 985 million
    words. BERT, released in the same year, was trained on a combined corpus of BookCorpus
    and English Wikipedia, totaling 3.3 billion words. Now, training corpora for LLMs
    reach up to trillions of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'A **foundation model** (sometimes: base model) is a large model, which was
    trained on an immense quantity of data at scale so that it can be adapted to a
    wide range of downstream tasks. In GPT models, this pre-training is done self-supervised
    learning.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Trained on 300 billion tokens, **GPT-3** has **175 billion parameters**, an
    unprecedented size for deep learning models. **GPT-4** is the most recent in the
    series, though its size and training details have not been published due to competitive
    and safety concerns, however, different estimates were putting it between **200
    and 500 billion parameters**. *Sam Altman*, the CEO of OpenAI has stated that
    the cost of training **GPT-4** was more than $100 million.ChatGPT, a conversation
    model, was released by **OpenAI** in November 2022\. Based on prior **GPT** models
    (particularly **GPT-3**) and optimized for dialogues, it uses a combination of
    human-generated roleplaying conversations and a dataset of human labeler demonstrations
    of the desired model behavior. The model exhibits excellent capabilities such
    as wide-ranging knowledge retention and precise context tracking in multi-turn
    dialogues.Another substantial advancement came with **GPT-4**, which extends beyond
    text input to include multimodal signals, in March 2023\. **GPT-4** provides superior
    performance on various evaluation tasks coupled with significantly better response
    avoidance to malicious or provocative queries due to six months of iterative alignment
    during training.Other notable foundational GPT models beside OpenAI's includes
    Google's **PaLM2**. Although GPT-4 leads most benchmarks in performance, these
    and other models demonstrate a comparable performance in some tasks, and have
    contributed to advancements in generative transformer-based language models. Meta's
    **LLaMA** was trained on **1.4 trillion tokens**, while **PaLM2**, the model behind
    Google's chatbot, **Bard**, consists of **340 billion parameters** - smaller than
    previous LLMs - appears to have a larger scale of training data in at least 100
    languages.
  prefs: []
  type: TYPE_NORMAL
- en: There are quite a few **companies and organizations developing LLMs**, and they
    are releasing them on different terms. OpenAI has released GPT-2 and subsequent
    models have been closed-source, but open for usage on their website or through
    an API. Meta is releasing models from RoBERTa, BART to LLaMA including parameters
    (the weights) of the models, although under a non-commercial license, and the
    source code for setting up and training the models. Google AI and their DeepMind
    division have developed a number of large language models, including BERT, GPT-2,
    LaMDA, Chinchilla, Gopher, PaLM, and PaLM2\. They've been releasing the code and
    weights of a few of their models under open-source licensing, even though recently
    they have moved towards more secrecy in their development. Microsoft has developed
    a number of large language models, including Turing NLG and Megatron-Turing NLG,
    however, they have integrated OpenAI models into Microsoft 365 and Bing. Technology
    Innovation Institute (TII), an Abu Dhabi government funded research institution,
    has open-sourced Falcon LLM for research and commercial usage.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: GPT models can also work with modalities beyond text for input and output, as
    seen in GPT-4's ability to process image input alongside text. Additionally, they
    serve as a foundation for text-to-image technologies like diffusion and parallel
    decoding, enabling the development of **visual foundation models** (**VFMs**)
    for systems that work with images.In summary, GPT models have evolved rapidly,
    enabling the creation of versatile foundational AI models suitable for a wide
    range of downstream tasks and modalities, ultimately driving innovation across
    various applications and industries.In the next section, we'll review the progress
    deep learning and generative models have been making over recent years that leads
    up to the current explosion of apparent capabilities and the attention these models
    have been getting.
  prefs: []
  type: TYPE_NORMAL
- en: Why now?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The success of generative AI coming into the public spotlight in 2022 can be
    attributed to several interlinked drivers. The development and success of generative
    models have relied on improved algorithms, considerable advances in compute power
    and hardware design, the availability of large labelled datasets, and an active
    and a collaborative research community helping evolve a set of tools and techniques.The
    development of more sophisticated mathematical and computational methods has played
    a vital role in the advancement of generative models. The backpropagation algorithm
    introduced in the 1980s by Geoffrey Hinton, David Rumelhart, and Ronald Williams
    is one such example. It provided a way to effectively train multi-layer neural
    networks.In the 2000s, **neural networks** began to regain popularity as researchers
    developed more complex architectures. However, it was the advent of deep learning,
    a type of neural network with numerous layers, which marked a significant turning
    point in the performance and capabilities of these models. Interestingly, although
    the concept of deep learning had existed for some time, the development and expansion
    of generative models correlate with significant advances in hardware, particularly
    **graphics processing units** (**GPUs**), which have been instrumental in propelling
    the field forward.As mentioned, the availability of cheaper and more powerful
    hardware has been a key factor in the development of deeper models. This is because
    deep learning models require a lot of computing power to train and to run. This
    concerns all aspects of processing power, memory, and disk space. This graph shows
    the cost of computer storage over time for different mediums such as disks, solid
    state, flash, and internal memory in terms of price in dollars per terabyte (source:
    Our World in Data; [https://ourworldindata.org/moores-law](https://ourworldindata.org/moores-law)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5: Cost of computer storage since the 1950s in dollars per terrabyte.](../media/file4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.5: Cost of computer storage since the 1950s in dollars per terrabyte.'
  prefs: []
  type: TYPE_NORMAL
- en: While the past, training a deep learning model was prohibitively expensive,
    as the cost of hardware has come down, it has become possible to train bigger
    models on much larger datasets. The model size is one of the factors determining
    how well a model can approximate (as measured in perplexity) the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**The importance of the number of parameters in an LLM**: The more parameters
    a model has, the higher its capacity to capture relationships between words and
    phrases as knowledge. As a simple example for these higher-order correlations,
    an LLM could learn that the word "cat" is more likely to be followed by the word
    "dog" if it is preceded by the word "chase," even if there are other words in
    between. Generally, the lower a model''s perplexity, the better it will perform,
    for example in terms of answering questions. Particularly, it seems that in models
    consisting of in the range of between 2 to 7 billion parameters new capabilities
    emerge such as the ability to generate different creative text formats, like poems,
    code, scripts, musical pieces, email, letters, and answer questions in an informative
    way, even if they are open-ended and challenging.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This trend towards larger models started around 2009, when Nvidia catalyzed
    what is often called the "big bang" of deep learning. GPUs are particularly well-suited
    for the matrix/vector computations necessary to train deep-learning neural networks,
    therefore significantly increasing the speed and efficiency of these systems by
    several orders of magnitude, and reducing running times from weeks to days. In
    particular, Nvidia''s **CUDA** platform, which allows direct programming of GPUs,
    has made it easier than ever for researchers and developers to experiment with
    and deploy complex generative models.In the 2010s, different types of generative
    models started getting traction. Autoencoders, a kind of neural network that can
    learn to compress data from the input layer to a representation, and then reconstructs
    the input, served as a basis for more advanced models like **Variational Autoencoders**
    (**VAEs**) first proposed in 2013\. **VAEs**, unlike traditional autoencoders,
    use variational inference to learn the distribution of data, also called the latent
    space of input data.Around the same time, **Generative Adversarial Networks**
    (**GANs**) were proposed by Ian Goodfellow and others in 2014\. The setup for
    training GANs is illustrated in this diagram (taken from "A Survey on Text Generation
    using Generative Adversarial Networks", G de Rosa and J P. Papa, 2022; [https://arxiv.org/pdf/2212.11119.pdf](https://arxiv.org/pdf/2212.11119.pdf)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.6: Generative Adversarial Network (GAN) training.](../media/file5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.6: Generative Adversarial Network (GAN) training.'
  prefs: []
  type: TYPE_NORMAL
- en: The GANs consist of two networks that are pitted against each other in a game-like
    setting - the generator that generates new data, often images, and the discriminator
    which estimates the probability of the new data being real. As they compete against
    each other, GANs gets better at its task, being able to generate realistic images
    and other types of data.Over the past decade, significant advancements have been
    made in the fundamental algorithms used in deep learning, such as better optimization
    methods, more sophisticated model architectures, and improved regularization techniques.Transformer
    models, introduced in 2017, built upon this progress and enabled the creation
    of large-scale models like GPT-3\. Transformers rely on attention mechanisms,
    and resulted in a further leap in the performance of generative models. These
    models, such as Google's BERT and OpenAI's GPT series, can generate highly coherent
    and contextually relevant text.The development of transfer learning techniques,
    which allow a model pre-trained on one task to be fine-tuned on another, similar
    task, has also been significant. These techniques have made it more efficient
    and practical to train large generative models.Moreover, part of the rise of generative
    models can be attributed to the development of software libraries and tools (**TensorFlow**,
    **PyTorch**, **Keras**) specifically designed to work with these artificial neural
    networks, streamlining the process of building, training, and deploying them.To
    further drive the development of generative models, the research community has
    regularly held challenges like ImageNet for image classification, and has started
    to do the same for generative models, with competitions such as the Generative
    Adversarial Networks (GAN) Competition.In addition to the availability of cheaper
    and more powerful hardware, the availability of large datasets of labeled data
    has also been a key factor in the development of generative models. This is because
    deep learning models and generative models in particular require vast amounts
    of text data for effective training. The explosion of data available from the
    internet, particularly in the last decade, created the suitable environment for
    such models to thrive. As the internet has become more popular, it has become
    easier to collect large datasets of text, images, and other data. This has made
    it possible to train generative models on much larger datasets than would have
    been possible in the past.In summary, generative modelling is a fascinating and
    rapidly evolving field. It has the potential to revolutionize the way we interact
    with computers and the way we create new content. I am excited to see what the
    future holds for this field.Let's get into the nitty gritty - how does this work?
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Models such as BERT and GPT were made possible by the **Transformer** deep
    neural network architecture, which has been a game changer for natural language
    processing. Designed to avoid recursion in order to allow parallel computation,
    the Transformer architecture, in different variations, is continuing to push the
    boundaries of what''s possible within the field of Natural Language Processing
    and Generative AI.One of the defining features of Transformers is the attention
    mechanism. Traditional sequence-to-sequence models often suffered from the problem
    of handling long dependencies - they had difficulty remembering relevant information
    if the sequences were too long. Transformer models introduced attention mechanisms
    to navigate this problem. The Self-Attention mechanism, often referred to as the
    core of the Transformer model, assigns a score to each word in the sequence, determining
    how much focus should be given to that word.Transformers consist of modules, which
    can be stacked, thereby creating very large models that can learn massive datasets.
    These are indicated in the diagram here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.7: The Transformer architecture (credit: Yuening Jia, Wikimedia
    Commons)](../media/file6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.7: The Transformer architecture (credit: Yuening Jia, Wikimedia Commons)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The architectural features that have contributed to the success of Transformers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Encoder-Decoder structure**: The Transformer model follows an Encoder-Decoder
    structure. The encoder takes the input sequence and computes a series of representations,
    (contextual embeddings), for each word. These representations consider not only
    the inherent meaning of the words (their semantic value) but also their context
    in the sequence. The decoder then uses this encoded information to generate the
    output sequence one item at a time, using the context of the previously generated
    items.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Positional Encoding**: Since Transformer doesn''t process words sequentially
    but instead processes all words simultaneously, it lacks any notion of the order
    of words. To remedy this, information about the position of words in the sequence
    is injected into the model using positional encodings. These encodings are added
    to the input embeddings representing each word, thus allowing the model to consider
    the order of words in a sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layer Normalization**: To stabilize the network''s learning, Transformer
    uses a technique called Layer Normalization. This technique normalizes the model''s
    inputs across the features dimension (instead of the batch dimension as in Batch
    Normalization), thus improving the overall speed and stability of learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-Head Attention**: Instead of applying attention once, the Transformer
    applies it multiple times in parallel — improving the model''s ability to focus
    on different types of information and thus capturing a richer combination of features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another, optional architectural feature, which is not specific to Transformers
    are Skip Connections (also: Residual Connections). These were introduced to mitigate
    degradation problems as networks get deeper, Skip Connections are used. This allows
    gradients to flow unchanged across layers by shortcutting the inputs to deeper
    layers.Transformers have pushed the envelope in NLP, especially in translation
    and language understanding. **Neural machine translation** (**NMT**) is a mainstream
    approach to machine translation that uses deep learning to capture long-range
    dependencies in the sentence. **NMT** based on Transformers outperformed previous
    approaches such as using recurrent neural networks, in particular **long short-term
    memory** (**LSTM**) networks. This can be attributed to this powerful architecture
    including first and foremost attention, which allows the Transformer model to
    treat word order in a flexible manner, no matter how far apart they are, that
    is optimal for each specific situation.Furthermore, the combination of these architectural
    features allows it to successfully tackle tasks that involve understanding and
    generating human language and other domains. OpenAI''s powerful GPT models for
    language generation is a Transformer as well, as is **DeepMind''s AlphaFold 2**,
    a model that predicts protein structure from their genetic sequences.Transformers
    have been able to maintain performance across longer sequences better than other
    models, for example recurrent neural networks. This has been contributing to their
    success, however, the transformer architecture means that they can capture only
    dependencies within a fixed input width. Early attention mechanisms scaled quadratically
    with the number of data points, rendering them inapplicable to settings with large
    amounts of inputs. There have been many proposed approaches to obtain efficiency
    gains such as sparse, low-rank self-attention, and latent bottlenecks to name
    just a few. Other work tried to extend sequences beyond the fixed input size,
    architectures such as Transformer-XL reintroduce recursion by storing hidden states
    of already encoded sentences to leverage them in the subsequent encoding of the
    next sentences.The particularity of GPTs and the origin of their name is the pre-training.
    Let''s see how these LLMs are trained!'
  prefs: []
  type: TYPE_NORMAL
- en: Pre-training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The first step in training a **Large Language Model** (**LLM**) is tokenization.
    This process involves converting words into numbers so that they can be processed
    by the model given that **LLMs** are mathematical functions that require numerical
    inputs and outputs.To carry out this tokenization, **LLMs** use a unique tokenizer.
    Tokenizers map words in text to corresponding lists of integers. Before training
    the **LLM**, the tokenizer is typically fitted to the entire training dataset
    and then frozen. A common type of tokenizer employed is byte pair encoding. It's
    important to note that tokenizers do not produce arbitrary integers. Instead,
    they output integers within a specific range - from to where represents the vocabulary
    size of the tokenizer.Now, considering outputs, when LLM receives a text, it mainly
    yields a vector falling within . This output vector is then passed through a softmax
    function to yield another vector, which is referred to as a probability vector.
    With its entries being non-negative and summing up to , this vector can be interpreted
    as the probability distribution over the vocabulary of the **LLM**.Also, it is
    necessary to point out that **LLMs** can only generate a token based on a sequence
    of tokens that does not exceed its context window. This context window refers
    to the length of the longest sequence of tokens that a **LLM** can use. If a sequence
    longer than this window is presented, the **LLM** would need to either truncate
    the sequence or employ algorithmic modifications to handle it. Typical context
    window sizes for **LLMs** can range from about 1,000 to 10,000 tokens.Training
    **LLMs** involves a specific process of tokenizing input data, feeding it into
    the model, and generating an output that is a probability distribution over the
    model's vocabulary. The specific mechanisms within this process, such as the softmax
    function and context window, help to facilitate **LLMs** understanding of and
    response to input data.**Negative Log-Likelihood** (**NLL**) and **Perplexity**
    (**PPL**) are important metrics used in the process of training and evaluating
    language models. **NLL** is a loss function used in machine learning algorithms,
    aimed at maximizing the probability of correct predictions. A lower **NLL** indicates
    that the network has successfully learned patterns from the training set, and
    therefore it will be able to accurately predict the labels of the training samples.
    It's important to mention that **NLL** is a value constrained within a positive
    interval.**Perplexity** (**PPL**), on the other hand, is an exponentiation of
    the **NLL**, providing a more intuitive way to understand the model's performance.
    Smaller **PPL** values indicate a well-trained network that can predict accurately
    while higher values indicate poor learning performance. Intuitively, we could
    say that a low perplexity means that the model is less surprised by the next word.
    Therefore, the goal in pre-training is to minimize perplexity which means the
    model's predictions align more with the actual outcomes.In comparing different
    language models, perplexity is often used as a benchmark metric across various
    tasks. It gives an idea about how well the language model is performing, where
    a lower perplexity indicates the model is more certain of its predictions. Hence,
    a model with lower perplexity would be considered better performing in comparison
    to others with higher perplexity.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is worth talking at least briefly about the choice of architecture, and why
    these models are as large as they are. In a paper from 2020 by researchers from
    OpenAI, Kaplan and others discussed scaling laws and choice of parameters.Interestingly,
    they compare lots of different architecture choices, and, among other things,
    show that transformers outperform **LSTMs** as language models in terms of perplexity
    in no small part due to improved use of long contexts - while these recurrent
    networks plateau after less than 100 tokens, transformers improve through the
    whole context. Therefore, transformers come not only with better training and
    inference speed with respect to the transformer, but also give better performance
    when looking at relevant contexts.Further, they found a power-law relationship
    of dataset size, model size (number of parameters), and the amount of compute
    for training, in the sense that in order to improve performance by a certain factor,
    one of these factors have to scaled-up by as the power of the factor, however,
    for optimal performance all three factors must be scaled in tandem in order not
    to get a bottleneck effect.Researchers at **DeepMind** (Hoffmann and others, 2022)
    analyzed training compute and dataset size of **LLMs**, and concluded that a **LLMs**
    are undertrained in terms of compute budget and dataset size as suggested by scaling
    laws. They predicted that large models would perform better if substantially smaller
    and trained much longer than, and - in fact - validated their prediction comparing
    a **70 billion parameter Chinchilla model** on a benchmark to their Gopher model,
    which consists of **280 billion parameters**.More recently, the team at has been
    found that longer training in terms of epochs or more compute in terms of petaflops
    didn't seem to improve performance anymore, and smaller networks and higher-quality
    datasets can give very competitive performance.
  prefs: []
  type: TYPE_NORMAL
- en: Conditioning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Conditioning large language models refers to adapting the model for specific
    tasks. Different methods of conditioning include fine-tuning, prompting, instruction
    tuning, and reinforcement learning:'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning involves modifying a pre-trained language model by training it on
    a specific task using supervised learning. For example, to make a model more amenable
    to chats with humans, the model is trained on examples of tasks formulated as
    natural language instructions (instruction tuning).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompting techniques present problems as text prompts and expect model completions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For fine-tuning, usually, reinforcement learning combines supervised fine-tuning
    with reinforcement learning using human feedback to train the model based on human
    preferences. LLMs can be trained on a training set of examples which are themselves
    generated by an LLM (bootstrapped from a small initial set of human-generated
    examples) such as in the training set for phi-1 by Microsoft Research ("Textbooks
    Are All You Need", June 2023).With prompting techniques text examples of similar
    problems and their solutions will be presented. Zero-shot prompting involves no
    solved examples, while few-shot prompting includes a small number of examples
    of similar (problem, solution) pairs.These conditioning methods continue to evolve,
    becoming more effective and useful for a wide range of applications. Prompt engineering
    and conditioning methods will be explored further in *Chapter 8*, *Prompt Engineering*.
  prefs: []
  type: TYPE_NORMAL
- en: How to give it a go?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can access OpenAI's model through their website or through their API. If
    you want to try other large language models on your laptop, open-source LLMs are
    a good place to get started. There is a whole zoo of stuff out there!You can access
    these models through Hugginface or other providers. You can even download them,
    finetune them, or - if you are feeling really fancy - fully train a model. We'll
    look at using these models in more detail in *Chapter 9*. *LLM applications in
    Production*.In the next section, we'll look at stable diffusion and how it works.
  prefs: []
  type: TYPE_NORMAL
- en: What is a Stable Diffusion model?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image generation models are a type of generative model that can be used to generate
    images. Image generation models are a powerful tool that can be used to generate
    realistic and creative images. They are still in their early stages of development,
    but they have the potential to revolutionize the way we create and consume images.One
    of the most popular image generation models is **Stable Diffusion**, another one
    is **Midjourney**. In simplest terms these are a deep learning models that creates
    images given text prompts. Google Brain announced the creation of two text-to-image
    models, **Imagen** and **Parti**, in 2022.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **Stable Diffusion** model is a deep learning, text-to-image model developed
    by researchers from the CompVis Group at Ludwig Maximilian University of Munich
    and Runway. It generates detailed images conditioned on text descriptions and
    utilizes a latent diffusion model architecture. The model''s source code and even
    the weights have been publicly released under the **CreativeML** **OpenRAIL**-**M
    License**, which "does not impose any restrictions on reuse, distribution, commercialization,
    adaptation." The model can be run on consumer hardware equipped with a modest
    GPU (for example the **GeForce 40** series).**Stable Diffusion** is a type of
    diffusion model that uses a Gumbel distribution to add noise to the image. The
    Gumbel distribution is a continuous probability distribution that is often used
    in machine learning because it is easy to sample from and it has the property
    to be more stable. Stability means that the model is less likely to get stuck
    in local minima, which can happen with other types of diffusion models.The model
    consists of a **variational autoencoder** (**VAE**), a **U-Net**, and a **text
    encoder**. The **VAE** has two parts, an encoder and a decoder, compressing the
    original high-dimensional image into a lower dimensional latent space and reconstructing
    it back into the image space. The latent space significantly decreases computational
    complexity, making the diffusion process faster.The **VAE** encoder compresses
    images into a latent space, while the **U-Net** performs denoising from forward
    diffusion to obtain a latent representation. The **VAE** decoder then generates
    the final image. The model can be flexibly conditioned on various modalities,
    including text, and leverages a cross-attention mechanism to incorporate conditioning
    information.A **U-Net** is a popular type of convolutional neural network (CNN)
    that has a symmetric encoder-decoder structure. It is commonly used for image
    segmentation tasks, but in the context of Stable Diffusion, it is utilized for
    predicting noise in the image. The U-Net takes the noisy image as input and processes
    it through a series of convolutional layers to extract features and learn representations.
    These convolutional layers, typically organized in a contracting path, reduce
    the spatial dimensions while increasing the number of channels.Once the contracting
    path reaches the bottleneck of the U-Net, it then expands through a symmetric
    expanding path. In the expanding path, transposed convolutions (also known as
    upsampling or deconvolutions) are applied to progressively upsample the spatial
    dimensions while reducing the number of channels.During the diffusion process,
    the U-Net''s expanding path takes the noisy image and reconstructs the latent
    representation from the forward diffusion. By comparing the reconstructed latent
    representation with the true latent representation, the U-Net predicts an estimation
    of the noise in the original image. This prediction of noise helps in the reverse
    diffusion process to recover the original image.Diffusion models operate through
    a process similar to diffusion in physics. It follows a **forward diffusion process**
    by adding noise to an image until it becomes uncharacteristic and noisy. This
    process is analogous to an ink drop falling into a glass of water and gradually
    diffusing.The unique aspect here is the **reverse diffusion process**, where the
    model attempts to recover the original image from a noisy, meaningless image.
    This result is achieved by subtracting estimated noise from the noisy image step-by-step,
    ultimately restoring an image resembling the original image.The denoising process
    is demonstrated in this plot (source: User Benlisquare on Wikimedia Commons):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.8: European-style castle in Japan, created using the Stable Diffusion
    V1-5 AI diffusion model. Only steps within a 40-step generation process are shown.](../media/file7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.8: European-style castle in Japan, created using the Stable Diffusion
    V1-5 AI diffusion model. Only steps within a 40-step generation process are shown.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the plot, you can see the image generation step by step, a U-Net denoising
    process using the DDIM sampling method, which repeatedly removes Gaussian noise,
    and then decodes the denoised output into pixel space.Stable Diffusion is a deep
    learning model that leverages a diffusion process to generate images from text
    prompts through several clear steps:'
  prefs: []
  type: TYPE_NORMAL
- en: It starts by producing a random tensor (random image) in the latent space, which
    serves as the noise for our initial image. ]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A noise predictor (**U-Net**) takes in both the latent noisy image and the provided
    text prompt and predicts the noise. ]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model then subtracts the latent noise from the latent image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Steps 2 and 3 are repeated for a set number of sampling steps, for instance,
    40 times as shown in the plot.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the decoder component of the **VAE** transforms the latent image back
    into pixel space, providing the final output image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During the training of image generation models, a loss function is used to evaluate
    the quality of the generated images. One commonly used loss function is the **mean
    squared error** (**MSE**) loss, which quantifies the difference between the generated
    image and the target image. The model is optimized to minimize this loss, encouraging
    it to generate images that closely resemble the desired output.The model was trained
    on a dataset called **LAION-5B**, derived from Common Crawl data, comprising billions
    of image-text pairs. The training dataset was classified based on language, resolution,
    watermark likelihood, and aesthetic score. Stable Diffusion was trained on subsets
    of this dataset. The model's training data had a diverse range of sources, with
    a significant portion coming from websites such as **Pinterest**, **WordPress**,
    **Blogspot**, **Flickr**, **DeviantArt**.Overall, image generation models such
    as Stable Diffusion and Midjourney process textual prompts into generated images,
    leveraging the concept of forward and reverse diffusion processes and operating
    in a lower dimensional latent space for efficiency.There are two main model versions
    so far of **Stable Diffusion**, **version 1** and **2**. Let's see how they differ.
  prefs: []
  type: TYPE_NORMAL
- en: Model differences
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Stable Diffusion v1** and **v2** are different in terms of text processing,
    training data and their results. In terms of text processing, **Stable Diffusion
    v2** uses **OpenClip** for text embedding while **v1** uses **Open AI''s** **CLIP
    ViT-L/14** for text embedding. **OpenClip** is five times larger than **CLIP**,
    which improves the image quality, and it also gives researchers more transparency
    in studying and optimizing the model.Regarding training data, **Stable Diffusion
    v1.4** is trained with three different datasets, while **Stable Diffusion v2**
    is trained on a subset of **LAION-5B** filtered for explicit pornographic material
    (**NSFW filter**) and an aesthetic score above a threshold. The **LAION 5B** dataset
    is a large-scale dataset consisting of 5.85 billion CLIP-filtered image-text pairs.
    Over 2.3 billion samples in the dataset contain English language, while **2.2
    billion samples** originate from over 100 other languages. The remaining one billion
    samples do not allow a certain language assignment, such as names.The acquisition
    pipeline for the dataset was complex and required a lot of processing. It includes
    distributed processing of a petabyte-scale Common Crawl dataset, distributed download
    of images, and few **GPU** node post-processing of the data, which produces the
    final dataset. The filtering also removed duplicate samples and trimmed the dataset
    from 50 billion candidates to just below 6 billion **CLIP**-filtered image-text
    pairs.In terms of outcomes, Stable Diffusion v2 is harder to use for controlling
    styles and generating celebrities. This difference is likely due to the training
    data difference, as Open AI''s proprietary data may have more artwork and celebrity
    photos, which are not included in Stable Diffusion v2 training data.In summary,
    **Stable Diffusion v2** used a different text embedding model and trained on a
    different subset of data, resulting in different outcomes compared to **Stable
    Diffusion v1**. While **Stable Diffusion v2** may be more transparent and better
    for long-term development, **Stable Diffusion v1** may have better results for
    particular use cases such as controlling styles or generating celebrities due
    to its training data.Now we''ll look at the conditioning for the model in the
    text-to-image use case.'
  prefs: []
  type: TYPE_NORMAL
- en: Conditioning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The conditioning process allow these models to be influenced by the input textual
    prompts or other input types like depth maps or outlines for greater precision
    in order to create relevant images. During conditioning, the prompt is tokenized,
    and each token is converted to an embedding, a vector of a certain length, sometimes
    768 values. These embeddings, which account for the semantic relationships between
    words, are then processed by a text transformer and fed to the noise predictor,
    steering it to produce an image that aligns with the text prompt.In the text-to-image
    process, the model uses a text prompt to generate a completely new image. The
    text prompt is encoded into the latent space, and a diffusion process gradually
    adds noise (controlled by denoising strength) to evolve the initial image towards
    the output image.Let's conclude the chapter!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative models like large language models (LLMs) have received considerable
    attention due to their potential to revolutionize numerous industries and tasks.
    Especially their applications in text generation and image synthesis have garnered
    significant media hype. Leading companies such as OpenAI are pushing the boundaries
    of LLMs, with their Generative Pre-trained Transformers (GPT) series capturing
    widespread attention for their impressive language generation capabilities. In
    this chapter, we've discussed the media attention the latest breakthroughs have
    been gathering, the recent history of deep learning and AI, generative models,
    and LLMs and Pre-trained Generative Models (GPTs) together with the theoretical
    ideas underpinning them, especially the Transformer architecture. We also discussed
    diffusion models for images, and applications for text, images, sound, and videos.The
    next chapter will explore the tooling of generative and particularly LLMs with
    the Langchain framework, focusing on the fundamentals, the implementation, and
    use of this particular tool in optimizing and enhancing LLMs.I think it's a good
    habit to check that you've digested the material when reading a technical book.
    I've created a few questions for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you''ve read and understood this chapter, you should be able to answer these
    questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a generative model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which applications exist for generative models?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What's a large language model (LLM) and what does it do?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we get bigger performance from LLMs?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the conditions that make these models possible?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which companies and organizations are the big players in developing LLMs?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a transformer and what does it consist of?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does GPT mean?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does stable diffusion work?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How is stable diffusion trained?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you struggle to answer these questions, please refer back to the corresponding
    sections in this chapter to make sure you've understood the material.
  prefs: []
  type: TYPE_NORMAL
