- en: 'REFERENCES AND CITATIONSGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
    Warde-Farley, D., Ozair, S., ... & Bengio, Y. (2014). Generative adversarial nets.
    In Advances in neural information processing systems (pp. 2672-2680). [Link: [https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)
    ]Radford, A., Metz, L., & Chintala, S. (2016). Unsupervised representation learning
    with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434\.
    [Link: [https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434) ]Brock,
    A., Donahue, J., & Simonyan, K. (2018). Large-scale GAN training for high-fidelity
    raw image synthesis. In International Conference on Learning Representations (ICLR).
    [Link: [https://openreview.net/pdf?id=B1xsqj09Fm](https://openreview.net/pdf?id=B1xsqj09Fm)
    ]Karras, T., Laine, S., & Aila, T. (2018). A style-based generator architecture
    for generative adversarial networks. In Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition (pp. 4401-4410). [Link: [https://openaccess.thecvf.com/content_cvpr_2018/papers/Karras_A_Style-Based_Generator_CVPR_2018_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2018/papers/Karras_A_Style-Based_Generator_CVPR_2018_paper.pdf)
    ]Huang, X., Li, Y., Poursaeed, O., Hopcroft, J., & Belongie, S. (2018). Stacked
    generative adversarial networks. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition (pp. 1866-1875). [Link: [https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Stacked_Generative_Adversarial_CVPR_2017_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Stacked_Generative_Adversarial_CVPR_2017_paper.pdf)
    ]Yang, Z., Yang, D., Dyer, C., He, X., Smola, A., & Hovy, E. (2017). Improved
    variational autoencoders for text modeling using dilated convolutions. In Proceedings
    of the 31st International Conference on Neural Information Processing Systems
    (pp. 2136-2146). [Link: [https://papers.nips.cc/paper/6706-improved-variational-autoencoders-for-text-modeling-using-dilated-convolutions.pdf](https://papers.nips.cc/paper/6706-improved-variational-autoencoders-for-text-modeling-using-dilated-convolutions.pdf)
    ]Salimans, T., Karpathy, A., & Chen, X. (2017). PixelCNN++: Improving the PixelCNN
    with discretized logistic mixture likelihood and other modifications. arXiv preprint
    arXiv:1701.05517\. [Link: [https://arxiv.org/abs/1701.05517](https://arxiv.org/abs/1701.05517)
    ]Brock, A., Lim, T., Ritchie, J. M., & Weston, N. (2019). Neural photo editing
    with introspective adversarial networks. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (pp. 6199-6208). [Link: [https://openaccess.thecvf.com/content_CVPR_2019/papers/Brock_Neural_Photo_Editing_With_Introspective_Adversarial_Networks_CVPR_2019_paper.pdf](https://openaccess.thecvf.com/content_CVPR_2019/papers/Brock_Neural_Photo_Editing_With_Introspective_Adversarial_Networks_CVPR_2019_paper.pdf)
    ]Liu, H., & Ma, L. (2020). A survey on generative adversarial networks: Algorithms,
    theory, and applications. IEEE Access, 8, 29529-295Brown, T. B., Mann, B., Ryder,
    N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Amodei, D. (2020). Language models
    are few-shot learners. arXiv preprint arXiv:2005.14165\. [Link: [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)
    ]Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019).
    Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 9\. [Link:
    [https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
    ]Gao, J., Wenzel, S., Lin, Y., Mansimov, E., Yu, L., & Bengio, Y. (2021). Neural-guided
    constraint logic programming for program synthesis from natural language. In Proceedings
    of the 2021 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies (pp. 1531-1541). [Link: [https://aclanthology.org/2021.naacl-main.123.pdf](https://aclanthology.org/2021.naacl-main.123.pdf)
    ]Dodge, J., & Gane, A. (2021). Fine-tuning pre-trained language models: Weight
    initializations, data orders, and early stopping. In Proceedings of the 2021 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies (pp. 3093-3103). [Link: [https://aclanthology.org/2021.naacl-main.290.pdf](https://aclanthology.org/2021.naacl-main.290.pdf)
    ]Keskar, N. S., McCann, B., Varshney, L. R., Liu, C., Fischer, I., & Bengio, Y.
    (2021). On the importance of initialization and momentum in deep learning. In
    Proceedings of the 38th International Conference on Machine Learning (ICML) (pp.
    5202-5212). [Link: [https://proceedings.icml.cc/static/paper_files/icml/2021/5208-Paper.pdf](https://proceedings.icml.cc/static/paper_files/icml/2021/5208-Paper.pdf)
    ]Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., & Bowman, S. R. (2019).
    GLUE: A multi-task benchmark and analysis platform for natural language understanding.
    In Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP) (pp. 3675-3685). [Link: [https://www.aclweb.org/anthology/D19-1231.pdf](https://www.aclweb.org/anthology/D19-1231.pdf)
    ]Lewis, M., Liu, Y. Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., ...
    & Zettlemoyer, L. (2020). Pretrained language models for conversational AI. arXiv
    preprint arXiv:2004.14294\. [Link: [https://arxiv.org/abs/2004.14294](https://arxiv.org/abs/2004.14294)Dodge,
    J., Gane, A., Zhang, X., Bordes, A., Chopra, S., & Miller, A. (2020). Fine-tuning
    language models from human preferences. arXiv preprint arXiv:2004.14228\. [Link:
    [https://arxiv.org/abs/2004.14228](https://arxiv.org/abs/2004.14228) ]Holtzman,
    A., Buys, J., Forbes, M., & Choi, Y. (2021). The curious case of neural text degeneration.
    arXiv preprint arXiv:2101.05961\. [Link: [https://arxiv.org/abs/2101.05961](https://arxiv.org/abs/2101.05961)
    ]Li, J., Li, W., Li, S., & Liu, X. (2021). A survey of limitations and opportunities
    in generative adversarial networks research. arXiv preprint arXiv:2103.01864\.
    [Link: [https://arxiv.org/abs/2103.01864](https://arxiv.org/abs/2103.01864) ]Keshavarzi,
    M., & Hashemi, S. M. (2021). A survey of the challenges and limitations of natural
    language generation. Natural Language Engineering, 27(2), 211-251\. [Link: [https://www.cambridge.org/core/journals/natural-language-engineering/article/survey-of-the-challenges-and-limitations-of-natural-language-generation/7C26143B9B04DCAE2B67051F7D42435A](https://www.cambridge.org/core/journals/natural-language-engineering/article/survey-of-the-challenges-and-limitations-of-natural-language-generation/7C26143B9B04DCAE2B67051F7D42435A)
    ]Liu, C., McCann, B., Keskar, N. S., Xiong, C., & Socher, R. (2021). On limitations
    of unsupervised bilingual dictionary induction. In Proceedings of the 2021 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies (pp. 1929-1939). [Link: [https://aclanthology.org/2021.naacl-main.151.pdf](https://aclanthology.org/2021.naacl-main.151.pdf)
    ]Yang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhutdinov, R., & Le, Q. V.
    (2019). XLNet: Generalized autoregressive pretraining for language understanding.
    In Advances in neural information processing systems (pp. 5754-5764). [Link: [https://proceedings.neurips.cc/paper/2019/file/2e9cf1a55c6bb74b6f8e3a9abfeaf60c-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/2e9cf1a55c6bb74b6f8e3a9abfeaf60c-Paper.pdf)
    ]'
  prefs: []
  type: TYPE_NORMAL
