- en: 1 What Are Generative Models?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 生成模型是什么？
- en: Join our book community on Discord
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们的书籍社区Discord
- en: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
- en: '![Qr code Description automatically generated](../media/file0.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的二维码描述](../media/file0.png)'
- en: '**Artificial Intelligence** (**AI**) has made significant advancements, impacting
    businesses, societies, and individuals. For about the last decade, deep learning
    has evolved to process and generate unstructured data like text, images, videos,
    and more. These advanced AI models, which are based on deep learning, have gained
    popularity in various industries, and include **large language models** (**LLMs**).There
    is currently a significant level of hype in both the media and the industry surrounding
    AI. This is driven by various factors, including advancements in technology, high-profile
    applications, and the potential for transformative impacts across multiple sectors.In
    this chapter, we''ll discuss generative models, in particular LLMs, and their
    application to domains such as text, image, sound, and video. We''ll go through
    some of the technical background that makes them tick, and how they are trained.We''ll
    start with an introduction clarifying where we are at in terms of the state-of-the-art,
    and what the hype is about.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工智能**（**AI**）取得了重大进展，影响着企业、社会和个人。在过去的十年左右，深度学习已经发展到可以处理和生成文本、图像、视频等非结构化数据。这些基于深度学习的先进AI模型在各行各业中备受青睐，包括**大型语言模型**（**LLMs**）。目前，媒体和行业对AI存在相当大的炒作。这是由多种因素驱动的，包括技术的进步、知名应用以及在多个领域产生变革性影响的潜力。在本章中，我们将讨论生成模型，特别是LLMs，以及它们在文本、图像、声音和视频等领域的应用。我们将介绍一些技术背景，解释它们的工作原理以及它们是如何训练的。我们将从介绍开始，澄清我们在技术发展的最前沿所处的位置，以及炒作的原因。'
- en: Why the hype?
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么会有这样的炒作？
- en: 'In the media, there is substantial coverage of AI-related breakthroughs and
    their potential implications. These range from advancements in natural language
    processing and computer vision to the development of sophisticated language models
    like GPT-3\. Media outlets often highlight AI''s capabilities and its potential
    to revolutionize industries such as healthcare, finance, transportation, and more.Particularly,
    generative models have received a lot of attention due to their ability to generate
    text, images, and other creative content that is often indistinguishable from
    human-generated content. These same models also provide a wide functionality including
    semantic search, content manipulation, and classification. This allows cost-savings
    by automation and can allow humans to leverage their creativity by an unprecedented
    level.This graph, inspired by a blog post about GPT-4 Predictions by Stephen McAleese
    on LessWrong, shows the improvements of LLMs in the **Massive Multitask Language
    Understanding** (**MMLU**) benchmark, which was designed to quantify knowledge
    and problem-solving ability in elementary mathematics, US history, computer science,
    law, and more:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 媒体广泛报道了与AI相关的突破性进展及其潜在影响。这些进展涵盖了从自然语言处理和计算机视觉的进步到像GPT-3这样的复杂语言模型的发展。媒体经常强调AI的能力以及其颠覆性潜力，例如在医疗保健、金融、交通等行业的革新。特别是，生成模型因其能够生成文本、图像和其他创意内容（往往难以与人类生成的内容区分开）而受到了广泛关注。这些模型还提供了广泛的功能，包括语义搜索、内容操作和分类。这可以通过自动化实现成本节约，并使人类能够以前所未有的水平发挥创造力。这张图表受到了Stephen
    McAleese在LessWrong上关于GPT-4预测的博文的启发，展示了LLMs在**大规模多任务语言理解**（**MMLU**）基准测试中的改进，该测试旨在量化基本数学、美国历史、计算机科学、法律等领域的知识和问题解决能力。
- en: '![Figure 1.1: Average performance on the Massive Multitask Language Understanding
    (MMLU) benchmark of Large Language Models (LLM). Please note that while most benchmark
    results come from 5-shot, a few the GPT-2, PaLM, and PaLM-2 results refer to fine-tuned
    models.](../media/file1.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![图1.1：大型语言模型（LLM）在大规模多任务语言理解（MMLU）基准测试中的平均表现。请注意，大多数基准测试结果来自5-shot，少数GPT-2、PaLM和PaLM-2的结果是指微调模型。](../media/file1.png)'
- en: 'Figure 1.1: Average performance on the Massive Multitask Language Understanding
    (MMLU) benchmark of Large Language Models (LLM). Please note that while most benchmark
    results come from 5-shot, a few the GPT-2, PaLM, and PaLM-2 results refer to fine-tuned
    models.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1：大型语言模型（LLM）在大规模多任务语言理解（MMLU）基准测试中的平均表现。请注意，大多数基准测试结果来自5-shot，少数GPT-2、PaLM和PaLM-2的结果是指微调模型。
- en: You can see the progress in recent years in the benchmark. Particularly to highlight
    is the progress of the models provided through a public user interface by OpenAI,
    especially the improvements between releases, from GTP-2 to GPT-3 and GPT-3.5
    to GPT-4\. These models only recently started to perform better than an average
    human rater, but still haven't reached the performance of a human expert. These
    achievements of human engineering are impressive; however, it should be noted
    that performance of these models depends on the field; most models are still performing
    poorly for on the GSM8K benchmark of grade school math word problems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来在基准测试中可以看到进展。特别值得强调的是OpenAI通过公共用户界面提供的模型的进步，特别是从GTP-2到GPT-3再到GPT-3.5再到GPT-4的改进。这些模型最近才开始表现优于平均人类评分者，但仍未达到人类专家的水平。这些人类工程的成就令人印象深刻；然而，应该注意到这些模型的表现取决于领域；大多数模型在小学数学单词问题的GSM8K基准测试中仍表现不佳。
- en: '**OpenAI** is an American AI research laboratory that aims to promote and develop
    friendly AI. II was established in 2015 with the support of several influential
    figures and companies, who pledged over $1 billion to the venture. The organization
    initially committed to non-profit, collaborating with other institutions and researchers
    by making its patents and research open to the public. In 2018, Elon Musk resigned
    from the board citing a potential conflict of interest with his role at Tesla.
    In 2019, OpenAI transitioned to a for-profit organization, and subsequently Microsoft
    made significant investments in OpenAI, leading to the integration of OpenAI systems
    with Microsoft''s Azure-based supercomputing platform and into the Bing search
    engine. The most significant achievements of the company include the OpenAI Gym
    for training reinforcement algorithms, and - more recently - the GPT-n models
    and DALL-E, another deep learning model that generates images from text.'
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**OpenAI**是一个旨在推广和发展友好人工智能的美国人工智能研究实验室。它成立于2015年，得到了几位有影响力的人物和公司的支持，他们承诺向这个项目投资超过10亿美元。该组织最初致力于非营利性，通过向公众开放其专利和研究成果与其他机构和研究人员合作。2018年，埃隆·马斯克因担心与特斯拉的角色存在潜在利益冲突而辞去了董事会职务。2019年，OpenAI转变为营利性组织，随后微软对OpenAI进行了重大投资，导致OpenAI系统与微软基于Azure的超级计算平台以及必应搜索引擎的整合。该公司最重要的成就包括用于训练强化学习算法的OpenAI
    Gym，以及最近推出的GPT-n模型和DALL-E，另一个能够从文本生成图像的深度学习模型。'
- en: '**Generative Pre-training Transformer** (**GPT**) models, like the recently
    launched OpenAI''s ChatGPT, are prime examples of AI advancements in the sphere
    of LLMs. ChatGPT has greatly improved chatbot capabilities by training at a much
    bigger scale and by being much bigger than previous models. These AI-based chatbots
    can generate human-like responses as real-time feedback to customers and can be
    applied to a wide range of use cases, from software development and testing to
    poetry and business communication. Within the industry, there is a growing sense
    of excitement around AI''s capabilities and its potential impact on business operations.
    We''ll look at this more in *Chapter 10*, *The Future of Generative Models*.As
    AI models like OpenAI''s GPT continue to improve, they could become indispensable
    assets to teams in need of diverse knowledge and skills. For example, GPT-4 could
    be considered a "polymath" that could work tirelessly without demanding compensation
    (beyond subscription or API fees), providing assistance in subjects like Math,
    Verbal, Stats, Macroeconomics, Biology, and even passing the Bar exam. As these
    AI models become more proficient and easily accessible, they are likely to play
    a significant role in shaping the future of work and learning.By making knowledge
    more accessible and adaptable, these models have the potential to level the playing
    field and create new opportunities for people from all walks of life. These models
    have shown potential in areas that require higher levels of reasoning and understanding,
    although progress varies depending on the complexity of the tasks involved.As
    for generative models with images, we could expect models with better capabilities
    to assist in creating visual content, and possibly improvements in computer vision
    tasks such as object detection, segmentation, captioning, and much more.Let''s
    get our terminology a bit clearer, and explain more in detail what is meant by
    generative model, artificial intelligence, deep learning, and machine learning.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成式预训练变换器**（**GPT**）模型，如最近推出的OpenAI的ChatGPT，是LLM领域人工智能进步的典范。ChatGPT通过在更大规模上训练并比以往模型更大，极大地提升了聊天机器人的能力。这些基于人工智能的聊天机器人可以生成类似人类的实时反馈给客户，并可应用于从软件开发和测试到诗歌和商业沟通等各种用例。在行业内，人们对人工智能的能力和其对业务运营的潜在影响感到越来越兴奋。我们将在*第10章*，*生成模型的未来*中更详细地探讨这一点。随着OpenAI的GPT等人工智能模型的不断改进，它们可能成为需要多样知识和技能的团队不可或缺的资产。例如，GPT-4可以被视为一个“博学多才”的人工智能，可以在不要求报酬（除了订阅或API费用）的情况下不知疲倦地工作，在数学、语言、统计学、宏观经济学、生物学甚至通过司法考试等学科提供帮助。随着这些人工智能模型变得更加熟练和易于访问，它们可能在塑造未来工作和学习方面发挥重要作用。通过使知识更易获取和适应，这些模型有潜力拉平竞争场地，为来自各行各业的人们创造新机会。这些模型在需要更高层次推理和理解的领域显示出潜力，尽管进展因所涉及任务的复杂性而有所不同。至于具有图像的生成模型，我们可以期待具有更好能力的模型协助创建视觉内容，并可能改进计算机视觉任务，如目标检测、分割、字幕等等。让我们更清晰地澄清术语，并更详细地解释生成模型、人工智能、深度学习和机器学习的含义。'
- en: What are Generative Models?
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成模型是什么？
- en: 'In the media, the term artificial intelligence is used a lot when referring
    to these new models. It''s worth distinguishing a bit more clearly how term generative
    model differs from artificial intelligence, deep learning, machine learning, and
    language model:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在媒体上，当提到这些新模型时，经常使用术语人工智能。值得更清楚地区分一下生成模型这个术语与人工智能、深度学习、机器学习和语言模型的区别：
- en: '**Artificial intelligence** (**AI**) is a broad field of computer science that
    deals with the creation of intelligent agents, which are systems that can reason,
    learn, and act autonomously.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人工智能**（**AI**）是计算机科学的一个广泛领域，涉及创建智能代理的系统，这些系统可以自主推理、学习和行动。'
- en: '**Machine learning** (**ML**) is a subset of AI that deals with the development
    of algorithms that can learn from data. ML algorithms are trained on a set of
    data, and then they can use that data to make predictions or decisions.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习**（**ML**）是人工智能的一个子集，涉及开发能够从数据中学习的算法。机器学习算法在一组数据上进行训练，然后可以使用该数据进行预测或决策。'
- en: '**Deep learning** (D**L**) is a subset of ML that uses artificial neural networks
    to learn from data. Neural networks are inspired by the human brain, and they
    are able to learn complex patterns from data.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度学习**（**DL**）是机器学习的一个子集，它使用人工神经网络从数据中学习。神经网络受人脑启发，能够从数据中学习复杂模式。'
- en: '**Generative models** are a type of ML model that can generate new data. Generative
    models are trained on a set of data, and then they can use that data to create
    new data that is similar to the training data.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成模型**是一种可以生成新数据的ML模型。生成模型在一组数据上进行训练，然后它们可以利用该数据创建类似于训练数据的新数据。'
- en: '**Language models** are statistical models that predict tokens (typically words)
    in a sequence. Some of these models, which are capable of more complex tasks,
    consist of many parameters (on the order of billions or even trillions), therefore
    they are called **large language model**.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言模型**是预测序列中的标记（通常是单词）的统计模型。其中一些模型，能够执行更复杂的任务，包含许多参数（数量达到数十亿甚至数万亿），因此被称为**大型语言模型**。'
- en: 'The main difference between generative models and other types of ML models
    is that generative models do not just make predictions or decisions. They can
    actually create new data. This makes generative models very powerful, and they
    can be used for a variety of tasks, such as generating images, text, music, and
    video.Here is a table that summarizes the differences between AI, ML, DL, language
    model, and generative models:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型与其他类型的ML模型的主要区别在于，生成模型不仅仅是做出预测或决策。它们实际上可以创建新数据。这使得生成模型非常强大，可以用于各种任务，如生成图像，文本，音乐和视频。以下是总结AI，ML，DL，语言模型和生成模型之间区别的表格：
- en: '| **Term** | **Definition** |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| **术语** | **定义** |'
- en: '| Artificial intelligence | A broad field of computer science that deals with
    the creation of intelligent agents. |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 人工智能 | 一门涉及智能代理创建的计算机科学广泛领域。 |'
- en: '| Machine learning | A subset of AI that deals with the development of algorithms
    that can learn from data. |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 机器学习 | 处理可以从数据中学习的算法开发的AI的一个子集。 |'
- en: '| Deep learning | A subset of ML that uses artificial neural networks to learn
    from data. |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 深度学习 | 使用人工神经网络从数据中学习的ML的一个子集。 |'
- en: '| Generative model | A type of ML model that can generate new data. |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 生成模型 | 一种可以生成新数据的ML模型。 |'
- en: '| Language model | A type of model, nowadays mostly a deep learning model,
    that predicts tokens in context. |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 语言模型 | 一种模型，现在主要是深度学习模型，可以预测上下文中的标记。 |'
- en: 'Figure 1.2: Terminology - Artificial Intelligence, Machine Learning, Deep Learning,
    and Generative Model.Generative models are a powerful type of AI that can generate
    new data samples that resemble the training data. Generative AI models have come
    a long way, enabling the generation of new examples from scratch using patterns
    in data. These models can handle different types of data and are employed across
    various domains, including text generation, image generation, music generation,
    and video generation.For language models, it''s important to note that some of
    them, particularly newer generations, are generative, in the sense that they can
    produce language (text), others are not. These generative models facilitate the
    creation of **synthetic data** to train AI models when real data is scarce or
    restricted. This type of data generation reduces labeling costs and improves training
    efficiency. Microsoft Research took this approach ("Textbooks Are All You Need",
    June 2023) for training their phi-1 model, where they created synthetic text books
    and exercises with GPT-3.5 as their training dataset.In the following sections,
    we''ll look at the different domains of generative models such as text, images,
    sound, video. The applications mostly revolve around content generation, editing,
    and processing (recognition). Let''s start with text!'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2：术语 - 人工智能，机器学习，深度学习和生成模型。生成模型是一种强大的AI类型，可以生成类似于训练数据的新数据样本。生成AI模型已经取得了长足的进步，通过数据中的模式生成新的示例。这些模型可以处理不同类型的数据，并在各个领域中使用，包括文本生成，图像生成，音乐生成和视频生成。对于语言模型，重要的是要注意，其中一些模型，特别是新一代的模型，是生成型的，可以生成语言（文本），而其他模型则不是。这些生成模型有助于创建**合成数据**来训练AI模型，当真实数据稀缺或受限时。这种数据生成方式降低了标记成本并提高了训练效率。微软研究采用了这种方法（“只需教科书”，2023年6月）来训练他们的phi-1模型，他们使用GPT-3.5创建了合成教科书和练习作为他们的训练数据集。在接下来的章节中，我们将探讨生成模型的不同领域，如文本，图像，声音，视频。这些应用主要围绕内容生成，编辑和处理（识别）。让我们从文本开始！
- en: Text
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本
- en: 'Text generation, such as GPT-4 by OpenAI, can generate coherent and grammatically
    correct poems, or code in different languages and extract features like keywords
    and topics. These models have practical applications in fields like content creation
    and **natural language processing** (**NLP**), where the ultimate goal is to create
    algorithms capable of interpreting human language.Language modeling aims to predict
    the next word, character, or even sentence based on the previous ones in a sequence.
    In this sense, language modeling serves as a way of encoding the rules and structures
    of a language in a way that can be understood by a machine. Large language models
    capture the structure of human language in terms of grammar, syntax, and semantics.
    These models are important as they form the backbone of a number of larger NLP
    tasks such as content creation, translation, summarization, machine translation,
    and text editing tasks such as spelling correction.At its core, language modeling,
    and more broadly Natural Language Processing, rely heavily on the quality of the
    representation learning. A well-trained language model encodes information about
    the text it''s trained on and generates new text based on those learnings, thereby
    taking on the task of text generation.Recently, large language models have found
    application for tasks like essay generation, code development, translation, and
    understanding genetic sequences. More broadly applications of language models
    involve multiple areas, such as:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 文本生成，例如OpenAI的GPT-4，可以生成连贯和语法正确的诗歌，或者用不同语言编写代码并提取关键词和主题等特征。这些模型在内容创作和**自然语言处理**（**NLP**）等领域具有实际应用，其最终目标是创建能够解释人类语言的算法。语言建模旨在根据序列中的前一个词、字符甚至句子来预测下一个词。在这个意义上，语言建模作为一种将语言的规则和结构编码成机器可理解的方式。大型语言模型捕捉了人类语言的语法、句法和语义结构。这些模型很重要，因为它们构成了许多更大的NLP任务的基础，如内容创作、翻译、摘要、机器翻译和文本编辑任务，如拼写纠正。在其核心，语言建模，以及更广泛的自然语言处理，严重依赖于表示学习的质量。一个训练良好的语言模型对其训练的文本编码信息，并根据这些学习生成新文本，从而承担文本生成的任务。最近，大型语言模型已经应用于文章生成、代码开发、翻译和理解基因序列等任务。语言模型的更广泛应用涉及多个领域，例如：
- en: '**Question answering**: AI chatbots and virtual assistants can provide personalized
    and efficient assistance, reducing response times in customer support and thereby
    enhancing customer experience. These systems can be used for solving specific
    problems like restaurant reservations and ticket booking.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问答系统**：AI聊天机器人和虚拟助手可以提供个性化和高效的帮助，减少客户支持中的响应时间，从而增强客户体验。这些系统可用于解决特定问题，如餐厅预订和购票。'
- en: '**Automatic summarization**: Language models can create concise summaries of
    articles, research papers, and other content, enabling users to consume and understand
    information rapidly.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动摘要**：语言模型可以创建文章、研究论文和其他内容的简洁摘要，使用户能够快速消化和理解信息。'
- en: '**Sentiment analysis**: Analyzing opinions and emotions in texts, language
    models can help businesses understand customer feedback and opinions more efficiently.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情感分析**：通过分析文本中的意见和情感，语言模型可以帮助企业更有效地理解客户反馈和意见。'
- en: '**Topic modeling** and **semantic search**: These models can identify, categorize
    by topics, and compress documents into concise vectors, making content management
    and discovery easier for organizations.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题建模**和**语义搜索**：这些模型可以识别、按主题分类和压缩文档为简洁向量，使组织更容易进行内容管理和发现。'
- en: '**Machine translation**: AI-powered language models can translate texts from
    one language into another, supporting businesses in their global expansion efforts.
    New generative models can perform competitively with commercial products (for
    example Google translate).'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器翻译**：由人工智能驱动的语言模型可以将一种语言的文本翻译成另一种语言，支持企业在全球扩张努力中。新的生成模型可以与商业产品（例如谷歌翻译）竞争。'
- en: Despite the remarkable achievements, language models still face limitations
    when dealing with complex mathematical or logical reasoning tasks. It remains
    uncertain whether continually increasing the scale of language models will inevitably
    lead to new reasoning capabilities. As mentioned, we also have to consider the
    importance of data quality and scale, as these factors play a significant role
    in improving language model performance in different tasks and areas.Another domain
    for generative models is image generation, let's see what that's about!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管取得了显著的成就，语言模型在处理复杂的数学或逻辑推理任务时仍面临限制。目前尚不清楚不断增加语言模型规模是否必然会导致新的推理能力。正如前文所述，我们还必须考虑数据质量和规模的重要性，因为这些因素在改善语言模型在不同任务和领域中的性能方面起着重要作用。生成模型的另一个领域是图像生成，让我们看看这是怎么回事！
- en: Images
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像
- en: 'Generative AI is extensively used in generating 3D images, avatars, videos,
    graphs, illustrations for virtual or augmented reality, video games graphics design,
    logo creation, image editing or enhancement.This graph illustrates image generation
    from a text prompt with stable diffusion (Source: "Restart Sampling for Improving
    Generative Processes" by Yilun Xu and others at MIT and Google Research, June
    2023; https://arxiv.org/pdf/2306.14878.pdf):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 生成人工智能广泛用于生成3D图像、头像、视频、图表、虚拟或增强现实中的插图、视频游戏图形设计、标志创建、图像编辑或增强。这张图展示了从具有稳定扩散的文本提示生成图像（来源：“改进生成过程的重新启动采样”作者为Yilun
    Xu等人，来自麻省理工学院和谷歌研究，2023年6月；https://arxiv.org/pdf/2306.14878.pdf）：
- en: '![Figure 1.3: Image generation from a text prompt "A transparent sculpture
    of a duck made out of glass".](../media/file2.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图1.3：从文本提示“由玻璃制成的透明鸭子雕塑”生成图像。](../media/file2.png)'
- en: 'Figure 1.3: Image generation from a text prompt "A transparent sculpture of
    a duck made out of glass".'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3：从文本提示“由玻璃制成的透明鸭子雕塑”生成图像。
- en: 'With stable diffusion models, you can see a wide variety of outcomes using
    only minimal changes to the initial setting of the model or - as in this case
    - numeric solvers and samplers. Although, they sometimes produce striking results,
    this instability and inconsistency is a significant challenge to applying these
    models more broadly.Services like **MidJourney**, **DALL-E 2**, and **Stable Diffusion**
    provide creative and realistic images derived from textual input or other images.
    Services like **DreamFusion**, **Magic3D**, and **Get3D** enable users to convert
    textual descriptions into 3D models and scenes, driving innovation in design,
    gaming, and virtual experiences.There are three main applications:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用稳定扩散模型，您可以看到仅通过对模型的初始设置进行最小更改或者 - 如本例中 - 数值求解器和采样器，就可以看到各种各样的结果。尽管它们有时会产生引人注目的结果，但这种不稳定性和不一致性是将这些模型更广泛应用的重要挑战。像**MidJourney**、**DALL-E
    2**和**Stable Diffusion**这样的服务提供了从文本输入或其他图像派生的创意和逼真图像。像**DreamFusion**、**Magic3D**和**Get3D**这样的服务使用户能够将文本描述转换为3D模型和场景，推动设计、游戏和虚拟体验的创新。主要有三个应用场景：
- en: '**Image generation**: models can generate images, such as paintings, photographs,
    and sketches. This can be used for a variety of purposes, such as creating art,
    designing products, and generating realistic visual effects.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**图像生成**：模型可以生成各种图像，如绘画、照片和草图。这可以用于各种目的，如创作艺术、设计产品和生成逼真的视觉效果。'
- en: '**Image editing**: Models can perform tasks such as removing objects, changing
    colors, and adding effects. This can be used to improve the quality of images,
    and to make them more visually appealing.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**图像编辑**：模型可以执行诸如移除对象、更改颜色和添加效果等任务。这可以用于提高图像质量，并使其更具视觉吸引力。'
- en: '**Image recognition**: large foundation models can be used to recognize images
    including classifying scenes, but also object detection, for example detecting
    faces.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**图像识别**：大型基础模型可用于识别图像，包括场景分类，还有物体检测，例如检测人脸。'
- en: Models like Generative Adversarial Networks (GANs) and DALL-E. GANs generate
    realistic images that have numerous business applications, while DALL-E creates
    images from textual descriptions, which is helpful in creative industries for
    designing advertisements, products, and fashion.Image editing involves modifying
    an image's semantics by changing content or style attributes using techniques
    like facial attribute editing or image morphing. Optimization- and learning-based
    approaches generate images with styles obtained via latent representations of
    pre-trained GAN models like StyleGAN. Diffusion models have recently been used
    for advanced image editing tasks such as connecting manually designed masked regions
    seamlessly or generating 3D object manipulations through text guidance. These
    techniques enable flexible image generation but face limited diversity issues
    that can be mitigated by incorporating other text inputs into the process.Into
    the category of image editing fall also tasks such as image restoration, which
    means restoring clean images from their degraded versions, which involves tasks
    like image super-resolution, inpainting, denoising, dehazing, and deblurring.
    Deep learning-based methods using CNN and transformer architectures are prevalent
    due to superior visual quality compared to traditional approaches. Generative
    models like GANs and Diffusion Models (DMs) are used for restoration but can suffer
    from complex training processes and mode collapse. Multi-distortion datasets and
    single-network approaches with attention modules or guiding sub-networks improve
    effectiveness for handling multiple degradation types.We'll see next what models
    can do with sound and music.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 像生成对抗网络（GANs）和 DALL-E 这样的模型。GANs 生成逼真的图像，具有许多商业应用，而 DALL-E 则根据文本描述创建图像，对于设计广告、产品和时尚等创意产业非常有帮助。图像编辑涉及通过改变内容或样式属性来修改图像的语义，使用面部属性编辑或图像变形等技术。基于优化和学习的方法通过使用预训练的
    GAN 模型（如 StyleGAN）的潜在表示生成具有风格的图像。扩散模型最近被用于高级图像编辑任务，例如通过文本引导无缝连接手动设计的遮罩区域或生成 3D
    对象操作。这些技术实现了灵活的图像生成，但面临着有限多样性问题，可以通过将其他文本输入纳入过程中来缓解。图像编辑的范畴还包括图像恢复等任务，这意味着从受损版本中恢复清晰图像，包括图像超分辨率、修补、去噪、去雾和去模糊等任务。基于深度学习的方法使用
    CNN 和 transformer 架构，由于与传统方法相比具有更优越的视觉质量。生成模型如 GANs 和扩散模型（DMs）用于恢复，但可能遭受复杂的训练过程和模式崩溃。多扭曲数据集和具有注意力模块或引导子网络的单网络方法提高了处理多种退化类型的效果。接下来我们将看看模型可以如何处理声音和音乐。
- en: Sound and Music
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 声音和音乐
- en: 'Generative models can develop songs and audio clips based on text inputs, recognize
    objects in videos and create accompanying audio, and create custom music. We can
    classify applications again roughly into generation, editing, and recognition:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型可以根据文本输入开发歌曲和音频剪辑，识别视频中的物体并创建相应的音频，以及创作定制音乐。我们可以粗略地将应用程序再次分类为生成、编辑和识别：
- en: '**Music generation**: Generative models can be used to generate music, such
    as songs, beats, and melodies. This can be used for a variety of purposes, such
    as creating new music, composing soundtracks, and generating personalized playlists.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**音乐生成**：生成模型可用于生成音乐，如歌曲、节拍和旋律。这可以用于各种目的，如创作新音乐、谱写配乐和生成个性化播放列表。'
- en: '**Sound editing**: Generative models can be used to edit sound, such as removing
    noise, changing pitch, and adding effects. This can be used to improve the quality
    of sound, and to make it more sonically appealing.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**声音编辑**：生成模型可用于编辑声音，如消除噪音、改变音调和添加效果。这可以用于提高声音质量，并使其在听觉上更具吸引力。'
- en: '**Sound recognition**: Generative models can be used to recognize sound, such
    as identifying instruments, classifying genres, and detecting speech. This can
    be used for a variety of purposes, such as music analysis, search, and recommendation
    systems.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**声音识别**：生成模型可用于识别声音，如识别乐器、分类流派和检测语音。这可以用于各种目的，如音乐分析、搜索和推荐系统。'
- en: Music generation algorithms started with algorithmic composition in the 1950s
    and have seen recent innovations like Google's WaveNet and OpenAI's Jukebox. These
    models have led to AI composer assistants, which can generate music in various
    styles and enable newer applications like speech synthesis.As a special case,
    speech-to-text generation, also known as **automatic speech recognition** (**ASR**),
    is the process of converting spoken language into text. They are trained on sounds
    and texts. ASR systems are becoming increasingly accurate, and are now used in
    a wide variety of applications. However, there are still some challenges that
    need to be addressed, such as the ability to handle noisy environments and different
    accents. With many potential applications such as voice dialing and computer-assisted
    personal assistance like Alexa and Siri, the technology behind ASR evolved from
    Markov Models to rely on GPTs.We'll see videos next.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 音乐生成算法始于20世纪50年代的算法作曲，并见证了像谷歌的WaveNet和OpenAI的Jukebox这样的最新创新。这些模型导致了AI作曲助手的出现，可以以各种风格生成音乐，并实现新的应用，如语音合成。作��一个特例，语音到文本生成，也称为**自动语音识别**（**ASR**），是将口语转换为文本的过程。它们是在声音和文本上进行训练的。ASR系统变得越来越准确，现在被广泛应用于各种应用中。然而，仍然存在一些需要解决的挑战，比如处理嘈杂环境和不同口音的能力。随着许多潜在应用，如语音拨号和像Alexa和Siri这样的计算机辅助个人助手，ASR背后的技术从马尔可夫模型发展到依赖于GPT。接下来我们将看到视频。
- en: Video
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 视频
- en: 'Video generation models like **DeepMind''s** Motion to Video and **NVIDIA''s**
    **Vid2Vid** rely on **GANs** for high-quality video synthesis. They can convert
    videos between different domains, modify existing videos, and animate still images,
    showing great potential for video editing and media production.Tools like Make-a-Video
    and Imagen Video convert natural language prompts into video clips, simplifying
    video production and content creation processes. The broad classes of applications
    are these:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 视频生成模型，如**DeepMind**的Motion to Video和**NVIDIA**的**Vid2Vid**依赖于**GANs**进行高质量视频合成。它们可以在不同领域之间转换视频，修改现有视频，并使静态图像动画化，展示了视频编辑和媒体制作的巨大潜力。像Make-a-Video和Imagen
    Video这样的工具将自然语言提示转换为视频片段，简化了视频制作和内容创作过程。这些应用的广泛类别是：
- en: '**Video generation**: Generative models can be used to generate videos, such
    as short films, animations, and commercials. This can be for creating new content,
    advertising products, and generating realistic visual effects.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**视频生成**：生成模型可以用于生成视频，如短片、动画和广告。这可以用于创作新内容、广告产品和生成逼真的视觉效果。'
- en: '**Video editing**: We can edit videos, such as removing objects, changing colors,
    and adding effects. This can help to improve the quality of videos, and to make
    them more visually appealing.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**视频编辑**：我们可以编辑视频，如移除物体、更改颜色和添加效果。这可以帮助提高视频质量，并使其更具视觉吸引力。'
- en: '**Video recognition**: Models can recognize video, such as identifying objects,
    classifying scenes, and detecting faces. This can help for applications such as
    security, search, and recommendation systems.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**视频识别**：模型可以识别视频，如识别物体、分类场景和检测人脸。这可以帮助应用于安全、搜索和推荐系统等领域。'
- en: Some models can generate content in more than one domain or modality. These
    are called multi-modal models.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型可以在多个领域或模态中生成内容。这些被称为多模型。
- en: Multi-Modal
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多模
- en: 'Multi-modal generative models can generate **text**, **images**, **sound**,
    and **video**. This allows them to create more realistic and immersive experiences.
    Multi-modal models are still in their early stages of development, but they have
    the potential to revolutionize the way we interact with computers and the way
    we experience the world. For example, these advancements have significantly improved
    performance in image captioning tasks, the process of describing an image''s content
    through natural language.Multi-modal models adopt generative architectures that
    fuse images and captions into a single model for shared learning space. The process
    involves a two-step encoder-decoder architecture: visual encoding and language
    decoding.We can distinguish these potential use cases:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 多模生成模型可以生成**文本**、**图像**、**声音**和**视频**。这使它们能够创造更加逼真和沉浸式的体验。多模型仍处于发展的早期阶段，但它们有潜力彻底改变我们与计算机互动和体验世界的方式。例如，这些进展显著提高了图像字幕任务的性能，即通过自然语言描述图像内容的过程。多模型采用融合图像和字幕的生成架构，形成一个共享学习空间的单一模型。这个过程涉及两步编码器-解码器架构：视觉编码和语言解码。我们可以区分这些潜在的用例：
- en: '**Virtual reality**: These models can be used to create virtual reality experiences
    that are more realistic and immersive. This can help in gaming, education, and
    training.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚拟现实**：这些模型可以用来创建更加逼真和沉浸式的虚拟现实体验。这对于游戏、教育和培训都有帮助。'
- en: '**Augmented reality**: They can create augmented reality experiences that overlay
    digital content on the real world. This is useful for navigation, shopping, and
    entertainment.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强现实**：它们可以创建增强现实体验，将数字内容叠加在现实世界上。这对于导航、购物和娱乐都是有用的。'
- en: In the next section, we'll discuss the technical background of large language
    models.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将讨论大型语言模型的技术背景。
- en: What is a GPT?
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是 GPT？
- en: '**Large Language Models** (**LLMs**) are deeply trained neural networks adept
    at understanding and generating human language. The current generation of LLMs
    such as ChatGPT are deep neural network architectures that utilize the Transformer
    model and undergo pre-training using unsupervised learning on extensive text data,
    enabling it to learn language patterns and structures.The notable strength of
    the latest generation of LLMs as conversational interface (ChatBot) lies in their
    ability to generate coherent and contextually appropriate responses, even in open-ended
    conversations. This generating the next word based on the preceding words repeatedly,
    the model produces fluent and coherent text often indistinguishable from text
    produced by humans. However, ChatGPT has been observed to "sometimes write plausible-sounding
    but incorrect or nonsensical answers" as expressed in a disclaimer by **OpenAI**.
    This is referred to as hallucination and is just one of the concerns around **LLMs**.A
    **Transformer**, is a deep learning architecture, first introduced in 2017 by
    researchers at Google and the University of Toronto (in an article called "Attention
    is All You Need"), which comprises self-attention and feed-forward neural networks,
    allowing it to effectively capture the word relationships in a sentence. The attention
    mechanism enables the model to focus on different parts of the input sequence.**Generative
    Pre-Trained Transformers** (**GPTs**) were introduced by researchers at OpenAI
    in 2018 together with the first of their eponymous GPT models, GPT-1, and published
    as "Improving Language Understanding by Generative Pre-Training". The pre-training
    process involves predicting the next word in a text sequence, enhancing the model''s
    grasp of language as measured in the quality of the output. Following pre-training,
    the model can be fine-tuned for specific language processing tasks like sentiment
    analysis, language translation, or chat. This combination of unsupervised and
    supervised learning enables GPT models to perform better across a range of NLP
    tasks and reduces the challenges associated with training LLMs.The size of the
    training corpus for LLMs has been increasing drastically. GPT-1, introduced by
    OpenAI in 2018, was trained on BookCorpus with 985 million words. BERT, released
    in the same year, was trained on a combined corpus of **BookCorpus** and **English
    Wikipedia**, totalling **3.3 billion words**. Now, training corpora for LLMs reach
    up to trillions of tokens.This graph illustrates how LLMs have been growing very
    large:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**大型语言模型**（**LLMs**）是深度训练的神经网络，擅长理解和生成人类语言。当前一代的LLMs，如ChatGPT，是利用Transformer模型的深度神经网络架构，在广泛的文本数据上进行无监督学习的预训练，使其能够学习语言模式和结构。最新一代LLMs的显著优势在于作为对话接口（ChatBot）时，能够生成连贯和上下文适当的回应，即使在开放式对话中也是如此。通过基于前面的单词生成下一个单词，该模型经常产生流畅和连贯的文本，往往难以与人类产生的文本区分开。然而，OpenAI在免责声明中表达了ChatGPT“有时会写出听起来合理但不正确或荒谬的答案”的观察。这被称为幻觉，这只是围绕LLMs的关注之一。**Transformer**是一种深度学习架构，于2017年首次由谷歌和多伦多大学的研究人员引入（在一篇名为“注意力机制是你所需要的一切”的文章中），它包括自注意力和前馈神经网络，使其能够有效地捕捉句子中的单词关系。注意机制使模型能够专注于输入序列的不同部分。**生成式预训练变换器**（**GPTs**）是由OpenAI的研究人员于2018年推出的，与他们的首个同名GPT模型GPT-1一起发布，并发表为“通过生成式预训练改进语言理解”。预训练过程涉及预测文本序列中的下一个单词，增强模型对语言的理解，这可以通过输出质量来衡量。在预训练之后，模型可以针对特定的语言处理任务进行微调，如情感分析、语言翻译或聊天。无监督和监督学习的结合使GPT模型在各种NLP任务中表现更好，并减少了训练LLMs所面临的挑战。LLMs的训练语料库规模急剧增加。OpenAI于2018年推出的GPT-1是在包含985百万个单词的BookCorpus上进行训练的。BERT在同一年发布，是在**BookCorpus**和**英文维基百科**的合并语料库上进行训练的，总计**33亿个单词**。现在，LLMs的训练语料库已经扩展到数万亿个标记。这张图说明了LLMs的规模一直在不断增长：'
- en: '![Figure 1.4: Large Language Models from BERT to GPT-4 - size, training budget,
    and organizations.](../media/file3.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图1.4：从BERT到GPT-4的大型语言模型 - 大小、训练预算和组织。](../media/file3.png)'
- en: 'Figure 1.4: Large Language Models from BERT to GPT-4 - size, training budget,
    and organizations.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4：从BERT到GPT-4的大型语言模型 - 大小、训练预算和组织。
- en: The size of the data points indicates training cost in terms of petaFLOP days.
    For some models, especially for proprietary and closed-source models this information
    is not known - in these cases, I've placed a cross. For example, for XLNet, the
    paper doesn't give the information about compute in flops, however, the training
    was done on 512 TPU v3 chips over 2.5 days. The colors of the data points show
    the company or organization developing the model - since these might not come
    out in the print or on the Kindle (unless you have a Kindle color), you can find
    a color version of this graph at this URL:The development of GPT models has seen
    significant progress, with OpenAI's GPT-n series leading the way in creating foundational
    AI models. The size of the training corpus for LLMs has been increasing drastically.
    GPT-1, introduced by OpenAI in 2018, was trained on BookCorpus with 985 million
    words. BERT, released in the same year, was trained on a combined corpus of BookCorpus
    and English Wikipedia, totaling 3.3 billion words. Now, training corpora for LLMs
    reach up to trillions of tokens.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 数据点的大小表示以petaFLOP天为单位的训练成本。对于一些模型，特别是专有和闭源模型，这些信息是未知的 - 在这些情况下，我放了一个叉。例如，对于XLNet，论文没有提供有关flops计算的信息，然而，训练是在512个TPU
    v3芯片上进行的，历时2.5天。数据点的颜色显示了开发模型的公司或组织 - 由于这些颜色可能在打印版或Kindle上不明显（除非您有一款彩色Kindle），您可以在此URL找到此图的彩色版本：GPT模型的发展取得了显著进展，OpenAI的GPT-n系列引领着创建基础AI模型的道路。LLM的训练语料库规模急剧增加。OpenAI于2018年推出的GPT-1是在包含985百万个单词的BookCorpus上进行训练的。BERT于同一年发布，是在BookCorpus和英文维基百科的合并语料库上进行训练的，总计33亿个单词。现在，LLM的训练语料库已经扩展到数万亿个标记。
- en: 'A **foundation model** (sometimes: base model) is a large model, which was
    trained on an immense quantity of data at scale so that it can be adapted to a
    wide range of downstream tasks. In GPT models, this pre-training is done self-supervised
    learning.'
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**基础模型**（有时称为基础模型）是一个大型模型，它在规模上经过大量数据的训练，以便可以适应各种下游任务。在GPT模型中，这种预训练是通过自监督学习完成的。'
- en: Trained on 300 billion tokens, **GPT-3** has **175 billion parameters**, an
    unprecedented size for deep learning models. **GPT-4** is the most recent in the
    series, though its size and training details have not been published due to competitive
    and safety concerns, however, different estimates were putting it between **200
    and 500 billion parameters**. *Sam Altman*, the CEO of OpenAI has stated that
    the cost of training **GPT-4** was more than $100 million.ChatGPT, a conversation
    model, was released by **OpenAI** in November 2022\. Based on prior **GPT** models
    (particularly **GPT-3**) and optimized for dialogues, it uses a combination of
    human-generated roleplaying conversations and a dataset of human labeler demonstrations
    of the desired model behavior. The model exhibits excellent capabilities such
    as wide-ranging knowledge retention and precise context tracking in multi-turn
    dialogues.Another substantial advancement came with **GPT-4**, which extends beyond
    text input to include multimodal signals, in March 2023\. **GPT-4** provides superior
    performance on various evaluation tasks coupled with significantly better response
    avoidance to malicious or provocative queries due to six months of iterative alignment
    during training.Other notable foundational GPT models beside OpenAI's includes
    Google's **PaLM2**. Although GPT-4 leads most benchmarks in performance, these
    and other models demonstrate a comparable performance in some tasks, and have
    contributed to advancements in generative transformer-based language models. Meta's
    **LLaMA** was trained on **1.4 trillion tokens**, while **PaLM2**, the model behind
    Google's chatbot, **Bard**, consists of **340 billion parameters** - smaller than
    previous LLMs - appears to have a larger scale of training data in at least 100
    languages.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 经过对3000亿标记的训练，**GPT-3**拥有**1750亿参数**，这是深度学习模型中前所未有的规模。**GPT-4**是该系列中最新的版本，尽管由于竞争和安全问题，其规模和训练细节尚未公布，但不同的估计将其参数放在**200到5000亿之间**。*Sam
    Altman*，OpenAI的首席执行官表示，训练**GPT-4**的成本超过了1亿美元。ChatGPT是一个对话模型，由**OpenAI**于2022年11月发布。基于之前的**GPT**模型（特别是**GPT-3**）并针对对话进行了优化，它使用人类生成的角色扮演对话和人类标记者演示所需模型行为的数据集。该模型展示了出色的能力，如广泛的知识保留和在多轮对话中精确的上下文跟踪。另一个重大进展是**GPT-4**，于2023年3月推出，它不仅限于文本输入，还包括多模态信号。**GPT-4**在各种评估任务上表现出优越的性能，同时由于训练期间六个月的迭代对齐，对恶意或挑衅性查询的响应避免能力显著提高。除OpenAI之外，其他值得注意的基础GPT模型还包括谷歌的**PaLM2**。尽管**GPT-4**在性能方面领先大多数基准测试，但这些和其他模型在某些任务中表现出可比较的性能，并促进了基于生成变压器的语言模型的进步。Meta的**LLaMA**训练了**1.4万亿标记**，而谷歌聊天机器人**Bard**背后的模型**PaLM2**由**3400亿参数**组成，比以前的LLM规模小，但在至少100种语言的训练数据上似乎具有更大的规模。
- en: There are quite a few **companies and organizations developing LLMs**, and they
    are releasing them on different terms. OpenAI has released GPT-2 and subsequent
    models have been closed-source, but open for usage on their website or through
    an API. Meta is releasing models from RoBERTa, BART to LLaMA including parameters
    (the weights) of the models, although under a non-commercial license, and the
    source code for setting up and training the models. Google AI and their DeepMind
    division have developed a number of large language models, including BERT, GPT-2,
    LaMDA, Chinchilla, Gopher, PaLM, and PaLM2\. They've been releasing the code and
    weights of a few of their models under open-source licensing, even though recently
    they have moved towards more secrecy in their development. Microsoft has developed
    a number of large language models, including Turing NLG and Megatron-Turing NLG,
    however, they have integrated OpenAI models into Microsoft 365 and Bing. Technology
    Innovation Institute (TII), an Abu Dhabi government funded research institution,
    has open-sourced Falcon LLM for research and commercial usage.
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 有相当多的**公司和组织正在开发LLM**，并以不同的条件发布它们。OpenAI发布了GPT-2，随后的模型是闭源的，但可以通过他们的网站或API使用。Meta发布了从RoBERTa、BART到LLaMA的模型，包括模型的参数（权重），尽管在非商业许可下，以及用于设置和训练模型的源代码。Google
    AI及其DeepMind部门开发了许多大型语言模型，包括BERT、GPT-2、LaMDA、Chinchilla、Gopher、PaLM和PaLM2。他们已经在开源许可下发布了一些模型的代码和权重，尽管最近他们在开发中更趋向于保密。微软开发了许多大型语言模型，包括Turing
    NLG和Megatron-Turing NLG，但他们已将OpenAI模型整合到Microsoft 365和必应中。阿布扎比政府资助的科技创新研究所（TII）已经为研究和商业用途开源了Falcon
    LLM。
- en: GPT models can also work with modalities beyond text for input and output, as
    seen in GPT-4's ability to process image input alongside text. Additionally, they
    serve as a foundation for text-to-image technologies like diffusion and parallel
    decoding, enabling the development of **visual foundation models** (**VFMs**)
    for systems that work with images.In summary, GPT models have evolved rapidly,
    enabling the creation of versatile foundational AI models suitable for a wide
    range of downstream tasks and modalities, ultimately driving innovation across
    various applications and industries.In the next section, we'll review the progress
    deep learning and generative models have been making over recent years that leads
    up to the current explosion of apparent capabilities and the attention these models
    have been getting.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Why now?
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The success of generative AI coming into the public spotlight in 2022 can be
    attributed to several interlinked drivers. The development and success of generative
    models have relied on improved algorithms, considerable advances in compute power
    and hardware design, the availability of large labelled datasets, and an active
    and a collaborative research community helping evolve a set of tools and techniques.The
    development of more sophisticated mathematical and computational methods has played
    a vital role in the advancement of generative models. The backpropagation algorithm
    introduced in the 1980s by Geoffrey Hinton, David Rumelhart, and Ronald Williams
    is one such example. It provided a way to effectively train multi-layer neural
    networks.In the 2000s, **neural networks** began to regain popularity as researchers
    developed more complex architectures. However, it was the advent of deep learning,
    a type of neural network with numerous layers, which marked a significant turning
    point in the performance and capabilities of these models. Interestingly, although
    the concept of deep learning had existed for some time, the development and expansion
    of generative models correlate with significant advances in hardware, particularly
    **graphics processing units** (**GPUs**), which have been instrumental in propelling
    the field forward.As mentioned, the availability of cheaper and more powerful
    hardware has been a key factor in the development of deeper models. This is because
    deep learning models require a lot of computing power to train and to run. This
    concerns all aspects of processing power, memory, and disk space. This graph shows
    the cost of computer storage over time for different mediums such as disks, solid
    state, flash, and internal memory in terms of price in dollars per terabyte (source:
    Our World in Data; [https://ourworldindata.org/moores-law](https://ourworldindata.org/moores-law)):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5: Cost of computer storage since the 1950s in dollars per terrabyte.](../media/file4.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.5: Cost of computer storage since the 1950s in dollars per terrabyte.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: While the past, training a deep learning model was prohibitively expensive,
    as the cost of hardware has come down, it has become possible to train bigger
    models on much larger datasets. The model size is one of the factors determining
    how well a model can approximate (as measured in perplexity) the training dataset.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，训练深度学习模型的成本非常昂贵，但随着硬件成本的降低，现在可以在更大的数据集上训练更大的模型。模型大小是决定模型能够多好地逼近（以困惑度衡量）训练数据集的因素之一。
- en: '**The importance of the number of parameters in an LLM**: The more parameters
    a model has, the higher its capacity to capture relationships between words and
    phrases as knowledge. As a simple example for these higher-order correlations,
    an LLM could learn that the word "cat" is more likely to be followed by the word
    "dog" if it is preceded by the word "chase," even if there are other words in
    between. Generally, the lower a model''s perplexity, the better it will perform,
    for example in terms of answering questions. Particularly, it seems that in models
    consisting of in the range of between 2 to 7 billion parameters new capabilities
    emerge such as the ability to generate different creative text formats, like poems,
    code, scripts, musical pieces, email, letters, and answer questions in an informative
    way, even if they are open-ended and challenging.'
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**LLM中参数数量的重要性**：模型的参数越多，其捕捉单词和短语之间关系的能力就越强。举个简单的例子来说明这些高阶相关性，LLM可以学习到单词“猫”如果前面是“追逐”这个词，那么后面更可能是“狗”，即使中间有其他词。一般来说，模型的困惑度越低，它的表现就越好，比如在回答问题方面。特别是，在由20亿到70亿参数组成的模型中，似乎会出现新的能力，比如生成不同的创意文本格式，如诗歌、代码、脚本、音乐作品、电子邮件、信件，并以信息丰富的方式回答问题，即使这些问题是开放性和具有挑战性的。'
- en: 'This trend towards larger models started around 2009, when Nvidia catalyzed
    what is often called the "big bang" of deep learning. GPUs are particularly well-suited
    for the matrix/vector computations necessary to train deep-learning neural networks,
    therefore significantly increasing the speed and efficiency of these systems by
    several orders of magnitude, and reducing running times from weeks to days. In
    particular, Nvidia''s **CUDA** platform, which allows direct programming of GPUs,
    has made it easier than ever for researchers and developers to experiment with
    and deploy complex generative models.In the 2010s, different types of generative
    models started getting traction. Autoencoders, a kind of neural network that can
    learn to compress data from the input layer to a representation, and then reconstructs
    the input, served as a basis for more advanced models like **Variational Autoencoders**
    (**VAEs**) first proposed in 2013\. **VAEs**, unlike traditional autoencoders,
    use variational inference to learn the distribution of data, also called the latent
    space of input data.Around the same time, **Generative Adversarial Networks**
    (**GANs**) were proposed by Ian Goodfellow and others in 2014\. The setup for
    training GANs is illustrated in this diagram (taken from "A Survey on Text Generation
    using Generative Adversarial Networks", G de Rosa and J P. Papa, 2022; [https://arxiv.org/pdf/2212.11119.pdf](https://arxiv.org/pdf/2212.11119.pdf)):'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这种朝着更大模型的趋势始于2009年，当Nvidia推动了深度学习的“大爆炸”。GPU特别适合进行训练深度学习神经网络所需的矩阵/向量计算，因此显著提高了这些系统的速度和效率，将运行时间从几周缩短到几天。特别是，Nvidia的**CUDA**平台允许直接编程GPU，使研究人员和开发人员可以更轻松地尝试和部署复杂的生成模型。在2010年代，不同类型的生成模型开始受到关注。自编码器是一种可以学习将数据从输入层压缩到表示层，然后重构输入的神经网络，为更高级的模型如2013年首次提出的**变分自编码器**（**VAEs**）奠定了基础。**VAEs**与传统自编码器不同，它使用变分推断来学习数据的分布，也称为输入数据的潜在空间。与此同时，**生成对抗网络**（**GANs**）在2014年由Ian
    Goodfellow等人提出。训练GANs的设置如下图所示（摘自“使用生成对抗网络进行文本生成的调查”，G de Rosa和J P. Papa，2022年；[https://arxiv.org/pdf/2212.11119.pdf](https://arxiv.org/pdf/2212.11119.pdf)）：
- en: '![Figure 1.6: Generative Adversarial Network (GAN) training.](../media/file5.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图1.6：生成对抗网络（GAN）训练。](../media/file5.png)'
- en: 'Figure 1.6: Generative Adversarial Network (GAN) training.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6：生成对抗网络（GAN）训练。
- en: The GANs consist of two networks that are pitted against each other in a game-like
    setting - the generator that generates new data, often images, and the discriminator
    which estimates the probability of the new data being real. As they compete against
    each other, GANs gets better at its task, being able to generate realistic images
    and other types of data.Over the past decade, significant advancements have been
    made in the fundamental algorithms used in deep learning, such as better optimization
    methods, more sophisticated model architectures, and improved regularization techniques.Transformer
    models, introduced in 2017, built upon this progress and enabled the creation
    of large-scale models like GPT-3\. Transformers rely on attention mechanisms,
    and resulted in a further leap in the performance of generative models. These
    models, such as Google's BERT and OpenAI's GPT series, can generate highly coherent
    and contextually relevant text.The development of transfer learning techniques,
    which allow a model pre-trained on one task to be fine-tuned on another, similar
    task, has also been significant. These techniques have made it more efficient
    and practical to train large generative models.Moreover, part of the rise of generative
    models can be attributed to the development of software libraries and tools (**TensorFlow**,
    **PyTorch**, **Keras**) specifically designed to work with these artificial neural
    networks, streamlining the process of building, training, and deploying them.To
    further drive the development of generative models, the research community has
    regularly held challenges like ImageNet for image classification, and has started
    to do the same for generative models, with competitions such as the Generative
    Adversarial Networks (GAN) Competition.In addition to the availability of cheaper
    and more powerful hardware, the availability of large datasets of labeled data
    has also been a key factor in the development of generative models. This is because
    deep learning models and generative models in particular require vast amounts
    of text data for effective training. The explosion of data available from the
    internet, particularly in the last decade, created the suitable environment for
    such models to thrive. As the internet has become more popular, it has become
    easier to collect large datasets of text, images, and other data. This has made
    it possible to train generative models on much larger datasets than would have
    been possible in the past.In summary, generative modelling is a fascinating and
    rapidly evolving field. It has the potential to revolutionize the way we interact
    with computers and the way we create new content. I am excited to see what the
    future holds for this field.Let's get into the nitty gritty - how does this work?
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: GANs由两个网络组成，在类似游戏的设置中相互对抗 - 生成器生成新数据，通常是图像，鉴别器估计新数据为真实数据的概率。随着它们相互竞争，GANs在任务中变得更加优秀，能够生成逼真的图像和其他类型的数据。在过去的十年中，深度学习中使用的基本算法取得了重大进展，如更好的优化方法，更复杂的模型架构和改进的正则化技术。Transformer模型于2017年推出，建立在这一进展的基础上，实现了像GPT-3这样的大规模模型的创建。Transformers依赖于注意力机制，并导致生成模型性能的进一步飞跃。这些模型，如谷歌的BERT和OpenAI的GPT系列，可以生成高度连贯和上下文相关的文本。迁移学习技术的发展也是显著的，它允许在一个任务上预训练的模型在另一个类似任务上进行微调，这些技术使训练大型生成模型更加高效和实用。此外，生成模型的崛起部分归因于专门设计用于处理这些人工神经网络的软件库和工具（**TensorFlow**，**PyTorch**，**Keras**），简化了构建、训练和部署它们的过程。为了进一步推动生成模型的发展，研究界定期举办像ImageNet这样的图像分类挑战赛，并已开始为生成模型做同样的事情，如生成对抗网络（GAN）竞赛。除了更便宜和更强大的硬件的可用性外，标记数据的大型数据集的可用性也是生成模型发展的关键因素。这是因为深度学习模型，特别是生成模型，需要大量的文本数据进行有效训练。互联网上数据的爆炸性增长，尤其是在过去的十年中，为这些模型蓬勃发展创造了适当的环境。随着互联网的普及，收集大量文本、图像和其他数据集变得更加容易。这使得在比过去可能的更大数据集上训练生成模型成为可能。总之，生成建模是一个迷人且快速发展的领域。它有潜力彻底改变我们与计算机的互动方式以及我们创造新内容的方式。我很期待看到这个领域的未来。让我们深入了解细节
    - 这是如何工作的？
- en: How does it work?
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这是如何工作的？
- en: 'Models such as BERT and GPT were made possible by the **Transformer** deep
    neural network architecture, which has been a game changer for natural language
    processing. Designed to avoid recursion in order to allow parallel computation,
    the Transformer architecture, in different variations, is continuing to push the
    boundaries of what''s possible within the field of Natural Language Processing
    and Generative AI.One of the defining features of Transformers is the attention
    mechanism. Traditional sequence-to-sequence models often suffered from the problem
    of handling long dependencies - they had difficulty remembering relevant information
    if the sequences were too long. Transformer models introduced attention mechanisms
    to navigate this problem. The Self-Attention mechanism, often referred to as the
    core of the Transformer model, assigns a score to each word in the sequence, determining
    how much focus should be given to that word.Transformers consist of modules, which
    can be stacked, thereby creating very large models that can learn massive datasets.
    These are indicated in the diagram here:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.7: The Transformer architecture (credit: Yuening Jia, Wikimedia
    Commons)](../media/file6.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.7: The Transformer architecture (credit: Yuening Jia, Wikimedia Commons)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'The architectural features that have contributed to the success of Transformers:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '**Encoder-Decoder structure**: The Transformer model follows an Encoder-Decoder
    structure. The encoder takes the input sequence and computes a series of representations,
    (contextual embeddings), for each word. These representations consider not only
    the inherent meaning of the words (their semantic value) but also their context
    in the sequence. The decoder then uses this encoded information to generate the
    output sequence one item at a time, using the context of the previously generated
    items.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Positional Encoding**: Since Transformer doesn''t process words sequentially
    but instead processes all words simultaneously, it lacks any notion of the order
    of words. To remedy this, information about the position of words in the sequence
    is injected into the model using positional encodings. These encodings are added
    to the input embeddings representing each word, thus allowing the model to consider
    the order of words in a sequence.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layer Normalization**: To stabilize the network''s learning, Transformer
    uses a technique called Layer Normalization. This technique normalizes the model''s
    inputs across the features dimension (instead of the batch dimension as in Batch
    Normalization), thus improving the overall speed and stability of learning.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-Head Attention**: Instead of applying attention once, the Transformer
    applies it multiple times in parallel — improving the model''s ability to focus
    on different types of information and thus capturing a richer combination of features.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another, optional architectural feature, which is not specific to Transformers
    are Skip Connections (also: Residual Connections). These were introduced to mitigate
    degradation problems as networks get deeper, Skip Connections are used. This allows
    gradients to flow unchanged across layers by shortcutting the inputs to deeper
    layers.Transformers have pushed the envelope in NLP, especially in translation
    and language understanding. **Neural machine translation** (**NMT**) is a mainstream
    approach to machine translation that uses deep learning to capture long-range
    dependencies in the sentence. **NMT** based on Transformers outperformed previous
    approaches such as using recurrent neural networks, in particular **long short-term
    memory** (**LSTM**) networks. This can be attributed to this powerful architecture
    including first and foremost attention, which allows the Transformer model to
    treat word order in a flexible manner, no matter how far apart they are, that
    is optimal for each specific situation.Furthermore, the combination of these architectural
    features allows it to successfully tackle tasks that involve understanding and
    generating human language and other domains. OpenAI''s powerful GPT models for
    language generation is a Transformer as well, as is **DeepMind''s AlphaFold 2**,
    a model that predicts protein structure from their genetic sequences.Transformers
    have been able to maintain performance across longer sequences better than other
    models, for example recurrent neural networks. This has been contributing to their
    success, however, the transformer architecture means that they can capture only
    dependencies within a fixed input width. Early attention mechanisms scaled quadratically
    with the number of data points, rendering them inapplicable to settings with large
    amounts of inputs. There have been many proposed approaches to obtain efficiency
    gains such as sparse, low-rank self-attention, and latent bottlenecks to name
    just a few. Other work tried to extend sequences beyond the fixed input size,
    architectures such as Transformer-XL reintroduce recursion by storing hidden states
    of already encoded sentences to leverage them in the subsequent encoding of the
    next sentences.The particularity of GPTs and the origin of their name is the pre-training.
    Let''s see how these LLMs are trained!'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Pre-training
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The first step in training a **Large Language Model** (**LLM**) is tokenization.
    This process involves converting words into numbers so that they can be processed
    by the model given that **LLMs** are mathematical functions that require numerical
    inputs and outputs.To carry out this tokenization, **LLMs** use a unique tokenizer.
    Tokenizers map words in text to corresponding lists of integers. Before training
    the **LLM**, the tokenizer is typically fitted to the entire training dataset
    and then frozen. A common type of tokenizer employed is byte pair encoding. It's
    important to note that tokenizers do not produce arbitrary integers. Instead,
    they output integers within a specific range - from to where represents the vocabulary
    size of the tokenizer.Now, considering outputs, when LLM receives a text, it mainly
    yields a vector falling within . This output vector is then passed through a softmax
    function to yield another vector, which is referred to as a probability vector.
    With its entries being non-negative and summing up to , this vector can be interpreted
    as the probability distribution over the vocabulary of the **LLM**.Also, it is
    necessary to point out that **LLMs** can only generate a token based on a sequence
    of tokens that does not exceed its context window. This context window refers
    to the length of the longest sequence of tokens that a **LLM** can use. If a sequence
    longer than this window is presented, the **LLM** would need to either truncate
    the sequence or employ algorithmic modifications to handle it. Typical context
    window sizes for **LLMs** can range from about 1,000 to 10,000 tokens.Training
    **LLMs** involves a specific process of tokenizing input data, feeding it into
    the model, and generating an output that is a probability distribution over the
    model's vocabulary. The specific mechanisms within this process, such as the softmax
    function and context window, help to facilitate **LLMs** understanding of and
    response to input data.**Negative Log-Likelihood** (**NLL**) and **Perplexity**
    (**PPL**) are important metrics used in the process of training and evaluating
    language models. **NLL** is a loss function used in machine learning algorithms,
    aimed at maximizing the probability of correct predictions. A lower **NLL** indicates
    that the network has successfully learned patterns from the training set, and
    therefore it will be able to accurately predict the labels of the training samples.
    It's important to mention that **NLL** is a value constrained within a positive
    interval.**Perplexity** (**PPL**), on the other hand, is an exponentiation of
    the **NLL**, providing a more intuitive way to understand the model's performance.
    Smaller **PPL** values indicate a well-trained network that can predict accurately
    while higher values indicate poor learning performance. Intuitively, we could
    say that a low perplexity means that the model is less surprised by the next word.
    Therefore, the goal in pre-training is to minimize perplexity which means the
    model's predictions align more with the actual outcomes.In comparing different
    language models, perplexity is often used as a benchmark metric across various
    tasks. It gives an idea about how well the language model is performing, where
    a lower perplexity indicates the model is more certain of its predictions. Hence,
    a model with lower perplexity would be considered better performing in comparison
    to others with higher perplexity.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Scaling
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is worth talking at least briefly about the choice of architecture, and why
    these models are as large as they are. In a paper from 2020 by researchers from
    OpenAI, Kaplan and others discussed scaling laws and choice of parameters.Interestingly,
    they compare lots of different architecture choices, and, among other things,
    show that transformers outperform **LSTMs** as language models in terms of perplexity
    in no small part due to improved use of long contexts - while these recurrent
    networks plateau after less than 100 tokens, transformers improve through the
    whole context. Therefore, transformers come not only with better training and
    inference speed with respect to the transformer, but also give better performance
    when looking at relevant contexts.Further, they found a power-law relationship
    of dataset size, model size (number of parameters), and the amount of compute
    for training, in the sense that in order to improve performance by a certain factor,
    one of these factors have to scaled-up by as the power of the factor, however,
    for optimal performance all three factors must be scaled in tandem in order not
    to get a bottleneck effect.Researchers at **DeepMind** (Hoffmann and others, 2022)
    analyzed training compute and dataset size of **LLMs**, and concluded that a **LLMs**
    are undertrained in terms of compute budget and dataset size as suggested by scaling
    laws. They predicted that large models would perform better if substantially smaller
    and trained much longer than, and - in fact - validated their prediction comparing
    a **70 billion parameter Chinchilla model** on a benchmark to their Gopher model,
    which consists of **280 billion parameters**.More recently, the team at has been
    found that longer training in terms of epochs or more compute in terms of petaflops
    didn't seem to improve performance anymore, and smaller networks and higher-quality
    datasets can give very competitive performance.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Conditioning
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Conditioning large language models refers to adapting the model for specific
    tasks. Different methods of conditioning include fine-tuning, prompting, instruction
    tuning, and reinforcement learning:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning involves modifying a pre-trained language model by training it on
    a specific task using supervised learning. For example, to make a model more amenable
    to chats with humans, the model is trained on examples of tasks formulated as
    natural language instructions (instruction tuning).
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompting techniques present problems as text prompts and expect model completions.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For fine-tuning, usually, reinforcement learning combines supervised fine-tuning
    with reinforcement learning using human feedback to train the model based on human
    preferences. LLMs can be trained on a training set of examples which are themselves
    generated by an LLM (bootstrapped from a small initial set of human-generated
    examples) such as in the training set for phi-1 by Microsoft Research ("Textbooks
    Are All You Need", June 2023).With prompting techniques text examples of similar
    problems and their solutions will be presented. Zero-shot prompting involves no
    solved examples, while few-shot prompting includes a small number of examples
    of similar (problem, solution) pairs.These conditioning methods continue to evolve,
    becoming more effective and useful for a wide range of applications. Prompt engineering
    and conditioning methods will be explored further in *Chapter 8*, *Prompt Engineering*.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: How to give it a go?
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can access OpenAI's model through their website or through their API. If
    you want to try other large language models on your laptop, open-source LLMs are
    a good place to get started. There is a whole zoo of stuff out there!You can access
    these models through Hugginface or other providers. You can even download them,
    finetune them, or - if you are feeling really fancy - fully train a model. We'll
    look at using these models in more detail in *Chapter 9*. *LLM applications in
    Production*.In the next section, we'll look at stable diffusion and how it works.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: What is a Stable Diffusion model?
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image generation models are a type of generative model that can be used to generate
    images. Image generation models are a powerful tool that can be used to generate
    realistic and creative images. They are still in their early stages of development,
    but they have the potential to revolutionize the way we create and consume images.One
    of the most popular image generation models is **Stable Diffusion**, another one
    is **Midjourney**. In simplest terms these are a deep learning models that creates
    images given text prompts. Google Brain announced the creation of two text-to-image
    models, **Imagen** and **Parti**, in 2022.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **Stable Diffusion** model is a deep learning, text-to-image model developed
    by researchers from the CompVis Group at Ludwig Maximilian University of Munich
    and Runway. It generates detailed images conditioned on text descriptions and
    utilizes a latent diffusion model architecture. The model''s source code and even
    the weights have been publicly released under the **CreativeML** **OpenRAIL**-**M
    License**, which "does not impose any restrictions on reuse, distribution, commercialization,
    adaptation." The model can be run on consumer hardware equipped with a modest
    GPU (for example the **GeForce 40** series).**Stable Diffusion** is a type of
    diffusion model that uses a Gumbel distribution to add noise to the image. The
    Gumbel distribution is a continuous probability distribution that is often used
    in machine learning because it is easy to sample from and it has the property
    to be more stable. Stability means that the model is less likely to get stuck
    in local minima, which can happen with other types of diffusion models.The model
    consists of a **variational autoencoder** (**VAE**), a **U-Net**, and a **text
    encoder**. The **VAE** has two parts, an encoder and a decoder, compressing the
    original high-dimensional image into a lower dimensional latent space and reconstructing
    it back into the image space. The latent space significantly decreases computational
    complexity, making the diffusion process faster.The **VAE** encoder compresses
    images into a latent space, while the **U-Net** performs denoising from forward
    diffusion to obtain a latent representation. The **VAE** decoder then generates
    the final image. The model can be flexibly conditioned on various modalities,
    including text, and leverages a cross-attention mechanism to incorporate conditioning
    information.A **U-Net** is a popular type of convolutional neural network (CNN)
    that has a symmetric encoder-decoder structure. It is commonly used for image
    segmentation tasks, but in the context of Stable Diffusion, it is utilized for
    predicting noise in the image. The U-Net takes the noisy image as input and processes
    it through a series of convolutional layers to extract features and learn representations.
    These convolutional layers, typically organized in a contracting path, reduce
    the spatial dimensions while increasing the number of channels.Once the contracting
    path reaches the bottleneck of the U-Net, it then expands through a symmetric
    expanding path. In the expanding path, transposed convolutions (also known as
    upsampling or deconvolutions) are applied to progressively upsample the spatial
    dimensions while reducing the number of channels.During the diffusion process,
    the U-Net''s expanding path takes the noisy image and reconstructs the latent
    representation from the forward diffusion. By comparing the reconstructed latent
    representation with the true latent representation, the U-Net predicts an estimation
    of the noise in the original image. This prediction of noise helps in the reverse
    diffusion process to recover the original image.Diffusion models operate through
    a process similar to diffusion in physics. It follows a **forward diffusion process**
    by adding noise to an image until it becomes uncharacteristic and noisy. This
    process is analogous to an ink drop falling into a glass of water and gradually
    diffusing.The unique aspect here is the **reverse diffusion process**, where the
    model attempts to recover the original image from a noisy, meaningless image.
    This result is achieved by subtracting estimated noise from the noisy image step-by-step,
    ultimately restoring an image resembling the original image.The denoising process
    is demonstrated in this plot (source: User Benlisquare on Wikimedia Commons):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.8: European-style castle in Japan, created using the Stable Diffusion
    V1-5 AI diffusion model. Only steps within a 40-step generation process are shown.](../media/file7.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.8: European-style castle in Japan, created using the Stable Diffusion
    V1-5 AI diffusion model. Only steps within a 40-step generation process are shown.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'In the plot, you can see the image generation step by step, a U-Net denoising
    process using the DDIM sampling method, which repeatedly removes Gaussian noise,
    and then decodes the denoised output into pixel space.Stable Diffusion is a deep
    learning model that leverages a diffusion process to generate images from text
    prompts through several clear steps:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: It starts by producing a random tensor (random image) in the latent space, which
    serves as the noise for our initial image. ]
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A noise predictor (**U-Net**) takes in both the latent noisy image and the provided
    text prompt and predicts the noise. ]
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model then subtracts the latent noise from the latent image.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Steps 2 and 3 are repeated for a set number of sampling steps, for instance,
    40 times as shown in the plot.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the decoder component of the **VAE** transforms the latent image back
    into pixel space, providing the final output image.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During the training of image generation models, a loss function is used to evaluate
    the quality of the generated images. One commonly used loss function is the **mean
    squared error** (**MSE**) loss, which quantifies the difference between the generated
    image and the target image. The model is optimized to minimize this loss, encouraging
    it to generate images that closely resemble the desired output.The model was trained
    on a dataset called **LAION-5B**, derived from Common Crawl data, comprising billions
    of image-text pairs. The training dataset was classified based on language, resolution,
    watermark likelihood, and aesthetic score. Stable Diffusion was trained on subsets
    of this dataset. The model's training data had a diverse range of sources, with
    a significant portion coming from websites such as **Pinterest**, **WordPress**,
    **Blogspot**, **Flickr**, **DeviantArt**.Overall, image generation models such
    as Stable Diffusion and Midjourney process textual prompts into generated images,
    leveraging the concept of forward and reverse diffusion processes and operating
    in a lower dimensional latent space for efficiency.There are two main model versions
    so far of **Stable Diffusion**, **version 1** and **2**. Let's see how they differ.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Model differences
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Stable Diffusion v1** and **v2** are different in terms of text processing,
    training data and their results. In terms of text processing, **Stable Diffusion
    v2** uses **OpenClip** for text embedding while **v1** uses **Open AI''s** **CLIP
    ViT-L/14** for text embedding. **OpenClip** is five times larger than **CLIP**,
    which improves the image quality, and it also gives researchers more transparency
    in studying and optimizing the model.Regarding training data, **Stable Diffusion
    v1.4** is trained with three different datasets, while **Stable Diffusion v2**
    is trained on a subset of **LAION-5B** filtered for explicit pornographic material
    (**NSFW filter**) and an aesthetic score above a threshold. The **LAION 5B** dataset
    is a large-scale dataset consisting of 5.85 billion CLIP-filtered image-text pairs.
    Over 2.3 billion samples in the dataset contain English language, while **2.2
    billion samples** originate from over 100 other languages. The remaining one billion
    samples do not allow a certain language assignment, such as names.The acquisition
    pipeline for the dataset was complex and required a lot of processing. It includes
    distributed processing of a petabyte-scale Common Crawl dataset, distributed download
    of images, and few **GPU** node post-processing of the data, which produces the
    final dataset. The filtering also removed duplicate samples and trimmed the dataset
    from 50 billion candidates to just below 6 billion **CLIP**-filtered image-text
    pairs.In terms of outcomes, Stable Diffusion v2 is harder to use for controlling
    styles and generating celebrities. This difference is likely due to the training
    data difference, as Open AI''s proprietary data may have more artwork and celebrity
    photos, which are not included in Stable Diffusion v2 training data.In summary,
    **Stable Diffusion v2** used a different text embedding model and trained on a
    different subset of data, resulting in different outcomes compared to **Stable
    Diffusion v1**. While **Stable Diffusion v2** may be more transparent and better
    for long-term development, **Stable Diffusion v1** may have better results for
    particular use cases such as controlling styles or generating celebrities due
    to its training data.Now we''ll look at the conditioning for the model in the
    text-to-image use case.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Conditioning
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The conditioning process allow these models to be influenced by the input textual
    prompts or other input types like depth maps or outlines for greater precision
    in order to create relevant images. During conditioning, the prompt is tokenized,
    and each token is converted to an embedding, a vector of a certain length, sometimes
    768 values. These embeddings, which account for the semantic relationships between
    words, are then processed by a text transformer and fed to the noise predictor,
    steering it to produce an image that aligns with the text prompt.In the text-to-image
    process, the model uses a text prompt to generate a completely new image. The
    text prompt is encoded into the latent space, and a diffusion process gradually
    adds noise (controlled by denoising strength) to evolve the initial image towards
    the output image.Let's conclude the chapter!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative models like large language models (LLMs) have received considerable
    attention due to their potential to revolutionize numerous industries and tasks.
    Especially their applications in text generation and image synthesis have garnered
    significant media hype. Leading companies such as OpenAI are pushing the boundaries
    of LLMs, with their Generative Pre-trained Transformers (GPT) series capturing
    widespread attention for their impressive language generation capabilities. In
    this chapter, we've discussed the media attention the latest breakthroughs have
    been gathering, the recent history of deep learning and AI, generative models,
    and LLMs and Pre-trained Generative Models (GPTs) together with the theoretical
    ideas underpinning them, especially the Transformer architecture. We also discussed
    diffusion models for images, and applications for text, images, sound, and videos.The
    next chapter will explore the tooling of generative and particularly LLMs with
    the Langchain framework, focusing on the fundamentals, the implementation, and
    use of this particular tool in optimizing and enhancing LLMs.I think it's a good
    habit to check that you've digested the material when reading a technical book.
    I've created a few questions for this chapter.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you''ve read and understood this chapter, you should be able to answer these
    questions:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: What is a generative model?
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which applications exist for generative models?
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What's a large language model (LLM) and what does it do?
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we get bigger performance from LLMs?
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the conditions that make these models possible?
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which companies and organizations are the big players in developing LLMs?
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a transformer and what does it consist of?
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does GPT mean?
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does stable diffusion work?
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How is stable diffusion trained?
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you struggle to answer these questions, please refer back to the corresponding
    sections in this chapter to make sure you've understood the material.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
