["```\n!pip install -U accelerate bitsandbytes datasets transformers peft trl sentencepiece wandb langchain\n```", "```\nimport notebook_login\nnotebook_login()\n```", "```\nimport os\nos.environ[\"WANDB_PROJECT\"] = \"finetuning\"\n```", "```\nif wandb.run is not None:\n    wandb.finish()\n```", "```\nfrom datasets import load_dataset\ndataset_name = \"squad_v2\" \ndataset = load_dataset(dataset_name, split=\"train\")\neval_dataset = load_dataset(dataset_name, split=\"validation\") \n```", "```\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 130319\n    })\n    validation: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 11873\n    })\n}) \n```", "```\n{'id': Value(dtype='string', id=None), \n 'title': Value(dtype='string', id=None), \n 'context': Value(dtype='string', id=None), \n 'question': Value(dtype='string', id=None), \n 'answers': Sequence(feature={'text': Value(dtype='string', id=None), \n 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}\n```", "```\nmodel_id = \"openlm-research/open_llama_3b_v2\" \nnew_model_name = f\"openllama-3b-peft-{dataset_name}\"\n```", "```\nimport torch\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\ndevice_map=\"auto\"\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\nbase_model.config.use_cache = False\n```", "```\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\n```", "```\noutput_dir = \"/content/gdrive/My Drive/results\"\n```", "```\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n```", "```\nfrom transformers import TrainingArguments, EarlyStoppingCallback\nfrom peft import LoraConfig\n# More info: https://github.com/huggingface/transformers/pull/24906\nbase_model.config.pretraining_tp = 1\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\ntraining_args = TrainingArguments(\n    output_dir=output_dir, \n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-4,\n    logging_steps=10,\n    max_steps=2000,\n    num_train_epochs=100,\n    evaluation_strategy=\"steps\",\n    eval_steps=5,\n    save_total_limit=5 \n    push_to_hub=False, \n    load_best_model_at_end=True,\n    report_to=\"wandb\"\n)\n```", "```\nfrom trl import SFTTrainer\ntrainer = SFTTrainer(\n    model=base_model,\n    train_dataset=dataset,\n    eval_dataset=eval_dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"question\",  # this depends on the dataset!\n    max_seq_length=512,\n    tokenizer=tokenizer,\n    args=training_args,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=200)]\n)\ntrainer.train()\n```", "```\ntrainer.model.save_pretrained(\n    os.path.join(output_dir, \"final_checkpoint\"),\n)\n```", "```\ntrainer.model.push_to_hub(\n    repo_id=new_model_name\n)\n```", "```\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nfrom langchain.llms import HuggingFacePipeline\nmodel_id = 'openlm-research/open_llama_3b_v2'\nconfig = PeftConfig.from_pretrained(\"benji1a/openllama-3b-peft-squad_v2\")\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\nmodel = PeftModel.from_pretrained(model, \"benji1a/openllama-3b-peft-squad_v2\")\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_length=256\n)\nllm = HuggingFacePipeline(pipeline=pipe)\n```", "```\nfrom skllm.models.palm import PaLMClassifier\nclf = PaLMClassifier(n_update_steps=100)\nclf.fit(X_train, y_train) # y_train is a list of labels\nlabels = clf.predict(X_test)\n```", "```\nfrom skllm.models.gpt import GPTClassifier\nclf = GPTClassifier(\n        base_model = \"gpt-3.5-turbo-0613\",\n        n_epochs = None, # int or None. When None, will be determined automatically by OpenAI\n        default_label = \"Random\", # optional\n)\nclf.fit(X_train, y_train) # y_train is a list of labels\nlabels = clf.predict(X_test)\n```", "```\nfrom langchain import PromptTemplate\nprompt = PromptTemplate(\"Tell me a {adjective} joke about {topic}\")\n```", "```\nprompt.format(adjective=\"funny\", topic=\"chickens\")\n# Output: \"Tell me a funny joke about chickens\"\n```", "```\nfrom langchain.prompts import ChatPromptTemplate \ntemplate = ChatPromptTemplate.from_messages([\n  (\"human\", \"Hello, how are you?\"),\n  (\"ai\", \"I am doing great, thanks!\"),\n  (\"human\", \"{user_input}\"),\n])\ntemplate.format_messages(user_input=\"What is your name?\")\n```", "```\nfrom langchain.prompts import FewShotPromptTemplate, PromptTemplate \nexample_prompt = PromptTemplate(\"{input} -> {output}\")\nexamples = [\n  {\"input\": \"2+2\", \"output\": \"4\"},\n  {\"input\": \"3+3\", \"output\": \"6\"}\n]\nprompt = FewShotPromptTemplate(\n  examples=examples,\n  example_prompt=example_prompt\n)\n```", "```\nfrom langchain.prompts import SemanticSimilarityExampleSelector\nselector = SemanticSimilarityExampleSelector(...) \nprompt = FewShotPromptTemplate(\n   example_selector=selector,\n   example_prompt=example_prompt\n)\n```", "```\nfrom langchain.prompts import PromptTemplate\nreasoning_prompt = \"Explain your reasoning step-by-step. Finally, state the answer: {question}\"\nprompt = PromptTemplate(\n  reasoning_prompt=reasoning_prompt,\n  input_variables=[\"questions\"]\n)\n```", "```\nsolutions_template = \"\"\"\nGenerate {num_solutions} distinct solutions for {problem}. Consider factors like {factors}.\nSolutions:\n\"\"\"\nsolutions_prompt = PromptTemplate(\n   template=solutions_template,\n   input_variables=[\"problem\", \"factors\", \"num_solutions\"]\n)\n```", "```\nevaluation_template = \"\"\"\nEvaluate each solution in {solutions} by analyzing pros, cons, feasibility, and probability of success.\nEvaluations:\n\"\"\"\nevaluation_prompt = PromptTemplate(\n  template=evaluation_template,\n  input_variables=[\"solutions\"]  \n)\n```", "```\nreasoning_template = \"\"\"\nFor the most promising solutions in {evaluations}, explain scenarios, implementation strategies, partnerships needed, and handling potential obstacles. \nEnhanced Reasoning: \n\"\"\"\nreasoning_prompt = PromptTemplate(\n  template=reasoning_template,\n  input_variables=[\"evaluations\"]\n)\n```", "```\nranking_template = \"\"\"\nBased on the evaluations and reasoning, rank the solutions in {enhanced_reasoning} from most to least promising.\nRanked Solutions:\n\"\"\"\nranking_prompt = PromptTemplate(\n  template=ranking_template, \n  input_variables=[\"enhanced_reasoning\"]\n)\n```", "```\nchain1 = LLMChain(\n   llm=SomeLLM(),\n   prompt=solutions_prompt,\n   output_key=\"solutions\"  \n)\nchain2 = LLMChain(\n   llm=SomeLLM(),\n   prompt=evaluation_prompt,\n   output_key=\"evaluations\"\n)\n```", "```\ntot_chain = SequentialChain(\n   chains=[chain1, chain2, chain3, chain4],\n   input_variables=[\"problem\", \"factors\", \"num_solutions\"], \n   output_variables=[\"ranked_solutions\"]\n)\ntot_chain.run(\n   problem=\"Prompt engineering\",\n   factors=\"Requirements for high task performance, low token use, and few calls to the LLM\",\n   num_solutions=3\n)\n```"]