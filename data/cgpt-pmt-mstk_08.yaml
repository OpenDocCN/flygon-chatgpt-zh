- en: '| ![image](d2d_images/chapter_title_corner_decoration_left.png) |  | ![image](d2d_images/chapter_title_corner_decoration_right.png)
    |'
  id: totrans-0
  prefs: []
  type: TYPE_TB
  zh: '| ![图像](d2d_images/chapter_title_corner_decoration_left.png) |  | ![图像](d2d_images/chapter_title_corner_decoration_right.png)
    |'
- en: '![image](d2d_images/chapter_title_above.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![图像](d2d_images/chapter_title_above.png)'
- en: Attention Mechanism and Self-Attention
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意机制和自注意力
- en: '![image](d2d_images/chapter_title_below.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![图像](d2d_images/chapter_title_below.png)'
- en: Attention mechanisms in neural networks help the model focus on relevant parts
    of the input when generating an output.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 注意机制在神经网络中帮助模型在生成输出时专注于输入的相关部分。
- en: Self-attention is a specific type of attention mechanism used in Transformer
    architecture, allowing the model to weigh the importance of different words or
    tokens in a given context.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力是Transformer架构中使用的一种特定类型的注意机制，允许模型权衡给定上下文中不同单词或标记的重要性。
- en: Imagine you're reading a book about the history of pizza. Your brain automatically
    focuses on the most relevant information, like ingredients and cooking techniques,
    and ignores less important details. Attention mechanisms work similarly in neural
    networks.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你正在阅读一本关于比萨历史的书。你的大脑会自动聚焦在最相关的信息上，比如配料和烹饪技巧，并忽略不太重要的细节。注意机制在神经网络中的工作原理与此类似。
