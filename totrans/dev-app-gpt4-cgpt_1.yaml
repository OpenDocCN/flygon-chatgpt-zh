- en: Chapter 1\. GPT-4 and ChatGPT Essentials
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章 GPT-4和ChatGPT基础知识
- en: Imagine a world where you can communicate with computers as quickly as you can
    with your friends. What would that look like? What applications could you create?
    This is the world that OpenAI is helping to build with its GPT models, bringing
    human-like conversational capabilities to our devices. As the latest advancements
    in AI, GPT-4 and other GPT models are large language models (LLMs) trained on
    massive amounts of data, enabling them to recognize and generate human-like text
    with very high accuracy.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，您可以与计算机的交流速度与与朋友的交流一样快。那会是什么样子？您可以创建什么应用程序？这就是OpenAI正在帮助构建的世界，它将人类般的对话能力带到我们的设备上。作为人工智能的最新进展，GPT-4和其他GPT模型是在大量数据上训练的大型语言模型（LLMs），使它们能够识别和生成非常准确的人类文本。
- en: The implications of these AI models go far beyond simple voice assistants. Thanks
    to OpenAI’s models, developers can now exploit the power of natural language processing
    (NLP) to create applications that understand our needs in ways that were once
    science fiction. From innovative customer support systems that learn and adapt
    to personalized educational tools that understand each student’s unique learning
    style, GPT-4 and ChatGPT open up a whole new world of possibilities.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这些人工智能模型的影响远远超出了简单的语音助手。由于OpenAI的模型，开发人员现在可以利用自然语言处理（NLP）的力量来创建能够以前只存在于科幻小说中的应用程序，这些应用程序能够理解我们的需求。从学习和适应个性化教育工具的创新客户支持系统，到理解每个学生独特学习风格的应用，GPT-4和ChatGPT打开了全新的可能性世界。
- en: But what *are* GPT-4 and ChatGPT? The goal of this chapter is to take a deep
    dive into the foundations, origins, and key features of these AI models. By understanding
    the basics of these models, you will be well on your way to building the next
    generation of LLM-powered applications.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 但是GPT-4和ChatGPT到底是什么？本章的目标是深入探讨这些人工智能模型的基础、起源和关键特性。通过了解这些模型的基础知识，您将能够着手构建下一代LLM驱动的应用程序。
- en: Introducing Large Language Models
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍大型语言模型
- en: This section lays down the fundamental building blocks that have shaped the
    development of GPT-4 and ChatGPT. We aim to provide a comprehensive understanding
    of language models and NLP, the role of transformer architectures, and the tokenization
    and prediction processes within GPT models.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本节阐述了塑造GPT-4和ChatGPT发展的基本构建模块。我们旨在全面了解语言模型和NLP，变压器架构的作用，以及GPT模型中的标记化和预测过程。
- en: Exploring the Foundations of Language Models and NLP
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索语言模型和NLP的基础
- en: As LLMs, GPT-4 and ChatGPT are the latest type of model obtained in the field
    of NLP, which is itself a subfield of machine learning (ML) and AI. Before delving
    into GPT-4 and ChatGPT, it is essential to take a look at NLP and its related
    fields.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作为LLMs，GPT-4和ChatGPT是NLP领域获得的最新模型类型，NLP本身是机器学习（ML）和人工智能（AI）的一个子领域。在深入研究GPT-4和ChatGPT之前，了解NLP及其相关领域至关重要。
- en: There are different definitions of AI, but one of them, more or less the consensus,
    says that AI is the development of computer systems that can perform tasks that
    typically require human intelligence. With this definition, many algorithms fall
    under the AI umbrella. Consider, for example, the traffic prediction task in GPS
    applications or the rule-based systems used in strategic video games. In these
    examples, seen from the outside, the machine seems to require intelligence to
    accomplish these tasks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: AI有不同的定义，但其中一个更或多或少的共识是，AI是开发能够执行通常需要人类智能的任务的计算机系统。根据这个定义，许多算法都属于AI范畴。例如，考虑GPS应用程序中的交通预测任务或战略视频游戏中使用的基于规则的系统。在这些示例中，从外部看，机器似乎需要智能来完成这些任务。
- en: ML is a subset of AI. In ML, we do not try to directly implement the decision
    rules used by the AI system. Instead, we try to develop algorithms that allow
    the system to learn by itself from examples. Since the 1950s, when ML research
    began, many ML algorithms have been proposed in the scientific literature.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ML是AI的一个子集。在ML中，我们不试图直接实现AI系统使用的决策规则。相反，我们试图开发算法，让系统能够从示例中自行学习。自从20世纪50年代开始进行ML研究以来，许多ML算法已经在科学文献中提出。
- en: Among them, deep learning algorithms have come to the fore. *Deep learning*
    is a branch of ML that focuses on algorithms inspired by the structure of the
    brain. These algorithms are called *artificial neural networks*. They can handle
    very large amounts of data and perform very well on tasks such as image and speech
    recognition and NLP.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，深度学习算法已经成为前沿。*深度学习*是ML的一个分支，专注于受大脑结构启发的算法。这些算法称为*人工神经网络*。它们可以处理非常大量的数据，并在图像和语音识别以及NLP等任务上表现非常出色。
- en: GPT-4 and ChatGPT are based on a particular type of deep learning algorithm
    called *transformers*. Transformers are like reading machines. They pay attention
    to different parts of a sentence or block of text to understand its context and
    produce a coherent response. They can also understand the order of words in a
    sentence and their context. This makes them highly effective at tasks such as
    language translation, question answering, and text generation. [Figure 1-1](#fig_1_a_nested_set_of_technologies_from_ai_to_transforme)
    illustrates the relationships among these terms.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4和ChatGPT基于一种称为*transformers*的特定类型的深度学习算法。变压器就像阅读机器一样。它们关注句子或文本块的不同部分，以理解其上下文并产生连贯的回应。它们还可以理解句子中的单词顺序和它们的上下文。这使它们在语言翻译、问题回答和文本生成等任务中非常有效。[图1-1](#fig_1_a_nested_set_of_technologies_from_ai_to_transforme)说明了这些术语之间的关系。
- en: '![](assets/dagc_0101.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dagc_0101.png)'
- en: Figure 1-1\. A nested set of technologies from AI to transformers
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-1。从AI到变压器的嵌套技术集
- en: 'NLP is a subfield of AI focused on enabling computers to process, interpret,
    and generate natural human language. Modern NLP solutions are based on ML algorithms.
    The goal of NLP is to allow computers to process natural language text. This goal
    covers a wide range of tasks:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: NLP是人工智能的一个子领域，专注于使计算机能够处理、解释和生成自然人类语言。现代NLP解决方案基于机器学习算法。NLP的目标是让计算机处理自然语言文本。这个目标涵盖了广泛的任务：
- en: Text classification
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类
- en: Categorizing input text into predefined groups. This includes, for example,
    sentiment analysis and topic categorization. Companies can use sentiment analysis
    to understand customers’ opinions about their services. Email filtering is an
    example of topic categorization in which email can be put into categories such
    as “Personal,” “Social,” “Promotions,” and “Spam.”
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入文本分类为预定义的组。这包括情感分析和主题分类，例如，公司可以使用情感分析来了解客户对其服务的意见。电子邮件过滤是主题分类的一个例子，其中电子邮件可以被归类为“个人”，“社交”，“促销”和“垃圾邮件”。
- en: Automatic translation
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 自动翻译
- en: Automatic translation of text from one language to another. Note that this can
    include areas like translating code from one programming language to another,
    such as from Python to C++.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本从一种语言自动翻译成另一种语言。请注意，这可能包括将代码从一种编程语言翻译成另一种语言，例如从Python到C++。
- en: Question answering
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 回答基于给定文本的问题。
- en: Answering questions based on a given text. For example, an online customer service
    portal could use an NLP model to answer FAQs about a product, or educational software
    could use NLP to provide answers to students’ questions about the topic being
    studied.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 根据给定文本回答问题。例如，在线客户服务门户可以使用NLP模型回答关于产品的常见问题，或者教育软件可以使用NLP为学生关于所学主题的问题提供答案。
- en: Text generation
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 文本生成
- en: Generating a coherent and relevant output text based on a given input text,
    called a prompt.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 根据给定的输入文本生成连贯和相关的输出文本，称为提示。
- en: As mentioned earlier, LLMs are ML models trying to solve text generation tasks,
    among others. LLMs enable computers to process, interpret, and generate human
    language, allowing for more effective human–machine communication. To be able
    to do this, LLMs analyze or *train* on vast amounts of text data and thereby learn
    patterns and relationships between words in sentences. A variety of data sources
    can be used to perform this learning process. This data can include text from
    Wikipedia, Reddit, the archive of thousands of books, or even the archive of the
    internet itself. Given an input text, this learning process allows the LLMs to
    make predictions about the likeliest following words and, in this way, can generate
    meaningful responses to the input text. The modern language models, published
    in the past few months, are so large and have been trained on so many texts that
    they can now directly perform most NLP tasks, such as text classification, machine
    translation, question answering, and many others. The GPT-4 and ChatGPT models
    are modern LLMs that excel at text generation tasks.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，LLM是试图解决文本生成任务的机器学习模型之一。LLM使计算机能够处理、解释和生成人类语言，从而实现更有效的人机通信。为了能够做到这一点，LLM分析或*训练*大量文本数据，从而学习句子中单词之间的模式和关系。可以使用各种数据源来执行这个学习过程。这些数据可以包括来自维基百科、Reddit、成千上万本书的档案，甚至是互联网本身的档案。在给定输入文本的情况下，这个学习过程使LLM能够对接下来最有可能的单词进行预测，并以这种方式生成对输入文本有意义的响应。最近几个月发布的现代语言模型非常庞大，并且已经在许多文本上进行了训练，以至于它们现在可以直接执行大多数NLP任务，如文本分类、机器翻译、问答等。GPT-4和ChatGPT模型是在文本生成任务方面表现出色的现代LLM。
- en: The development of LLMs goes back several years. It started with simple language
    models such as *n-grams*, which tried to predict the next word in a sentence based
    on the previous words. N-gram models use *frequency* to do this. The predicted
    next word is the most frequent word that follows the previous words in the text
    the n-gram model was trained on. While this approach was a good start, n-gram
    models’ need for improvement in understanding context and grammar resulted in
    inconsistent text generation.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的发展可以追溯到几年前。它始于简单的语言模型，如*n-grams*，它试图根据先前的单词预测句子中的下一个单词。N-gram模型使用*频率*来做到这一点。预测的下一个单词是在n-gram模型训练的文本中跟随前面单词的最常见的单词。虽然这种方法是一个很好的开始，但n-gram模型对于理解上下文和语法的改进需要导致不一致的文本生成。
- en: To improve the performance of n-gram models, more advanced learning algorithms
    were introduced, including recurrent neural networks (RNNs) and long short-term
    memory (LSTM) networks. These models could learn longer sequences and analyze
    the context better than n-grams, but they still needed help processing large amounts
    of data efficiently. These types of recurrent models were the most efficient ones
    for a long time and therefore were the most used in tools such as automatic machine
    translation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改善n-gram模型的性能，引入了更先进的学习算法，包括循环神经网络（RNN）和长短期记忆（LSTM）网络。这些模型可以学习更长的序列，并且比n-grams更好地分析上下文，但它们仍然需要帮助有效地处理大量数据。这些类型的循环模型长时间以来一直是最有效的模型，因此在自动机器翻译等工具中被最广泛使用。
- en: Understanding the Transformer Architecture and Its Role in LLMs
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解变压器架构及其在LLM中的作用
- en: 'The Transformer architecture revolutionized NLP, primarily because transformers
    effectively address one of the critical limitations of previous NLP models such
    as RNNs: their struggle with long text sequences and maintaining context over
    these lengths. In other words, while RNNs tended to forget the context in longer
    sequences (the infamous “catastrophic forgetting”), transformers came with the
    ability to handle and encode this context effectively.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器架构革命性地改变了NLP，主要是因为变压器有效地解决了以前NLP模型（如RNN）的一个关键限制：它们在处理长文本序列和保持这些长度上下文方面的困难。换句话说，虽然RNN在处理更长序列时往往会忘记上下文（臭名昭著的“灾难性遗忘”），但变压器具有处理和有效编码这种上下文的能力。
- en: The central pillar of this revolution is the *attention mechanism*, a simple
    yet powerful idea. Instead of treating all words in a text sequence as equally
    important, the model “pays attention” to the most relevant terms for each step
    of its task. Cross-attention and self-attention are two architectural blocks based
    on this attention mechanism, and they are often found in LLMs. The Transformer
    architecture makes extensive use of these cross-attention and self-attention blocks.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '*Cross-attention* helps the model determine the relevance of the different
    parts of the input text for accurately predicting the next word in the output
    text. It’s like a spotlight that shines on words or phrases in the input text,
    highlighting the relevant information needed to make the next word prediction
    while ignoring less important details.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this, let’s take an example of a simple sentence translation task.
    Imagine we have an input English sentence, “Alice enjoyed the sunny weather in
    Brussels,” which should be translated into French as “Alice a profité du temps
    ensoleillé à Bruxelles.” In this example, let us focus on generating the French
    word *ensoleillé*, which means *sunny*. For this prediction, cross-attention would
    give more weight to the English words *sunny* and *weather* since they are both
    relevant to the meaning of *ensoleillé*. By focusing on these two words, cross-attention
    helps the model generate an accurate translation for this part of the sentence.
    [Figure 1-2](#fig_2_cross_attention_uses_the_attention_mechanism_to_fo) illustrates
    this example.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dagc_0102.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: Figure 1-2\. Cross-attention uses the attention mechanism to focus on essential
    parts of the input text (English sentence) to predict the next word in the output
    text (French sentence)
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*Self-attention* refers to the ability of a model to focus on different parts
    of its input text. In the context of NLP, the model can evaluate the importance
    of each word in a sentence with the other words. This allows it to better understand
    the relationships between the words and helps the model build new *concepts* from
    multiple words in the input text.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'As a more specific example, consider the following: “Alice received praise
    from her colleagues.” Assume that the model is trying to understand the meaning
    of the word *her* in the sentence. The self-attention mechanism assigns different
    weights to the words in the sentence, highlighting the words relevant to *her*
    in this context. In this example, self-attention would place more weight on the
    words *Alice* and *colleagues*. Self-attention helps the model build new concepts
    from these words. In this example, one of the concepts that could emerge would
    be “Alice’s colleagues,” as shown in [Figure 1-3](#fig_3_self_attention_allows_the_emergence_of_the_alice).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dagc_0103.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: Figure 1-3\. Self-attention allows the emergence of the “Alice’s colleagues”
    concept
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Unlike the recurrent architecture, transformers also have the advantage of being
    easily *parallelized*. This means the Transformer architecture can process multiple
    parts of the input text simultaneously rather than sequentially. This allows faster
    computation and training because different parts of the model can work in parallel
    without waiting for previous steps to complete, unlike recurrent architectures,
    which require sequential processing. The parallel processing capability of transformer
    models fits perfectly with the architecture of graphics processing units (GPUs),
    which are designed to handle multiple computations simultaneously. Therefore,
    GPUs are ideal for training and running these transformer models because of their
    high parallelism and computational power. This advance allowed data scientists
    to train models on much larger datasets, paving the way for developing LLMs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'The Transformer architecture, introduced in 2017 by Vaswani et al. from Google
    in the paper “[Attention Is All You Need”](https://oreil.ly/jVZW1), was originally
    developed for sequence-to-sequence tasks such as machine translation. A standard
    transformer consists of two primary components: an encoder and a decoder, both
    of which rely heavily on attention mechanisms. The task of the encoder is to process
    the input text, identify valuable features, and generate a meaningful representation
    of that text, known as *embedding*. The decoder then uses this embedding to produce
    an output, such as a translation or summary. This output effectively interprets
    the encoded information.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构于2017年由Google的Vaswani等人在论文“[Attention Is All You Need”](https://oreil.ly/jVZW1)中引入，最初是为了序列到序列的任务，如机器翻译。标准的Transformer由两个主要组件组成：编码器和解码器，两者都严重依赖于注意机制。编码器的任务是处理输入文本，识别有价值的特征，并生成该文本的有意义表示，称为*嵌入*。解码器然后使用这个嵌入来产生一个输出，比如翻译或摘要。这个输出有效地解释了编码信息。
- en: '*Generative pre-trained transformers*, commonly known as *GPT*, are a family
    of models that are based on the Transformer architecture and that specifically
    utilize the decoder part of the original architecture. In GPT, the encoder is
    not present, so there is no need for cross-attention to integrate the embeddings
    produced by an encoder. As a result, GPT relies solely on the self-attention mechanism
    within the decoder to generate context-aware representations and predictions.
    Note that other well-known models, such as BERT (Bidirectional Encoder Representations
    from Transformers), are based on the encoder part. We don’t cover this type of
    model in this book. [Figure 1-4](#fig_4_the_evolution_of_nlp_techniques_from_n_grams_to_th)
    illustrates the evolution of these different models.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*生成式预训练变换器*，通常称为*GPT*，是基于Transformer架构的一系列模型，专门利用原始架构的解码器部分。在GPT中，编码器不存在，因此不需要跨注意力来整合编码器产生的嵌入。因此，GPT仅依赖于解码器内的自注意机制来生成上下文感知的表示和预测。请注意，其他众所周知的模型，如BERT（来自变压器的双向编码器表示），是基于编码器部分的。我们在本书中不涵盖这种类型的模型。[图1-4](#fig_4_the_evolution_of_nlp_techniques_from_n_grams_to_th)说明了这些不同模型的演变。'
- en: '![](assets/dagc_0104.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dagc_0104.png)'
- en: Figure 1-4\. The evolution of NLP techniques from n-grams to the emergence of
    LLMs
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-4。从n-gram到LLM的NLP技术的演变
- en: Demystifying the Tokenization and Prediction Steps in GPT Models
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 揭秘GPT模型中的标记化和预测步骤
- en: LLMs in the GPT family receive a prompt as input, and in response they generate
    a text. This process is known as *text completion*. For example, the prompt could
    be “*The weather is nice today, so I decided to*” and the model output might be
    “*go for a walk*”. You may be wondering how the LLM model builds this output text
    from the input prompt. As you will see, it’s mostly just a question of probabilities.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: GPT系列中的LLM接收提示作为输入，并生成文本作为响应。这个过程被称为*文本完成*。例如，提示可以是“*天气今天很好，所以我决定*”，模型的输出可能是“*去散步*”。你可能想知道LLM模型如何从输入提示构建这个输出文本。正如你将看到的，这主要是一个概率问题。
- en: 'When a prompt is sent to an LLM, it first breaks the input into smaller pieces
    called *tokens*. These tokens represent single words, parts of words, or spaces
    and punctuation. For example, the preceding prompt could be broken like this:
    [“*The”, “wea”, “ther”, “is”, “nice”, “today”, “,”, “so”, “I”, “de”, “ci”, “ded”,
    “to*”]. Each language model comes with its own tokenizer. The GPT-4 tokenizer
    is not available at the time of this writing, but you can test the [GPT-3 tokenizer](https://platform.openai.com/tokenizer).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当提示被发送到LLM时，它首先将输入分成称为*标记*的较小部分。这些标记代表单词、单词部分、空格和标点符号。例如，前面的提示可以被分成这样：[“*The”,
    “wea”, “ther”, “is”, “nice”, “today”, “,”, “so”, “I”, “de”, “ci”, “ded”, “to*”]。每个语言模型都配备了自己的标记器。在撰写本文时，GPT-4标记器尚不可用，但你可以测试[GPT-3标记器](https://platform.openai.com/tokenizer)。
- en: Tip
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: A rule of thumb for understanding tokens in terms of word length is that 100
    tokens equal approximately 75 words for an English text.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 理解单词长度方面的令人信服的规则是，对于英文文本，100个标记大约相当于75个单词。
- en: Thanks to the attention principle and the Transformer architecture introduced
    earlier, the LLM processes these tokens and can interpret the relationships between
    them and the overall meaning of the prompt. The Transformer architecture allows
    a model to efficiently identify the critical information and the context within
    the text.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于之前介绍的注意原则和Transformer架构，LLM处理这些标记，并可以解释它们之间的关系以及提示的整体含义。Transformer架构允许模型有效地识别文本中的关键信息和上下文。
- en: To create a new sentence, the LLM predicts the tokens most likely to follow,
    based on the context of the prompt. OpenAI produced two versions of GPT-4, with
    context windows of 8,192 tokens and 32,768 tokens. Unlike the previous recurrent
    models, which had difficulty handling long input sequences, the Transformer architecture
    with the attention mechanism allows the modern LLM to consider the context as
    a whole. Based on this context, the model assigns a probability score for each
    potential subsequent token. The token with the highest probability is then selected
    as the next token in the sequence. In our example, after “The weather is nice
    today, so I decided to”, the next best token could be “go”.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个新的句子，LLM预测最有可能跟随的标记，基于提示的上下文。OpenAI制作了两个版本的GPT-4，上下文窗口分别为8,192个标记和32,768个标记。与以前的循环模型不同，它们难以处理长输入序列，具有注意机制的Transformer架构允许现代LLM将上下文作为一个整体来考虑。基于这个上下文，模型为每个潜在的后续标记分配一个概率分数。然后选择具有最高概率的标记作为序列中的下一个标记。在我们的例子中，在“天气今天很好，所以我决定”之后，下一个最佳标记可能是“去”。
- en: 'This process is then repeated, but now the context becomes “The weather is
    nice today, so I decided to go”, where the previously predicted token “go” is
    added to the original prompt. The second token that the model might predict could
    be “for”. This process is repeated until a complete sentence is formed: “go for
    a walk”. This process relies on the LLM’s ability to learn the next most probable
    word from massive text data. [Figure 1-5](#fig_5_the_completion_process_is_iterative_token_by_toke)
    illustrates this process.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程然后被重复，但现在上下文变成了“今天天气很好，所以我决定去”，先前预测的“去”被添加到原始提示中。模型可能预测的第二个标记可能是“散步”。这个过程重复进行，直到形成一个完整的句子：“去散步”。这个过程依赖于LLM学习从大量文本数据中学习下一个最有可能的单词的能力。[图1-5](#fig_5_the_completion_process_is_iterative_token_by_toke)说明了这个过程。
- en: '![](assets/dagc_0105.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dagc_0105.png)'
- en: Figure 1-5\. The completion process is iterative, token by token
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-5。完成过程是迭代的，逐个标记
- en: 'A Brief History: From GPT-1 to GPT-4'
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简史：从GPT-1到GPT-4
- en: In this section, we will review the evolution of the OpenAI GPT models from
    GPT-1 to GPT-4\.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将回顾OpenAI GPT模型从GPT-1到GPT-4的演变。
- en: GPT-1
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-1
- en: In mid-2018, just one year after the invention of the Transformer architecture,
    OpenAI published a paper titled [“Improving Language Understanding by Generative
    Pre-Training”](https://oreil.ly/Yakwa), by Radford et al., in which the company
    introduced the Generative Pre-trained Transformer, also known as GPT-1\.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年中，就在变压器架构被发明的一年后，OpenAI发表了一篇题为[“通过生成式预训练改进语言理解”](https://oreil.ly/Yakwa)的论文，作者是Radford等人，在这篇论文中，该公司介绍了生成式预训练变压器，也被称为GPT-1。
- en: Before GPT-1, the common approach to building high-performance NLP neural models
    relied on supervised learning. These learning techniques use large amounts of
    manually labeled data. For example, in a sentiment analysis task where the goal
    is to classify whether a given text has positive or negative sentiment, a common
    strategy would require collecting thousands of manually labeled text examples
    to build an effective classification model. However, the need for large amounts
    of well-annotated, supervised data has limited the performance of these techniques
    because such datasets are both difficult and expensive to generate.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPT-1之前，构建高性能NLP神经模型的常见方法依赖于监督学习。这些学习技术使用大量手动标记的数据。例如，在情感分析任务中，目标是对给定文本的情感进行分类，一种常见的策略需要收集成千上万个手动标记的文本示例来构建有效的分类模型。然而，对大量良好注释的监督数据的需求限制了这些技术的性能，因为这样的数据集既难以生成又昂贵。
- en: 'In their paper, the authors of GPT-1 proposed a new learning process in which
    an unsupervised pre-training step is introduced. In this pre-training step, no
    labeled data is needed. Instead, the model is trained to predict what the next
    token is. Thanks to the use of the Transformer architecture, which allows parallelization,
    this pre-training was performed on a large amount of data. For the pre-training,
    the GPT-1 model used the *BookCorpus dataset*, which contains the text of approximately
    11,000 unpublished books. This dataset was initially presented in 2015 in the
    scientific paper [“Aligning Books and Movies: Towards Story-Like Visual Explanations
    by Watching Movies and Reading Books”](https://oreil.ly/3hWl1) by Zhu et al.,
    and was initially made available on a University of Toronto web page. However,
    today the official version of the original dataset is no longer publicly accessible.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的论文中，GPT-1的作者提出了一个新的学习过程，其中引入了一个无监督的预训练步骤。在这个预训练步骤中，不需要标记的数据。相反，模型被训练来预测下一个标记是什么。由于变压器架构的使用，可以并行化，这个预训练是在大量数据上进行的。对于预训练，GPT-1模型使用了*BookCorpus数据集*，其中包含大约11000本未发表书籍的文本。这个数据集最初是在2015年的科学论文[“将书籍和电影对齐：通过观看电影和阅读书籍实现类似故事的视觉解释”](https://oreil.ly/3hWl1)中首次提出，作者是Zhu等人，最初在多伦多大学的网页上提供。然而，今天，原始数据集的官方版本不再公开可访问。
- en: The GPT-1 model was found to be effective in a variety of basic completion tasks.
    In the unsupervised learning phase, the model learned to predict the next item
    in the texts of the BookCorpus dataset. However, since GPT-1 is a small model,
    it was unable to perform complex tasks without fine-tuning. Therefore, fine-tuning
    was performed as a second supervised learning step on a small set of manually
    labeled data to adapt the model to a specific target task. For example, in a classification
    task such as sentiment analysis, it may be necessary to retrain the model on a
    small set of manually labeled text examples to achieve reasonable accuracy. This
    process allowed the parameters learned in the initial pre-training phase to be
    modified to better fit the task at hand.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-1模型在各种基本完成任务中被发现是有效的。在无监督学习阶段，该模型学会了预测BookCorpus数据集中文本的下一个项目。然而，由于GPT-1是一个小模型，它无法在没有微调的情况下执行复杂的任务。因此，微调作为第二个监督学习步骤在一小部分手动标记的数据上进行，以使模型适应特定的目标任务。例如，在情感分析等分类任务中，可能需要在一小部分手动标记的文本示例上重新训练模型以达到合理的准确性。这个过程允许在初始预训练阶段学习的参数被修改以更好地适应手头的任务。
- en: Despite its relatively small size, GPT-1 showed remarkable performance on several
    NLP tasks using only a small amount of manually labeled data for fine-tuning.
    The GPT-1 architecture consisted of a decoder similar to the original transformer,
    which was introduced in 2017 and had 117 million parameters. This first GPT model
    paved the way for more powerful models with larger datasets and more parameters
    to take better advantage of the potential of the Transformer architecture.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管规模相对较小，但GPT-1在几个NLP任务上表现出了显著的性能，仅使用少量手动标记的数据进行微调。GPT-1架构包括一个类似于2017年引入的原始变压器的解码器，具有1.17亿个参数。这个第一个GPT模型为更强大的模型铺平了道路，这些模型具有更大的数据集和更多的参数，以更好地利用Transformer架构的潜力。
- en: GPT-2
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-2
- en: In early 2019, OpenAI proposed GPT-2, a scaled-up version of the GPT-1 model
    that increased the number of parameters and the size of the training dataset tenfold.
    The number of parameters of this new version was 1.5 billion, trained on 40 GB
    of text. In November 2019, OpenAI released the full version of the GPT-2 language
    model.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年初，OpenAI提出了GPT-2，这是GPT-1模型的一个放大版本，它将参数数量和训练数据集的大小增加了十倍。这个新版本的参数数量为15亿，训练了40GB的文本。2019年11月，OpenAI发布了完整版本的GPT-2语言模型。
- en: Note
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: GPT-2 is publicly available and can be downloaded from [Hugging Face](https://huggingface.co/gpt2)
    or [GitHub](https://github.com/openai/gpt-2).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2是公开可用的，可以从[Hugging Face](https://huggingface.co/gpt2)或[GitHub](https://github.com/openai/gpt-2)下载。
- en: GPT-2 showed that training a larger language model on a larger dataset improves
    the ability of a language model to process tasks and outperforms the state of
    the art on many jobs. It also showed that even larger language models can process
    natural language better.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2表明，将更大的语言模型训练在更大的数据集上可以提高语言模型处理任务的能力，并在许多任务上胜过现有技术。它还表明，即使更大的语言模型也可以更好地处理自然语言。
- en: GPT-3
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-3
- en: OpenAI released version 3 of GPT in June 2020\. The main differences between
    GPT-2 and GPT-3 are the size of the model and the quantity of data used for the
    training. GPT-3 is a much larger model than GPT-2, with 175 billion parameters,
    allowing it to capture more complex patterns. In addition, GPT-3 was trained on
    a more extensive dataset. This includes [Common Crawl](https://commoncrawl.org),
    a large web archive containing text from billions of web pages and other sources,
    such as Wikipedia. This training dataset, which includes content from websites,
    books, and articles, allows GPT-3 to develop a deeper understanding of the language
    and context. As a result, GPT-3 demonstrates improved performance on a variety
    of linguistics tasks. It also demonstrates superior coherence and creativity in
    its generated texts. It is even capable of writing code snippets, such as SQL
    queries, and performing other intelligent tasks. Furthermore, GPT-3 eliminates
    the need for a fine-tuning step, which was mandatory for its predecessors.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI于2020年6月发布了GPT的第3版。GPT-2和GPT-3之间的主要区别在于模型的大小和训练使用的数据量。GPT-3比GPT-2模型要大得多，有1750亿个参数，使其能够捕捉更复杂的模式。此外，GPT-3是在更广泛的数据集上进行训练的。这包括[Common
    Crawl](https://commoncrawl.org)，一个包含来自数十亿网页和其他来源的文本的大型网络存档，如维基百科。这个训练数据集包括来自网站、书籍和文章的内容，使GPT-3能够更深入地理解语言和上下文。因此，GPT-3在各种语言任务上表现出更好的性能。它还在生成的文本中表现出更高的连贯性和创造力。它甚至能够编写代码片段，如SQL查询，并执行其他智能任务。此外，GPT-3消除了以前版本中必须进行的微调步骤。
- en: However, with GPT-3 there is a problem of misalignment between the tasks given
    by end users and what the model has seen during its training. As we have seen,
    language models are trained to predict the next token based on the input context.
    This training process is not necessarily directly aligned with the tasks end users
    want the model to perform. In addition, increasing the size of language models
    does not inherently make them better at following user intent or instructions.
    Moreover, models like GPT-3 were trained on data from different sources on the
    internet. Although a cleanup has been made in the selection of sources, the learning
    data may contain false or problematic text, including racist text, sexist text,
    misinformation, or disinformation. As a result, the model may sometimes say the
    wrong things or even toxic things. In 2021 a new release of the GPT-3 model was
    published, called the *Instruct series*. Unlike the original GPT-3 base model,
    the instruct models are optimized by *reinforcement learning* with human feedback,
    meaning that they use the feedback to learn and to improve over time. This allows
    the models to learn from human instructions while making them more truthful and
    less toxic.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，GPT-3存在一个问题，即用户给出的任务与模型在训练过程中看到的内容之间存在不一致。正如我们所见，语言模型是根据输入上下文来预测下一个标记的。这个训练过程不一定与最终用户希望模型执行的任务直接对齐。此外，增加语言模型的大小并不会使它们本质上更擅长遵循用户意图或指令。此外，像GPT-3这样的模型是在互联网上的不同来源的数据上进行训练的。尽管在选择来源时进行了清理，但学习数据可能包含错误或有问题的文本，包括种族主义文本、性别歧视文本、错误信息或虚假信息。因此，模型有时可能会说错话，甚至是有毒的话。2021年，GPT-3模型的新版本被发布，称为*Instruct系列*。与原始的GPT-3基础模型不同，instruct模型通过*强化学习*和人类反馈进行优化，这意味着它们利用反馈来学习和随着时间的推移改进。这使得模型能够从人类指令中学习，使它们更真实，更少有毒。
- en: 'To illustrate the difference, we input the prompt “Explain what is meant by
    time complexity”, and we receive the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明区别，我们输入提示“解释时间复杂度的含义”，我们得到以下结果：
- en: 'With the standard GPT-3, we obtain the output: “*Explain what is meant by space
    complexity. Explain what is meant by the big-O notation*.”'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用标准的GPT-3，我们得到输出：“*解释空间复杂度的含义。解释大O符号的含义*。”
- en: 'With the instructGPT-3 model, we obtain: “*Time complexity is a way of measuring
    the amount of time it takes for an algorithm to run and complete its task. It
    is usually expressed using Big O notation, which measures the complexity of an
    algorithm in terms of the number of operations it performs. The time complexity
    of an algorithm is important because it determines how efficient the algorithm
    is and how well it scales with larger inputs.*”'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用instructGPT-3模型，我们得到：“*时间复杂度是衡量算法运行和完成任务所需时间的一种方式。通常使用大O符号来表示，它衡量算法的复杂度是指它执行的操作数量。算法的时间复杂度很重要，因为它决定了算法的效率以及随着更大输入的规模如何扩展*。”
- en: We can see that for the same input, the first model cannot answer the question
    (the answer is even weird), whereas the second model does answer the question.
    It is, of course, possible to obtain the desired response with a standard GPT-3
    model. However, contrary to instruction models, it is necessary to apply specific
    prompt design and optimization techniques to obtain the desired output from the
    GPT-3 model. This technique is called *prompt engineering* and will be detailed
    in the coming chapters.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: From GPT-3 to InstructGPT
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenAI explains how the Instruct series was constructed in the scientific paper
    [“Training Language Models to Follow Instructions with Human Feedback”](https://oreil.ly/sz90A)
    by Ouyang et al.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'The training recipe has two main stages to go from a GPT-3 model to an instructed
    GPT-3 model: *supervised fine-tuning* (*SFT*) and *reinforcement learning from
    human feedback* (*RLHF*). In each stage, the results of the prior stage are fine-tuned.
    That is, the SFT stage receives the GPT-3 model and returns a new model, which
    is sent to the RLHF stage to obtain the instructed version.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 1-6](#fig_6_the_steps_to_obtain_the_instructed_models_from_an), from
    the scientific paper from OpenAI, details the entire process.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dagc_0106.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
- en: Figure 1-6\. The steps to obtain the instructed models (redrawn from an image
    by Ouyang et al.)
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We will step through these stages one by one.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: In the SFT stage, the original GPT-3 model is fine-tuned with straightforward
    supervised learning (step 1 in [Figure 1-6](#fig_6_the_steps_to_obtain_the_instructed_models_from_an)).
    OpenAI has a collection of prompts made by end users. The process starts with
    the random selection of a prompt from the set of available prompts. A human (called
    a *labeler*) is then asked to write an example of an ideal answer to this prompt.
    This process is repeated thousands of times to obtain a supervised training set
    composed of prompts and the corresponding ideal responses. This dataset is then
    used to fine-tune the GPT-3 model to give more consistent answers to user requests.
    The resulting model is called the SFT model.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: The RLHF stage is divided into two substeps. First a reward model (RM) is built
    (step 2 in [Figure 1-6](#fig_6_the_steps_to_obtain_the_instructed_models_from_an)),
    and then the RM is used for reinforcement learning (step 3 in [Figure 1-6](#fig_6_the_steps_to_obtain_the_instructed_models_from_an)).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the RM is to automatically give a score to a response to a prompt.
    When the response matches what is indicated in the prompt, the RM score should
    be high; when it doesn’t match, it should be low. To construct the RM, OpenAI
    begins by randomly selecting a question and using the SFT model to produce several
    possible answers. As we will see later, it is possible to produce many responses
    with the same input prompt via a parameter called *temperature*. A human labeler
    is then asked to rank the responses based on criteria such as fit with the prompt
    and toxicity of the response. After running this procedure many times, a dataset
    is used to fine-tune the SFT model for scoring. This RM will be used to build
    the final InstructGPT model.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: The final step in training InstructGPT models involves reinforcement learning,
    which is an iterative process. It starts with an initial generative model, such
    as the SFT model. Then a random prompt is selected, and the model predicts an
    output, which the RM evaluates. Based on the reward received, the generative model
    is updated accordingly. This process can be repeated countless times without human
    intervention, providing a more efficient and automated approach to adapting the
    model for better performance.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: InstructGPT models are better at producing accurate completions for what people
    give as input in the prompt. OpenAI recommends using the InstructGPT series rather
    than the original series.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3.5, Codex, and ChatGPT
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In March 2022, OpenAI made available new versions of GPT-3\. These new models
    can edit text or insert content into text. They have been trained on data through
    June 2021 and are described as more powerful than previous versions. At the end
    of November 2022, OpenAI began referring to these models as belonging to the GPT-3.5
    series.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 2022年3月，OpenAI推出了GPT-3的新版本。这些新模型可以编辑文本或将内容插入文本。它们经过了截至2021年6月的数据训练，并被描述为比以前的版本更强大。2022年11月底，OpenAI开始将这些模型称为GPT-3.5系列的一部分。
- en: OpenAI also proposed the Codex model, a GPT-3 model that is fine-tuned on billions
    of lines of code and that powers the [GitHub Copilot](https://github.com/features/copilot)
    autocompletion programming tool to assist developers of many text editors including
    Visual Studio Code, JetBrains, and even Neovim. However, the Codex model was deprecated
    by OpenAI in March 2023\. Instead, OpenAI recommends that users switch from Codex
    to GPT-3.5 Turbo or GPT-4\. At the same time, GitHub released Copilot X, which
    is based on GPT-4 and provides much more functionality than the previous version.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI还提出了Codex模型，这是一个在数十亿行代码上进行了微调的GPT-3模型，它驱动着[GitHub Copilot](https://github.com/features/copilot)自动补全编程工具，以协助许多文本编辑器的开发人员，包括Visual
    Studio Code、JetBrains，甚至Neovim。然而，Codex模型在2023年3月被OpenAI弃用。相反，OpenAI建议用户从Codex切换到GPT-3.5
    Turbo或GPT-4。与此同时，GitHub发布了基于GPT-4的Copilot X，提供比之前版本更多的功能。
- en: Warning
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'OpenAI’s deprecation of the Codex model serves as a stark reminder of the inherent
    risk of working with APIs: they can be subject to changes or discontinuation over
    time as newer, more efficient models are developed and rolled out.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI对Codex模型的弃用提醒了使用API的固有风险：随着更新、更高效的模型的开发和推出，它们可能会随时间发生变化或停止。
- en: In November 2022, OpenAI introduced [ChatGPT](https://chat.openai.com) as an
    experimental conversational model. This model has been fine-tuned to excel at
    interactive dialogue, using a technique similar to that shown in [Figure 1-6](#fig_6_the_steps_to_obtain_the_instructed_models_from_an).
    ChatGPT has its roots in the GPT-3.5 series, which served as the basis for its
    development.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 2022年11月，OpenAI推出了[ChatGPT](https://chat.openai.com)作为一种实验性对话模型。该模型经过了微调，以在交互式对话中表现出色，使用了类似于[图1-6](#fig_6_the_steps_to_obtain_the_instructed_models_from_an)中所示的技术。ChatGPT源于GPT-3.5系列，这成为了其发展的基础。
- en: Note
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: It can be argued that ChatGPT is an application powered by an LLM, not an actual
    LLM. The LLM behind ChatGPT is GPT-3.5 Turbo. However, OpenAI itself refers to
    ChatGPT as a model in its [release note](https://openai.com/blog/chatgpt). In
    this book, we use *ChatGPT* as a generic term for both the application and the
    model, unless we are manipulating code, in which case we use `gpt-3.5-turbo`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT可以被认为是由LLM驱动的应用程序，而不是实际的LLM。ChatGPT背后的LLM是GPT-3.5 Turbo。然而，OpenAI本身在其[发布说明](https://openai.com/blog/chatgpt)中将ChatGPT称为一个模型。在本书中，我们将*ChatGPT*用作既指代应用程序又指代模型的通用术语，除非我们在处理代码，那时我们使用`gpt-3.5-turbo`。
- en: GPT-4
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPT-4
- en: In March 2023, OpenAI made GPT-4 available. We know very little about the architecture
    of this new model, as OpenAI has provided little information. It is OpenAI’s most
    advanced system to date and should produce more secure and useful answers. The
    company claims that GPT-4 surpasses ChatGPT in its advanced reasoning capabilities.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年3月，OpenAI推出了GPT-4。我们对这个新模型的架构了解甚少，因为OpenAI提供了很少的信息。这是OpenAI迄今为止最先进的系统，应该能够产生更安全和有用的答案。该公司声称GPT-4在其先进的推理能力方面超过了ChatGPT。
- en: Unlike the other models in the OpenAI GPT family, GPT-4 is the first multimodal
    model capable of receiving not only text but also images. This means that GPT-4
    considers both the images and the text in the context that the model uses to generate
    an output sentence, which makes it possible to add an image to a prompt and ask
    questions about it. Note that OpenAI has not yet made this feature publicly available
    as of the writing of this book.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 与OpenAI GPT系列中的其他模型不同，GPT-4是第一个能够接收文本和图像的多模态模型。这意味着GPT-4在生成输出句子时考虑了图像和文本的上下文，这使得可以向提示添加图像并对其进行提问。需要注意的是，截至本书撰写时，OpenAI尚未公开提供此功能。
- en: The models have also been evaluated on various tests, and GPT-4 has outperformed
    ChatGPT by scoring in higher percentiles among the test takers. For example, on
    the [Uniform Bar Exam](https://oreil.ly/opXec), ChatGPT scored in the 10th percentile,
    while GPT-4 scored in the 90th percentile. The same goes for the [International
    Biology Olympiad](https://oreil.ly/a8CP6), in which ChatGPT scored in the 31st
    percentile and GPT-4 in the 99th percentile. This progress is very impressive,
    especially considering that it was achieved in less than one year.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型也经过了各种测试的评估，GPT-4在各项测试中的得分都超过了ChatGPT。例如，在[统一法律考试](https://oreil.ly/opXec)上，ChatGPT得分位于第10百分位数，而GPT-4得分位于第90百分位数。在[国际生物奥林匹克竞赛](https://oreil.ly/a8CP6)中也是如此，ChatGPT得分位于第31百分位数，而GPT-4得分位于第99百分位数。这种进步非常令人印象深刻，尤其是考虑到这是在不到一年的时间内实现的。
- en: '[Table 1-1](#table-1-1) summarizes the evolution of the GPT models.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[表1-1](#table-1-1)总结了GPT模型的演变。'
- en: Table 1-1\. Evolution of the GPT models
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 表1-1。GPT模型的演变
- en: '| 2017 | The paper “Attention Is All You Need” by Vaswani et al. is published.
    |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: 2017年，Vaswani等人发表了论文“Attention Is All You Need”。
- en: '| 2018 | The first GPT model is introduced with 117 million parameters. |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: 2018年，引入了拥有1.17亿参数的第一个GPT模型。
- en: '| 2019 | The GPT-2 model is introduced with 1.5 billion parameters. |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: 2019年，引入了拥有15亿参数的GPT-2模型。
- en: '| 2020 | The GPT-3 model is introduced with 175 billion parameters. |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: 2020年，引入了拥有1750亿参数的GPT-3模型。
- en: '| 2022 | The GPT-3.5 (ChatGPT) model is introduced with 175 billion parameters.
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: 2022年，引入了拥有1750亿参数的GPT-3.5（ChatGPT）模型。
- en: '| 2023 | The GPT-4 model is introduced, but the number of parameters is not
    disclosed. |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: 2023年，引入了GPT-4模型，但参数数量未公开。
- en: Note
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You may have heard the term *foundation model*. While LLMs like GPT are trained
    to process human language, a foundation model is a broader concept. These models
    are trained on many types of data, not just text, and they can be fine-tuned for
    various tasks, including but not limited to NLP. Thus, all LLMs are foundation
    models, but not all foundation models are LLMs.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: LLM Use Cases and Example Products
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenAI includes many inspiring customer stories on its website. This section
    explores some of these applications, use cases, and product examples. We will
    discover how these models may transform our society and open new opportunities
    for business and creativity. As you will see, many businesses already use these
    new technologies, but there is room for more ideas. It is now up to you.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Be My Eyes
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since 2012, [Be My Eyes](https://www.bemyeyes.com) has created technologies
    for a community of several million people who are blind or have limited vision.
    For example, it has an app that connects volunteers with blind or visually impaired
    persons who need help with everyday tasks, such as identifying a product or navigating
    in an airport. With only one click in the app, the person who needs help is contacted
    by a volunteer who, through video and microphone sharing, can help the person.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: The new multimodal capacity of GPT-4 makes it possible to process both text
    and images, so Be My Eyes began developing a new virtual volunteer based on GPT-4\.
    This new virtual volunteer aims to reach the same level of assistance and understanding
    as a human volunteer.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: “The implications for global accessibility are profound. In the not-so-distant
    future, the blind and low-vision community will utilize these tools not only for
    a host of visual interpretation needs but also to have a greater degree of independence
    in their lives,” says Michael Buckley, CEO of Be My Eyes.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: At the time of this writing, the virtual volunteer is still in the beta version.
    To gain access to it, you must register to be put on a waiting list in the app,
    but initial feedback from beta testers is very positive.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Morgan Stanley
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Morgan Stanley](https://www.morganstanley.com) is a multinational investment
    bank and financial services company in the United States. As a leader in wealth
    management, Morgan Stanley has a content library of hundreds of thousands of pages
    of knowledge and insight covering investment strategies, market research and commentary,
    and analyst opinions. This vast amount of information is spread across multiple
    internal sites and is mostly in PDF format. This means consultants must search
    a large number of documents to find answers to their questions. As you can imagine,
    this search can be long and tedious.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: The company evaluated how it could leverage its intellectual capital with GPT’s
    integrated research capabilities. The resulting internally developed model will
    power a chatbot that performs a comprehensive search of wealth management content
    and efficiently unlocks Morgan Stanley’s accumulated knowledge. In this way, GPT-4
    has provided a way to analyze all this information in a format that is much easier
    to use.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Khan Academy
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Khan Academy](https://www.khanacademy.org) is a US-based nonprofit educational
    organization founded in 2008 by Sal Khan. Its mission is to create a set of free
    online tools to help educate students worldwide. The organization offers thousands
    of math, science, and social studies lessons for students of all ages. In addition,
    the organization produces short lessons through videos and blogs, and recently
    it began offering Khanmigo.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Khanmigo is a new AI assistant powered by GPT-4\. Khanmigo can do a lot of things
    for students, such as guiding and encouraging them, asking questions, and preparing
    them for tests. Khanmigo is designed to be a friendly chatbot that helps students
    with their classwork. It does not give students answers directly, but instead
    guides them in the learning process. Khanmigo can also support teachers by helping
    them make lesson plans, complete administrative tasks, and create lesson books,
    among other things.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: “We think GPT-4 is opening up new frontiers in education. A lot of people have
    dreamed about this kind of technology for a long time. It’s transformative, and
    we plan to proceed responsibly with testing to explore if it can be used effectively
    for learning and teaching,” says Kristen DiCerbo, chief learning officer at Khan
    Academy.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: At the time of this writing, access to Khanmigo’s pilot program is limited to
    selected people. To participate in the program, you must be placed on a [waiting
    list](https://oreil.ly/oP6KN).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Duolingo
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Duolingo](https://www.duolingo.com) is a US-based educational technology company,
    founded in 2011, that produces applications used by millions of people who want
    to learn a second language. Duolingo users need to understand the rules of grammar
    to learn the basics of a language. They need to have conversations, ideally with
    a native speaker, to understand those grammar rules and master the language. This
    is not possible for everyone.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'Duolingo has added two new features to the product using OpenAI’s GPT-4: Role
    Play and Explain My Answer. These features are available in a new subscription
    level called Duolingo Max. With these features, Duolingo has bridged the gap between
    theoretical knowledge and the practical application of language. Thanks to LLMs,
    Duolingo allows learners to immerse themselves in real-world scenarios.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: The Role Play feature simulates conversations with native speakers, allowing
    users to practice their language skills in a variety of settings. The Explain
    My Answer feature provides personalized feedback on grammar errors, facilitating
    a deeper understanding of the structure of the language.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: “We wanted AI-powered features that were deeply integrated into the app and
    leveraged the gamified aspect of Duolingo that our learners love,” says Edwin
    Bodge, principal product manager at Duolingo.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: The integration of GPT-4 into Duolingo Max not only enhances the overall learning
    experience but also paves the way for more effective language acquisition, especially
    for those without access to native speakers or immersive environments. This innovative
    approach should transform the way learners master a second language and contribute
    to better long-term learning outcomes.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Yabble
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Yabble](https://www.yabble.com) is a market research company that uses AI
    to analyze consumer data in order to deliver actionable insights to businesses.
    Its platform transforms raw, unstructured data into visualizations, enabling businesses
    to make informed decisions based on customer needs.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: The integration of advanced AI technologies such as GPT into Yabble’s platform
    has enhanced its consumer data processing capabilities. This enhancement allows
    for a more effective understanding of complex questions and answers, enabling
    businesses to gain deeper insights based on the data. As a result, organizations
    can make more informed decisions by identifying key areas for improvement based
    on customer feedback.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: “We knew that if we wanted to expand our existing offers, we needed artificial
    intelligence to do a lot of the heavy lifting so that we could spend our time
    and creative energy elsewhere. OpenAI fit the bill perfectly,” says Ben Roe, Head
    of Product at Yabble.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Waymark
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Waymark](https://waymark.com) provides a platform for creating video ads.
    This platform uses AI to help businesses easily create high-quality videos without
    the need for technical skills or expensive equipment.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Waymark has integrated GPT into its platform, which has significantly improved
    the scripting process for platform users. This GPT-powered enhancement allows
    the platform to generate custom scripts for businesses in seconds. This allows
    users to focus more on their primary goals, as they spend less time editing scripts
    and more time creating video ads. The integration of GPT into Waymark’s platform
    therefore provides a more efficient and personalized video creation experience.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: “I’ve tried every AI-powered product available over the last five years but
    found nothing that could effectively summarize a business’s online footprint,
    let alone write effective marketing copy, until GPT-3,” says Waymark founder,
    Nathan Labenz.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Inworld AI
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Inworld AI](https://www.inworld.ai) provides a developer platform for creating
    AI characters with distinct personalities, multimodal expression, and contextual
    awareness.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: One of the main use cases of the Inworld AI platform is video games. The integration
    of GPT as the basis for the character engine of Inworld AI enables efficient and
    rapid video game character development. By combining GPT with other ML models,
    the platform can generate unique personalities, emotions, memory, and behaviors
    for AI characters. This process allows game developers to focus on storytelling
    and other topics without having to invest significant time in creating language
    models from scratch.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: “With GPT-3, we had more time and creative energy to invest in our proprietary
    technology that powers the next generation of non-player characters (NPCs),” says
    Kylan Gibbs, chief product officer and cofounder of Inworld.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'Beware of AI Hallucinations: Limitations and Considerations'
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you have seen, an LLM generates an answer by predicting the next words (or
    tokens) one by one based on a given input prompt. In most situations, the model’s
    output is relevant and entirely usable for your task, but it is essential to be
    careful when you are using language models in your applications because they can
    give incoherent answers. These answers are often referred to as *hallucinations*.
    AI hallucinations occur when AI gives you a confident response that is false or
    that refers to imaginary facts. This can be dangerous for users who rely on GPT.
    You need to double-check and critically examine the model’s response.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following example. We start by asking the model to do a simple
    calculation: 2 + 2\. As expected, it answers 4\. So it is correct. Excellent!
    We then ask it to do a more complex calculation: 3,695 × 123,548\. Although the
    correct answer is 456,509,860, the model gives with great confidence a wrong answer,
    as you can see in [Figure 1-7](#fig_7_chatgpt_hallucinating_bad_math_chatgpt_april_22).
    And when we ask it to check and recalculate, it still gives a wrong answer.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dagc_0107.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
- en: Figure 1-7\. ChatGPT hallucinating bad math (ChatGPT, April 22, 2023)
  id: totrans-143
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Although, as we will see, you can add new features to GPT using a plug-in system,
    GPT does not include a calculator by default. To answer our question of what is
    2 + 2, GPT generates each token one at a time. It answers correctly because it
    probably has often seen “2 + 2 equals 4” in the texts used for its training. It
    doesn’t really do the calculation—it is just text completion.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is unlikely that GPT has seen the numbers we chose for the multiplication
    problem, 3,695 × 123,548, many times in its training. This is why it makes a mistake.
    And as you can see, even when it makes a mistake, it can be reasonably sure about
    its wrong output. Be careful, mainly if you use the model in one of your applications.
    If GPT makes mistakes, your application may get inconsistent results.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that ChatGPT’s result is *close* to the correct answer and not completely
    random. It is an interesting side effect of its algorithm: even though it has
    no mathematical capabilities, it can give a close estimation with a language approach
    only.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: OpenAI introduced the ability to use plug-ins with GPT-4\. As we will see in
    [Chapter 5](ch05.html#advancing_llm_capabilities_with_the_langchain_fram), these
    tools allow you to add additional functionality to the LLM. One tool is a calculator
    that helps GPT correctly answer these types of questions.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, ChatGPT made a mistake. But in some cases, it can
    even be deliberately deceitful, such as shown in [Figure 1-8](#fig_8_asking_chatgpt_to_count_zebras_on_a_wikipedia_pict).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dagc_0108.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: Figure 1-8\. Asking ChatGPT to count zebras on a Wikipedia picture (ChatGPT,
    April 5, 2023)
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: ChatGPT begins by claiming that it cannot access the internet. However, if we
    insist, something interesting happens (see [Figure 1-9](#fig_9_chatgpt_claiming_it_accessed_the_wikipedia_link)).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dagc_0109.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: Figure 1-9\. ChatGPT claiming it accessed the Wikipedia link
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: ChatGPT now implies that it *did* access the link. However, this is definitely
    not possible at the moment. ChatGPT is blatantly leading the user to think that
    it has capabilities it doesn’t have. By the way, as [Figure 1-10](#fig_10_the_zebras_chatgpt_didn_t_really_count)
    shows, there are more than three zebras in the image.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dagc_0110.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
- en: Figure 1-10\. The zebras ChatGPT didn’t really count
  id: totrans-158
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Warning
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'ChatGPT and other GPT-4 models are, by design, not reliable: they can make
    mistakes, give false information, or even mislead the user.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: In summary, we highly recommend using pure GPT-based solutions for creative
    applications, not question answering where the truth matters—such as for medical
    tools. For such use cases, as you will see, plug-ins are probably an ideal solution.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing GPT Models with Plug-ins and Fine-Tuning
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In addition to its simple completion feature, more advanced techniques can
    be used to further exploit the capabilities of the language models provided by
    OpenAI. This book looks at two of these methods:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Plug-ins
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT has some limitations, for example, with calculations. As you’ve seen, GPT
    can correctly answer simple math problems like 2 + 2 but may struggle with more
    complex calculations like 3,695 × 123,548\. Moreover, it does not have direct
    access to the internet, which means that GPT models lack access to new information
    and are limited to the data they were trained on. For GPT-4, the last knowledge
    update occurred in September 2021\. The plug-in service provided by OpenAI allows
    the model to be connected to applications that may be developed by third parties.
    These plug-ins enable the models to interact with developer-defined APIs, and
    this process can potentially greatly enhance the capabilities of the GPT models,
    as they *can* access the outside world through a wide range of actions.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: For developers, plug-ins potentially open up many new opportunities. Consider
    that in the future, each company may want to have its own plug-in for LLMs. There
    could be collections of plug-ins similar to what we find today in smartphone app
    stores. The number of applications that could be added via plug-ins could be enormous.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'On its website, OpenAI says that plug-ins can allow ChatGPT to do things such
    as the following:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Retrieve real-time information, such as sports scores, stock prices, the latest
    news, and so forth
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieve knowledge-based information, such as company docs, personal notes,
    and more
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform actions on behalf of the user, such as booking a flight, ordering food,
    and so on
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Execute accurate math calculations
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are just a few examples of use cases; it is up to you to find new ones.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: This book also examines fine-tuning techniques. As you will see, fine-tuning
    can improve the accuracy of an existing model for a specific task. The fine-tuning
    process involves retraining an existing GPT model on a particular set of new data.
    This new model is designed for a specific task, and this additional training process
    allows the model to adjust its internal parameters to learn the nuances of this
    given task. The resulting fine-tuned model should perform better on the task for
    which it has been fine-tuned. For example, a model fine-tuned on financial textual
    data should be able to better answer queries in that domain and generate more
    relevant content.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 本书还探讨了微调技术。正如你将看到的，微调可以提高现有模型在特定任务上的准确性。微调过程涉及在特定一组新数据上重新训练现有的GPT模型。这个新模型是为特定任务设计的，这个额外的训练过程允许模型调整其内部参数以学习这个给定任务的细微差别。结果微调的模型应该在其被微调的任务上表现更好。例如，在金融文本数据上微调的模型应该能够更好地回答该领域的查询并生成更相关的内容。
- en: Summary
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: LLMs have come a long way, starting with simple n-gram models and moving to
    RNNs, LSTMs, and advanced transformer-based architectures. LLMs are computer programs
    that can process and generate human-like language, with ML techniques to analyze
    vast amounts of text data. By using self-attention and cross-attention mechanisms,
    transformers have greatly enhanced language understanding.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: LLM已经走过了很长的路，从简单的n-gram模型发展到RNNs、LSTMs和先进的基于transformer的架构。LLM是能够处理和生成类似人类语言的计算机程序，利用机器学习技术来分析大量的文本数据。通过使用自注意力和交叉注意力机制，transformers大大增强了语言理解能力。
- en: This book explores how to use GPT-4 and ChatGPT, as they offer advanced capabilities
    for understanding and generating context. Building applications with them goes
    beyond the scope of traditional BERT or LSTM models to provide human-like interactions.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 本书探讨了如何使用GPT-4和ChatGPT，因为它们提供了理解和生成上下文的先进能力。利用它们构建应用程序超越了传统的BERT或LSTM模型的范围，提供了类似人类的互动。
- en: Since early 2023, ChatGPT and GPT-4 have demonstrated remarkable capabilities
    in NLP. As a result, they have contributed to the rapid advancement of AI-enabled
    applications in various industries. Different use cases already exist, ranging
    from applications such as Be My Eyes to platforms such as Waymark, which are testaments
    to the potential of these models to revolutionize how we interact with technology.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 自2023年初以来，ChatGPT和GPT-4在自然语言处理方面展现出了非凡的能力。因此，它们为各行各业的人工智能应用的快速发展做出了贡献。不同的用例已经存在，从Be
    My Eyes等应用到Waymark等平台，这些都证明了这些模型改变我们与技术互动方式的潜力。
- en: It is important to keep in mind the potential risks of using these LLMs. As
    a developer of applications that will use the OpenAI API, you should be sure that
    users know the risk of errors and can verify the AI-generated information.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要牢记使用这些LLM的潜在风险。作为将使用OpenAI API的应用程序开发人员，您应该确保用户知道错误的风险，并能验证由AI生成的信息。
- en: The next chapter will give you the tools and information to use the OpenAI models
    available as a service and help you be part of this incredible transformation
    we are living today.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将为您提供使用OpenAI模型作为服务的工具和信息，并帮助您成为我们今天生活中这一不可思议的转变的一部分。
