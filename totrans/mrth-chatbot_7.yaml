- en: © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023M.
    Kurpicz-BrikiMore than a Chatbot[https://doi.org/10.1007/978-3-031-37690-0_7](https://doi.org/10.1007/978-3-031-37690-0_7)
  prefs: []
  type: TYPE_NORMAL
- en: 7. The Future of Humans and Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mascha Kurpicz-Briki^([1](#Aff2)  )(1)Applied Machine Intelligence, Bern University
    of Applied Sciences, Biel/Bienne, Switzerland
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have seen in this book how the technology behind state-of-the-art language
    models and other text processing software is working. Based on this understanding,
    I want you to participate actively in the public discourse on how our future’s
    society will be shaped by this technology. This discussion requires technical
    experts to collaborate closely with scholars from other domains as well as lay
    people, and a basic technological understanding like this book has provided is
    crucial to have a fruitful discussion on how our societies should deal with such
    majorly altering technologies.
  prefs: []
  type: TYPE_NORMAL
- en: In this final chapter, I thus want to raise some points of discussion and point
    some potential directions as food for thoughts.
  prefs: []
  type: TYPE_NORMAL
- en: The Future of Humans
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To discuss the different visions of the future of machine and human collaboration,
    we have to look at two different kinds of artificial intelligence that are typically
    differentiated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[…] weak or narrow AI on the one hand and strong AI on the other. Weak AI is
    capable only of solving specific problems—playing chess, for example, or recognizing
    what lies in a picture. Strong AI, by contrast, would designate a computer system
    that responds intelligently at a general level, including in situations where
    precise factual information is missing or the objectives are unclear. (Zweig [2022](#CR43),
    S. 90)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The strong AI is also referred to as general artificial intelligence (or artificial
    general intelligence AGI) and is the “Hollywood version” (Broussard [2018](#CR5),
    p. 10), the kind of AI that is used in sci-fi movies with robots taking over the
    government and giving us a dark vision of the future of humans. But why should
    we create such a monster that aims to wipe us out completely someday?
  prefs: []
  type: TYPE_NORMAL
- en: The intention and wish of creating an *artificial general intelligence* have
    been inspirations for many science-fiction tales and movies and at the same time
    divide the community of researchers in the field. Researchers disagree on the
    forecast, when (or if) such an artificial general intelligence will be achieved,
    and whether it is even worth targeting this kind of artificial intelligence. Artificial
    general intelligence can also be defined as the “ability to accomplish any cognitive
    task at least as well as humans” (Tegmark [2018](#CR36), p. 39).
  prefs: []
  type: TYPE_NORMAL
- en: Any cognitive task. That’s quite complete. As Tegmark ([2018](#CR36)) describes,
    based on such a general artificial intelligence, a *superintelligence* could be
    built, which can be used or unleased to take over the world. With superintelligence,
    he refers to “general intelligence far beyond human level” (Tegmark [2018](#CR36),
    S. 39). This hypothetical moment in time, with unforeseen consequences for human
    civilization, is also referred to as *technology singularity*.
  prefs: []
  type: TYPE_NORMAL
- en: Certainly, there would still be a long way to go to reach something like this,
    at a technical level. Will we ever achieve something like this? Hard to predict.
    Does it make sense to aim at building this kind of a superintelligence (or a system
    with capabilities to potentially *break out* and create itself a superintelligence)?
    That’s something we all should discuss, as a society.
  prefs: []
  type: TYPE_NORMAL
- en: Take a step back and consider the use cases where advanced technology could
    be beneficial as a tool to humans, as we are using other tools like calculators
    or cars. Now consider the scenario of creating a superintelligence, in form of
    armed robots or just by fully simulating being a human. One could be wondering,
    *why humans*? Would you be wanting to do that?
  prefs: []
  type: TYPE_NORMAL
- en: I encourage us as a society to think about what is useful for us in terms of
    technology, rather than developing technology that aims apocalyptic scenarios
    for mankind. Whereas there might be a fascination to be the creator of a *superintelligence*,
    it is not what the society or humanity needs.
  prefs: []
  type: TYPE_NORMAL
- en: Coming back to the language models. The way state-of-the-art language models
    or chatbot talk to us, simulating how humans would write, yet with major differences
    when it comes to meaning or communication intention, might be impressive. Still,
    they are a tool, a tool that provides us many opportunities and at the same time
    a tool that brings some risks along. The risks for the near future do not lay
    in the apocalyptic scenario of ChatGPT taking over the world but in other important
    discussions that need to be conducted now.
  prefs: []
  type: TYPE_NORMAL
- en: This split of the AI community was also visible in the case of *the letter*
    in spring 2023\. As a consequence of the release of OpenAIs ChatGPT and in particular
    the GPT-4 model, the Future of Life Institute published an open letter signed
    by more than 10,000 supporters (Future of Life Institute [2023](#CR14)). Among
    the supporters, you can find professors from the field of AI, a Turing prize winner,
    and co-founders of well-known tech companies. In this letter, they call on all
    centers developing AI to pause for at least 6 months the training of AI systems
    more powerful than GPT-4\. They argued that contemporary AI systems are becoming
    human-competitive at general tasks and that they should only be developed once
    we are confident that their effects will be positive and their risks manageable.
    The proposed pause of 6 months should be public and verifiable and, if necessary,
    enforced by governments applying a moratorium.
  prefs: []
  type: TYPE_NORMAL
- en: 'The letter was heavily discussed in the media. In addition to the need of a
    pause, in particular, the feasibility of the proposed a moratorium was put in
    question. The authors of the paper describing large language models as stochastic
    parrots (Bender et al. [2021](#CR3)), which we have encountered earlier, have
    shortly after published a statement regarding the letter (Gebru et al. [2023](#CR16)).
    In their statement, they discussed the need for regulatory efforts focusing on
    transparency, accountability, and prevention of exploitive labor practice, with
    a focus on AI that is already now real and present, deployed in automated systems.
    In particular, they criticized the fearmongering with hypothetical risks like
    “human-competitive intelligence” or “powerful digital minds.” They argued that
    the letter ignores harms such as worker exploitation, massive data theft, synthetic
    media data reproducing systems of oppression and endangering the information ecosystem,
    and the concentration of power which exacerbates social inequities. Especially,
    they warned that:'
  prefs: []
  type: TYPE_NORMAL
- en: Such language that inflates the capabilities of automated systems and anthropomorphizes
    them, as we note in Stochastic Parrots, deceives people into thinking that there
    is a sentient being behind the synthetic media. This not only lures people into
    uncritically trusting the outputs of systems like ChatGPT, but also misattributes
    agency. Accountability properly lies not with the artifacts but with their builders.
    (Gebru et al. [2023](#CR16))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Therefore, they underlined the need for regulation that enforces transparency
    and that regulations should protect the rights and interests of people when this
    technology is being applied by corporations.
  prefs: []
  type: TYPE_NORMAL
- en: Whether today’s existing models like GPT-4 do have first signs of human-like
    intelligence or not is also influenced by the definition of intelligence itself.
    Different definitions are being used and are discussed, and how to measure this
    intelligence is not finally decided. To sharpen the discussion, common definitions
    will need to be developed in the public discourse.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen throughout this book, language models can hallucinate and require
    additional e-literacy skills to be handled in a responsible manner. At the same
    time, people might interpret information provided by a chatbot differently from
    the information provided in a bullet list in the results of a search engine. There
    are risks when it comes to discrimination and bias in these systems, as well as
    expensive ecological consequences. Finally, the way machines and humans collaborate,
    in terms of work or learning, might change, requiring an adaptation of how we
    have been doing things so far. In a similar way, an adaptation was required when
    calculators were entering the market.
  prefs: []
  type: TYPE_NORMAL
- en: So, rather than worrying about the Terminator AI, let’s look at the more pressing
    changes these new tools bring to our society and how to deal with them.
  prefs: []
  type: TYPE_NORMAL
- en: The Future of Responsible AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From a technical perspective, the problems of hallucinations or bias in language
    models are difficult to address. Different technical methods exist to reduce hallucinations,
    either being applied to the data used for training or to the training process
    itself (Ji et al. [2023](#CR22)). In the training process, the components such
    as the encoder, the decoder, or the attention mechanism can be optimized to have
    a better semantic understanding of the input. For the data, the dataset can be
    increased or validated. One method that can be used is to create a (more) faithful
    dataset in collaboration with humans. As we have seen in different parts of this
    book, a language model can only be as good as the data it was trained on. Obtaining
    high-quality and faithful training data in the required quantity is challenging.
    One way to improve the training data is by employing human annotators. Human annotators
    can either write new texts from scratch or go through the training data and correcting
    or improving the collected texts.^([1](#Fn1)) In both cases, the required resources
    in terms of human labor, and thus costs, is very large. Therefore, this is often
    in the best case feasible for very domain-specific tasks and lacks generalization.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we take the entire training set of a large language model, containing
    billions of words, it is not feasible to manually review all of this by humans.
    However, if we say that we are in particular interested in being sure that the
    system is all correct about strawberries, we could pick out from the training
    data all sentences containing the word *strawberry*. This would probably reduce
    the number of sentences to be reviewed by hand by a lot, and maybe it would become
    feasible.
  prefs: []
  type: TYPE_NORMAL
- en: At a technical level, the quality of the training data and transparency are
    two major enablers for responsible AI. Responsible AI, or in the context of this
    book *responsible natural language processing*, is a field that should interest
    us in order to shape the digital society we would like to have for the future.
    This raises also to the question of whether there is *the* digital society or
    whether there will be several digital societies involving different groups or
    regions.
  prefs: []
  type: TYPE_NORMAL
- en: To enable transparency about data sets, we first need a standard for documentation
    of datasets. Such a standard was proposed in the Datasheets for Datasets paper
    in 2021 (Gebru et al. [2021](#CR15)). The authors argue that the characteristics
    of the training data set influence the model’s behavior, and thus the provenance,
    the creation, and the use of such data sets, need to be well documented. They
    suggest that each data set is accompanied by a data sheet, containing all this
    information. Sounds plausible and simple, but, unfortunately, it is currently
    not (yet) the default standard for the AI industry.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from having more knowledge about the training data sets, we also want
    to be transparent about the machine learning models. And here it is getting a
    bit trickier. Whereas decisions obtained with the basic methods we have seen earlier
    in the beginning of the book, such as logistic regression, can be explained easier,
    when it comes to neural networks, this is very challenging. This problem is addressed
    by the research field of *explainable AI*. In the context of explainable AI, tools
    and frameworks are developed to understand and interpret the decisions such systems
    make. A better understanding of how decisions are made is required to be transparent
    about machine learning models. Unfortunately, more work is required in this field,
    and fully explaining how a 175 billion parameter language model generates a sentence
    is far from being solved.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, as mentioned earlier, regulations of AI are another topic of the current
    discussion. Whereas most people agree that regulations are required, how they
    shall be implemented technically or enforced is subject to discussion. Whereas
    there might be few people being against fair and transparent AI software, it is
    challenging to fully address this at a technical level. However, we need this
    transparency and will thus need to rethink the way such software is developed
    and deployed. There is work ahead.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen now, the technical solution to these problems is still work
    in progress and, by design, difficult to fix. However, language models are used
    more and more, so we have to address some of these problems also at a societal
    level.
  prefs: []
  type: TYPE_NORMAL
- en: The Future of Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: New tools may change the way human work happens. To what extent should on one
    side be defined by what is technically possible but also by what is beneficial
    from a financial perspective and, most importantly, by what makes sense from a
    societal perspective. Personal computers and printers have revolutionized the
    technical way in which we write and produce text, shifting from handwritten notes
    to digital documents. With the introduction of smartphones, yet new ways of written
    communication such as SMS or messaging services have been introduced. With a new
    generation of tools like ChatGPT, text production by humans is challenged on a
    new level.
  prefs: []
  type: TYPE_NORMAL
- en: 'Whereas at a technical level it is now possible to produce texts that look
    somewhat eloquent and legit, they have major limitations in terms of content and
    world knowledge, as we have seen throughout this book. When with the rise of ChatGPT
    people suggested adding such a generative AI as coauthor to their scientific paper,
    it is them putting the technology in the job of a human. This is more a societal
    rather than a technical problem, and we should ask ourselves how we see our role
    and technology’s role in all of this. If you use a generative AI to support you
    generating the structure of your scientific article, do you consider it your coauthor?
    Or let me rephrase it: Did you ever consider putting your text-processing tool
    like Word or Latex or the search engine you used to find related work as a coauthor?'
  prefs: []
  type: TYPE_NORMAL
- en: How we define the power relationship between humans and machines for different
    tasks is crucial. Being a coauthor means seeing the tool at the same power level
    as your human coauthor. Having an AI application give severe instructions to human
    workers might reduce the acceptance of the technology as opposed to a human worker
    seeing a software as a smart tool supporting their working processes. For example,
    worker surveillance tools would probably be seen much more critical than an AI
    tool that is used as a programming assistant for software developers. To represent
    their processes in a useful and acceptable way in digital workflows and enable
    a human-centered digital transformation requires user involvement in all steps.
    This is especially true for novel technologies such as generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, with the stereotypes and limitations we have seen previously,
    I can only encourage having a *human in the loop*. As we have seen in the very
    early sections of this book, machine learning models, for example, classifiers,
    make an *estimation*. This guess, which seems the most probable for the given
    scenario, can also be *wrong*. Depending on what kind of critical decisions we
    are taking, we need to include more than the knowledge from the training data
    and reflect the proposed decisions by humans having world knowledge and experience.
    Additionally, as humans can be biased too, the processes and documentation around
    relevant decisions are crucial, also when being made by humans.
  prefs: []
  type: TYPE_NORMAL
- en: Are all AI applications critical in the same way? Probably not. When considering
    a software to sort different types of screws into boxes, there are probably much
    less ethical and human-in-the-loop problems as compared to other applications
    in the legal or medical domain. Systems that call for regulation and monitoring
    are those making decisions about “people, resources that concern people, issues
    that affect people’s ability to participate in society” (Zweig [2022](#CR43),
    p. 8).
  prefs: []
  type: TYPE_NORMAL
- en: With all that being said, I want to explain you why I do not like the term *artificial
    intelligence*. As mentioned, the goal of the society should be to produce tools
    supporting us with our tasks, and not aiming to develop a fully humanlike general
    artificial intelligence. The term *artificial intelligence*, even though established
    nowadays and widely used, is therefore misleading.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the term *augmented intelligence*^([2](#Fn2)) is more and more used
    instead. It aims at the creation of technology that augments the human intelligence
    with smart tools, rather than replacing humans. This vision for the digital society
    of the future leaves the human in the control position and assesses the information
    provided by the software systems. As seen before, this is particularly relevant
    for critical use cases, such as decisions about humans.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in Fig. [7.1](#Fig1), augmented intelligence aims to empower human
    decision-making, providing additional information and insights rather than replacing
    the human in the loop by a software.![](../images/604345_1_En_7_Chapter/604345_1_En_7_Fig1_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: A diagram illustrates the augmented intelligence model, which empowers humans
    with technology by assisting them in the decision-making process.
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 7.1
  prefs: []
  type: TYPE_NORMAL
- en: 'Augmented intelligence: empowering humans rather than replacing them'
  prefs: []
  type: TYPE_NORMAL
- en: The Future of Education
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s now discuss what these technologies mean for education. Given the new
    possibilities of text generation with models like ChatGPT, several challenges
    arise for the way we were, until a few months ago quite successfully, teaching
    and assessing students at different levels. By prompting ChatGPT (or other similar
    tools), it is now in many cases possible to solve programming exercises or write
    essays just by pasting the task description as a prompt. In other domains, it
    was reported that ChatGPT successfully passed exams that require domain-specific
    knowledge, as for example in medicine. What does this mean for our education system?
  prefs: []
  type: TYPE_NORMAL
- en: We have to review the way we teach and what the skills are that need to be developed
    by students but also for teachers or the broad public. The main challenge here
    is that technology is evolving fast, and to reflect the way we teach is taking
    more time. Rather than banning these tools, I suggest teaching our students how
    to use them responsibly and, in particular, what are their limitations and pitfalls.
  prefs: []
  type: TYPE_NORMAL
- en: We certainly have to adapt and review certain types of exams and some of the
    contents and skills we are aiming to teach and maybe have more direct interactions
    in the assessment rather than just an essay submitted at the end of the semester.
    But the new technologies offer much more than that. New generative AI tools also
    enable new great opportunities in the field of education, for example, to generate
    new use cases to discuss and critically reflect with students and to provide them
    with smart learning environments with individual feedback and suggestions. Human
    creativity along with the latest technology will shape the education of the future.
  prefs: []
  type: TYPE_NORMAL
- en: Apart impacting the way we work, learn, and teach, language models can provide
    also many interesting new questions for other fields of research, to be unfolded
    over the coming years. As the mathematician Stephen Wolfram is suggesting in his
    recent book about ChatGPT, “human language (and the patterns of thinking behind
    it) are somehow simpler and more *law like* in their structure than we thought”
    (Wolfram [2023](#CR42), p. 108). Maybe those new technologies can finally help
    us in some way or the other to better understand ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conclusion: Shaping the Future'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have seen in this book what is behind the scenes of state-of-the-art language
    models and in particular chatbots. Even though we have seen that from the inside
    language models are less magical than one might have thought in the beginning,
    their results are still impressive. By providing responses that might most likely
    look like good human answers to the proposed prompts, they give us the impression
    of eloquent conversation partners. These pitfalls and limitations need to be considered,
    yet there are nearly endless new opportunities that these technologies bring along
    for our society. It is what we make from it. And with *we*, I mean all of us.
    I want to encourage a public discourse about technology, involving technical experts
    as well as people from other domains. It is in our hands.
  prefs: []
  type: TYPE_NORMAL
