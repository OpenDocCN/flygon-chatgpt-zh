### 第7章

## ChatGPT的对话提示工程

了解与ChatGPT对话的动态特性

对话工程是一个迅速发展的领域，由于ChatGPT等聊天机器人的开发和日益广泛使用而变得越来越受欢迎。与基于机器的对话者交谈需要与与人类交流不同的方法，了解这种类型互动的微妙之处至关重要。

与ChatGPT交谈的第一步是了解其工作原理。ChatGPT使用自然语言处理（NLP）算法，使其能够分析文本输入并提供适当的回应。这些回应是基于输入系统的大量数据中观察到的模式生成的。与人类交谈和ChatGPT之间的一个重要区别是，机器只会根据它们从先前交互中学到的知识做出回应，而人类可以利用他们过去经验之外的知识。这意味着与ChatGPT等机器的对话可能更可预测，但也可能更受限制。

与ChatGPT交谈的另一个重要方面是需要结构化提示。提示是指引对话朝向特定主题或信息的输入文本。正确构建提示可以通过引导基于机器的对话者产生特定回应来实现更有吸引力的对话。在生成有意义的对话时，提示的多样性至关重要，因为它可以防止重复和单调。有效提示的例子包括开放式问题、需要同意或不同意的陈述，或要求对先前陈述进行详细说明。

由于ChatGPT缺乏类似人类直觉或超出其编程模式的理解能力，对话中可能会出现歧义。然而，有策略可以有效处理模糊的回复。其中一种策略涉及重新表述不清晰的陈述或询问后续问题，直到达到清晰度。处理歧义还包括在必要时向基于机器的对话者寻求澄清。例如，如果一个答案看起来矛盾或令人困惑，请求进一步解释可以帮助澄清回复。

导航偏见是与ChatGPT交谈的另一个关键方面。由于输入系统的数据或开发人员或用户的先入之见，偏见可能会在基于机器的交互中出现。这些偏见可能是有害的，并且可能会持续传播刻板印象，因此识别它们并避免在与ChatGPT的对话中持续传播是至关重要的。保持道德对话涉及避免持续传播有害的偏见或刻板印象，同时参与人机交互。了解偏见如何影响沟通以及它们可能如何影响某些群体是至关重要的。开发人员必须努力创建促进所有人健康对话的无偏见的聊天机器人。

当与像ChatGPT这样的机器进行交流时，了解对话工程的动态需要了解结构化提示、处理模糊性、导航偏见以及保持道德对话。从本章中获得的见解将为读者打下坚实的基础，使他们在深入研究ChatGPT的对话提示工程时有所裨益。

为引人入胜的对话构建提示

当我们深入探讨像ChatGPT这样的聊天机器人的对话提示工程领域时，了解提示的结构如何引发机器对话者特定回应是很重要的。合适的提示可以产生有意义的对话，让用户保持参与并对他们的互动感到满意。在本章中，我们将讨论提示中多样性的重要性，并提供一些有效提示的示例以及它们为何有效。

多样性是构建提示时的关键。重复使用相同类型的提示可能导致单调的对话，无法吸引用户。为了避免这种情况，开发人员应该创建各种各样的提示，涉及到人类心理和情感的不同方面。例如，一个询问某人最喜欢的爱好的提示可以涉及到他们的兴趣和激情，而询问他们在工作中的一天的提示可以唤起与生产力或压力相关的情感。

提示的结构也在引发ChatGPT引人入胜回应中扮演着重要角色。一个结构良好的提示应该清晰、简洁，并且开放性足够，以允许多样化的回应，同时仍专注于特定主题或主题。例如，不要问“你对政治有什么看法？”这可能导致模糊或无趣的回应，试着问“你对哪个政治问题感触最深？”这不仅限定了话题，还能引发更详细和充满激情的回应。

处理模糊性和寻求澄清

随着我们继续探索ChatGPT的对话提示工程之旅，我们必须探讨在机器对话中处理模糊性和寻求澄清的挑战。虽然ChatGPT可以生成在连贯性和相关性方面令人印象深刻的回应，但并不免疫于模糊性或不精确性。在本章中，我们将讨论如何处理这些问题。首先，让我们定义一下我们所说的模糊性。当ChatGPT的回应可以被解释为多种解释或缺乏具体性时，就会出现模糊性。这可能是因为提示的结构方式或机器知识库中的空白。例如，如果我们询问ChatGPT关于一个特定事件而没有提供足够的背景信息，它可能会回应不完整或不准确的信息。

那么我们如何处理ChatGPT的模糊响应呢？一种策略是使用追问问题，寻求对特定细节的澄清。这些问题应设计为在避免过于宽泛的询问的同时，获取关于特定主题更多信息。另一种方法是用更精确的语言重述原始问题。这种技术可以帮助聚焦ChatGPT的注意力到先前不清楚或被忽视的具体细节。此外，将复杂的提示分解成较小的部分可以帮助避免混淆并减少模糊性。

在向ChatGPT澄清问题时，重要的是要记住，这项技术基于其编程和数据来源具有局限性。因此，我们可能无法始终获得完全满足我们需求的答案；但是，通过使用这些处理模糊性和寻求澄清的技巧，我们可以提高生成准确答案的机会。

现在让我们看一些实际生活中这些策略如何运作的例子：

Chatbot: "抱歉，我不理解你在问什么。"

User: "你能告诉我一些关于莎士比亚的信息吗？"

Chatbot: "莎士比亚是一位英国剧作家和诗人。"

User: "你能提供更多关于他的作品的信息吗？"

Chatbot: "莎士比亚写了许多剧作，包括《罗密欧与朱丽叶》、《哈姆雷特》和《麦克白》。"

User: "你最喜欢的剧本是哪一个？"

Chatbot: "抱歉，我没有偏好的能力。"

在这个例子中，用户提出了一个关于莎士比亚的广泛问题，ChatGPT用一般信息回答了用户。然后用户使用追问的方式来引出关于莎士比亚作品更具体的细节。然而，当用户问ChatGPT它最喜欢的剧本时，ChatGPT正确地回答说它没有个人偏好。

在另一个例子中：

User: "法国的首都是什么？"

Chatbot: "法国的首都是巴黎。"

User: "其他哪些法国城市是受欢迎的旅游目的地？"

Chatbot: "法国有许多受欢迎的旅游目的地，包括尼斯、马赛和里昂。"

User: "你会推荐先参观哪一个？"

Chatbot: "抱歉，我不能做出推荐，因为我没有个人经历或观点。"

在这里，我们看到通过询问ChatGPT关于除了巴黎之外的其他受欢迎旅游目的地的探询性问题，并要求ChatGPT推荐哪个城市先访问，结果是得到一个回答说它无法做出推荐。处理模糊性并寻求澄清是与像ChatGPT这样的基于机器的对话者交谈时的基本技能。通过使用寻求具体细节的探询性问题或用更精确的语言重新表达提示可以帮助避免混淆并产生准确的回应。此外，了解这项技术的局限性可以帮助我们在与之交流时管理期望。通过掌握处理模糊性和寻求澄清的技巧，如与ChatGPT等基于机器的对话中，将帮助我们在利用这些技术时改善我们的整体体验，因为它们继续朝着潜力发展。

应对偏见并保持道德对话

随着人工智能（AI）技术的进步，认识到在处理机器交互中的偏见时出现的挑战是至关重要的。像ChatGPT这样的聊天机器人是通过来自各种来源的数据编程的，包括人类对话和网站。因此，在进行人类与AI对话时识别道德考虑是至关重要的。在AI中最重要的道德问题之一是避免持续有害的偏见或刻板印象。机器学习算法只有在其训练数据无偏见时才是无偏见的。如果数据集包含有偏见的信息，可能会导致问题性的结果。例如，如果AI聊天机器人是在主要由男性撰写的数据集上训练的，它可能难以理解某些女性中心的���题或强化性别刻板���象。

另一个挑战是应对人类与技术互动产生的潜在偏见。人类对机器的能力和不能力有先入为主的观念，这可能导致在与像ChatGPT这样的AI聊天机器人互动时出现无意识的偏见。例如，如果用户认为ChatGPT无法理解复杂情绪或社交暗示，因为它不够像人类，他们在对话中可能会限制自己。

为了解决这些问题并与像ChatGPT这样的AI聊天机器人保持道德对话，应采用几种策略：

1) 多样化训练数据：为了减轻像ChatGPT这样的机器学习模型中的算法偏见，需要开发反映性别认同/种族/年龄/地理位置/国籍/阶级等多样化观点的提示，而不仅仅是一个主导群体。

2) 监控对话：定期监控对话提示和回复有助于识别出现在单个对话或随着时间推移在所有对话中出现的任何问题模式，比如使用冒犯性语言等模式。

3) 建立基本规则：为聊天机器人对话设定明确的基本规则对于维护人与机器之间的道德对话至关重要。规则可能包括要避免的话题，例如仇恨言论或煽动性语言。

4) 提供透明度：提供关于聊天机器人能力和限制的透明度可以帮助用户了解可以从基于机器的对话者那里期望什么。这种透明度应包括聊天机器人的训练数据、其偏见以及信息处理方式。

5) 定期更新聊天机器人训练数据：定期更新训练数据有助于通过将新的观点纳入AI对话中来减少偏见。例如，如果有关性别认同的新研究发布，将ChatGPT的数据更新为这些信息可以改善其对非二元个体观点的理解。

随着人工智能技术的快速发展，重要的是要引导偏见并与像ChatGPT这样的AI聊天机器人保持道德对话。随着我们将来进一步发展这些技术，我们需要确保它们不受内在偏见的影响或延续可能造成重大伤害的有害刻板印象。通过多样化培训数据集，定期监控对话，建立人与机器之间对话的基本规则，为ChatGPT这样的聊天机器人提供透明度，我们可以帮助随着时间的推移减轻这些风险。
