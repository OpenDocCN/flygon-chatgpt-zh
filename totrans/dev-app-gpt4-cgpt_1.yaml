- en: Chapter 1\. GPT-4 and ChatGPT Essentials
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章 GPT-4和ChatGPT基础知识
- en: Imagine a world where you can communicate with computers as quickly as you can
    with your friends. What would that look like? What applications could you create?
    This is the world that OpenAI is helping to build with its GPT models, bringing
    human-like conversational capabilities to our devices. As the latest advancements
    in AI, GPT-4 and other GPT models are large language models (LLMs) trained on
    massive amounts of data, enabling them to recognize and generate human-like text
    with very high accuracy.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，您可以与计算机的交流速度与与朋友的交流一样快。那会是什么样子？您可以创建什么应用程序？这就是OpenAI正在帮助构建的世界，它将人类般的对话能力带到我们的设备上。作为人工智能的最新进展，GPT-4和其他GPT模型是在大量数据上训练的大型语言模型（LLMs），使它们能够识别和生成非常准确的人类文本。
- en: The implications of these AI models go far beyond simple voice assistants. Thanks
    to OpenAI’s models, developers can now exploit the power of natural language processing
    (NLP) to create applications that understand our needs in ways that were once
    science fiction. From innovative customer support systems that learn and adapt
    to personalized educational tools that understand each student’s unique learning
    style, GPT-4 and ChatGPT open up a whole new world of possibilities.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这些人工智能模型的影响远远超出了简单的语音助手。由于OpenAI的模型，开发人员现在可以利用自然语言处理（NLP）的力量来创建能够以前只存在于科幻小说中的应用程序，这些应用程序能够理解我们的需求。从学习和适应个性化教育工具的创新客户支持系统，到理解每个学生独特学习风格的应用，GPT-4和ChatGPT打开了全新的可能性世界。
- en: But what *are* GPT-4 and ChatGPT? The goal of this chapter is to take a deep
    dive into the foundations, origins, and key features of these AI models. By understanding
    the basics of these models, you will be well on your way to building the next
    generation of LLM-powered applications.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 但是GPT-4和ChatGPT到底是什么？本章的目标是深入探讨这些人工智能模型的基础、起源和关键特性。通过了解这些模型的基础知识，您将能够着手构建下一代LLM驱动的应用程序。
- en: Introducing Large Language Models
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍大型语言模型
- en: This section lays down the fundamental building blocks that have shaped the
    development of GPT-4 and ChatGPT. We aim to provide a comprehensive understanding
    of language models and NLP, the role of transformer architectures, and the tokenization
    and prediction processes within GPT models.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本节阐述了塑造GPT-4和ChatGPT发展的基本构建模块。我们旨在全面了解语言模型和NLP，变压器架构的作用，以及GPT模型中的标记化和预测过程。
- en: Exploring the Foundations of Language Models and NLP
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索语言模型和NLP的基础
- en: As LLMs, GPT-4 and ChatGPT are the latest type of model obtained in the field
    of NLP, which is itself a subfield of machine learning (ML) and AI. Before delving
    into GPT-4 and ChatGPT, it is essential to take a look at NLP and its related
    fields.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作为LLMs，GPT-4和ChatGPT是NLP领域获得的最新模型类型，NLP本身是机器学习（ML）和人工智能（AI）的一个子领域。在深入研究GPT-4和ChatGPT之前，了解NLP及其相关领域至关重要。
- en: There are different definitions of AI, but one of them, more or less the consensus,
    says that AI is the development of computer systems that can perform tasks that
    typically require human intelligence. With this definition, many algorithms fall
    under the AI umbrella. Consider, for example, the traffic prediction task in GPS
    applications or the rule-based systems used in strategic video games. In these
    examples, seen from the outside, the machine seems to require intelligence to
    accomplish these tasks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: AI有不同的定义，但其中一个更或多或少的共识是，AI是开发能够执行通常需要人类智能的任务的计算机系统。根据这个定义，许多算法都属于AI范畴。例如，考虑GPS应用程序中的交通预测任务或战略视频游戏中使用的基于规则的系统。在这些示例中，从外部看，机器似乎需要智能来完成这些任务。
- en: ML is a subset of AI. In ML, we do not try to directly implement the decision
    rules used by the AI system. Instead, we try to develop algorithms that allow
    the system to learn by itself from examples. Since the 1950s, when ML research
    began, many ML algorithms have been proposed in the scientific literature.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ML是AI的一个子集。在ML中，我们不试图直接实现AI系统使用的决策规则。相反，我们试图开发算法，让系统能够从示例中自行学习。自从20世纪50年代开始进行ML研究以来，许多ML算法已经在科学文献中提出。
- en: Among them, deep learning algorithms have come to the fore. *Deep learning*
    is a branch of ML that focuses on algorithms inspired by the structure of the
    brain. These algorithms are called *artificial neural networks*. They can handle
    very large amounts of data and perform very well on tasks such as image and speech
    recognition and NLP.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，深度学习算法已经成为前沿。*深度学习*是ML的一个分支，专注于受大脑结构启发的算法。这些算法称为*人工神经网络*。它们可以处理非常大量的数据，并在图像和语音识别以及NLP等任务上表现非常出色。
- en: GPT-4 and ChatGPT are based on a particular type of deep learning algorithm
    called *transformers*. Transformers are like reading machines. They pay attention
    to different parts of a sentence or block of text to understand its context and
    produce a coherent response. They can also understand the order of words in a
    sentence and their context. This makes them highly effective at tasks such as
    language translation, question answering, and text generation. [Figure 1-1](#fig_1_a_nested_set_of_technologies_from_ai_to_transforme)
    illustrates the relationships among these terms.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4和ChatGPT基于一种称为*transformers*的特定类型的深度学习算法。变压器就像阅读机器一样。它们关注句子或文本块的不同部分，以理解其上下文并产生连贯的回应。它们还可以理解句子中的单词顺序和它们的上下文。这使它们在语言翻译、问题回答和文本生成等任务中非常有效。[图1-1](#fig_1_a_nested_set_of_technologies_from_ai_to_transforme)说明了这些术语之间的关系。
- en: '![](assets/dagc_0101.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dagc_0101.png)'
- en: Figure 1-1\. A nested set of technologies from AI to transformers
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-1。从AI到变压器的嵌套技术集
- en: 'NLP is a subfield of AI focused on enabling computers to process, interpret,
    and generate natural human language. Modern NLP solutions are based on ML algorithms.
    The goal of NLP is to allow computers to process natural language text. This goal
    covers a wide range of tasks:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: NLP是人工智能的一个子领域，专注于使计算机能够处理、解释和生成自然人类语言。现代NLP解决方案基于机器学习算法。NLP的目标是让计算机处理自然语言文本。这个目标涵盖了广泛的任务：
- en: Text classification
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类
- en: Categorizing input text into predefined groups. This includes, for example,
    sentiment analysis and topic categorization. Companies can use sentiment analysis
    to understand customers’ opinions about their services. Email filtering is an
    example of topic categorization in which email can be put into categories such
    as “Personal,” “Social,” “Promotions,” and “Spam.”
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入文本分类为预定义的组。这包括情感分析和主题分类，例如，公司可以使用情感分析来了解客户对其服务的意见。电子邮件过滤是主题分类的一个例子，其中电子邮件可以被归类为“个人”，“社交”，“促销”和“垃圾邮件”。
- en: Automatic translation
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 自动翻译
- en: Automatic translation of text from one language to another. Note that this can
    include areas like translating code from one programming language to another,
    such as from Python to C++.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本从一种语言自动翻译成另一种语言。请注意，这可能包括将代码从一种编程语言翻译成另一种语言，例如从Python到C++。
- en: Question answering
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 回答基于给定文本的问题。
- en: Answering questions based on a given text. For example, an online customer service
    portal could use an NLP model to answer FAQs about a product, or educational software
    could use NLP to provide answers to students’ questions about the topic being
    studied.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 根据给定文本回答问题。例如，在线客户服务门户可以使用NLP模型回答关于产品的常见问题，或者教育软件可以使用NLP为学生关于所学主题的问题提供答案。
- en: Text generation
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 文本生成
- en: Generating a coherent and relevant output text based on a given input text,
    called a prompt.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 根据给定的输入文本生成连贯和相关的输出文本，称为提示。
- en: As mentioned earlier, LLMs are ML models trying to solve text generation tasks,
    among others. LLMs enable computers to process, interpret, and generate human
    language, allowing for more effective human–machine communication. To be able
    to do this, LLMs analyze or *train* on vast amounts of text data and thereby learn
    patterns and relationships between words in sentences. A variety of data sources
    can be used to perform this learning process. This data can include text from
    Wikipedia, Reddit, the archive of thousands of books, or even the archive of the
    internet itself. Given an input text, this learning process allows the LLMs to
    make predictions about the likeliest following words and, in this way, can generate
    meaningful responses to the input text. The modern language models, published
    in the past few months, are so large and have been trained on so many texts that
    they can now directly perform most NLP tasks, such as text classification, machine
    translation, question answering, and many others. The GPT-4 and ChatGPT models
    are modern LLMs that excel at text generation tasks.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，LLM是试图解决文本生成任务的机器学习模型之一。LLM使计算机能够处理、解释和生成人类语言，从而实现更有效的人机通信。为了能够做到这一点，LLM分析或*训练*大量文本数据，从而学习句子中单词之间的模式和关系。可以使用各种数据源来执行这个学习过程。这些数据可以包括来自维基百科、Reddit、成千上万本书的档案，甚至是互联网本身的档案。在给定输入文本的情况下，这个学习过程使LLM能够对接下来最有可能的单词进行预测，并以这种方式生成对输入文本有意义的响应。最近几个月发布的现代语言模型非常庞大，并且已经在许多文本上进行了训练，以至于它们现在可以直接执行大多数NLP任务，如文本分类、机器翻译、问答等。GPT-4和ChatGPT模型是在文本生成任务方面表现出色的现代LLM。
- en: The development of LLMs goes back several years. It started with simple language
    models such as *n-grams*, which tried to predict the next word in a sentence based
    on the previous words. N-gram models use *frequency* to do this. The predicted
    next word is the most frequent word that follows the previous words in the text
    the n-gram model was trained on. While this approach was a good start, n-gram
    models’ need for improvement in understanding context and grammar resulted in
    inconsistent text generation.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的发展可以追溯到几年前。它始于简单的语言模型，如*n-grams*，它试图根据先前的单词预测句子中的下一个单词。N-gram模型使用*频率*来做到这一点。预测的下一个单词是在n-gram模型训练的文本中跟随前面单词的最常见的单词。虽然这种方法是一个很好的开始，但n-gram模型对于理解上下文和语法的改进需要导致不一致的文本生成。
- en: To improve the performance of n-gram models, more advanced learning algorithms
    were introduced, including recurrent neural networks (RNNs) and long short-term
    memory (LSTM) networks. These models could learn longer sequences and analyze
    the context better than n-grams, but they still needed help processing large amounts
    of data efficiently. These types of recurrent models were the most efficient ones
    for a long time and therefore were the most used in tools such as automatic machine
    translation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改善n-gram模型的性能，引入了更先进的学习算法，包括循环神经网络（RNN）和长短期记忆（LSTM）网络。这些模型可以学习更长的序列，并且比n-grams更好地分析上下文，但它们仍然需要帮助有效地处理大量数据。这些类型的循环模型长时间以来一直是最有效的模型，因此在自动机器翻译等工具中被最广泛使用。
- en: Understanding the Transformer Architecture and Its Role in LLMs
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解变压器架构及其在LLM中的作用
- en: 'The Transformer architecture revolutionized NLP, primarily because transformers
    effectively address one of the critical limitations of previous NLP models such
    as RNNs: their struggle with long text sequences and maintaining context over
    these lengths. In other words, while RNNs tended to forget the context in longer
    sequences (the infamous “catastrophic forgetting”), transformers came with the
    ability to handle and encode this context effectively.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器架构革命性地改变了NLP，主要是因为变压器有效地解决了以前NLP模型（如RNN）的一个关键限制：它们在处理长文本序列和保持这些长度上下文方面的困难。换句话说，虽然RNN在处理更长序列时往往会忘记上下文（臭名昭著的“灾难性遗忘”），但变压器具有处理和有效编码这种上下文的能力。
- en: The central pillar of this revolution is the *attention mechanism*, a simple
    yet powerful idea. Instead of treating all words in a text sequence as equally
    important, the model “pays attention” to the most relevant terms for each step
    of its task. Cross-attention and self-attention are two architectural blocks based
    on this attention mechanism, and they are often found in LLMs. The Transformer
    architecture makes extensive use of these cross-attention and self-attention blocks.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这一革命的核心是*注意机制*，这是一个简单而强大的想法。模型不再将文本序列中的所有单词视为同等重要，而是“关注”每一步任务中最相关的术语。交叉注意力和自注意力是基于这一注意机制的两个架构模块，它们经常出现在LLM中。变压器架构广泛使用这些交叉注意力和自注意力模块。
- en: '*Cross-attention* helps the model determine the relevance of the different
    parts of the input text for accurately predicting the next word in the output
    text. It’s like a spotlight that shines on words or phrases in the input text,
    highlighting the relevant information needed to make the next word prediction
    while ignoring less important details.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*交叉注意力*帮助模型确定输入文本的不同部分与准确预测输出文本中下一个单词的相关性。这就像一个聚光灯，照亮输入文本中的单词或短语，突出显示生成下一个单词预测所需的相关信息，同时忽略不太重要的细节。'
- en: To illustrate this, let’s take an example of a simple sentence translation task.
    Imagine we have an input English sentence, “Alice enjoyed the sunny weather in
    Brussels,” which should be translated into French as “Alice a profité du temps
    ensoleillé à Bruxelles.” In this example, let us focus on generating the French
    word *ensoleillé*, which means *sunny*. For this prediction, cross-attention would
    give more weight to the English words *sunny* and *weather* since they are both
    relevant to the meaning of *ensoleillé*. By focusing on these two words, cross-attention
    helps the model generate an accurate translation for this part of the sentence.
    [Figure 1-2](#fig_2_cross_attention_uses_the_attention_mechanism_to_fo) illustrates
    this example.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，让我们以一个简单的句子翻译任务为例。假设我们有一个输入的英文句子，“爱丽丝在布鲁塞尔享受了阳光明媚的天气”，应该翻译成法文为“爱丽丝 a
    profité du temps ensoleillé à Bruxelles.”在这个例子中，让我们专注于生成法文单词*ensoleillé*，意思是*阳光明媚*。对于这个预测，交叉注意力会更加关注英文单词*sunny*和*weather*，因为它们都与*ensoleillé*的含义相关。通过专注于这两个单词，交叉注意力帮助模型为句子的这部分生成准确的翻译。[图1-2](#fig_2_cross_attention_uses_the_attention_mechanism_to_fo)说明了这个例子。
- en: '![](assets/dagc_0102.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dagc_0102.png)'
- en: Figure 1-2\. Cross-attention uses the attention mechanism to focus on essential
    parts of the input text (English sentence) to predict the next word in the output
    text (French sentence)
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-2。交叉注意力使用注意机制专注于输入文本（英文句子）的关键部分，以预测输出文本（法文句子）中的下一个单词
- en: '*Self-attention* refers to the ability of a model to focus on different parts
    of its input text. In the context of NLP, the model can evaluate the importance
    of each word in a sentence with the other words. This allows it to better understand
    the relationships between the words and helps the model build new *concepts* from
    multiple words in the input text.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*自注意力*指的是模型专注于其输入文本的不同部分的能力。在NLP的背景下，模型可以评估句子中每个单词与其他单词的重要性。这使它能够更好地理解单词之间的关系，并帮助模型从输入文本中的多个单词中构建新的*概念*。'
- en: 'As a more specific example, consider the following: “Alice received praise
    from her colleagues.” Assume that the model is trying to understand the meaning
    of the word *her* in the sentence. The self-attention mechanism assigns different
    weights to the words in the sentence, highlighting the words relevant to *her*
    in this context. In this example, self-attention would place more weight on the
    words *Alice* and *colleagues*. Self-attention helps the model build new concepts
    from these words. In this example, one of the concepts that could emerge would
    be “Alice’s colleagues,” as shown in [Figure 1-3](#fig_3_self_attention_allows_the_emergence_of_the_alice).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个更具体的例子，考虑以下情况：“爱丽丝受到了同事们的赞扬。”假设模型试图理解句子中“她”这个词的含义。自注意力机制为句子中的单词分配不同的权重，突出显示与这个上下文中“她”相关的单词。在这个例子中，自注意力会更加关注“爱丽丝”和“同事们”这两个词。自注意力帮助模型从这些词中建立新的概念。在这个例子中，可能出现的一个概念是“爱丽丝的同事们”，如[图1-3](#fig_3_self_attention_allows_the_emergence_of_the_alice)所示。
- en: '![](assets/dagc_0103.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dagc_0103.png)'
- en: Figure 1-3\. Self-attention allows the emergence of the “Alice’s colleagues”
    concept
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-3。自注意力允许“爱丽丝的同事们”概念的出现
- en: Unlike the recurrent architecture, transformers also have the advantage of being
    easily *parallelized*. This means the Transformer architecture can process multiple
    parts of the input text simultaneously rather than sequentially. This allows faster
    computation and training because different parts of the model can work in parallel
    without waiting for previous steps to complete, unlike recurrent architectures,
    which require sequential processing. The parallel processing capability of transformer
    models fits perfectly with the architecture of graphics processing units (GPUs),
    which are designed to handle multiple computations simultaneously. Therefore,
    GPUs are ideal for training and running these transformer models because of their
    high parallelism and computational power. This advance allowed data scientists
    to train models on much larger datasets, paving the way for developing LLMs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与循环架构不同，变压器还具有易于*并行化*的优势。这意味着变压器架构可以同时处理输入文本的多个部分，而不是顺序处理。这样可以加快计算和训练，因为模型的不同部分可以并行工作，而不必等待前面的步骤完成，这与需要顺序处理的循环架构不同。变压器模型的并行处理能力与图形处理单元（GPU）的架构完美契合，GPU的设计是为了同时处理多个计算。因此，GPU非常适合训练和运行这些变压器模型，因为它们具有高度的并行性和计算能力。这一进步使数据科学家能够在更大的数据集上训练模型，为发展LLM铺平了道路。
- en: 'The Transformer architecture, introduced in 2017 by Vaswani et al. from Google
    in the paper “[Attention Is All You Need”](https://oreil.ly/jVZW1), was originally
    developed for sequence-to-sequence tasks such as machine translation. A standard
    transformer consists of two primary components: an encoder and a decoder, both
    of which rely heavily on attention mechanisms. The task of the encoder is to process
    the input text, identify valuable features, and generate a meaningful representation
    of that text, known as *embedding*. The decoder then uses this embedding to produce
    an output, such as a translation or summary. This output effectively interprets
    the encoded information.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构于2017年由Google的Vaswani等人在论文“[Attention Is All You Need”](https://oreil.ly/jVZW1)中引入，最初是为了序列到序列的任务，如机器翻译。标准的Transformer由两个主要组件组成：编码器和解码器，两者都严重依赖于注意机制。编码器的任务是处理输入文本，识别有价值的特征，并生成该文本的有意义表示，称为*嵌入*。解码器然后使用这个嵌入来产生一个输出，比如翻译或摘要。这个输出有效地解释了编码信息。
- en: '*Generative pre-trained transformers*, commonly known as *GPT*, are a family
    of models that are based on the Transformer architecture and that specifically
    utilize the decoder part of the original architecture. In GPT, the encoder is
    not present, so there is no need for cross-attention to integrate the embeddings
    produced by an encoder. As a result, GPT relies solely on the self-attention mechanism
    within the decoder to generate context-aware representations and predictions.
    Note that other well-known models, such as BERT (Bidirectional Encoder Representations
    from Transformers), are based on the encoder part. We don’t cover this type of
    model in this book. [Figure 1-4](#fig_4_the_evolution_of_nlp_techniques_from_n_grams_to_th)
    illustrates the evolution of these different models.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*生成式预训练变换器*，通常称为*GPT*，是基于Transformer架构的一系列模型，专门利用原始架构的解码器部分。在GPT中，编码器不存在，因此不需要跨注意力来整合编码器产生的嵌入。因此，GPT仅依赖于解码器内的自注意机制来生成上下文感知的表示和预测。请注意，其他众所周知的模型，如BERT（来自变压器的双向编码器表示），是基于编码器部分的。我们在本书中不涵盖这种类型的模型。[图1-4](#fig_4_the_evolution_of_nlp_techniques_from_n_grams_to_th)说明了这些不同模型的演变。'
- en: '![](assets/dagc_0104.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dagc_0104.png)'
- en: Figure 1-4\. The evolution of NLP techniques from n-grams to the emergence of
    LLMs
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-4。从n-gram到LLM的NLP技术的演变
- en: Demystifying the Tokenization and Prediction Steps in GPT Models
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 揭秘GPT模型中的标记化和预测步骤
- en: LLMs in the GPT family receive a prompt as input, and in response they generate
    a text. This process is known as *text completion*. For example, the prompt could
    be “*The weather is nice today, so I decided to*” and the model output might be
    “*go for a walk*”. You may be wondering how the LLM model builds this output text
    from the input prompt. As you will see, it’s mostly just a question of probabilities.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: GPT系列中的LLM接收提示作为输入，并生成文本作为响应。这个过程被称为*文本完成*。例如，提示可以是“*天气今天很好，所以我决定*”，模型的输出可能是“*去散步*”。你可能想知道LLM模型如何从输入提示构建这个输出文本。正如你将看到的，这主要是一个概率问题。
- en: 'When a prompt is sent to an LLM, it first breaks the input into smaller pieces
    called *tokens*. These tokens represent single words, parts of words, or spaces
    and punctuation. For example, the preceding prompt could be broken like this:
    [“*The”, “wea”, “ther”, “is”, “nice”, “today”, “,”, “so”, “I”, “de”, “ci”, “ded”,
    “to*”]. Each language model comes with its own tokenizer. The GPT-4 tokenizer
    is not available at the time of this writing, but you can test the [GPT-3 tokenizer](https://platform.openai.com/tokenizer).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当提示被发送到LLM时，它首先将输入分成称为*标记*的较小部分。这些标记代表单词、单词部分、空格和标点符号。例如，前面的提示可以被分成这样：[“*The”,
    “wea”, “ther”, “is”, “nice”, “today”, “,”, “so”, “I”, “de”, “ci”, “ded”, “to*”]。每个语言模型都配备了自己的标记器。在撰写本文时，GPT-4标记器尚不可用，但你可以测试[GPT-3标记器](https://platform.openai.com/tokenizer)。
- en: Tip
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: A rule of thumb for understanding tokens in terms of word length is that 100
    tokens equal approximately 75 words for an English text.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 理解单词长度方面的令人信服的规则是，对于英文文本，100个标记大约相当于75个单词。
- en: Thanks to the attention principle and the Transformer architecture introduced
    earlier, the LLM processes these tokens and can interpret the relationships between
    them and the overall meaning of the prompt. The Transformer architecture allows
    a model to efficiently identify the critical information and the context within
    the text.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于之前介绍的注意原则和Transformer架构，LLM处理这些标记，并可以解释它们之间的关系以及提示的整体含义。Transformer架构允许模型有效地识别文本中的关键信息和上下文。
- en: To create a new sentence, the LLM predicts the tokens most likely to follow,
    based on the context of the prompt. OpenAI produced two versions of GPT-4, with
    context windows of 8,192 tokens and 32,768 tokens. Unlike the previous recurrent
    models, which had difficulty handling long input sequences, the Transformer architecture
    with the attention mechanism allows the modern LLM to consider the context as
    a whole. Based on this context, the model assigns a probability score for each
    potential subsequent token. The token with the highest probability is then selected
    as the next token in the sequence. In our example, after “The weather is nice
    today, so I decided to”, the next best token could be “go”.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个新的句子，LLM预测最有可能跟随的标记，基于提示的上下文。OpenAI制作了两个版本的GPT-4，上下文窗口分别为8,192个标记和32,768个标记。与以前的循环模型不同，它们难以处理长输入序列，具有注意机制的Transformer架构允许现代LLM将上下文作为一个整体来考虑。基于这个上下文，模型为每个潜在的后续标记分配一个概率分数。然后选择具有最高概率的标记作为序列中的下一个标记。在我们的例子中，在“天气今天很好，所以我决定”之后，下一个最佳标记可能是“去”。
- en: 'This process is then repeated, but now the context becomes “The weather is
    nice today, so I decided to go”, where the previously predicted token “go” is
    added to the original prompt. The second token that the model might predict could
    be “for”. This process is repeated until a complete sentence is formed: “go for
    a walk”. This process relies on the LLM’s ability to learn the next most probable
    word from massive text data. [Figure 1-5](#fig_5_the_completion_process_is_iterative_token_by_toke)
    illustrates this process.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程然后被重复，但现在上下文变成了“今天天气很好，所以我决定去”，先前预测的“去”被添加到原始提示中。模型可能预测的第二个标记可能是“散步”。这个过程重复进行，直到形成一个完整的句子：“去散步”。这个过程依赖于LLM学习从大量文本数据中学习下一个最有可能的单词的能力。[图1-5](#fig_5_the_completion_process_is_iterative_token_by_toke)说明了这个过程。
- en: '![](assets/dagc_0105.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dagc_0105.png)'
- en: Figure 1-5\. The completion process is iterative, token by token
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-5。完成过程是迭代的，逐个标记
- en: 'A Brief History: From GPT-1 to GPT-4'
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简史：从GPT-1到GPT-4
- en: In this section, we will review the evolution of the OpenAI GPT models from
    GPT-1 to GPT-4\.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将回顾OpenAI GPT模型从GPT-1到GPT-4的演变。
- en: GPT-1
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-1
- en: In mid-2018, just one year after the invention of the Transformer architecture,
    OpenAI published a paper titled [“Improving Language Understanding by Generative
    Pre-Training”](https://oreil.ly/Yakwa), by Radford et al., in which the company
    introduced the Generative Pre-trained Transformer, also known as GPT-1\.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年中，就在变压器架构被发明的一年后，OpenAI发表了一篇题为[“通过生成式预训练改进语言理解”](https://oreil.ly/Yakwa)的论文，作者是Radford等人，在这篇论文中，该公司介绍了生成式预训练变压器，也被称为GPT-1。
- en: Before GPT-1, the common approach to building high-performance NLP neural models
    relied on supervised learning. These learning techniques use large amounts of
    manually labeled data. For example, in a sentiment analysis task where the goal
    is to classify whether a given text has positive or negative sentiment, a common
    strategy would require collecting thousands of manually labeled text examples
    to build an effective classification model. However, the need for large amounts
    of well-annotated, supervised data has limited the performance of these techniques
    because such datasets are both difficult and expensive to generate.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPT-1之前，构建高性能NLP神经模型的常见方法依赖于监督学习。这些学习技术使用大量手动标记的数据。例如，在情感分析任务中，目标是对给定文本的情感进行分类，一种常见的策略需要收集成千上万个手动标记的文本示例来构建有效的分类模型。然而，对大量良好注释的监督数据的需求限制了这些技术的性能，因为这样的数据集既难以生成又昂贵。
- en: 'In their paper, the authors of GPT-1 proposed a new learning process in which
    an unsupervised pre-training step is introduced. In this pre-training step, no
    labeled data is needed. Instead, the model is trained to predict what the next
    token is. Thanks to the use of the Transformer architecture, which allows parallelization,
    this pre-training was performed on a large amount of data. For the pre-training,
    the GPT-1 model used the *BookCorpus dataset*, which contains the text of approximately
    11,000 unpublished books. This dataset was initially presented in 2015 in the
    scientific paper [“Aligning Books and Movies: Towards Story-Like Visual Explanations
    by Watching Movies and Reading Books”](https://oreil.ly/3hWl1) by Zhu et al.,
    and was initially made available on a University of Toronto web page. However,
    today the official version of the original dataset is no longer publicly accessible.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的论文中，GPT-1的作者提出了一个新的学习过程，其中引入了一个无监督的预训练步骤。在这个预训练步骤中，不需要标记的数据。相反，模型被训练来预测下一个标记是什么。由于变压器架构的使用，可以并行化，这个预训练是在大量数据上进行的。对于预训练，GPT-1模型使用了*BookCorpus数据集*，其中包含大约11000本未发表书籍的文本。这个数据集最初是在2015年的科学论文[“将书籍和电影对齐：通过观看电影和阅读书籍实现类似故事的视觉解释”](https://oreil.ly/3hWl1)中首次提出，作者是Zhu等人，最初在多伦多大学的网页上提供。然而，今天，原始数据集的官方版本不再公开可访问。
- en: The GPT-1 model was found to be effective in a variety of basic completion tasks.
    In the unsupervised learning phase, the model learned to predict the next item
    in the texts of the BookCorpus dataset. However, since GPT-1 is a small model,
    it was unable to perform complex tasks without fine-tuning. Therefore, fine-tuning
    was performed as a second supervised learning step on a small set of manually
    labeled data to adapt the model to a specific target task. For example, in a classification
    task such as sentiment analysis, it may be necessary to retrain the model on a
    small set of manually labeled text examples to achieve reasonable accuracy. This
    process allowed the parameters learned in the initial pre-training phase to be
    modified to better fit the task at hand.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-1模型在各种基本完成任务中被发现是有效的。在无监督学习阶段，该模型学会了预测BookCorpus数据集中文本的下一个项目。然而，由于GPT-1是一个小模型，它无法在没有微调的情况下执行复杂的任务。因此，微调作为第二个监督学习步骤在一小部分手动标记的数据上进行，以使模型适应特定的目标任务。例如，在情感分析等分类任务中，可能需要在一小部分手动标记的文本示例上重新训练模型以达到合理的准确性。这个过程允许在初始预训练阶段学习的参数被修改以更好地适应手头的任务。
- en: Despite its relatively small size, GPT-1 showed remarkable performance on several
    NLP tasks using only a small amount of manually labeled data for fine-tuning.
    The GPT-1 architecture consisted of a decoder similar to the original transformer,
    which was introduced in 2017 and had 117 million parameters. This first GPT model
    paved the way for more powerful models with larger datasets and more parameters
    to take better advantage of the potential of the Transformer architecture.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管规模相对较小，但GPT-1在几个NLP任务上表现出了显著的性能，仅使用少量手动标记的数据进行微调。GPT-1架构包括一个类似于2017年引入的原始变压器的解码器，具有1.17亿个参数。这个第一个GPT模型为更强大的模型铺平了道路，这些模型具有更大的数据集和更多的参数，以更好地利用Transformer架构的潜力。
- en: GPT-2
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-2
- en: In early 2019, OpenAI proposed GPT-2, a scaled-up version of the GPT-1 model
    that increased the number of parameters and the size of the training dataset tenfold.
    The number of parameters of this new version was 1.5 billion, trained on 40 GB
    of text. In November 2019, OpenAI released the full version of the GPT-2 language
    model.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年初，OpenAI提出了GPT-2，这是GPT-1模型的一个放大版本，它将参数数量和训练数据集的大小增加了十倍。这个新版本的参数数量为15亿，训练了40GB的文本。2019年11月，OpenAI发布了完整版本的GPT-2语言模型。
- en: Note
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: GPT-2 is publicly available and can be downloaded from [Hugging Face](https://huggingface.co/gpt2)
    or [GitHub](https://github.com/openai/gpt-2).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2是公开可用的，可以从[Hugging Face](https://huggingface.co/gpt2)或[GitHub](https://github.com/openai/gpt-2)下载。
- en: GPT-2 showed that training a larger language model on a larger dataset improves
    the ability of a language model to process tasks and outperforms the state of
    the art on many jobs. It also showed that even larger language models can process
    natural language better.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2表明，将更大的语言模型训练在更大的数据集上可以提高语言模型处理任务的能力，并在许多任务上胜过现有技术。它还表明，即使更大的语言模型也可以更好地处理自然语言。
- en: GPT-3
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-3
- en: OpenAI released version 3 of GPT in June 2020\. The main differences between
    GPT-2 and GPT-3 are the size of the model and the quantity of data used for the
    training. GPT-3 is a much larger model than GPT-2, with 175 billion parameters,
    allowing it to capture more complex patterns. In addition, GPT-3 was trained on
    a more extensive dataset. This includes [Common Crawl](https://commoncrawl.org),
    a large web archive containing text from billions of web pages and other sources,
    such as Wikipedia. This training dataset, which includes content from websites,
    books, and articles, allows GPT-3 to develop a deeper understanding of the language
    and context. As a result, GPT-3 demonstrates improved performance on a variety
    of linguistics tasks. It also demonstrates superior coherence and creativity in
    its generated texts. It is even capable of writing code snippets, such as SQL
    queries, and performing other intelligent tasks. Furthermore, GPT-3 eliminates
    the need for a fine-tuning step, which was mandatory for its predecessors.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI于2020年6月发布了GPT的第3版。GPT-2和GPT-3之间的主要区别在于模型的大小和训练使用的数据量。GPT-3比GPT-2模型要大得多，有1750亿个参数，使其能够捕捉更复杂的模式。此外，GPT-3是在更广泛的数据集上进行训练的。这包括[Common
    Crawl](https://commoncrawl.org)，一个包含来自数十亿网页和其他来源的文本的大型网络存档，如维基百科。这个训练数据集包括来自网站、书籍和文章的内容，使GPT-3能够更深入地理解语言和上下文。因此，GPT-3在各种语言任务上表现出更好的性能。它还在生成的文本中表现出更高的连贯性和创造力。它甚至能够编写代码片段，如SQL查询，并执行其他智能任务。此外，GPT-3消除了以前版本中必须进行的微调步骤。
- en: However, with GPT-3 there is a problem of misalignment between the tasks given
    by end users and what the model has seen during its training. As we have seen,
    language models are trained to predict the next token based on the input context.
    This training process is not necessarily directly aligned with the tasks end users
    want the model to perform. In addition, increasing the size of language models
    does not inherently make them better at following user intent or instructions.
    Moreover, models like GPT-3 were trained on data from different sources on the
    internet. Although a cleanup has been made in the selection of sources, the learning
    data may contain false or problematic text, including racist text, sexist text,
    misinformation, or disinformation. As a result, the model may sometimes say the
    wrong things or even toxic things. In 2021 a new release of the GPT-3 model was
    published, called the *Instruct series*. Unlike the original GPT-3 base model,
    the instruct models are optimized by *reinforcement learning* with human feedback,
    meaning that they use the feedback to learn and to improve over time. This allows
    the models to learn from human instructions while making them more truthful and
    less toxic.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，GPT-3存在一个问题，即用户给出的任务与模型在训练过程中看到的内容之间存在不一致。正如我们所见，语言模型是根据输入上下文来预测下一个标记的。这个训练过程不一定与最终用户希望模型执行的任务直接对齐。此外，增加语言模型的大小并不会使它们本质上更擅长遵循用户意图或指令。此外，像GPT-3这样的模型是在互联网上的不同来源的数据上进行训练的。尽管在选择来源时进行了清理，但学习数据可能包含错误或有问题的文本，包括种族主义文本、性别歧视文本、错误信息或虚假信息。因此，模型有时可能会说错话，甚至是有毒的话。2021年，GPT-3模型的新版本被发布，称为*Instruct系列*。与原始的GPT-3基础模型不同，instruct模型通过*强化学习*和人类反馈进行优化，这意味着它们利用反馈来学习和随着时间的推移改进。这使得模型能够从人类指令中学习，使它们更真实，更少有毒。
- en: 'To illustrate the difference, we input the prompt “Explain what is meant by
    time complexity”, and we receive the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明区别，我们输入提示“解释时间复杂度的含义”，我们得到以下结果：
- en: 'With the standard GPT-3, we obtain the output: “*Explain what is meant by space
    complexity. Explain what is meant by the big-O notation*.”'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用标准的GPT-3，我们得到输出：“*解释空间复杂度的含义。解释大O符号的含义*。”
- en: 'With the instructGPT-3 model, we obtain: “*Time complexity is a way of measuring
    the amount of time it takes for an algorithm to run and complete its task. It
    is usually expressed using Big O notation, which measures the complexity of an
    algorithm in terms of the number of operations it performs. The time complexity
    of an algorithm is important because it determines how efficient the algorithm
    is and how well it scales with larger inputs.*”'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用instructGPT-3模型，我们得到：“*时间复杂度是衡量算法运行和完成任务所需时间的一种方式。通常使用大O符号来表示，它衡量算法的复杂度是指它执行的操作数量。算法的时间复杂度很重要，因为它决定了算法的效率以及随着更大输入的规模如何扩展*。”
- en: We can see that for the same input, the first model cannot answer the question
    (the answer is even weird), whereas the second model does answer the question.
    It is, of course, possible to obtain the desired response with a standard GPT-3
    model. However, contrary to instruction models, it is necessary to apply specific
    prompt design and optimization techniques to obtain the desired output from the
    GPT-3 model. This technique is called *prompt engineering* and will be detailed
    in the coming chapters.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，对于相同的输入，第一个模型无法回答问题（甚至答案很奇怪），而第二个模型可以回答问题。当然，使用标准的GPT-3模型也可以获得期望的响应。然而，与指导模型相反，需要应用特定的提示设计和优化技术才能从GPT-3模型中获得期望的输出。这种技术称为*提示工程*，将在接下来的章节中详细介绍。
- en: From GPT-3 to InstructGPT
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从GPT-3到InstructGPT
- en: OpenAI explains how the Instruct series was constructed in the scientific paper
    [“Training Language Models to Follow Instructions with Human Feedback”](https://oreil.ly/sz90A)
    by Ouyang et al.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI解释了科学论文《使用人类反馈训练语言模型遵循指示》中Instruct系列是如何构建的。
- en: 'The training recipe has two main stages to go from a GPT-3 model to an instructed
    GPT-3 model: *supervised fine-tuning* (*SFT*) and *reinforcement learning from
    human feedback* (*RLHF*). In each stage, the results of the prior stage are fine-tuned.
    That is, the SFT stage receives the GPT-3 model and returns a new model, which
    is sent to the RLHF stage to obtain the instructed version.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 训练配方有两个主要阶段，从GPT-3模型到指导的GPT-3模型：*监督微调*（SFT）和*来自人类反馈的强化学习*（RLHF）。在每个阶段中，前一阶段的结果都会进行微调。也就是说，SFT阶段接收GPT-3模型并返回一个新模型，然后将其发送到RLHF阶段以获得指导版本。
- en: '[Figure 1-6](#fig_6_the_steps_to_obtain_the_instructed_models_from_an), from
    the scientific paper from OpenAI, details the entire process.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图1-6，来自OpenAI的科学论文，详细介绍了整个过程。
- en: '![](assets/dagc_0106.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dagc_0106.png)'
- en: Figure 1-6\. The steps to obtain the instructed models (redrawn from an image
    by Ouyang et al.)
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-6。获取指导模型的步骤（重新绘制自欧阳等人的图像）。
- en: We will step through these stages one by one.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐个步骤地进行这些阶段。
- en: In the SFT stage, the original GPT-3 model is fine-tuned with straightforward
    supervised learning (step 1 in [Figure 1-6](#fig_6_the_steps_to_obtain_the_instructed_models_from_an)).
    OpenAI has a collection of prompts made by end users. The process starts with
    the random selection of a prompt from the set of available prompts. A human (called
    a *labeler*) is then asked to write an example of an ideal answer to this prompt.
    This process is repeated thousands of times to obtain a supervised training set
    composed of prompts and the corresponding ideal responses. This dataset is then
    used to fine-tune the GPT-3 model to give more consistent answers to user requests.
    The resulting model is called the SFT model.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在SFT阶段，原始的GPT-3模型通过直接监督学习进行微调（图1-6中的第1步）。OpenAI拥有一系列由最终用户制作的提示。该过程从可用提示集中随机选择一个提示开始。然后要求人类（称为*标注者*）写出对这个提示的理想答案的示例。这个过程重复数千次，以获得由提示和相应理想响应组成的监督训练集。然后使用这个数据集对GPT-3模型进行微调，以使其对用户请求给出更一致的答案。得到的模型称为SFT模型。
- en: The RLHF stage is divided into two substeps. First a reward model (RM) is built
    (step 2 in [Figure 1-6](#fig_6_the_steps_to_obtain_the_instructed_models_from_an)),
    and then the RM is used for reinforcement learning (step 3 in [Figure 1-6](#fig_6_the_steps_to_obtain_the_instructed_models_from_an)).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF阶段分为两个子步骤。首先建立奖励模型（RM）（图1-6中的第2步），然后使用RM进行强化学习（图1-6中的第3步）。
- en: The goal of the RM is to automatically give a score to a response to a prompt.
    When the response matches what is indicated in the prompt, the RM score should
    be high; when it doesn’t match, it should be low. To construct the RM, OpenAI
    begins by randomly selecting a question and using the SFT model to produce several
    possible answers. As we will see later, it is possible to produce many responses
    with the same input prompt via a parameter called *temperature*. A human labeler
    is then asked to rank the responses based on criteria such as fit with the prompt
    and toxicity of the response. After running this procedure many times, a dataset
    is used to fine-tune the SFT model for scoring. This RM will be used to build
    the final InstructGPT model.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: RM的目标是自动给出对提示的响应的分数。当响应与提示中指示的内容匹配时，RM分数应该很高；当不匹配时，应该很低。为了构建RM，OpenAI首先随机选择一个问题，并使用SFT模型产生几个可能的答案。正如我们将在后面看到的，通过称为*温度*的参数，可以使用相同的输入提示产生许多响应。然后要求人类标注者根据诸如与提示的匹配度和响应的毒性等标准对响应进行排名。经过多次运行这个过程后，使用数据集对SFT模型进行微调以进行评分。这个RM将用于构建最终的InstructGPT模型。
- en: The final step in training InstructGPT models involves reinforcement learning,
    which is an iterative process. It starts with an initial generative model, such
    as the SFT model. Then a random prompt is selected, and the model predicts an
    output, which the RM evaluates. Based on the reward received, the generative model
    is updated accordingly. This process can be repeated countless times without human
    intervention, providing a more efficient and automated approach to adapting the
    model for better performance.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 训练InstructGPT模型的最后一步涉及强化学习，这是一个迭代过程。它从初始生成模型开始，比如SFT模型。然后选择一个随机提示，模型预测输出，RM进行评估。根据收到的奖励，生成模型相应地进行更新。这个过程可以在没有人类干预的情况下重复无数次，为改进模型性能提供更高效和自动化的方法。
- en: InstructGPT models are better at producing accurate completions for what people
    give as input in the prompt. OpenAI recommends using the InstructGPT series rather
    than the original series.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: InstructGPT模型更擅长为人们在提示中给出的输入生成准确的完成。OpenAI建议使用InstructGPT系列而不是原始系列。
- en: GPT-3.5, Codex, and ChatGPT
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-3.5、Codex和ChatGPT
- en: In March 2022, OpenAI made available new versions of GPT-3\. These new models
    can edit text or insert content into text. They have been trained on data through
    June 2021 and are described as more powerful than previous versions. At the end
    of November 2022, OpenAI began referring to these models as belonging to the GPT-3.5
    series.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 2022年3月，OpenAI推出了GPT-3的新版本。这些新模型可以编辑文本或将内容插入文本。它们经过了截至2021年6月的数据训练，并被描述为比以前的版本更强大。2022年11月底，OpenAI开始将这些模型称为GPT-3.5系列的一部分。
- en: OpenAI also proposed the Codex model, a GPT-3 model that is fine-tuned on billions
    of lines of code and that powers the [GitHub Copilot](https://github.com/features/copilot)
    autocompletion programming tool to assist developers of many text editors including
    Visual Studio Code, JetBrains, and even Neovim. However, the Codex model was deprecated
    by OpenAI in March 2023\. Instead, OpenAI recommends that users switch from Codex
    to GPT-3.5 Turbo or GPT-4\. At the same time, GitHub released Copilot X, which
    is based on GPT-4 and provides much more functionality than the previous version.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI还提出了Codex模型，这是一个在数十亿行代码上进行了微调的GPT-3模型，它驱动着[GitHub Copilot](https://github.com/features/copilot)自动补全编程工具，以协助许多文本编辑器的开发人员，包括Visual
    Studio Code、JetBrains，甚至Neovim。然而，Codex模型在2023年3月被OpenAI弃用。相反，OpenAI建议用户从Codex切换到GPT-3.5
    Turbo或GPT-4。与此同时，GitHub发布了基于GPT-4的Copilot X，提供比之前版本更多的功能。
- en: Warning
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'OpenAI’s deprecation of the Codex model serves as a stark reminder of the inherent
    risk of working with APIs: they can be subject to changes or discontinuation over
    time as newer, more efficient models are developed and rolled out.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI对Codex模型的弃用提醒了使用API的固有风险：随着更新、更高效的模型的开发和推出，它们可能会随时间发生变化或停止。
- en: In November 2022, OpenAI introduced [ChatGPT](https://chat.openai.com) as an
    experimental conversational model. This model has been fine-tuned to excel at
    interactive dialogue, using a technique similar to that shown in [Figure 1-6](#fig_6_the_steps_to_obtain_the_instructed_models_from_an).
    ChatGPT has its roots in the GPT-3.5 series, which served as the basis for its
    development.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 2022年11月，OpenAI推出了[ChatGPT](https://chat.openai.com)作为一种实验性对话模型。该模型经过了微调，以在交互式对话中表现出色，使用了类似于[图1-6](#fig_6_the_steps_to_obtain_the_instructed_models_from_an)中所示的技术。ChatGPT源于GPT-3.5系列，这成为了其发展的基础。
- en: Note
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: It can be argued that ChatGPT is an application powered by an LLM, not an actual
    LLM. The LLM behind ChatGPT is GPT-3.5 Turbo. However, OpenAI itself refers to
    ChatGPT as a model in its [release note](https://openai.com/blog/chatgpt). In
    this book, we use *ChatGPT* as a generic term for both the application and the
    model, unless we are manipulating code, in which case we use `gpt-3.5-turbo`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT可以被认为是由LLM驱动的应用程序，而不是实际的LLM。ChatGPT背后的LLM是GPT-3.5 Turbo。然而，OpenAI本身在其[发布说明](https://openai.com/blog/chatgpt)中将ChatGPT称为一个模型。在本书中，我们将*ChatGPT*用作既指代应用程序又指代模型的通用术语，除非我们在处理代码，那时我们使用`gpt-3.5-turbo`。
- en: GPT-4
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPT-4
- en: In March 2023, OpenAI made GPT-4 available. We know very little about the architecture
    of this new model, as OpenAI has provided little information. It is OpenAI’s most
    advanced system to date and should produce more secure and useful answers. The
    company claims that GPT-4 surpasses ChatGPT in its advanced reasoning capabilities.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年3月，OpenAI推出了GPT-4。我们对这个新模型的架构了解甚少，因为OpenAI提供了很少的信息。这是OpenAI迄今为止最先进的系统，应该能够产生更安全和有用的答案。该公司声称GPT-4在其先进的推理能力方面超过了ChatGPT。
- en: Unlike the other models in the OpenAI GPT family, GPT-4 is the first multimodal
    model capable of receiving not only text but also images. This means that GPT-4
    considers both the images and the text in the context that the model uses to generate
    an output sentence, which makes it possible to add an image to a prompt and ask
    questions about it. Note that OpenAI has not yet made this feature publicly available
    as of the writing of this book.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 与OpenAI GPT系列中的其他模型不同，GPT-4是第一个能够接收文本和图像的多模态模型。这意味着GPT-4在生成输出句子时考虑了图像和文本的上下文，这使得可以向提示添加图像并对其进行提问。需要注意的是，截至本书撰写时，OpenAI尚未公开提供此功能。
- en: The models have also been evaluated on various tests, and GPT-4 has outperformed
    ChatGPT by scoring in higher percentiles among the test takers. For example, on
    the [Uniform Bar Exam](https://oreil.ly/opXec), ChatGPT scored in the 10th percentile,
    while GPT-4 scored in the 90th percentile. The same goes for the [International
    Biology Olympiad](https://oreil.ly/a8CP6), in which ChatGPT scored in the 31st
    percentile and GPT-4 in the 99th percentile. This progress is very impressive,
    especially considering that it was achieved in less than one year.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型也经过了各种测试的评估，GPT-4在各项测试中的得分都超过了ChatGPT。例如，在[统一法律考试](https://oreil.ly/opXec)上，ChatGPT得分位于第10百分位数，而GPT-4得分位于第90百分位数。在[国际生物奥林匹克竞赛](https://oreil.ly/a8CP6)中也是如此，ChatGPT得分位于第31百分位数，而GPT-4得分位于第99百分位数。这种进步非常令人印象深刻，尤其是考虑到这是在不到一年的时间内实现的。
- en: '[Table 1-1](#table-1-1) summarizes the evolution of the GPT models.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[表1-1](#table-1-1)总结了GPT模型的演变。'
- en: Table 1-1\. Evolution of the GPT models
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 表1-1。GPT模型的演变
- en: '| 2017 | The paper “Attention Is All You Need” by Vaswani et al. is published.
    |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: 2017年，Vaswani等人发表了论文“Attention Is All You Need”。
- en: '| 2018 | The first GPT model is introduced with 117 million parameters. |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: 2018年，引入了拥有1.17亿参数的第一个GPT模型。
- en: '| 2019 | The GPT-2 model is introduced with 1.5 billion parameters. |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: 2019年，引入了拥有15亿参数的GPT-2模型。
- en: '| 2020 | The GPT-3 model is introduced with 175 billion parameters. |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: 2020年，引入了拥有1750亿参数的GPT-3模型。
- en: '| 2022 | The GPT-3.5 (ChatGPT) model is introduced with 175 billion parameters.
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: 2022年，引入了拥有1750亿参数的GPT-3.5（ChatGPT）模型。
- en: '| 2023 | The GPT-4 model is introduced, but the number of parameters is not
    disclosed. |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: 2023年，引入了GPT-4模型，但参数数量未公开。
- en: Note
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You may have heard the term *foundation model*. While LLMs like GPT are trained
    to process human language, a foundation model is a broader concept. These models
    are trained on many types of data, not just text, and they can be fine-tuned for
    various tasks, including but not limited to NLP. Thus, all LLMs are foundation
    models, but not all foundation models are LLMs.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能听说过“基础模型”这个术语。虽然像GPT这样的LLM被训练来处理人类语言，但基础模型是一个更广泛的概念。这些模型接受多种类型的数据训练，不仅仅是文本，它们可以针对各种任务进行微调，包括但不限于自然语言处理。因此，所有LLM都是基础模型，但并非所有基础模型都是LLM。
- en: LLM Use Cases and Example Products
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM使用案例和示例产品
- en: OpenAI includes many inspiring customer stories on its website. This section
    explores some of these applications, use cases, and product examples. We will
    discover how these models may transform our society and open new opportunities
    for business and creativity. As you will see, many businesses already use these
    new technologies, but there is room for more ideas. It is now up to you.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI在其网站上包含许多鼓舞人心的客户故事。本节探讨了其中一些应用、使用案例和产品示例。我们将发现这些模型可能如何改变我们的社会，并为商业和创造力开辟新的机会。正如你将看到的，许多企业已经在使用这些新技术，但还有更多的想法可以实现。现在轮到你了。
- en: Be My Eyes
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Be My Eyes
- en: Since 2012, [Be My Eyes](https://www.bemyeyes.com) has created technologies
    for a community of several million people who are blind or have limited vision.
    For example, it has an app that connects volunteers with blind or visually impaired
    persons who need help with everyday tasks, such as identifying a product or navigating
    in an airport. With only one click in the app, the person who needs help is contacted
    by a volunteer who, through video and microphone sharing, can help the person.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 自2012年以来，[Be My Eyes](https://www.bemyeyes.com)为数百万盲人或视力有限的人群创建了技术。例如，它有一个应用程序，将志愿者与需要帮助的盲人或视障人士联系起来，例如识别产品或在机场导航等日常任务。在应用程序中点击一次，需要帮助的人就会被一名志愿者联系，通过视频和麦克风共享，志愿者可以帮助这个人。
- en: The new multimodal capacity of GPT-4 makes it possible to process both text
    and images, so Be My Eyes began developing a new virtual volunteer based on GPT-4\.
    This new virtual volunteer aims to reach the same level of assistance and understanding
    as a human volunteer.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4的新的多模态能力使其能够处理文本和图像，因此Be My Eyes开始开发一个基于GPT-4的新虚拟志愿者。这个新的虚拟志愿者旨在达到与人类志愿者相同的帮助和理解水平。
- en: “The implications for global accessibility are profound. In the not-so-distant
    future, the blind and low-vision community will utilize these tools not only for
    a host of visual interpretation needs but also to have a greater degree of independence
    in their lives,” says Michael Buckley, CEO of Be My Eyes.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: “全球可访问性的影响是深远的。在不久的将来，盲人和低视力社区将利用这些工具，不仅满足各种视觉解释需求，还将在生活中拥有更大程度的独立性，”Be My Eyes的CEO迈克尔·巴克利说。
- en: At the time of this writing, the virtual volunteer is still in the beta version.
    To gain access to it, you must register to be put on a waiting list in the app,
    but initial feedback from beta testers is very positive.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，虚拟志愿者仍处于测试版阶段。要获得访问权限，您必须在应用程序中注册等待名单，但测试版测试者的初步反馈非常积极。
- en: Morgan Stanley
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摩根士丹利
- en: '[Morgan Stanley](https://www.morganstanley.com) is a multinational investment
    bank and financial services company in the United States. As a leader in wealth
    management, Morgan Stanley has a content library of hundreds of thousands of pages
    of knowledge and insight covering investment strategies, market research and commentary,
    and analyst opinions. This vast amount of information is spread across multiple
    internal sites and is mostly in PDF format. This means consultants must search
    a large number of documents to find answers to their questions. As you can imagine,
    this search can be long and tedious.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[摩根士丹利](https://www.morganstanley.com)是一家美国的跨国投资银行和金融服务公司。作为财富管理领域的领导者，摩根士丹利拥有数十万页的投资策略、市场研究和评论以及分析师意见的内容库。这些大量的信息分布在多个内部网站上，大部分以PDF格式存在。这意味着顾问必须搜索大量文档才能找到他们的问题的答案。可以想象，这种搜索可能会很漫长和乏味。'
- en: The company evaluated how it could leverage its intellectual capital with GPT’s
    integrated research capabilities. The resulting internally developed model will
    power a chatbot that performs a comprehensive search of wealth management content
    and efficiently unlocks Morgan Stanley’s accumulated knowledge. In this way, GPT-4
    has provided a way to analyze all this information in a format that is much easier
    to use.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 该公司评估了如何利用GPT的综合研究能力来发挥其知识资产。由此产生的内部开发模型将驱动一个聊天机器人，该机器人可以对财富管理内容进行全面搜索，并高效地解锁摩根士丹利积累的知识。通过GPT-4，以一种更易于使用的格式分析所有这些信息提供了一种方式。
- en: Khan Academy
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可汗学院
- en: '[Khan Academy](https://www.khanacademy.org) is a US-based nonprofit educational
    organization founded in 2008 by Sal Khan. Its mission is to create a set of free
    online tools to help educate students worldwide. The organization offers thousands
    of math, science, and social studies lessons for students of all ages. In addition,
    the organization produces short lessons through videos and blogs, and recently
    it began offering Khanmigo.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[Khan Academy](https://www.khanacademy.org)是一家总部位于美国的非营利教育组织，由Sal Khan于2008年创立。其使命是创建一套免费在线工具，帮助全球教育学生。该组织为各个年龄段的学生提供数千种数学、科学和社会学课程。此外，该组织通过视频和博客制作短课程，最近开始提供Khanmigo。'
- en: Khanmigo is a new AI assistant powered by GPT-4\. Khanmigo can do a lot of things
    for students, such as guiding and encouraging them, asking questions, and preparing
    them for tests. Khanmigo is designed to be a friendly chatbot that helps students
    with their classwork. It does not give students answers directly, but instead
    guides them in the learning process. Khanmigo can also support teachers by helping
    them make lesson plans, complete administrative tasks, and create lesson books,
    among other things.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Khanmigo是由GPT-4提供动力的新的AI助手。Khanmigo可以为学生做很多事情，比如指导和鼓励他们，提问，并为他们准备考试。Khanmigo旨在成为一个友好的聊天机器人，帮助学生完成课业。它不会直接给学生答案，而是引导他们学习过程。Khanmigo还可以通过帮助他们制定教学计划、完成行政任务和创建课本等方式支持教师。
- en: “We think GPT-4 is opening up new frontiers in education. A lot of people have
    dreamed about this kind of technology for a long time. It’s transformative, and
    we plan to proceed responsibly with testing to explore if it can be used effectively
    for learning and teaching,” says Kristen DiCerbo, chief learning officer at Khan
    Academy.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: “我们认为GPT-4正在教育领域开辟新的前沿。很多人长期以来都梦想拥有这种技术。这是变革性的，我们计划负责地进行测试，探索它是否可以有效地用于学习和教学，”Khan
    Academy的首席学习官克里斯汀·迪塞博说道。
- en: At the time of this writing, access to Khanmigo’s pilot program is limited to
    selected people. To participate in the program, you must be placed on a [waiting
    list](https://oreil.ly/oP6KN).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Khanmigo的试点计划的访问权限仅限于特定人员。要参加该计划，您必须被放在等待名单上。
- en: Duolingo
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Duolingo
- en: '[Duolingo](https://www.duolingo.com) is a US-based educational technology company,
    founded in 2011, that produces applications used by millions of people who want
    to learn a second language. Duolingo users need to understand the rules of grammar
    to learn the basics of a language. They need to have conversations, ideally with
    a native speaker, to understand those grammar rules and master the language. This
    is not possible for everyone.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Duolingo是一家总部位于美国的教育科技公司，成立于2011年，生产应用程序，被数百万想要学习第二语言的人使用。Duolingo用户需要理解语法规则来学习语言的基础知识。他们需要进行对话，最好是与母语为该语言的人，以理解这些语法规则并掌握语言。这对于每个人来说都是不可能的。
- en: 'Duolingo has added two new features to the product using OpenAI’s GPT-4: Role
    Play and Explain My Answer. These features are available in a new subscription
    level called Duolingo Max. With these features, Duolingo has bridged the gap between
    theoretical knowledge and the practical application of language. Thanks to LLMs,
    Duolingo allows learners to immerse themselves in real-world scenarios.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Duolingo已经在产品中使用OpenAI的GPT-4添加了两个新功能：角色扮演和解释我的答案。这些功能在名为Duolingo Max的新订阅级别中可用。有了这些功能，Duolingo已经弥合了理论知识和语言的实际应用之间的差距。多亏了LLMs，Duolingo允许学习者沉浸在现实世界的场景中。
- en: The Role Play feature simulates conversations with native speakers, allowing
    users to practice their language skills in a variety of settings. The Explain
    My Answer feature provides personalized feedback on grammar errors, facilitating
    a deeper understanding of the structure of the language.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 角色扮演功能模拟与母语为该语言的人的对话，使用户能够在各种环境中练习语言技能。解释我的答案功能提供有关语法错误的个性化反馈，促进对语言结构的更深入理解。
- en: “We wanted AI-powered features that were deeply integrated into the app and
    leveraged the gamified aspect of Duolingo that our learners love,” says Edwin
    Bodge, principal product manager at Duolingo.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: “我们希望AI功能能够深度整合到应用程序中，并利用Duolingo学习者喜爱的游戏化特性，”Duolingo的首席产品经理埃德温·博奇说道。
- en: The integration of GPT-4 into Duolingo Max not only enhances the overall learning
    experience but also paves the way for more effective language acquisition, especially
    for those without access to native speakers or immersive environments. This innovative
    approach should transform the way learners master a second language and contribute
    to better long-term learning outcomes.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 将GPT-4整合到Duolingo Max中不仅增强了整体学习体验，还为更有效地习得语言，特别是对于那些无法接触母语为该语言的人或沉浸式环境的人铺平了道路。这种创新的方法应该改变学习者掌握第二语言的方式，并有助于更好的长期学习成果。
- en: Yabble
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Yabble
- en: '[Yabble](https://www.yabble.com) is a market research company that uses AI
    to analyze consumer data in order to deliver actionable insights to businesses.
    Its platform transforms raw, unstructured data into visualizations, enabling businesses
    to make informed decisions based on customer needs.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Yabble是一家市场研究公司，利用人工智能分析消费者数据，以向企业提供可操作的见解。其平台将原始的非结构化数据转化为可视化数据，使企业能够根据客户需求做出明智的决策。
- en: The integration of advanced AI technologies such as GPT into Yabble’s platform
    has enhanced its consumer data processing capabilities. This enhancement allows
    for a more effective understanding of complex questions and answers, enabling
    businesses to gain deeper insights based on the data. As a result, organizations
    can make more informed decisions by identifying key areas for improvement based
    on customer feedback.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 像GPT这样的先进AI技术的整合到Yabble的平台中增强了其消费者数据处理能力。这种增强使得更有效地理解复杂问题和答案成为可能，从而使企业能够根据数据获得更深入的见解。因此，组织可以通过根据客户反馈识别改进的关键领域，做出更明智的决策。
- en: “We knew that if we wanted to expand our existing offers, we needed artificial
    intelligence to do a lot of the heavy lifting so that we could spend our time
    and creative energy elsewhere. OpenAI fit the bill perfectly,” says Ben Roe, Head
    of Product at Yabble.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: “我们知道，如果我们想要扩展我们现有的服务，我们需要人工智能来做很多繁重的工作，这样我们就可以把时间和创造性精力花在其他地方。OpenAI完全符合我们的要求，”Yabble的产品负责人本·罗说。
- en: Waymark
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Waymark
- en: '[Waymark](https://waymark.com) provides a platform for creating video ads.
    This platform uses AI to help businesses easily create high-quality videos without
    the need for technical skills or expensive equipment.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Waymark提供一个创建视频广告的平台。该平台利用人工智能帮助企业轻松创建高质量的视频，而无需技术技能或昂贵的设备。
- en: Waymark has integrated GPT into its platform, which has significantly improved
    the scripting process for platform users. This GPT-powered enhancement allows
    the platform to generate custom scripts for businesses in seconds. This allows
    users to focus more on their primary goals, as they spend less time editing scripts
    and more time creating video ads. The integration of GPT into Waymark’s platform
    therefore provides a more efficient and personalized video creation experience.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Waymark已将GPT集成到其平台中，这显著改进了平台用户的脚本编写过程。这种由GPT驱动的增强功能使平台能够在几秒钟内为企业生成定制脚本。这使用户能够更多地专注于他们的主要目标，因为他们花费更少的时间编辑脚本，更多的时间创建视频广告。因此，将GPT集成到Waymark的平台中提供了更高效和个性化的视频创作体验。
- en: “I’ve tried every AI-powered product available over the last five years but
    found nothing that could effectively summarize a business’s online footprint,
    let alone write effective marketing copy, until GPT-3,” says Waymark founder,
    Nathan Labenz.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: “在过去五年里，我尝试了所有可用的AI产品，但没有找到任何能够有效总结企业在线足迹，更不用说撰写有效营销文案的产品，直到GPT-3，”Waymark创始人Nathan
    Labenz表示。
- en: Inworld AI
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Inworld AI
- en: '[Inworld AI](https://www.inworld.ai) provides a developer platform for creating
    AI characters with distinct personalities, multimodal expression, and contextual
    awareness.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[Inworld AI](https://www.inworld.ai)提供了一个开发平台，用于创建具有独特个性、多模态表达和情境意识的AI角色。'
- en: One of the main use cases of the Inworld AI platform is video games. The integration
    of GPT as the basis for the character engine of Inworld AI enables efficient and
    rapid video game character development. By combining GPT with other ML models,
    the platform can generate unique personalities, emotions, memory, and behaviors
    for AI characters. This process allows game developers to focus on storytelling
    and other topics without having to invest significant time in creating language
    models from scratch.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Inworld AI平台的主要用例之一是视频游戏。将GPT作为Inworld AI角色引擎的基础集成，可以实现高效快速的视频游戏角色开发。通过将GPT与其他ML模型结合，该平台可以为AI角色生成独特的个性、情感、记忆和行为。这个过程使游戏开发人员能够专注于叙事和其他主题，而无需花费大量时间从头开始创建语言模型。
- en: “With GPT-3, we had more time and creative energy to invest in our proprietary
    technology that powers the next generation of non-player characters (NPCs),” says
    Kylan Gibbs, chief product officer and cofounder of Inworld.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: “Inworld的首席产品官兼联合创始人Kylan Gibbs表示：“有了GPT-3，我们有更多的时间和创造性的精力投入到支持下一代非玩家角色（NPC）的专有技术中。”
- en: 'Beware of AI Hallucinations: Limitations and Considerations'
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 警惕AI幻觉：限制和考虑
- en: As you have seen, an LLM generates an answer by predicting the next words (or
    tokens) one by one based on a given input prompt. In most situations, the model’s
    output is relevant and entirely usable for your task, but it is essential to be
    careful when you are using language models in your applications because they can
    give incoherent answers. These answers are often referred to as *hallucinations*.
    AI hallucinations occur when AI gives you a confident response that is false or
    that refers to imaginary facts. This can be dangerous for users who rely on GPT.
    You need to double-check and critically examine the model’s response.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，LLM通过根据给定的输入提示逐个预测下一个单词（或标记）来生成答案。在大多数情况下，模型的输出对您的任务是相关的和完全可用的，但在使用语言模型时要小心，因为它们可能会给出不连贯的答案。这些答案通常被称为*幻觉*。当AI给出一个自信的但错误的回答，或者涉及虚构事实时，就会发生AI幻觉。这对依赖GPT的用户可能是危险的。您需要仔细检查和批判性地审查模型的回应。
- en: 'Consider the following example. We start by asking the model to do a simple
    calculation: 2 + 2\. As expected, it answers 4\. So it is correct. Excellent!
    We then ask it to do a more complex calculation: 3,695 × 123,548\. Although the
    correct answer is 456,509,860, the model gives with great confidence a wrong answer,
    as you can see in [Figure 1-7](#fig_7_chatgpt_hallucinating_bad_math_chatgpt_april_22).
    And when we ask it to check and recalculate, it still gives a wrong answer.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下例子。我们首先让模型进行简单的计算：2 + 2。如预期的那样，它回答4。所以它是正确的。太棒了！然后我们让它进行更复杂的计算：3,695 × 123,548。尽管正确答案是456,509,860，但模型非常自信地给出了错误答案，如您在[图1-7](#fig_7_chatgpt_hallucinating_bad_math_chatgpt_april_22)中所见。当我们要求它检查和重新计算时，它仍然给出了错误答案。
- en: '![](assets/dagc_0107.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dagc_0107.png)'
- en: Figure 1-7\. ChatGPT hallucinating bad math (ChatGPT, April 22, 2023)
  id: totrans-143
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-7。ChatGPT产生错误的数学幻觉（ChatGPT，2023年4月22日）
- en: Although, as we will see, you can add new features to GPT using a plug-in system,
    GPT does not include a calculator by default. To answer our question of what is
    2 + 2, GPT generates each token one at a time. It answers correctly because it
    probably has often seen “2 + 2 equals 4” in the texts used for its training. It
    doesn’t really do the calculation—it is just text completion.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管，正如我们将看到的，您可以使用插件系统向GPT添加新功能，但GPT默认不包括计算器。为了回答我们的问题2 + 2等于多少，GPT逐个生成每个标记。它之所以回答正确，可能是因为它在训练中经常看到“2
    + 2等于4”的文本。它并没有真正进行计算，它只是文本完成。
- en: Warning
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: It is unlikely that GPT has seen the numbers we chose for the multiplication
    problem, 3,695 × 123,548, many times in its training. This is why it makes a mistake.
    And as you can see, even when it makes a mistake, it can be reasonably sure about
    its wrong output. Be careful, mainly if you use the model in one of your applications.
    If GPT makes mistakes, your application may get inconsistent results.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: GPT很少在训练中多次看到我们选择的乘法问题3,695 × 123,548中的数字。这就是为什么它会犯错。而且正如您所见，即使它犯了错，它对错误的输出也可以相当确定。要小心，特别是如果您在您的应用程序中使用该模型。如果GPT犯了错，您的应用程序可能会得到不一致的结果。
- en: 'Notice that ChatGPT’s result is *close* to the correct answer and not completely
    random. It is an interesting side effect of its algorithm: even though it has
    no mathematical capabilities, it can give a close estimation with a language approach
    only.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，ChatGPT的结果*接近*正确答案，而不是完全随机。这是其算法的一个有趣的副作用：即使它没有数学能力，也可以通过语言方法给出一个接近的估计。
- en: Note
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: OpenAI introduced the ability to use plug-ins with GPT-4\. As we will see in
    [Chapter 5](ch05.html#advancing_llm_capabilities_with_the_langchain_fram), these
    tools allow you to add additional functionality to the LLM. One tool is a calculator
    that helps GPT correctly answer these types of questions.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI引入了在GPT-4中使用插件的功能。正如我们将在[第5章](ch05.html#advancing_llm_capabilities_with_the_langchain_fram)中看到的，这些工具允许你为LLM添加额外的功能。其中一个工具是一个计算器，可以帮助GPT正确回答这类问题。
- en: In the preceding example, ChatGPT made a mistake. But in some cases, it can
    even be deliberately deceitful, such as shown in [Figure 1-8](#fig_8_asking_chatgpt_to_count_zebras_on_a_wikipedia_pict).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，ChatGPT犯了一个错误。但在某些情况下，它甚至可以故意欺骗，就像[图1-8](#fig_8_asking_chatgpt_to_count_zebras_on_a_wikipedia_pict)中所示的那样。
- en: '![](assets/dagc_0108.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dagc_0108.png)'
- en: Figure 1-8\. Asking ChatGPT to count zebras on a Wikipedia picture (ChatGPT,
    April 5, 2023)
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-8。询问ChatGPT在维基百科图片上数斑马（ChatGPT，2023年4月5日）
- en: ChatGPT begins by claiming that it cannot access the internet. However, if we
    insist, something interesting happens (see [Figure 1-9](#fig_9_chatgpt_claiming_it_accessed_the_wikipedia_link)).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT开始声称它无法访问互联网。然而，如果我们坚持，会发生一些有趣的事情（见[图1-9](#fig_9_chatgpt_claiming_it_accessed_the_wikipedia_link)）。
- en: '![](assets/dagc_0109.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dagc_0109.png)'
- en: Figure 1-9\. ChatGPT claiming it accessed the Wikipedia link
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-9。ChatGPT声称它访问了维基百科链接
- en: ChatGPT now implies that it *did* access the link. However, this is definitely
    not possible at the moment. ChatGPT is blatantly leading the user to think that
    it has capabilities it doesn’t have. By the way, as [Figure 1-10](#fig_10_the_zebras_chatgpt_didn_t_really_count)
    shows, there are more than three zebras in the image.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT现在暗示它*确实*访问了链接。然而，目前这绝对是不可能的。ChatGPT明显让用户误以为它具有它实际上没有的能力。顺便说一句，正如[图1-10](#fig_10_the_zebras_chatgpt_didn_t_really_count)所示，图片中有三只以上的斑马。
- en: '![](assets/dagc_0110.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dagc_0110.png)'
- en: Figure 1-10\. The zebras ChatGPT didn’t really count
  id: totrans-158
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-10。ChatGPT并没有真正数过斑马
- en: Warning
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'ChatGPT and other GPT-4 models are, by design, not reliable: they can make
    mistakes, give false information, or even mislead the user.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT和其他GPT-4模型从设计上来说是不可靠的：它们可能会犯错，提供错误信息，甚至误导用户。
- en: In summary, we highly recommend using pure GPT-based solutions for creative
    applications, not question answering where the truth matters—such as for medical
    tools. For such use cases, as you will see, plug-ins are probably an ideal solution.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们强烈建议在创意应用中使用纯GPT-based解决方案，而不是在医疗工具等真相至关重要的问题上使用。对于这样的用例，正如你将看到的，插件可能是一个理想的解决方案。
- en: Optimizing GPT Models with Plug-ins and Fine-Tuning
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过插件和微调优化GPT模型
- en: 'In addition to its simple completion feature, more advanced techniques can
    be used to further exploit the capabilities of the language models provided by
    OpenAI. This book looks at two of these methods:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 除了其简单的完成功能之外，还可以使用更高级的技术来进一步利用OpenAI提供的语言模型的能力。本书将介绍其中的两种方法：
- en: Plug-ins
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 插件
- en: Fine-tuning
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调
- en: GPT has some limitations, for example, with calculations. As you’ve seen, GPT
    can correctly answer simple math problems like 2 + 2 but may struggle with more
    complex calculations like 3,695 × 123,548\. Moreover, it does not have direct
    access to the internet, which means that GPT models lack access to new information
    and are limited to the data they were trained on. For GPT-4, the last knowledge
    update occurred in September 2021\. The plug-in service provided by OpenAI allows
    the model to be connected to applications that may be developed by third parties.
    These plug-ins enable the models to interact with developer-defined APIs, and
    this process can potentially greatly enhance the capabilities of the GPT models,
    as they *can* access the outside world through a wide range of actions.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: GPT有一些限制，例如在计算方面。正如你所见，GPT可以正确回答简单的数学问题，比如2 + 2，但可能会在更复杂的计算中遇到困难，比如3,695 × 123,548。此外，它没有直接访问互联网的权限，这意味着GPT模型无法获取新信息，只能限于它们训练时的数据。对于GPT-4，最后一次知识更新发生在2021年9月。OpenAI提供的插件服务允许模型连接到第三方开发的应用程序。这些插件使模型能够与开发者定义的API进行交互，这个过程可能会极大地增强GPT模型的能力，因为它们*可以*通过各种行为访问外部世界。
- en: For developers, plug-ins potentially open up many new opportunities. Consider
    that in the future, each company may want to have its own plug-in for LLMs. There
    could be collections of plug-ins similar to what we find today in smartphone app
    stores. The number of applications that could be added via plug-ins could be enormous.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对于开发者来说，插件可能会开启许多新机会。考虑到将来，每家公司可能都希望为LLMs拥有自己的插件。可能会有类似智能手机应用商店中所找到的插件集合。通过插件添加的应用程序数量可能是巨大的。
- en: 'On its website, OpenAI says that plug-ins can allow ChatGPT to do things such
    as the following:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在其网站上，OpenAI表示插件可以让ChatGPT做以下事情：
- en: Retrieve real-time information, such as sports scores, stock prices, the latest
    news, and so forth
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索实时信息，比如体育比分、股票价格、最新新闻等
- en: Retrieve knowledge-based information, such as company docs, personal notes,
    and more
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索基于知识的信息，比如公司文件、个人笔记等
- en: Perform actions on behalf of the user, such as booking a flight, ordering food,
    and so on
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代表用户执行操作，比如预订航班、订餐等
- en: Execute accurate math calculations
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行准确的数学计算
- en: These are just a few examples of use cases; it is up to you to find new ones.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是用例的几个例子；发现新的用例取决于你。
- en: This book also examines fine-tuning techniques. As you will see, fine-tuning
    can improve the accuracy of an existing model for a specific task. The fine-tuning
    process involves retraining an existing GPT model on a particular set of new data.
    This new model is designed for a specific task, and this additional training process
    allows the model to adjust its internal parameters to learn the nuances of this
    given task. The resulting fine-tuned model should perform better on the task for
    which it has been fine-tuned. For example, a model fine-tuned on financial textual
    data should be able to better answer queries in that domain and generate more
    relevant content.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 本书还探讨了微调技术。正如你将看到的，微调可以提高现有模型在特定任务上的准确性。微调过程涉及在特定一组新数据上重新训练现有的GPT模型。这个新模型是为特定任务设计的，这个额外的训练过程允许模型调整其内部参数以学习这个给定任务的细微差别。结果微调的模型应该在其被微调的任务上表现更好。例如，在金融文本数据上微调的模型应该能够更好地回答该领域的查询并生成更相关的内容。
- en: Summary
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: LLMs have come a long way, starting with simple n-gram models and moving to
    RNNs, LSTMs, and advanced transformer-based architectures. LLMs are computer programs
    that can process and generate human-like language, with ML techniques to analyze
    vast amounts of text data. By using self-attention and cross-attention mechanisms,
    transformers have greatly enhanced language understanding.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: LLM已经走过了很长的路，从简单的n-gram模型发展到RNNs、LSTMs和先进的基于transformer的架构。LLM是能够处理和生成类似人类语言的计算机程序，利用机器学习技术来分析大量的文本数据。通过使用自注意力和交叉注意力机制，transformers大大增强了语言理解能力。
- en: This book explores how to use GPT-4 and ChatGPT, as they offer advanced capabilities
    for understanding and generating context. Building applications with them goes
    beyond the scope of traditional BERT or LSTM models to provide human-like interactions.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 本书探讨了如何使用GPT-4和ChatGPT，因为它们提供了理解和生成上下文的先进能力。利用它们构建应用程序超越了传统的BERT或LSTM模型的范围，提供了类似人类的互动。
- en: Since early 2023, ChatGPT and GPT-4 have demonstrated remarkable capabilities
    in NLP. As a result, they have contributed to the rapid advancement of AI-enabled
    applications in various industries. Different use cases already exist, ranging
    from applications such as Be My Eyes to platforms such as Waymark, which are testaments
    to the potential of these models to revolutionize how we interact with technology.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 自2023年初以来，ChatGPT和GPT-4在自然语言处理方面展现出了非凡的能力。因此，它们为各行各业的人工智能应用的快速发展做出了贡献。不同的用例已经存在，从Be
    My Eyes等应用到Waymark等平台，这些都证明了这些模型改变我们与技术互动方式的潜力。
- en: It is important to keep in mind the potential risks of using these LLMs. As
    a developer of applications that will use the OpenAI API, you should be sure that
    users know the risk of errors and can verify the AI-generated information.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要牢记使用这些LLM的潜在风险。作为将使用OpenAI API的应用程序开发人员，您应该确保用户知道错误的风险，并能验证由AI生成的信息。
- en: The next chapter will give you the tools and information to use the OpenAI models
    available as a service and help you be part of this incredible transformation
    we are living today.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将为您提供使用OpenAI模型作为服务的工具和信息，并帮助您成为我们今天生活中这一不可思议的转变的一部分。
