["```\nbrew install python\n```", "```\nsudo apt-get updatesudo apt-get install python3.10\n```", "```\njupyter notebook\n```", "```\njupyter lab\n```", "```\ngit clone https://github.com/benman1/generative_ai_with_langchain.git\ncd generative_ai_with_langchain\n```", "```\ndocker build -t langchain_ai\n```", "```\ndocker run -it langchain_ai\n```", "```\nconda env create --file langchain_ai.yml\n```", "```\nconda activate langchain_ai\n```", "```\njupyter notebook\n```", "```\npip install numpy\n```", "```\npip install numpy==1.0\n```", "```\npip install -r requirements.txt\n```", "```\n# create a new environment myenv:\nvirtualenv myenv\n# activate the myenv environment:\nsource myenv/bin/activate\n# install dependencies or run python, for example:\npython\n# leave the environment again:\ndeactivate\nPlease note that in Windows, the activation command is slightly different \u2013 you'd run a shell script:\n# activate the myenv environment:\nmyenv\\Scripts\\activate.bat\n```", "```\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"<your token>\"\n```", "```\nexport OPENAI_API_KEY=<your token>\n```", "```\nfrom langchain.llms.fake import FakeListLLM\nfrom langchain.agents import load_tools\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentType\ntools = load_tools([\"python_repl\"])\nresponses = [\"Action: Python_REPL\\nAction Input: print(2 + 2)\", \"Final Answer: 4\"]\nllm = FakeListLLM(responses=responses)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\nagent.run(\"whats 2 + 2\")\n```", "```\nclass PythonREPLTool(BaseTool):\n    \"\"\"A tool for running python code in a REPL.\"\"\"\n    name = \"Python_REPL\"\n    description = (\n        \"A Python shell. Use this to execute python commands. \"\n        \"Input should be a valid python command. \"\n        \"If you want to see the output of a value, you should print it out \"\n        \"with `print(...)`.\"\n    )\n```", "```\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0., model=\"text-davinci-003\")\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\nagent.run(\"whats 4 + 4\")\n```", "```\n> Entering new  chain...\n I need to add two numbers\nAction: Python_REPL\nAction Input: print(4 + 4)\nObservation: 8\nThought: I now know the final answer\nFinal Answer: 4 + 4 = 8\n> Finished chain.\n'4 + 4 = 8'\n```", "```\nfrom langchain.llms import HuggingFaceHub\nllm = HuggingFaceHub(\n    model_kwargs={\"temperature\": 0.5, \"max_length\": 64},\n    repo_id=\"google/flan-t5-xxl\"\n)\nprompt = \"In which country is Tokyo?\"\ncompletion = llm(prompt)\nprint(completion)\n```", "```\ngcloud auth application-default login\n```", "```\nfrom langchain.llms import VertexAI\nfrom langchain import PromptTemplate, LLMChain\ntemplate = \"\"\"Question: {question}\nAnswer: Let's think step by step.\"\"\"\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\nllm = VertexAI()\nllm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\nquestion = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\nllm_chain.run(question)\n```", "```\n[1m> Entering new chain...[0m\nPrompt after formatting:\n[[Question: What NFL team won the Super Bowl in the year Justin Beiber was born?\nAnswer: Let's think step by step.[0m\n[1m> Finished chain.[0m\nJustin Beiber was born on March 1, 1994\\. The Super Bowl in 1994 was won by the San Francisco 49ers.\n```", "```\nquestion = \"\"\"\nGiven an integer n, return a string array answer (1-indexed) where:\nanswer[i] == \"FizzBuzz\" if i is divisible by 3 and 5.\nanswer[i] == \"Fizz\" if i is divisible by 3.\nanswer[i] == \"Buzz\" if i is divisible by 5.\nanswer[i] == i (as a string) if none of the above conditions are true.\n\"\"\"\nllm = VertexAI(model_name=\"code-bison\")\nllm_chain = LLMChain(prompt=prompt, llm=llm)\nprint(llm_chain.run(question))\n```", "```\n```", "```\n```", "```\nfrom langchain.chat_models import JinaChat\nfrom langchain.schema import HumanMessage\nchat = JinaChat(temperature=0.)\nmessages = [\n    HumanMessage(\n        content=\"Translate this sentence from English to French: I love generative AI!\"\n    )\n]\nchat(messages)\n```", "```\nWe should be seeing \n```", "```\nAIMessage(content=\"J'adore l'IA g\u00e9n\u00e9rative !\", additional_kwargs={}, example=False).\n```", "```\nchat = JinaChat(temperature=0.)\nchat(\n    [\n        SystemMessage(\n            content=\"You help a user find a nutritious and tasty food to eat in one word.\"\n        ),\n        HumanMessage(\n            content=\"I like pasta with cheese, but I need to eat more vegetables, what should I eat?\"\n        )\n    ]\n)\n```", "```\nAIMessage(content='A tasty and nutritious option could be a vegetable pasta dish. Depending on your taste, you can choose a sauce that complements the vegetables. Try adding broccoli, spinach, bell peppers, and zucchini to your pasta with some grated parmesan cheese on top. This way, you get to enjoy your pasta with cheese while incorporating some veggies into your meal.', additional_kwargs={}, example=False)\n```", "```\nfrom langchain.llms import Replicate\ntext2image = Replicate(\n    model=\"stability-ai/stable-diffusion:db21e45d3f7023abc2a46ee38a23973f6dce16bb082a930b0c49861f96d1e5bf\",\n    input={\"image_dimensions\": \"512x512\"},\n)\nimage_url = text2image(\"a book cover for a book about creating generative ai applications in Python\")\n```", "```\nfrom transformers import pipeline\nimport torch\ngenerate_text = pipeline(\n    model=\"aisquared/dlite-v1-355m\",\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n    framework=\"pt\"\n)\ngenerate_text(\"In this chapter, we'll discuss first steps with generative AI in Python.\")\n```", "```\nfrom langchain import PromptTemplate, LLMChain\ntemplate = \"\"\"Question: {question}\nAnswer: Let's think step by step.\"\"\"\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\nllm_chain = LLMChain(prompt=prompt, llm=generate_text)\nquestion = \"What is electroencephalography?\"\nprint(llm_chain.run(question))\n```", "```\nbrew install md5sha1sum\n```", "```\ngit clone https://github.com/ggerganov/llama.cpp.git\n```", "```\ncd llama.cpp\npip install -r requirements.txt\n```", "```\nmake -C . -j4 # runs make in subdir with 4 processes\n```", "```\npython3 convert.py models/7B/\n./quantize ./models/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.bin q4_0\n```", "```\nllm = LlamaCpp(\n    model_path=\"./ggml-model-q4_0.bin\",\n    verbose=True\n```", "```\nGiven this text, what is the sentiment conveyed? Is it positive, neutral, or negative?\nText: {sentence}\nSentiment:\n```", "```\ndef list_most_popular(task: str):\n    for rank, model in enumerate(\n        list_models(filter=task, sort=\"downloads\", direction=-1)\n):\n        if rank == 5:\n            break\n        print(f\"{model.id}, {model.downloads}\\n\")\nlist_most_popular(\"text-classification\")\n```", "```\nI've asked GPT-3.5 to put together a long rambling customer email complaining about a coffee machine. You can find the email on GitHub. Let's see what our sentiment model has to say:\nfrom transformers import pipeline\nsentiment_model = pipeline(\n    task=\"sentiment-analysis\",\n    model=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n)\nprint(sentiment_model(customer_email))\n```", "```\n[{'label': '2 stars', 'score': 0.28999224305152893}]\n```", "```\nfrom langchain import HuggingFaceHub\nsummarizer = HuggingFaceHub(\n    repo_id=\"facebook/bart-large-cnn\",\n    model_kwargs={\"temperature\":0, \"max_length\":180}\n)\ndef summarize(llm, text) -> str:\n    return llm(f\"Summarize this: {text}!\")\nsummarize(summarizer, customer_email)\n```", "```\nfrom langchain.llms import VertexAI\nfrom langchain import PromptTemplate, LLMChain\ntemplate = \"\"\"Given this text, decide what is the issue the customer is concerned about. Valid categories are these:\n* product issues\n* delivery problems\n* missing or late orders\n* wrong product\n* cancellation request\n* refund or exchange\n* bad support experience\n* no clear reason to be upset\nText: {email}\nCategory:\n\"\"\"\nprompt = PromptTemplate(template=template, input_variables=[\"email\"])\nllm = VertexAI()\nllm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\nprint(llm_chain.run(customer_email))\n```"]